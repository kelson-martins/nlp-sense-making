{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Summary:**\n",
    "\n",
    "**Task 1:** BERT\n",
    "\n",
    "**Task 2:** BERT\n",
    "\n",
    "**Task 2:** GPT\n",
    "\n",
    "\n",
    "### **Task 1**\n",
    "\n",
    "For task 1, BERT pre-trainned **uncased_L-12_H-768_A-12** model was used to tackle the task.\n",
    "\n",
    "Results were achieved by fine-tuning BERT and the approach to tackle the problem was to breakdown the data in such a way to allow a binary classification method trainnig.\n",
    "\n",
    "This was done by breaking each line (which has 2 sentences) into 2 lines, allocating also its target label for trainning.\n",
    "\n",
    "The Code for Task1 presents all steps in the pre-processing with appropriate comments.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "![task1_trainning](images/task1_img1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sanity check for train data\n",
      "\n",
      "\n",
      "   0  1                              2\n",
      "0  0  0  He poured milk on his cereal.\n",
      "1  1  0                He drinks milk.\n",
      "2  2  0         I like to ride my bike\n",
      "3  3  0    he pour water in to the jug\n",
      "4  4  0      Kids find eggs on Easter.\n",
      "\n",
      "\n",
      "Sanity check for test data\n",
      "\n",
      "\n",
      "    0                                                  1\n",
      "0  id                                              sent0\n",
      "1   1                 he put an elephant into the fridge\n",
      "2   2  my sister eats an apple after breakfast every day\n",
      "3   3                  money can be used for buying cars\n",
      "4   4  New York is located in the northeastern part o...\n",
      "\n",
      "\n",
      "Sanity check for bert train dataset\n",
      "\n",
      "\n",
      "   id  label alpha                           text\n",
      "0   0      0     a  he poured milk on his cereal.\n",
      "1   1      0     a                he drinks milk.\n",
      "2   2      0     a         i like to ride my bike\n",
      "3   3      0     a    he pour water in to the jug\n",
      "4   4      0     a      kids find eggs on easter.\n",
      "\n",
      "\n",
      "Sanity check for the bert dataframe train\n",
      "\n",
      "\n",
      "          id  label alpha                                               text\n",
      "11016  11016      1     a           his new shoes are made of peanut butter.\n",
      "19749  19749      0     a         a credit card allows me to pay money later\n",
      "12192  12192      1     a                      the picture is looking at me.\n",
      "17086  17086      0     a  a person wants to share his life with someone ...\n",
      "17339  17339      0     a  it's his wedding ceremony today, so he chose a...\n",
      "\n",
      "\n",
      "Sanity check for bert dataframe dev\n",
      "\n",
      "\n",
      "          id  label alpha                                               text\n",
      "10271  10271      1     a                  generally people sleep during day\n",
      "383      383      0     a    he closed one eye means he opened the other eye\n",
      "18617  18617      0     a                i go to the gym to play basketball.\n",
      "17380  17380      0     a  don't forget to bring a shopping bag to the su...\n",
      "4565    4565      0     a              joanna made a pot of chili for dinner\n",
      "\n",
      "\n",
      "Sanity check for BERT dataframe test data\n",
      "\n",
      "\n",
      "   id                                               text\n",
      "0   0                                              sent0\n",
      "1   1                 he put an elephant into the fridge\n",
      "2   2  my sister eats an apple after breakfast every day\n",
      "3   3                  money can be used for buying cars\n",
      "4   4  new york is located in the northeastern part o...\n"
     ]
    }
   ],
   "source": [
    "# Pre-Processing data for Task 1\n",
    "train_df = pd.read_csv('task1_data/task1_train.csv', header=None)\n",
    "test_df = pd.read_csv('task1_data/task1_test.csv', header=None)\n",
    "\n",
    "# sanity check for train data\n",
    "print(\"\\n\\nSanity check for train data\\n\\n\")\n",
    "print(train_df.head())\n",
    "\n",
    "# sanity check for test data\n",
    "print(\"\\n\\nSanity check for test data\\n\\n\")\n",
    "print(test_df.head())\n",
    "\n",
    "\n",
    "# creating bert dataframe for the train data\n",
    "df_bert = pd.DataFrame({\n",
    "    'id':range(len(train_df)),\n",
    "    'label':train_df[1],\n",
    "    'alpha':['a']*train_df.shape[0],\n",
    "    'text': train_df[2].replace(r'\\n', ' ', regex=True).str.lower()\n",
    "})\n",
    "\n",
    "# sanity check for bert train dataset\n",
    "print(\"\\n\\nSanity check for bert train dataset\\n\\n\")\n",
    "print(df_bert.head())\n",
    "\n",
    "\n",
    "# splitting training data file into train and *dev*\n",
    "df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n",
    "\n",
    "# sanity check for the bert dataframe train\n",
    "print(\"\\n\\nSanity check for the bert dataframe train\\n\\n\")\n",
    "print(df_bert_train.head())\n",
    "\n",
    "# sanity check for bert dataframe dev\n",
    "print(\"\\n\\nSanity check for bert dataframe dev\\n\\n\")\n",
    "print(df_bert_dev.head())\n",
    "\n",
    "# creating test dataframe according to BERT\n",
    "df_bert_test = pd.DataFrame({\n",
    "    'id':range(len(test_df)),\n",
    "    'text': test_df[1].replace(r'\\n', ' ', regex=True).str.lower()\n",
    "})\n",
    "\n",
    "# sanity check for BERT dataframe test data\n",
    "print(\"\\n\\nSanity check for BERT dataframe test data\\n\\n\")\n",
    "print(df_bert_test.head())\n",
    "\n",
    "# saving dataframes to .tsv format as required by BERT\n",
    "df_bert_train.to_csv('task1_data/train.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_dev.to_csv('task1_data/dev.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_test.to_csv('task1_data/test.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 Pre-Processing Output\n",
    "\n",
    "The output above shows how data was pre-processed in such a way to allow it to be fed to BERT.\n",
    "\n",
    "The data requires tsv extension for files, so that was performed using pandas. The same process was done for the Trainning and Test data. The Test data being what we want to predict.\n",
    "\n",
    "Data Ready to be fed to BERT\n",
    "\n",
    "![task1_processed](images/task1_img2.png)\n",
    "\n",
    "The final step is then to Fine-Tune BERT, and that is done below, using the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:981: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:784: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W1117 09:38:59.057229 4595733952 deprecation_wrapper.py:119] From run_classifier.py:784: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:784: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "W1117 09:38:59.057347 4595733952 deprecation_wrapper.py:119] From run_classifier.py:784: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1117 09:38:59.057614 4595733952 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:808: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W1117 09:38:59.058318 4595733952 deprecation_wrapper.py:119] From run_classifier.py:808: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1117 09:39:00.001732 4595733952 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:199: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1117 09:39:00.002474 4595733952 deprecation_wrapper.py:119] From run_classifier.py:199: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x1a3be9f158>) includes params argument, but params are not passed to Estimator.\n",
      "W1117 09:39:00.105290 4595733952 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x1a3be9f158>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a3b97a978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "I1117 09:39:00.106031 4595733952 estimator.py:209] Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a3b97a978>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "I1117 09:39:00.106451 4595733952 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "W1117 09:39:00.106577 4595733952 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n",
      "WARNING:tensorflow:From run_classifier.py:483: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W1117 09:39:00.106709 4595733952 deprecation_wrapper.py:119] From run_classifier.py:483: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:487: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W1117 09:39:00.107733 4595733952 deprecation_wrapper.py:119] From run_classifier.py:487: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Writing example 0 of 19800\n",
      "I1117 09:39:00.107829 4595733952 run_classifier.py:487] Writing example 0 of 19800\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 09:39:00.108169 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-0\n",
      "I1117 09:39:00.108286 4595733952 run_classifier.py:462] guid: train-0\n",
      "INFO:tensorflow:tokens: [CLS] he took some water by a book to drink [SEP]\n",
      "I1117 09:39:00.108365 4595733952 run_classifier.py:464] tokens: [CLS] he took some water by a book to drink [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2002 2165 2070 2300 2011 1037 2338 2000 4392 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.108456 4595733952 run_classifier.py:465] input_ids: 101 2002 2165 2070 2300 2011 1037 2338 2000 4392 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.108546 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.108633 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 09:39:00.108697 4595733952 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 09:39:00.109410 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-1\n",
      "I1117 09:39:00.109500 4595733952 run_classifier.py:462] guid: train-1\n",
      "INFO:tensorflow:tokens: [CLS] people built temple to watch movies [SEP]\n",
      "I1117 09:39:00.109568 4595733952 run_classifier.py:464] tokens: [CLS] people built temple to watch movies [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2111 2328 3379 2000 3422 5691 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.109653 4595733952 run_classifier.py:465] input_ids: 101 2111 2328 3379 2000 3422 5691 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.109740 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.109874 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 09:39:00.109961 4595733952 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 09:39:00.110622 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-2\n",
      "I1117 09:39:00.110739 4595733952 run_classifier.py:462] guid: train-2\n",
      "INFO:tensorflow:tokens: [CLS] you would feed your family because you want them to be dismay and healthy [SEP]\n",
      "I1117 09:39:00.110819 4595733952 run_classifier.py:464] tokens: [CLS] you would feed your family because you want them to be dismay and healthy [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2017 2052 5438 2115 2155 2138 2017 2215 2068 2000 2022 20006 1998 7965 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.110913 4595733952 run_classifier.py:465] input_ids: 101 2017 2052 5438 2115 2155 2138 2017 2215 2068 2000 2022 20006 1998 7965 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.111004 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.111151 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 09:39:00.111237 4595733952 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 09:39:00.111742 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-3\n",
      "I1117 09:39:00.111822 4595733952 run_classifier.py:462] guid: train-3\n",
      "INFO:tensorflow:tokens: [CLS] cater ##pi ##llar ##s grow into butterflies . [SEP]\n",
      "I1117 09:39:00.111886 4595733952 run_classifier.py:464] tokens: [CLS] cater ##pi ##llar ##s grow into butterflies . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 23488 8197 17305 2015 4982 2046 15023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.111971 4595733952 run_classifier.py:465] input_ids: 101 23488 8197 17305 2015 4982 2046 15023 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.112056 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.112205 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 09:39:00.112287 4595733952 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 09:39:00.112837 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-4\n",
      "I1117 09:39:00.112915 4595733952 run_classifier.py:462] guid: train-4\n",
      "INFO:tensorflow:tokens: [CLS] children don ' t like to go to hospitals for treatment . [SEP]\n",
      "I1117 09:39:00.112982 4595733952 run_classifier.py:464] tokens: [CLS] children don ' t like to go to hospitals for treatment . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2336 2123 1005 1056 2066 2000 2175 2000 8323 2005 3949 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.113066 4595733952 run_classifier.py:465] input_ids: 101 2336 2123 1005 1056 2066 2000 2175 2000 8323 2005 3949 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.113152 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 09:39:00.113297 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 09:39:00.113381 4595733952 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 19800\n",
      "I1117 09:39:03.637513 4595733952 run_classifier.py:487] Writing example 10000 of 19800\n",
      "INFO:tensorflow:***** Running training *****\n",
      "I1117 09:39:07.104597 4595733952 run_classifier.py:871] ***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 19800\n",
      "I1117 09:39:07.104722 4595733952 run_classifier.py:872]   Num examples = 19800\n",
      "INFO:tensorflow:  Batch size = 32\n",
      "I1117 09:39:07.104887 4595733952 run_classifier.py:873]   Batch size = 32\n",
      "INFO:tensorflow:  Num steps = 1856\n",
      "I1117 09:39:07.104944 4595733952 run_classifier.py:874]   Num steps = 1856\n",
      "WARNING:tensorflow:From run_classifier.py:514: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "W1117 09:39:07.105039 4595733952 deprecation_wrapper.py:119] From run_classifier.py:514: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W1117 09:39:07.117227 4595733952 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From run_classifier.py:550: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "W1117 09:39:07.145562 4595733952 deprecation.py:323] From run_classifier.py:550: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W1117 09:39:07.145714 4595733952 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From run_classifier.py:523: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "W1117 09:39:07.146794 4595733952 deprecation_wrapper.py:119] From run_classifier.py:523: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:530: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1117 09:39:07.151352 4595733952 deprecation.py:323] From run_classifier.py:530: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I1117 09:39:07.166707 4595733952 estimator.py:1145] Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "I1117 09:39:07.166891 4595733952 tpu_estimator.py:2965] Running train on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I1117 09:39:07.167155 4595733952 run_classifier.py:627] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (32, 128)\n",
      "I1117 09:39:07.167268 4595733952 run_classifier.py:629]   name = input_ids, shape = (32, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (32, 128)\n",
      "I1117 09:39:07.167363 4595733952 run_classifier.py:629]   name = input_mask, shape = (32, 128)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (32,)\n",
      "I1117 09:39:07.167440 4595733952 run_classifier.py:629]   name = is_real_example, shape = (32,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (32,)\n",
      "I1117 09:39:07.167524 4595733952 run_classifier.py:629]   name = label_ids, shape = (32,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (32, 128)\n",
      "I1117 09:39:07.167603 4595733952 run_classifier.py:629]   name = segment_ids, shape = (32, 128)\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1117 09:39:07.168432 4595733952 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1117 09:39:07.169977 4595733952 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W1117 09:39:07.190114 4595733952 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1117 09:39:07.215424 4595733952 deprecation.py:506] From /Users/kelson/repository/personal/github/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1117 09:39:07.227701 4595733952 deprecation.py:323] From /Users/kelson/repository/personal/github/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda97f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda97f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:07.371706 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda97f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda97f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ba897b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ba897b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:07.452373 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ba897b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ba897b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b885fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b885fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:07.533014 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b885fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b885fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:07.633600 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb39e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb39e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:07.734437 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb39e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb39e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb28198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb28198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:07.818819 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb28198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb28198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:07.919556 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.002329 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.084914 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c463390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.192754 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bda9630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c39d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c39d588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.302359 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c39d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c39d588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.390902 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5c3a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5c3a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.494144 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5c3a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5c3a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.574996 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.658195 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c631ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5dd5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5dd5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.763192 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5dd5f8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c5dd5f8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb08860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb08860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.870003 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb08860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb08860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c42e780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c42e780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:08.959392 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c42e780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c42e780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c778780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c778780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.063883 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c778780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c778780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.147301 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.230115 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7e6320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c76ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c76ccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.335294 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c76ccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c76ccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.444186 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4e7940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4e7940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.533421 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4e7940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4e7940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.637893 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c889978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c889978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.726930 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c889978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c889978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.813011 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c92c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fcbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fcbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:09.924518 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fcbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fcbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.097697 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.188678 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c3bc470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.292652 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.378864 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.463579 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cad8518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb35509400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb35509400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.564954 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb35509400>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb35509400>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.669175 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fc0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fc0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.757559 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fc0f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c8fc0f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc92240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc92240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.865345 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc92240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc92240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:10.948400 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.032229 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ccb81d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c6e4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c6e4e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.141588 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c6e4e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c6e4e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.245793 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.337988 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c72b6a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce73128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce73128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.445113 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce73128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce73128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.528186 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.617379 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce5c358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce19cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce19cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.721374 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce19cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ce19cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.822276 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:11.910198 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c7fccf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.014164 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.097360 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d013978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d05dc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d05dc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.177870 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d05dc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d05dc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cff5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cff5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.279558 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cff5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cff5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cb105c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cb105c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.385299 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cb105c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cb105c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.495510 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c4faa20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.611760 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.700751 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1e6668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cec9fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cec9fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.794322 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cec9fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cec9fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf4a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf4a240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:12.909255 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf4a240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf4a240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc13d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc13d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.023113 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc13d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc13d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.190918 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c59d748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.300617 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.383254 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.464847 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a8c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf8c278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf8c278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.577057 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf8c278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf8c278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d136048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d136048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.684195 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d136048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d136048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1ace80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1ace80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.772305 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1ace80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1ace80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.884687 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:13.974293 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:14.063730 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:14.170842 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d546be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceecdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceecdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:14.277014 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceecdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceecdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2f2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2f2cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:14.368437 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2f2cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2f2cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bae0588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bae0588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 09:39:14.497584 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bae0588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bae0588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From run_classifier.py:661: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "W1117 09:39:14.525356 4595733952 deprecation_wrapper.py:119] From run_classifier.py:661: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:**** Trainable Variables ****\r\n",
      "I1117 09:39:15.020996 4595733952 run_classifier.py:663] **** Trainable Variables ****\r\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021170 4595733952 run_classifier.py:669]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021281 4595733952 run_classifier.py:669]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021361 4595733952 run_classifier.py:669]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021437 4595733952 run_classifier.py:669]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021516 4595733952 run_classifier.py:669]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021593 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021666 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021733 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021802 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021867 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.021936 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022001 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022071 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022156 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022222 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022287 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022356 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022420 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022491 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022557 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022621 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022686 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022756 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022825 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022895 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.022960 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023029 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023093 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023162 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023237 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023308 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023371 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023439 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023509 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023586 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023652 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023717 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023783 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023853 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023918 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.023985 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024053 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024121 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024185 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024255 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024329 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024395 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024461 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024532 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024595 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024665 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024739 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024813 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024884 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.024956 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025029 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025110 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025180 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025262 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025338 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025409 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025481 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025547 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025612 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025680 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025745 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025815 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025880 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.025943 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026008 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026077 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026144 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026213 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026277 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026346 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026411 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026479 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026544 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026608 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026672 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026741 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026818 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026890 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.026956 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027021 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027085 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027153 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027217 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027287 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027351 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027419 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027484 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027553 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027617 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027682 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027745 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027814 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027879 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.027948 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028012 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028078 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028141 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028212 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028276 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028346 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028410 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028480 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028543 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028612 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028676 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028741 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028805 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028873 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.028938 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029007 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029071 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029135 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029201 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029271 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029337 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029404 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029469 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029536 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029601 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029669 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029733 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029797 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029861 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029929 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.029994 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030064 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030128 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030192 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030257 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030328 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030392 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030459 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030524 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030591 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030657 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030725 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030789 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030853 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030916 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.030983 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031047 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031116 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031213 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031280 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031344 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031414 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031481 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031548 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031612 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031681 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031744 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031813 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031877 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.031941 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032005 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032073 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032136 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032206 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032271 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032337 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032409 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032479 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032546 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032615 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032680 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032751 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032815 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032883 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.032948 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033014 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033077 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033146 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033211 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033282 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033361 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033435 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033501 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033573 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033701 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033798 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033875 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.033951 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034020 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034090 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034157 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034222 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034287 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034357 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034423 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034492 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034559 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034624 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034687 4595733952 run_classifier.py:669]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 09:39:15.034757 4595733952 run_classifier.py:669]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\r\n",
      "I1117 09:39:15.034851 4595733952 run_classifier.py:669]   name = output_weights:0, shape = (2, 768)\r\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,)\r\n",
      "I1117 09:39:15.034942 4595733952 run_classifier.py:669]   name = output_bias:0, shape = (2,)\r\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n",
      "\r\n",
      "W1117 09:39:15.035079 4595733952 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\r\n",
      "\r\n",
      "W1117 09:39:15.035921 4595733952 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Deprecated in favor of operator or tf.math.divide.\r\n",
      "W1117 09:39:15.039618 4595733952 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Deprecated in favor of operator or tf.math.divide.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1117 09:39:15.199326 4595733952 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I1117 09:39:21.246029 4595733952 estimator.py:1147] Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "I1117 09:39:21.247060 4595733952 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I1117 09:39:24.005728 4595733952 monitored_session.py:240] Graph was finalized.\n",
      "2019-11-17 09:39:24.006863: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I1117 09:39:28.956590 4595733952 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I1117 09:39:29.149226 4595733952 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./bert_output/model.ckpt.\n",
      "I1117 09:39:35.092417 4595733952 basic_session_run_hooks.py:606] Saving checkpoints for 0 into ./bert_output/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0596542\n",
      "I1117 09:40:16.122075 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0596542\n",
      "INFO:tensorflow:examples/sec: 1.90894\n",
      "I1117 09:40:16.125690 4595733952 tpu_estimator.py:2160] examples/sec: 1.90894\n",
      "INFO:tensorflow:global_step/sec: 0.0791488\n",
      "I1117 09:40:28.756567 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0791488\n",
      "INFO:tensorflow:examples/sec: 2.53276\n",
      "I1117 09:40:28.756796 4595733952 tpu_estimator.py:2160] examples/sec: 2.53276\n",
      "INFO:tensorflow:global_step/sec: 0.0791221\n",
      "I1117 09:40:41.394446 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0791221\n",
      "INFO:tensorflow:examples/sec: 2.53191\n",
      "I1117 09:40:41.394634 4595733952 tpu_estimator.py:2160] examples/sec: 2.53191\n",
      "INFO:tensorflow:global_step/sec: 0.0811633\n",
      "I1117 09:40:53.715321 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0811633\n",
      "INFO:tensorflow:examples/sec: 2.59722\n",
      "I1117 09:40:53.715532 4595733952 tpu_estimator.py:2160] examples/sec: 2.59722\n",
      "INFO:tensorflow:global_step/sec: 0.0824824\n",
      "I1117 09:41:05.839112 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824824\n",
      "INFO:tensorflow:examples/sec: 2.63944\n",
      "I1117 09:41:05.839499 4595733952 tpu_estimator.py:2160] examples/sec: 2.63944\n",
      "INFO:tensorflow:global_step/sec: 0.0834855\n",
      "I1117 09:41:17.817219 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0834855\n",
      "INFO:tensorflow:examples/sec: 2.67154\n",
      "I1117 09:41:17.817409 4595733952 tpu_estimator.py:2160] examples/sec: 2.67154\n",
      "INFO:tensorflow:global_step/sec: 0.0824028\n",
      "I1117 09:41:29.952728 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824028\n",
      "INFO:tensorflow:examples/sec: 2.63689\n",
      "I1117 09:41:29.952919 4595733952 tpu_estimator.py:2160] examples/sec: 2.63689\n",
      "INFO:tensorflow:global_step/sec: 0.0842535\n",
      "I1117 09:41:41.821666 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842535\n",
      "INFO:tensorflow:examples/sec: 2.69611\n",
      "I1117 09:41:41.821870 4595733952 tpu_estimator.py:2160] examples/sec: 2.69611\n",
      "INFO:tensorflow:global_step/sec: 0.0847127\n",
      "I1117 09:41:53.626268 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0847127\n",
      "INFO:tensorflow:examples/sec: 2.71081\n",
      "I1117 09:41:53.626480 4595733952 tpu_estimator.py:2160] examples/sec: 2.71081\n",
      "INFO:tensorflow:global_step/sec: 0.0793969\n",
      "I1117 09:42:06.221231 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0793969\n",
      "INFO:tensorflow:examples/sec: 2.5407\n",
      "I1117 09:42:06.221435 4595733952 tpu_estimator.py:2160] examples/sec: 2.5407\n",
      "INFO:tensorflow:global_step/sec: 0.0819001\n",
      "I1117 09:42:18.431234 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0819001\n",
      "INFO:tensorflow:examples/sec: 2.6208\n",
      "I1117 09:42:18.431454 4595733952 tpu_estimator.py:2160] examples/sec: 2.6208\n",
      "INFO:tensorflow:global_step/sec: 0.0828762\n",
      "I1117 09:42:30.497419 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0828762\n",
      "INFO:tensorflow:examples/sec: 2.65204\n",
      "I1117 09:42:30.497776 4595733952 tpu_estimator.py:2160] examples/sec: 2.65204\n",
      "INFO:tensorflow:global_step/sec: 0.0819912\n",
      "I1117 09:42:42.693845 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0819912\n",
      "INFO:tensorflow:examples/sec: 2.62372\n",
      "I1117 09:42:42.694221 4595733952 tpu_estimator.py:2160] examples/sec: 2.62372\n",
      "INFO:tensorflow:global_step/sec: 0.0809261\n",
      "I1117 09:42:55.050810 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0809261\n",
      "INFO:tensorflow:examples/sec: 2.58964\n",
      "I1117 09:42:55.051025 4595733952 tpu_estimator.py:2160] examples/sec: 2.58964\n",
      "INFO:tensorflow:global_step/sec: 0.0814921\n",
      "I1117 09:43:07.321928 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0814921\n",
      "INFO:tensorflow:examples/sec: 2.60775\n",
      "I1117 09:43:07.322127 4595733952 tpu_estimator.py:2160] examples/sec: 2.60775\n",
      "INFO:tensorflow:global_step/sec: 0.0805366\n",
      "I1117 09:43:19.738678 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0805366\n",
      "INFO:tensorflow:examples/sec: 2.57717\n",
      "I1117 09:43:19.738882 4595733952 tpu_estimator.py:2160] examples/sec: 2.57717\n",
      "INFO:tensorflow:global_step/sec: 0.0799071\n",
      "I1117 09:43:32.253172 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0799071\n",
      "INFO:tensorflow:examples/sec: 2.55703\n",
      "I1117 09:43:32.253359 4595733952 tpu_estimator.py:2160] examples/sec: 2.55703\n",
      "INFO:tensorflow:global_step/sec: 0.080644\n",
      "I1117 09:43:44.653366 4595733952 tpu_estimator.py:2159] global_step/sec: 0.080644\n",
      "INFO:tensorflow:examples/sec: 2.58061\n",
      "I1117 09:43:44.653560 4595733952 tpu_estimator.py:2160] examples/sec: 2.58061\n",
      "INFO:tensorflow:global_step/sec: 0.0825188\n",
      "I1117 09:43:56.771878 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0825188\n",
      "INFO:tensorflow:examples/sec: 2.6406\n",
      "I1117 09:43:56.772157 4595733952 tpu_estimator.py:2160] examples/sec: 2.6406\n",
      "INFO:tensorflow:global_step/sec: 0.0868473\n",
      "I1117 09:44:08.286321 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0868473\n",
      "INFO:tensorflow:examples/sec: 2.77911\n",
      "I1117 09:44:08.286577 4595733952 tpu_estimator.py:2160] examples/sec: 2.77911\n",
      "INFO:tensorflow:global_step/sec: 0.091113\n",
      "I1117 09:44:19.261665 4595733952 tpu_estimator.py:2159] global_step/sec: 0.091113\n",
      "INFO:tensorflow:examples/sec: 2.91562\n",
      "I1117 09:44:19.261877 4595733952 tpu_estimator.py:2160] examples/sec: 2.91562\n",
      "INFO:tensorflow:global_step/sec: 0.0892239\n",
      "I1117 09:44:30.469434 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892239\n",
      "INFO:tensorflow:examples/sec: 2.85517\n",
      "I1117 09:44:30.469652 4595733952 tpu_estimator.py:2160] examples/sec: 2.85517\n",
      "INFO:tensorflow:global_step/sec: 0.0151097\n",
      "I1117 09:45:36.652187 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0151097\n",
      "INFO:tensorflow:examples/sec: 0.483509\n",
      "I1117 09:45:36.652374 4595733952 tpu_estimator.py:2160] examples/sec: 0.483509\n",
      "INFO:tensorflow:global_step/sec: 0.0564278\n",
      "I1117 09:45:54.373946 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0564278\n",
      "INFO:tensorflow:examples/sec: 1.80569\n",
      "I1117 09:45:54.374134 4595733952 tpu_estimator.py:2160] examples/sec: 1.80569\n",
      "INFO:tensorflow:global_step/sec: 0.0794661\n",
      "I1117 09:46:06.957977 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0794661\n",
      "INFO:tensorflow:examples/sec: 2.54291\n",
      "I1117 09:46:06.958260 4595733952 tpu_estimator.py:2160] examples/sec: 2.54291\n",
      "INFO:tensorflow:global_step/sec: 0.0794669\n",
      "I1117 09:46:19.541801 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0794669\n",
      "INFO:tensorflow:examples/sec: 2.54294\n",
      "I1117 09:46:19.541978 4595733952 tpu_estimator.py:2160] examples/sec: 2.54294\n",
      "INFO:tensorflow:global_step/sec: 0.075279\n",
      "I1117 09:46:32.825762 4595733952 tpu_estimator.py:2159] global_step/sec: 0.075279\n",
      "INFO:tensorflow:examples/sec: 2.40893\n",
      "I1117 09:46:32.826042 4595733952 tpu_estimator.py:2160] examples/sec: 2.40893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0832551\n",
      "I1117 09:46:44.836996 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832551\n",
      "INFO:tensorflow:examples/sec: 2.66416\n",
      "I1117 09:46:44.837178 4595733952 tpu_estimator.py:2160] examples/sec: 2.66416\n",
      "INFO:tensorflow:global_step/sec: 0.0634362\n",
      "I1117 09:47:00.600891 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0634362\n",
      "INFO:tensorflow:examples/sec: 2.02996\n",
      "I1117 09:47:00.601103 4595733952 tpu_estimator.py:2160] examples/sec: 2.02996\n",
      "INFO:tensorflow:global_step/sec: 0.081249\n",
      "I1117 09:47:12.909499 4595733952 tpu_estimator.py:2159] global_step/sec: 0.081249\n",
      "INFO:tensorflow:examples/sec: 2.59997\n",
      "I1117 09:47:12.909808 4595733952 tpu_estimator.py:2160] examples/sec: 2.59997\n",
      "INFO:tensorflow:global_step/sec: 0.0779671\n",
      "I1117 09:47:25.734647 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0779671\n",
      "INFO:tensorflow:examples/sec: 2.49495\n",
      "I1117 09:47:25.734833 4595733952 tpu_estimator.py:2160] examples/sec: 2.49495\n",
      "INFO:tensorflow:global_step/sec: 0.0696999\n",
      "I1117 09:47:40.081859 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0696999\n",
      "INFO:tensorflow:examples/sec: 2.2304\n",
      "I1117 09:47:40.082051 4595733952 tpu_estimator.py:2160] examples/sec: 2.2304\n",
      "INFO:tensorflow:global_step/sec: 0.0780084\n",
      "I1117 09:47:52.901077 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0780084\n",
      "INFO:tensorflow:examples/sec: 2.49627\n",
      "I1117 09:47:52.901561 4595733952 tpu_estimator.py:2160] examples/sec: 2.49627\n",
      "INFO:tensorflow:global_step/sec: 0.079972\n",
      "I1117 09:48:05.405393 4595733952 tpu_estimator.py:2159] global_step/sec: 0.079972\n",
      "INFO:tensorflow:examples/sec: 2.5591\n",
      "I1117 09:48:05.405597 4595733952 tpu_estimator.py:2160] examples/sec: 2.5591\n",
      "INFO:tensorflow:global_step/sec: 0.0833238\n",
      "I1117 09:48:17.406757 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0833238\n",
      "INFO:tensorflow:examples/sec: 2.66636\n",
      "I1117 09:48:17.406971 4595733952 tpu_estimator.py:2160] examples/sec: 2.66636\n",
      "INFO:tensorflow:global_step/sec: 0.0829325\n",
      "I1117 09:48:29.464751 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0829325\n",
      "INFO:tensorflow:examples/sec: 2.65384\n",
      "I1117 09:48:29.464957 4595733952 tpu_estimator.py:2160] examples/sec: 2.65384\n",
      "INFO:tensorflow:global_step/sec: 0.0833415\n",
      "I1117 09:48:41.463587 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0833415\n",
      "INFO:tensorflow:examples/sec: 2.66693\n",
      "I1117 09:48:41.463795 4595733952 tpu_estimator.py:2160] examples/sec: 2.66693\n",
      "INFO:tensorflow:global_step/sec: 0.0834091\n",
      "I1117 09:48:53.452703 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0834091\n",
      "INFO:tensorflow:examples/sec: 2.66909\n",
      "I1117 09:48:53.452901 4595733952 tpu_estimator.py:2160] examples/sec: 2.66909\n",
      "INFO:tensorflow:global_step/sec: 0.0803671\n",
      "I1117 09:49:05.895596 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0803671\n",
      "INFO:tensorflow:examples/sec: 2.57175\n",
      "I1117 09:49:05.895792 4595733952 tpu_estimator.py:2160] examples/sec: 2.57175\n",
      "INFO:tensorflow:global_step/sec: 0.0803376\n",
      "I1117 09:49:18.343050 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0803376\n",
      "INFO:tensorflow:examples/sec: 2.5708\n",
      "I1117 09:49:18.343252 4595733952 tpu_estimator.py:2160] examples/sec: 2.5708\n",
      "INFO:tensorflow:global_step/sec: 0.0780055\n",
      "I1117 09:49:31.162657 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0780055\n",
      "INFO:tensorflow:examples/sec: 2.49618\n",
      "I1117 09:49:31.162837 4595733952 tpu_estimator.py:2160] examples/sec: 2.49618\n",
      "INFO:tensorflow:global_step/sec: 0.08153\n",
      "I1117 09:49:43.428058 4595733952 tpu_estimator.py:2159] global_step/sec: 0.08153\n",
      "INFO:tensorflow:examples/sec: 2.60896\n",
      "I1117 09:49:43.428242 4595733952 tpu_estimator.py:2160] examples/sec: 2.60896\n",
      "INFO:tensorflow:global_step/sec: 0.0823718\n",
      "I1117 09:49:55.568135 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0823718\n",
      "INFO:tensorflow:examples/sec: 2.6359\n",
      "I1117 09:49:55.568323 4595733952 tpu_estimator.py:2160] examples/sec: 2.6359\n",
      "INFO:tensorflow:global_step/sec: 0.0776656\n",
      "I1117 09:50:08.443866 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0776656\n",
      "INFO:tensorflow:examples/sec: 2.4853\n",
      "I1117 09:50:08.444072 4595733952 tpu_estimator.py:2160] examples/sec: 2.4853\n",
      "INFO:tensorflow:global_step/sec: 0.0804274\n",
      "I1117 09:50:20.877470 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0804274\n",
      "INFO:tensorflow:examples/sec: 2.57368\n",
      "I1117 09:50:20.877748 4595733952 tpu_estimator.py:2160] examples/sec: 2.57368\n",
      "INFO:tensorflow:global_step/sec: 0.0796255\n",
      "I1117 09:50:33.436236 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0796255\n",
      "INFO:tensorflow:examples/sec: 2.54802\n",
      "I1117 09:50:33.436635 4595733952 tpu_estimator.py:2160] examples/sec: 2.54802\n",
      "INFO:tensorflow:global_step/sec: 0.0832295\n",
      "I1117 09:50:45.451189 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832295\n",
      "INFO:tensorflow:examples/sec: 2.66334\n",
      "I1117 09:50:45.451388 4595733952 tpu_estimator.py:2160] examples/sec: 2.66334\n",
      "INFO:tensorflow:global_step/sec: 0.0831803\n",
      "I1117 09:50:57.473289 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0831803\n",
      "INFO:tensorflow:examples/sec: 2.66177\n",
      "I1117 09:50:57.473736 4595733952 tpu_estimator.py:2160] examples/sec: 2.66177\n",
      "INFO:tensorflow:global_step/sec: 0.0846542\n",
      "I1117 09:51:09.286033 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0846542\n",
      "INFO:tensorflow:examples/sec: 2.70894\n",
      "I1117 09:51:09.286228 4595733952 tpu_estimator.py:2160] examples/sec: 2.70894\n",
      "INFO:tensorflow:global_step/sec: 0.0842683\n",
      "I1117 09:51:21.152899 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842683\n",
      "INFO:tensorflow:examples/sec: 2.69659\n",
      "I1117 09:51:21.153100 4595733952 tpu_estimator.py:2160] examples/sec: 2.69659\n",
      "INFO:tensorflow:global_step/sec: 0.0847143\n",
      "I1117 09:51:32.957267 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0847143\n",
      "INFO:tensorflow:examples/sec: 2.71086\n",
      "I1117 09:51:32.957462 4595733952 tpu_estimator.py:2160] examples/sec: 2.71086\n",
      "INFO:tensorflow:global_step/sec: 0.0802746\n",
      "I1117 09:51:45.414519 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0802746\n",
      "INFO:tensorflow:examples/sec: 2.56879\n",
      "I1117 09:51:45.414737 4595733952 tpu_estimator.py:2160] examples/sec: 2.56879\n",
      "INFO:tensorflow:global_step/sec: 0.0842769\n",
      "I1117 09:51:57.280146 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842769\n",
      "INFO:tensorflow:examples/sec: 2.69686\n",
      "I1117 09:51:57.280339 4595733952 tpu_estimator.py:2160] examples/sec: 2.69686\n",
      "INFO:tensorflow:global_step/sec: 0.0902354\n",
      "I1117 09:52:08.362290 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902354\n",
      "INFO:tensorflow:examples/sec: 2.88753\n",
      "I1117 09:52:08.362497 4595733952 tpu_estimator.py:2160] examples/sec: 2.88753\n",
      "INFO:tensorflow:global_step/sec: 0.0906958\n",
      "I1117 09:52:19.388177 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906958\n",
      "INFO:tensorflow:examples/sec: 2.90227\n",
      "I1117 09:52:19.388384 4595733952 tpu_estimator.py:2160] examples/sec: 2.90227\n",
      "INFO:tensorflow:global_step/sec: 0.090189\n",
      "I1117 09:52:30.475997 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090189\n",
      "INFO:tensorflow:examples/sec: 2.88605\n",
      "I1117 09:52:30.476217 4595733952 tpu_estimator.py:2160] examples/sec: 2.88605\n",
      "INFO:tensorflow:global_step/sec: 0.0900891\n",
      "I1117 09:52:41.576159 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900891\n",
      "INFO:tensorflow:examples/sec: 2.88285\n",
      "I1117 09:52:41.576375 4595733952 tpu_estimator.py:2160] examples/sec: 2.88285\n",
      "INFO:tensorflow:global_step/sec: 0.0898486\n",
      "I1117 09:52:52.705946 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898486\n",
      "INFO:tensorflow:examples/sec: 2.87515\n",
      "I1117 09:52:52.706155 4595733952 tpu_estimator.py:2160] examples/sec: 2.87515\n",
      "INFO:tensorflow:global_step/sec: 0.0906771\n",
      "I1117 09:53:03.734112 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906771\n",
      "INFO:tensorflow:examples/sec: 2.90167\n",
      "I1117 09:53:03.734354 4595733952 tpu_estimator.py:2160] examples/sec: 2.90167\n",
      "INFO:tensorflow:global_step/sec: 0.0896785\n",
      "I1117 09:53:14.885039 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896785\n",
      "INFO:tensorflow:examples/sec: 2.86971\n",
      "I1117 09:53:14.885249 4595733952 tpu_estimator.py:2160] examples/sec: 2.86971\n",
      "INFO:tensorflow:global_step/sec: 0.0906085\n",
      "I1117 09:53:25.921542 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906085\n",
      "INFO:tensorflow:examples/sec: 2.89947\n",
      "I1117 09:53:25.921754 4595733952 tpu_estimator.py:2160] examples/sec: 2.89947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0905974\n",
      "I1117 09:53:36.959357 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905974\n",
      "INFO:tensorflow:examples/sec: 2.89912\n",
      "I1117 09:53:36.959552 4595733952 tpu_estimator.py:2160] examples/sec: 2.89912\n",
      "INFO:tensorflow:global_step/sec: 0.0900395\n",
      "I1117 09:53:48.065618 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900395\n",
      "INFO:tensorflow:examples/sec: 2.88126\n",
      "I1117 09:53:48.065842 4595733952 tpu_estimator.py:2160] examples/sec: 2.88126\n",
      "INFO:tensorflow:global_step/sec: 0.0893669\n",
      "I1117 09:53:59.255445 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893669\n",
      "INFO:tensorflow:examples/sec: 2.85974\n",
      "I1117 09:53:59.255670 4595733952 tpu_estimator.py:2160] examples/sec: 2.85974\n",
      "INFO:tensorflow:global_step/sec: 0.0897947\n",
      "I1117 09:54:10.391979 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897947\n",
      "INFO:tensorflow:examples/sec: 2.87343\n",
      "I1117 09:54:10.392189 4595733952 tpu_estimator.py:2160] examples/sec: 2.87343\n",
      "INFO:tensorflow:global_step/sec: 0.0896924\n",
      "I1117 09:54:21.541168 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896924\n",
      "INFO:tensorflow:examples/sec: 2.87016\n",
      "I1117 09:54:21.541383 4595733952 tpu_estimator.py:2160] examples/sec: 2.87016\n",
      "INFO:tensorflow:global_step/sec: 0.0908246\n",
      "I1117 09:54:32.551385 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908246\n",
      "INFO:tensorflow:examples/sec: 2.90639\n",
      "I1117 09:54:32.551587 4595733952 tpu_estimator.py:2160] examples/sec: 2.90639\n",
      "INFO:tensorflow:global_step/sec: 0.0876866\n",
      "I1117 09:54:43.955647 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876866\n",
      "INFO:tensorflow:examples/sec: 2.80597\n",
      "I1117 09:54:43.955863 4595733952 tpu_estimator.py:2160] examples/sec: 2.80597\n",
      "INFO:tensorflow:global_step/sec: 0.0890039\n",
      "I1117 09:54:55.191103 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890039\n",
      "INFO:tensorflow:examples/sec: 2.84812\n",
      "I1117 09:54:55.191296 4595733952 tpu_estimator.py:2160] examples/sec: 2.84812\n",
      "INFO:tensorflow:global_step/sec: 0.0865453\n",
      "I1117 09:55:06.745763 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865453\n",
      "INFO:tensorflow:examples/sec: 2.76945\n",
      "I1117 09:55:06.745985 4595733952 tpu_estimator.py:2160] examples/sec: 2.76945\n",
      "INFO:tensorflow:global_step/sec: 0.0883728\n",
      "I1117 09:55:18.061457 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883728\n",
      "INFO:tensorflow:examples/sec: 2.82793\n",
      "I1117 09:55:18.061682 4595733952 tpu_estimator.py:2160] examples/sec: 2.82793\n",
      "INFO:tensorflow:global_step/sec: 0.0888082\n",
      "I1117 09:55:29.321659 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888082\n",
      "INFO:tensorflow:examples/sec: 2.84186\n",
      "I1117 09:55:29.321867 4595733952 tpu_estimator.py:2160] examples/sec: 2.84186\n",
      "INFO:tensorflow:global_step/sec: 0.0894643\n",
      "I1117 09:55:40.499312 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894643\n",
      "INFO:tensorflow:examples/sec: 2.86286\n",
      "I1117 09:55:40.499528 4595733952 tpu_estimator.py:2160] examples/sec: 2.86286\n",
      "INFO:tensorflow:global_step/sec: 0.0893988\n",
      "I1117 09:55:51.685157 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893988\n",
      "INFO:tensorflow:examples/sec: 2.86076\n",
      "I1117 09:55:51.685381 4595733952 tpu_estimator.py:2160] examples/sec: 2.86076\n",
      "INFO:tensorflow:global_step/sec: 0.0898934\n",
      "I1117 09:56:02.809463 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898934\n",
      "INFO:tensorflow:examples/sec: 2.87659\n",
      "I1117 09:56:02.809679 4595733952 tpu_estimator.py:2160] examples/sec: 2.87659\n",
      "INFO:tensorflow:global_step/sec: 0.0899806\n",
      "I1117 09:56:13.922933 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899806\n",
      "INFO:tensorflow:examples/sec: 2.87938\n",
      "I1117 09:56:13.923133 4595733952 tpu_estimator.py:2160] examples/sec: 2.87938\n",
      "INFO:tensorflow:global_step/sec: 0.0890381\n",
      "I1117 09:56:25.154081 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890381\n",
      "INFO:tensorflow:examples/sec: 2.84922\n",
      "I1117 09:56:25.154284 4595733952 tpu_estimator.py:2160] examples/sec: 2.84922\n",
      "INFO:tensorflow:global_step/sec: 0.0889478\n",
      "I1117 09:56:36.396638 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889478\n",
      "INFO:tensorflow:examples/sec: 2.84633\n",
      "I1117 09:56:36.396844 4595733952 tpu_estimator.py:2160] examples/sec: 2.84633\n",
      "INFO:tensorflow:global_step/sec: 0.0892213\n",
      "I1117 09:56:47.604719 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892213\n",
      "INFO:tensorflow:examples/sec: 2.85508\n",
      "I1117 09:56:47.604933 4595733952 tpu_estimator.py:2160] examples/sec: 2.85508\n",
      "INFO:tensorflow:global_step/sec: 0.0888447\n",
      "I1117 09:56:58.860311 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888447\n",
      "INFO:tensorflow:examples/sec: 2.84303\n",
      "I1117 09:56:58.860527 4595733952 tpu_estimator.py:2160] examples/sec: 2.84303\n",
      "INFO:tensorflow:global_step/sec: 0.0895287\n",
      "I1117 09:57:10.029903 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895287\n",
      "INFO:tensorflow:examples/sec: 2.86492\n",
      "I1117 09:57:10.030117 4595733952 tpu_estimator.py:2160] examples/sec: 2.86492\n",
      "INFO:tensorflow:global_step/sec: 0.088681\n",
      "I1117 09:57:21.306266 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088681\n",
      "INFO:tensorflow:examples/sec: 2.83779\n",
      "I1117 09:57:21.306468 4595733952 tpu_estimator.py:2160] examples/sec: 2.83779\n",
      "INFO:tensorflow:global_step/sec: 0.0898709\n",
      "I1117 09:57:32.433363 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898709\n",
      "INFO:tensorflow:examples/sec: 2.87587\n",
      "I1117 09:57:32.433583 4595733952 tpu_estimator.py:2160] examples/sec: 2.87587\n",
      "INFO:tensorflow:global_step/sec: 0.089265\n",
      "I1117 09:57:43.635953 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089265\n",
      "INFO:tensorflow:examples/sec: 2.85648\n",
      "I1117 09:57:43.636165 4595733952 tpu_estimator.py:2160] examples/sec: 2.85648\n",
      "INFO:tensorflow:global_step/sec: 0.0902386\n",
      "I1117 09:57:54.717690 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902386\n",
      "INFO:tensorflow:examples/sec: 2.88763\n",
      "I1117 09:57:54.719270 4595733952 tpu_estimator.py:2160] examples/sec: 2.88763\n",
      "INFO:tensorflow:global_step/sec: 0.0902159\n",
      "I1117 09:58:05.802200 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902159\n",
      "INFO:tensorflow:examples/sec: 2.88691\n",
      "I1117 09:58:05.802411 4595733952 tpu_estimator.py:2160] examples/sec: 2.88691\n",
      "INFO:tensorflow:global_step/sec: 0.0894266\n",
      "I1117 09:58:16.984551 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894266\n",
      "INFO:tensorflow:examples/sec: 2.86165\n",
      "I1117 09:58:16.984773 4595733952 tpu_estimator.py:2160] examples/sec: 2.86165\n",
      "INFO:tensorflow:global_step/sec: 0.0893192\n",
      "I1117 09:58:28.180365 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893192\n",
      "INFO:tensorflow:examples/sec: 2.85822\n",
      "I1117 09:58:28.180593 4595733952 tpu_estimator.py:2160] examples/sec: 2.85822\n",
      "INFO:tensorflow:global_step/sec: 0.089454\n",
      "I1117 09:58:39.359294 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089454\n",
      "INFO:tensorflow:examples/sec: 2.86253\n",
      "I1117 09:58:39.359519 4595733952 tpu_estimator.py:2160] examples/sec: 2.86253\n",
      "INFO:tensorflow:global_step/sec: 0.0864723\n",
      "I1117 09:58:50.923676 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0864723\n",
      "INFO:tensorflow:examples/sec: 2.76711\n",
      "I1117 09:58:50.923909 4595733952 tpu_estimator.py:2160] examples/sec: 2.76711\n",
      "INFO:tensorflow:global_step/sec: 0.0839643\n",
      "I1117 09:59:02.833497 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839643\n",
      "INFO:tensorflow:examples/sec: 2.68686\n",
      "I1117 09:59:02.833702 4595733952 tpu_estimator.py:2160] examples/sec: 2.68686\n",
      "INFO:tensorflow:global_step/sec: 0.0869462\n",
      "I1117 09:59:14.334840 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869462\n",
      "INFO:tensorflow:examples/sec: 2.78228\n",
      "I1117 09:59:14.335026 4595733952 tpu_estimator.py:2160] examples/sec: 2.78228\n",
      "INFO:tensorflow:global_step/sec: 0.0842417\n",
      "I1117 09:59:26.205499 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842417\n",
      "INFO:tensorflow:examples/sec: 2.69573\n",
      "I1117 09:59:26.205718 4595733952 tpu_estimator.py:2160] examples/sec: 2.69573\n",
      "INFO:tensorflow:global_step/sec: 0.0855925\n",
      "I1117 09:59:37.888764 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0855925\n",
      "INFO:tensorflow:examples/sec: 2.73896\n",
      "I1117 09:59:37.889037 4595733952 tpu_estimator.py:2160] examples/sec: 2.73896\n",
      "INFO:tensorflow:global_step/sec: 0.0871374\n",
      "I1117 09:59:49.364873 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871374\n",
      "INFO:tensorflow:examples/sec: 2.7884\n",
      "I1117 09:59:49.365084 4595733952 tpu_estimator.py:2160] examples/sec: 2.7884\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0836731\n",
      "I1117 10:00:01.316160 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836731\n",
      "INFO:tensorflow:examples/sec: 2.67754\n",
      "I1117 10:00:01.316395 4595733952 tpu_estimator.py:2160] examples/sec: 2.67754\n",
      "INFO:tensorflow:global_step/sec: 0.0819656\n",
      "I1117 10:00:13.516381 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0819656\n",
      "INFO:tensorflow:examples/sec: 2.6229\n",
      "I1117 10:00:13.516561 4595733952 tpu_estimator.py:2160] examples/sec: 2.6229\n",
      "INFO:tensorflow:global_step/sec: 0.0853586\n",
      "I1117 10:00:25.231662 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853586\n",
      "INFO:tensorflow:examples/sec: 2.73148\n",
      "I1117 10:00:25.231855 4595733952 tpu_estimator.py:2160] examples/sec: 2.73148\n",
      "INFO:tensorflow:global_step/sec: 0.0885479\n",
      "I1117 10:00:36.524978 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885479\n",
      "INFO:tensorflow:examples/sec: 2.83353\n",
      "I1117 10:00:36.525186 4595733952 tpu_estimator.py:2160] examples/sec: 2.83353\n",
      "INFO:tensorflow:global_step/sec: 0.0863348\n",
      "I1117 10:00:48.107748 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863348\n",
      "INFO:tensorflow:examples/sec: 2.76271\n",
      "I1117 10:00:48.108055 4595733952 tpu_estimator.py:2160] examples/sec: 2.76271\n",
      "INFO:tensorflow:global_step/sec: 0.0644446\n",
      "I1117 10:01:03.624963 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0644446\n",
      "INFO:tensorflow:examples/sec: 2.06223\n",
      "I1117 10:01:03.625139 4595733952 tpu_estimator.py:2160] examples/sec: 2.06223\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 101 vs previous value: 101. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 10:01:03.629861 4595733952 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 101 vs previous value: 101. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0822968\n",
      "I1117 10:01:15.776165 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0822968\n",
      "INFO:tensorflow:examples/sec: 2.6335\n",
      "I1117 10:01:15.776357 4595733952 tpu_estimator.py:2160] examples/sec: 2.6335\n",
      "INFO:tensorflow:global_step/sec: 0.0838404\n",
      "I1117 10:01:27.703563 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0838404\n",
      "INFO:tensorflow:examples/sec: 2.68289\n",
      "I1117 10:01:27.703820 4595733952 tpu_estimator.py:2160] examples/sec: 2.68289\n",
      "INFO:tensorflow:global_step/sec: 0.0855769\n",
      "I1117 10:01:39.388954 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0855769\n",
      "INFO:tensorflow:examples/sec: 2.73846\n",
      "I1117 10:01:39.389319 4595733952 tpu_estimator.py:2160] examples/sec: 2.73846\n",
      "INFO:tensorflow:global_step/sec: 0.0853585\n",
      "I1117 10:01:51.104243 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853585\n",
      "INFO:tensorflow:examples/sec: 2.73147\n",
      "I1117 10:01:51.104439 4595733952 tpu_estimator.py:2160] examples/sec: 2.73147\n",
      "INFO:tensorflow:global_step/sec: 0.0863994\n",
      "I1117 10:02:02.678405 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863994\n",
      "INFO:tensorflow:examples/sec: 2.76478\n",
      "I1117 10:02:02.680236 4595733952 tpu_estimator.py:2160] examples/sec: 2.76478\n",
      "INFO:tensorflow:global_step/sec: 0.0866045\n",
      "I1117 10:02:14.225155 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866045\n",
      "INFO:tensorflow:examples/sec: 2.77135\n",
      "I1117 10:02:14.225363 4595733952 tpu_estimator.py:2160] examples/sec: 2.77135\n",
      "INFO:tensorflow:global_step/sec: 0.0861018\n",
      "I1117 10:02:25.839304 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861018\n",
      "INFO:tensorflow:examples/sec: 2.75526\n",
      "I1117 10:02:25.839503 4595733952 tpu_estimator.py:2160] examples/sec: 2.75526\n",
      "INFO:tensorflow:global_step/sec: 0.0863576\n",
      "I1117 10:02:37.419076 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863576\n",
      "INFO:tensorflow:examples/sec: 2.76344\n",
      "I1117 10:02:37.419291 4595733952 tpu_estimator.py:2160] examples/sec: 2.76344\n",
      "INFO:tensorflow:global_step/sec: 0.0862712\n",
      "I1117 10:02:49.010415 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862712\n",
      "INFO:tensorflow:examples/sec: 2.76068\n",
      "I1117 10:02:49.010636 4595733952 tpu_estimator.py:2160] examples/sec: 2.76068\n",
      "INFO:tensorflow:global_step/sec: 0.0862025\n",
      "I1117 10:03:00.610993 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862025\n",
      "INFO:tensorflow:examples/sec: 2.75848\n",
      "I1117 10:03:00.611190 4595733952 tpu_estimator.py:2160] examples/sec: 2.75848\n",
      "INFO:tensorflow:global_step/sec: 0.0865738\n",
      "I1117 10:03:12.161840 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865738\n",
      "INFO:tensorflow:examples/sec: 2.77036\n",
      "I1117 10:03:12.162042 4595733952 tpu_estimator.py:2160] examples/sec: 2.77036\n",
      "INFO:tensorflow:global_step/sec: 0.0863092\n",
      "I1117 10:03:23.748115 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863092\n",
      "INFO:tensorflow:examples/sec: 2.7619\n",
      "I1117 10:03:23.748327 4595733952 tpu_estimator.py:2160] examples/sec: 2.7619\n",
      "INFO:tensorflow:global_step/sec: 0.0835661\n",
      "I1117 10:03:35.714664 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0835661\n",
      "INFO:tensorflow:examples/sec: 2.67412\n",
      "I1117 10:03:35.715064 4595733952 tpu_estimator.py:2160] examples/sec: 2.67412\n",
      "INFO:tensorflow:global_step/sec: 0.0866847\n",
      "I1117 10:03:47.250730 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866847\n",
      "INFO:tensorflow:examples/sec: 2.77391\n",
      "I1117 10:03:47.251125 4595733952 tpu_estimator.py:2160] examples/sec: 2.77391\n",
      "INFO:tensorflow:global_step/sec: 0.0862239\n",
      "I1117 10:03:58.848433 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862239\n",
      "INFO:tensorflow:examples/sec: 2.75917\n",
      "I1117 10:03:58.848649 4595733952 tpu_estimator.py:2160] examples/sec: 2.75917\n",
      "INFO:tensorflow:global_step/sec: 0.0875756\n",
      "I1117 10:04:10.267122 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875756\n",
      "INFO:tensorflow:examples/sec: 2.80242\n",
      "I1117 10:04:10.267320 4595733952 tpu_estimator.py:2160] examples/sec: 2.80242\n",
      "INFO:tensorflow:global_step/sec: 0.0870436\n",
      "I1117 10:04:21.755653 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870436\n",
      "INFO:tensorflow:examples/sec: 2.78539\n",
      "I1117 10:04:21.755883 4595733952 tpu_estimator.py:2160] examples/sec: 2.78539\n",
      "INFO:tensorflow:global_step/sec: 0.0873256\n",
      "I1117 10:04:33.207026 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873256\n",
      "INFO:tensorflow:examples/sec: 2.79442\n",
      "I1117 10:04:33.208815 4595733952 tpu_estimator.py:2160] examples/sec: 2.79442\n",
      "INFO:tensorflow:global_step/sec: 0.0867974\n",
      "I1117 10:04:44.728113 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867974\n",
      "INFO:tensorflow:examples/sec: 2.77752\n",
      "I1117 10:04:44.728325 4595733952 tpu_estimator.py:2160] examples/sec: 2.77752\n",
      "INFO:tensorflow:global_step/sec: 0.0840765\n",
      "I1117 10:04:56.622042 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840765\n",
      "INFO:tensorflow:examples/sec: 2.69045\n",
      "I1117 10:04:56.622241 4595733952 tpu_estimator.py:2160] examples/sec: 2.69045\n",
      "INFO:tensorflow:global_step/sec: 0.0836927\n",
      "I1117 10:05:08.570518 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836927\n",
      "INFO:tensorflow:examples/sec: 2.67817\n",
      "I1117 10:05:08.570709 4595733952 tpu_estimator.py:2160] examples/sec: 2.67817\n",
      "INFO:tensorflow:global_step/sec: 0.0829687\n",
      "I1117 10:05:20.623276 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0829687\n",
      "INFO:tensorflow:examples/sec: 2.655\n",
      "I1117 10:05:20.623560 4595733952 tpu_estimator.py:2160] examples/sec: 2.655\n",
      "INFO:tensorflow:global_step/sec: 0.086928\n",
      "I1117 10:05:32.127017 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086928\n",
      "INFO:tensorflow:examples/sec: 2.7817\n",
      "I1117 10:05:32.127253 4595733952 tpu_estimator.py:2160] examples/sec: 2.7817\n",
      "INFO:tensorflow:global_step/sec: 0.0899567\n",
      "I1117 10:05:43.243523 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899567\n",
      "INFO:tensorflow:examples/sec: 2.87861\n",
      "I1117 10:05:43.243731 4595733952 tpu_estimator.py:2160] examples/sec: 2.87861\n",
      "INFO:tensorflow:global_step/sec: 0.0853688\n",
      "I1117 10:05:54.957358 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853688\n",
      "INFO:tensorflow:examples/sec: 2.7318\n",
      "I1117 10:05:54.957562 4595733952 tpu_estimator.py:2160] examples/sec: 2.7318\n",
      "INFO:tensorflow:global_step/sec: 0.0873456\n",
      "I1117 10:06:06.406142 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873456\n",
      "INFO:tensorflow:examples/sec: 2.79506\n",
      "I1117 10:06:06.406344 4595733952 tpu_estimator.py:2160] examples/sec: 2.79506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0875063\n",
      "I1117 10:06:17.833893 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875063\n",
      "INFO:tensorflow:examples/sec: 2.8002\n",
      "I1117 10:06:17.834100 4595733952 tpu_estimator.py:2160] examples/sec: 2.8002\n",
      "INFO:tensorflow:global_step/sec: 0.0887413\n",
      "I1117 10:06:29.102598 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887413\n",
      "INFO:tensorflow:examples/sec: 2.83972\n",
      "I1117 10:06:29.102803 4595733952 tpu_estimator.py:2160] examples/sec: 2.83972\n",
      "INFO:tensorflow:global_step/sec: 0.0870773\n",
      "I1117 10:06:40.586656 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870773\n",
      "INFO:tensorflow:examples/sec: 2.78647\n",
      "I1117 10:06:40.586880 4595733952 tpu_estimator.py:2160] examples/sec: 2.78647\n",
      "INFO:tensorflow:global_step/sec: 0.0806893\n",
      "I1117 10:06:52.979878 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0806893\n",
      "INFO:tensorflow:examples/sec: 2.58206\n",
      "I1117 10:06:52.980314 4595733952 tpu_estimator.py:2160] examples/sec: 2.58206\n",
      "INFO:tensorflow:global_step/sec: 0.0870068\n",
      "I1117 10:07:04.473241 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870068\n",
      "INFO:tensorflow:examples/sec: 2.78422\n",
      "I1117 10:07:04.473455 4595733952 tpu_estimator.py:2160] examples/sec: 2.78422\n",
      "INFO:tensorflow:global_step/sec: 0.0852541\n",
      "I1117 10:07:16.202853 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0852541\n",
      "INFO:tensorflow:examples/sec: 2.72813\n",
      "I1117 10:07:16.203146 4595733952 tpu_estimator.py:2160] examples/sec: 2.72813\n",
      "INFO:tensorflow:global_step/sec: 0.0873173\n",
      "I1117 10:07:27.655369 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873173\n",
      "INFO:tensorflow:examples/sec: 2.79415\n",
      "I1117 10:07:27.655586 4595733952 tpu_estimator.py:2160] examples/sec: 2.79415\n",
      "INFO:tensorflow:global_step/sec: 0.0877141\n",
      "I1117 10:07:39.056041 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877141\n",
      "INFO:tensorflow:examples/sec: 2.80685\n",
      "I1117 10:07:39.056240 4595733952 tpu_estimator.py:2160] examples/sec: 2.80685\n",
      "INFO:tensorflow:global_step/sec: 0.0873694\n",
      "I1117 10:07:50.501689 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873694\n",
      "INFO:tensorflow:examples/sec: 2.79582\n",
      "I1117 10:07:50.501895 4595733952 tpu_estimator.py:2160] examples/sec: 2.79582\n",
      "INFO:tensorflow:global_step/sec: 0.0889537\n",
      "I1117 10:08:01.743493 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889537\n",
      "INFO:tensorflow:examples/sec: 2.84652\n",
      "I1117 10:08:01.743875 4595733952 tpu_estimator.py:2160] examples/sec: 2.84652\n",
      "INFO:tensorflow:global_step/sec: 0.0865044\n",
      "I1117 10:08:13.303617 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865044\n",
      "INFO:tensorflow:examples/sec: 2.76814\n",
      "I1117 10:08:13.303839 4595733952 tpu_estimator.py:2160] examples/sec: 2.76814\n",
      "INFO:tensorflow:global_step/sec: 0.0861105\n",
      "I1117 10:08:24.916570 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861105\n",
      "INFO:tensorflow:examples/sec: 2.75554\n",
      "I1117 10:08:24.916773 4595733952 tpu_estimator.py:2160] examples/sec: 2.75554\n",
      "INFO:tensorflow:global_step/sec: 0.084753\n",
      "I1117 10:08:36.715577 4595733952 tpu_estimator.py:2159] global_step/sec: 0.084753\n",
      "INFO:tensorflow:examples/sec: 2.7121\n",
      "I1117 10:08:36.715775 4595733952 tpu_estimator.py:2160] examples/sec: 2.7121\n",
      "INFO:tensorflow:global_step/sec: 0.0862157\n",
      "I1117 10:08:48.314383 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862157\n",
      "INFO:tensorflow:examples/sec: 2.7589\n",
      "I1117 10:08:48.314580 4595733952 tpu_estimator.py:2160] examples/sec: 2.7589\n",
      "INFO:tensorflow:global_step/sec: 0.0850684\n",
      "I1117 10:09:00.069643 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0850684\n",
      "INFO:tensorflow:examples/sec: 2.72219\n",
      "I1117 10:09:00.069850 4595733952 tpu_estimator.py:2160] examples/sec: 2.72219\n",
      "INFO:tensorflow:global_step/sec: 0.0853478\n",
      "I1117 10:09:11.786539 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853478\n",
      "INFO:tensorflow:examples/sec: 2.73113\n",
      "I1117 10:09:11.786742 4595733952 tpu_estimator.py:2160] examples/sec: 2.73113\n",
      "INFO:tensorflow:global_step/sec: 0.0803376\n",
      "I1117 10:09:24.233882 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0803376\n",
      "INFO:tensorflow:examples/sec: 2.5708\n",
      "I1117 10:09:24.234055 4595733952 tpu_estimator.py:2160] examples/sec: 2.5708\n",
      "INFO:tensorflow:global_step/sec: 0.0833458\n",
      "I1117 10:09:36.232987 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0833458\n",
      "INFO:tensorflow:examples/sec: 2.66707\n",
      "I1117 10:09:36.233345 4595733952 tpu_estimator.py:2160] examples/sec: 2.66707\n",
      "INFO:tensorflow:global_step/sec: 0.0867494\n",
      "I1117 10:09:47.759556 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867494\n",
      "INFO:tensorflow:examples/sec: 2.77598\n",
      "I1117 10:09:47.759979 4595733952 tpu_estimator.py:2160] examples/sec: 2.77598\n",
      "INFO:tensorflow:global_step/sec: 0.0859278\n",
      "I1117 10:09:59.397197 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859278\n",
      "INFO:tensorflow:examples/sec: 2.74969\n",
      "I1117 10:09:59.397372 4595733952 tpu_estimator.py:2160] examples/sec: 2.74969\n",
      "INFO:tensorflow:global_step/sec: 0.0856459\n",
      "I1117 10:10:11.073212 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0856459\n",
      "INFO:tensorflow:examples/sec: 2.74067\n",
      "I1117 10:10:11.073544 4595733952 tpu_estimator.py:2160] examples/sec: 2.74067\n",
      "INFO:tensorflow:global_step/sec: 0.0832342\n",
      "I1117 10:10:23.087497 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832342\n",
      "INFO:tensorflow:examples/sec: 2.66349\n",
      "I1117 10:10:23.087719 4595733952 tpu_estimator.py:2160] examples/sec: 2.66349\n",
      "INFO:tensorflow:global_step/sec: 0.0830358\n",
      "I1117 10:10:35.130519 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0830358\n",
      "INFO:tensorflow:examples/sec: 2.65714\n",
      "I1117 10:10:35.131450 4595733952 tpu_estimator.py:2160] examples/sec: 2.65714\n",
      "INFO:tensorflow:global_step/sec: 0.0820238\n",
      "I1117 10:10:47.322088 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0820238\n",
      "INFO:tensorflow:examples/sec: 2.62476\n",
      "I1117 10:10:47.322447 4595733952 tpu_estimator.py:2160] examples/sec: 2.62476\n",
      "INFO:tensorflow:global_step/sec: 0.0873421\n",
      "I1117 10:10:58.771337 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873421\n",
      "INFO:tensorflow:examples/sec: 2.79495\n",
      "I1117 10:10:58.771550 4595733952 tpu_estimator.py:2160] examples/sec: 2.79495\n",
      "INFO:tensorflow:global_step/sec: 0.0879864\n",
      "I1117 10:11:10.136716 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879864\n",
      "INFO:tensorflow:examples/sec: 2.81557\n",
      "I1117 10:11:10.136929 4595733952 tpu_estimator.py:2160] examples/sec: 2.81557\n",
      "INFO:tensorflow:global_step/sec: 0.0879322\n",
      "I1117 10:11:21.509114 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879322\n",
      "INFO:tensorflow:examples/sec: 2.81383\n",
      "I1117 10:11:21.509317 4595733952 tpu_estimator.py:2160] examples/sec: 2.81383\n",
      "INFO:tensorflow:global_step/sec: 0.0869006\n",
      "I1117 10:11:33.016501 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869006\n",
      "INFO:tensorflow:examples/sec: 2.78082\n",
      "I1117 10:11:33.016826 4595733952 tpu_estimator.py:2160] examples/sec: 2.78082\n",
      "INFO:tensorflow:global_step/sec: 0.0876242\n",
      "I1117 10:11:44.428859 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876242\n",
      "INFO:tensorflow:examples/sec: 2.80398\n",
      "I1117 10:11:44.429176 4595733952 tpu_estimator.py:2160] examples/sec: 2.80398\n",
      "INFO:tensorflow:global_step/sec: 0.0840551\n",
      "I1117 10:11:56.325818 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840551\n",
      "INFO:tensorflow:examples/sec: 2.68976\n",
      "I1117 10:11:56.325998 4595733952 tpu_estimator.py:2160] examples/sec: 2.68976\n",
      "INFO:tensorflow:global_step/sec: 0.0833101\n",
      "I1117 10:12:08.329174 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0833101\n",
      "INFO:tensorflow:examples/sec: 2.66592\n",
      "I1117 10:12:08.329359 4595733952 tpu_estimator.py:2160] examples/sec: 2.66592\n",
      "INFO:tensorflow:global_step/sec: 0.0853694\n",
      "I1117 10:12:20.042974 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853694\n",
      "INFO:tensorflow:examples/sec: 2.73182\n",
      "I1117 10:12:20.043256 4595733952 tpu_estimator.py:2160] examples/sec: 2.73182\n",
      "INFO:tensorflow:global_step/sec: 0.0848897\n",
      "I1117 10:12:31.822974 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0848897\n",
      "INFO:tensorflow:examples/sec: 2.71647\n",
      "I1117 10:12:31.823168 4595733952 tpu_estimator.py:2160] examples/sec: 2.71647\n",
      "INFO:tensorflow:global_step/sec: 0.0885113\n",
      "I1117 10:12:43.120975 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885113\n",
      "INFO:tensorflow:examples/sec: 2.83236\n",
      "I1117 10:12:43.121193 4595733952 tpu_estimator.py:2160] examples/sec: 2.83236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0824386\n",
      "I1117 10:12:55.251220 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824386\n",
      "INFO:tensorflow:examples/sec: 2.63803\n",
      "I1117 10:12:55.251397 4595733952 tpu_estimator.py:2160] examples/sec: 2.63803\n",
      "INFO:tensorflow:global_step/sec: 0.0849828\n",
      "I1117 10:13:07.018320 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0849828\n",
      "INFO:tensorflow:examples/sec: 2.71945\n",
      "I1117 10:13:07.018516 4595733952 tpu_estimator.py:2160] examples/sec: 2.71945\n",
      "INFO:tensorflow:global_step/sec: 0.0865573\n",
      "I1117 10:13:18.571409 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865573\n",
      "INFO:tensorflow:examples/sec: 2.76983\n",
      "I1117 10:13:18.571630 4595733952 tpu_estimator.py:2160] examples/sec: 2.76983\n",
      "INFO:tensorflow:global_step/sec: 0.0861325\n",
      "I1117 10:13:30.181379 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861325\n",
      "INFO:tensorflow:examples/sec: 2.75624\n",
      "I1117 10:13:30.181601 4595733952 tpu_estimator.py:2160] examples/sec: 2.75624\n",
      "INFO:tensorflow:global_step/sec: 0.0849605\n",
      "I1117 10:13:41.951543 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0849605\n",
      "INFO:tensorflow:examples/sec: 2.71874\n",
      "I1117 10:13:41.951920 4595733952 tpu_estimator.py:2160] examples/sec: 2.71874\n",
      "INFO:tensorflow:global_step/sec: 0.0843962\n",
      "I1117 10:13:53.800462 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843962\n",
      "INFO:tensorflow:examples/sec: 2.70068\n",
      "I1117 10:13:53.800839 4595733952 tpu_estimator.py:2160] examples/sec: 2.70068\n",
      "INFO:tensorflow:global_step/sec: 0.0824743\n",
      "I1117 10:14:05.925424 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824743\n",
      "INFO:tensorflow:examples/sec: 2.63918\n",
      "I1117 10:14:05.925626 4595733952 tpu_estimator.py:2160] examples/sec: 2.63918\n",
      "INFO:tensorflow:global_step/sec: 0.0828022\n",
      "I1117 10:14:18.002413 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0828022\n",
      "INFO:tensorflow:examples/sec: 2.64967\n",
      "I1117 10:14:18.002632 4595733952 tpu_estimator.py:2160] examples/sec: 2.64967\n",
      "INFO:tensorflow:global_step/sec: 0.0842955\n",
      "I1117 10:14:29.865462 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842955\n",
      "INFO:tensorflow:examples/sec: 2.69746\n",
      "I1117 10:14:29.865963 4595733952 tpu_estimator.py:2160] examples/sec: 2.69746\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 170 vs previous value: 170. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 10:14:29.866226 4595733952 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 170 vs previous value: 170. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0886471\n",
      "I1117 10:14:41.146131 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886471\n",
      "INFO:tensorflow:examples/sec: 2.83671\n",
      "I1117 10:14:41.146392 4595733952 tpu_estimator.py:2160] examples/sec: 2.83671\n",
      "INFO:tensorflow:global_step/sec: 0.0880971\n",
      "I1117 10:14:52.497228 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880971\n",
      "INFO:tensorflow:examples/sec: 2.81911\n",
      "I1117 10:14:52.497466 4595733952 tpu_estimator.py:2160] examples/sec: 2.81911\n",
      "INFO:tensorflow:global_step/sec: 0.0878572\n",
      "I1117 10:15:03.879328 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878572\n",
      "INFO:tensorflow:examples/sec: 2.81143\n",
      "I1117 10:15:03.879533 4595733952 tpu_estimator.py:2160] examples/sec: 2.81143\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 173 vs previous value: 173. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 10:15:03.879662 4595733952 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 173 vs previous value: 173. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0864216\n",
      "I1117 10:15:15.450492 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0864216\n",
      "INFO:tensorflow:examples/sec: 2.76549\n",
      "I1117 10:15:15.450693 4595733952 tpu_estimator.py:2160] examples/sec: 2.76549\n",
      "INFO:tensorflow:global_step/sec: 0.0857755\n",
      "I1117 10:15:27.108843 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857755\n",
      "INFO:tensorflow:examples/sec: 2.74482\n",
      "I1117 10:15:27.109029 4595733952 tpu_estimator.py:2160] examples/sec: 2.74482\n",
      "INFO:tensorflow:global_step/sec: 0.0863403\n",
      "I1117 10:15:38.690926 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863403\n",
      "INFO:tensorflow:examples/sec: 2.76289\n",
      "I1117 10:15:38.691148 4595733952 tpu_estimator.py:2160] examples/sec: 2.76289\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 176 vs previous value: 176. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 10:15:38.691290 4595733952 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 176 vs previous value: 176. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0874277\n",
      "I1117 10:15:50.128936 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874277\n",
      "INFO:tensorflow:examples/sec: 2.79769\n",
      "I1117 10:15:50.129143 4595733952 tpu_estimator.py:2160] examples/sec: 2.79769\n",
      "INFO:tensorflow:global_step/sec: 0.0868523\n",
      "I1117 10:16:01.642747 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0868523\n",
      "INFO:tensorflow:examples/sec: 2.77927\n",
      "I1117 10:16:01.642944 4595733952 tpu_estimator.py:2160] examples/sec: 2.77927\n",
      "INFO:tensorflow:global_step/sec: 0.0883552\n",
      "I1117 10:16:12.960680 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883552\n",
      "INFO:tensorflow:examples/sec: 2.82737\n",
      "I1117 10:16:12.961112 4595733952 tpu_estimator.py:2160] examples/sec: 2.82737\n",
      "INFO:tensorflow:global_step/sec: 0.0875987\n",
      "I1117 10:16:24.376384 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875987\n",
      "INFO:tensorflow:examples/sec: 2.80316\n",
      "I1117 10:16:24.376609 4595733952 tpu_estimator.py:2160] examples/sec: 2.80316\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 180 vs previous value: 180. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 10:16:24.376748 4595733952 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 180 vs previous value: 180. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0866275\n",
      "I1117 10:16:35.920060 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866275\n",
      "INFO:tensorflow:examples/sec: 2.77208\n",
      "I1117 10:16:35.920275 4595733952 tpu_estimator.py:2160] examples/sec: 2.77208\n",
      "INFO:tensorflow:global_step/sec: 0.0874425\n",
      "I1117 10:16:47.356154 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874425\n",
      "INFO:tensorflow:examples/sec: 2.79816\n",
      "I1117 10:16:47.356359 4595733952 tpu_estimator.py:2160] examples/sec: 2.79816\n",
      "INFO:tensorflow:global_step/sec: 0.0880791\n",
      "I1117 10:16:58.709578 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880791\n",
      "INFO:tensorflow:examples/sec: 2.81853\n",
      "I1117 10:16:58.709787 4595733952 tpu_estimator.py:2160] examples/sec: 2.81853\n",
      "INFO:tensorflow:global_step/sec: 0.0888275\n",
      "I1117 10:17:09.967343 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888275\n",
      "INFO:tensorflow:examples/sec: 2.84248\n",
      "I1117 10:17:09.967689 4595733952 tpu_estimator.py:2160] examples/sec: 2.84248\n",
      "INFO:tensorflow:global_step/sec: 0.0884257\n",
      "I1117 10:17:21.276266 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884257\n",
      "INFO:tensorflow:examples/sec: 2.82962\n",
      "I1117 10:17:21.276459 4595733952 tpu_estimator.py:2160] examples/sec: 2.82962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0875946\n",
      "I1117 10:17:32.692506 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875946\n",
      "INFO:tensorflow:examples/sec: 2.80303\n",
      "I1117 10:17:32.692775 4595733952 tpu_estimator.py:2160] examples/sec: 2.80303\n",
      "INFO:tensorflow:global_step/sec: 0.0875783\n",
      "I1117 10:17:44.110860 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875783\n",
      "INFO:tensorflow:examples/sec: 2.80251\n",
      "I1117 10:17:44.111075 4595733952 tpu_estimator.py:2160] examples/sec: 2.80251\n",
      "INFO:tensorflow:global_step/sec: 0.0899478\n",
      "I1117 10:17:55.228416 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899478\n",
      "INFO:tensorflow:examples/sec: 2.87833\n",
      "I1117 10:17:55.228623 4595733952 tpu_estimator.py:2160] examples/sec: 2.87833\n",
      "INFO:tensorflow:global_step/sec: 0.0853651\n",
      "I1117 10:18:06.942808 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853651\n",
      "INFO:tensorflow:examples/sec: 2.73168\n",
      "I1117 10:18:06.943249 4595733952 tpu_estimator.py:2160] examples/sec: 2.73168\n",
      "INFO:tensorflow:global_step/sec: 0.0664824\n",
      "I1117 10:18:21.984382 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0664824\n",
      "INFO:tensorflow:examples/sec: 2.12744\n",
      "I1117 10:18:21.984566 4595733952 tpu_estimator.py:2160] examples/sec: 2.12744\n",
      "INFO:tensorflow:global_step/sec: 0.0882314\n",
      "I1117 10:18:33.318277 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882314\n",
      "INFO:tensorflow:examples/sec: 2.8234\n",
      "I1117 10:18:33.318490 4595733952 tpu_estimator.py:2160] examples/sec: 2.8234\n",
      "INFO:tensorflow:global_step/sec: 0.0894697\n",
      "I1117 10:18:44.495187 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894697\n",
      "INFO:tensorflow:examples/sec: 2.86303\n",
      "I1117 10:18:44.495389 4595733952 tpu_estimator.py:2160] examples/sec: 2.86303\n",
      "INFO:tensorflow:global_step/sec: 0.0886271\n",
      "I1117 10:18:55.778426 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886271\n",
      "INFO:tensorflow:examples/sec: 2.83607\n",
      "I1117 10:18:55.778639 4595733952 tpu_estimator.py:2160] examples/sec: 2.83607\n",
      "INFO:tensorflow:global_step/sec: 0.0866218\n",
      "I1117 10:19:07.322865 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866218\n",
      "INFO:tensorflow:examples/sec: 2.7719\n",
      "I1117 10:19:07.324664 4595733952 tpu_estimator.py:2160] examples/sec: 2.7719\n",
      "INFO:tensorflow:global_step/sec: 0.085642\n",
      "I1117 10:19:18.999361 4595733952 tpu_estimator.py:2159] global_step/sec: 0.085642\n",
      "INFO:tensorflow:examples/sec: 2.74054\n",
      "I1117 10:19:18.999561 4595733952 tpu_estimator.py:2160] examples/sec: 2.74054\n",
      "INFO:tensorflow:global_step/sec: 0.0868794\n",
      "I1117 10:19:30.509585 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0868794\n",
      "INFO:tensorflow:examples/sec: 2.78014\n",
      "I1117 10:19:30.509802 4595733952 tpu_estimator.py:2160] examples/sec: 2.78014\n",
      "INFO:tensorflow:global_step/sec: 0.0859118\n",
      "I1117 10:19:42.149415 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859118\n",
      "INFO:tensorflow:examples/sec: 2.74918\n",
      "I1117 10:19:42.149611 4595733952 tpu_estimator.py:2160] examples/sec: 2.74918\n",
      "INFO:tensorflow:global_step/sec: 0.0859007\n",
      "I1117 10:19:53.790768 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859007\n",
      "INFO:tensorflow:examples/sec: 2.74882\n",
      "I1117 10:19:53.790975 4595733952 tpu_estimator.py:2160] examples/sec: 2.74882\n",
      "INFO:tensorflow:global_step/sec: 0.0876575\n",
      "I1117 10:20:05.198807 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876575\n",
      "INFO:tensorflow:examples/sec: 2.80504\n",
      "I1117 10:20:05.199024 4595733952 tpu_estimator.py:2160] examples/sec: 2.80504\n",
      "INFO:tensorflow:global_step/sec: 0.0859836\n",
      "I1117 10:20:16.828911 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859836\n",
      "INFO:tensorflow:examples/sec: 2.75147\n",
      "I1117 10:20:16.829107 4595733952 tpu_estimator.py:2160] examples/sec: 2.75147\n",
      "INFO:tensorflow:global_step/sec: 0.0884261\n",
      "I1117 10:20:28.137837 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884261\n",
      "INFO:tensorflow:examples/sec: 2.82964\n",
      "I1117 10:20:28.138046 4595733952 tpu_estimator.py:2160] examples/sec: 2.82964\n",
      "INFO:tensorflow:global_step/sec: 0.0855641\n",
      "I1117 10:20:39.824962 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0855641\n",
      "INFO:tensorflow:examples/sec: 2.73805\n",
      "I1117 10:20:39.825172 4595733952 tpu_estimator.py:2160] examples/sec: 2.73805\n",
      "INFO:tensorflow:global_step/sec: 0.0887877\n",
      "I1117 10:20:51.087769 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887877\n",
      "INFO:tensorflow:examples/sec: 2.84121\n",
      "I1117 10:20:51.087973 4595733952 tpu_estimator.py:2160] examples/sec: 2.84121\n",
      "INFO:tensorflow:global_step/sec: 0.0892873\n",
      "I1117 10:21:02.287573 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892873\n",
      "INFO:tensorflow:examples/sec: 2.8572\n",
      "I1117 10:21:02.287789 4595733952 tpu_estimator.py:2160] examples/sec: 2.8572\n",
      "INFO:tensorflow:global_step/sec: 0.0895183\n",
      "I1117 10:21:13.458469 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895183\n",
      "INFO:tensorflow:examples/sec: 2.86459\n",
      "I1117 10:21:13.458685 4595733952 tpu_estimator.py:2160] examples/sec: 2.86459\n",
      "INFO:tensorflow:global_step/sec: 0.088793\n",
      "I1117 10:21:24.720618 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088793\n",
      "INFO:tensorflow:examples/sec: 2.84138\n",
      "I1117 10:21:24.720812 4595733952 tpu_estimator.py:2160] examples/sec: 2.84138\n",
      "INFO:tensorflow:global_step/sec: 0.0895198\n",
      "I1117 10:21:35.891354 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895198\n",
      "INFO:tensorflow:examples/sec: 2.86463\n",
      "I1117 10:21:35.891561 4595733952 tpu_estimator.py:2160] examples/sec: 2.86463\n",
      "INFO:tensorflow:global_step/sec: 0.0880477\n",
      "I1117 10:21:47.248817 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880477\n",
      "INFO:tensorflow:examples/sec: 2.81753\n",
      "I1117 10:21:47.249030 4595733952 tpu_estimator.py:2160] examples/sec: 2.81753\n",
      "INFO:tensorflow:global_step/sec: 0.0890333\n",
      "I1117 10:21:58.480569 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890333\n",
      "INFO:tensorflow:examples/sec: 2.84906\n",
      "I1117 10:21:58.480762 4595733952 tpu_estimator.py:2160] examples/sec: 2.84906\n",
      "INFO:tensorflow:global_step/sec: 0.0877465\n",
      "I1117 10:22:09.877036 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877465\n",
      "INFO:tensorflow:examples/sec: 2.80789\n",
      "I1117 10:22:09.877254 4595733952 tpu_estimator.py:2160] examples/sec: 2.80789\n",
      "INFO:tensorflow:global_step/sec: 0.0870873\n",
      "I1117 10:22:21.359758 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870873\n",
      "INFO:tensorflow:examples/sec: 2.78679\n",
      "I1117 10:22:21.359963 4595733952 tpu_estimator.py:2160] examples/sec: 2.78679\n",
      "INFO:tensorflow:global_step/sec: 0.0879191\n",
      "I1117 10:22:32.733862 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879191\n",
      "INFO:tensorflow:examples/sec: 2.81341\n",
      "I1117 10:22:32.734076 4595733952 tpu_estimator.py:2160] examples/sec: 2.81341\n",
      "INFO:tensorflow:global_step/sec: 0.0890573\n",
      "I1117 10:22:43.962589 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890573\n",
      "INFO:tensorflow:examples/sec: 2.84983\n",
      "I1117 10:22:43.962802 4595733952 tpu_estimator.py:2160] examples/sec: 2.84983\n",
      "INFO:tensorflow:global_step/sec: 0.0866458\n",
      "I1117 10:22:55.503831 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866458\n",
      "INFO:tensorflow:examples/sec: 2.77266\n",
      "I1117 10:22:55.504176 4595733952 tpu_estimator.py:2160] examples/sec: 2.77266\n",
      "INFO:tensorflow:global_step/sec: 0.086537\n",
      "I1117 10:23:07.059576 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086537\n",
      "INFO:tensorflow:examples/sec: 2.76918\n",
      "I1117 10:23:07.059772 4595733952 tpu_estimator.py:2160] examples/sec: 2.76918\n",
      "INFO:tensorflow:global_step/sec: 0.085293\n",
      "I1117 10:23:18.783874 4595733952 tpu_estimator.py:2159] global_step/sec: 0.085293\n",
      "INFO:tensorflow:examples/sec: 2.72937\n",
      "I1117 10:23:18.784075 4595733952 tpu_estimator.py:2160] examples/sec: 2.72937\n",
      "INFO:tensorflow:global_step/sec: 0.0885597\n",
      "I1117 10:23:30.075697 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885597\n",
      "INFO:tensorflow:examples/sec: 2.83391\n",
      "I1117 10:23:30.075898 4595733952 tpu_estimator.py:2160] examples/sec: 2.83391\n",
      "INFO:tensorflow:global_step/sec: 0.0877036\n",
      "I1117 10:23:41.477775 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877036\n",
      "INFO:tensorflow:examples/sec: 2.80652\n",
      "I1117 10:23:41.478171 4595733952 tpu_estimator.py:2160] examples/sec: 2.80652\n",
      "INFO:tensorflow:global_step/sec: 0.0844543\n",
      "I1117 10:23:53.318531 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844543\n",
      "INFO:tensorflow:examples/sec: 2.70254\n",
      "I1117 10:23:53.318734 4595733952 tpu_estimator.py:2160] examples/sec: 2.70254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0862862\n",
      "I1117 10:24:04.907797 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862862\n",
      "INFO:tensorflow:examples/sec: 2.76116\n",
      "I1117 10:24:04.908004 4595733952 tpu_estimator.py:2160] examples/sec: 2.76116\n",
      "INFO:tensorflow:global_step/sec: 0.088284\n",
      "I1117 10:24:16.234884 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088284\n",
      "INFO:tensorflow:examples/sec: 2.82509\n",
      "I1117 10:24:16.235095 4595733952 tpu_estimator.py:2160] examples/sec: 2.82509\n",
      "INFO:tensorflow:global_step/sec: 0.0873274\n",
      "I1117 10:24:27.686053 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873274\n",
      "INFO:tensorflow:examples/sec: 2.79448\n",
      "I1117 10:24:27.686275 4595733952 tpu_estimator.py:2160] examples/sec: 2.79448\n",
      "INFO:tensorflow:global_step/sec: 0.0884877\n",
      "I1117 10:24:38.987113 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884877\n",
      "INFO:tensorflow:examples/sec: 2.83161\n",
      "I1117 10:24:38.987325 4595733952 tpu_estimator.py:2160] examples/sec: 2.83161\n",
      "INFO:tensorflow:global_step/sec: 0.0881205\n",
      "I1117 10:24:50.335145 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881205\n",
      "INFO:tensorflow:examples/sec: 2.81986\n",
      "I1117 10:24:50.335354 4595733952 tpu_estimator.py:2160] examples/sec: 2.81986\n",
      "INFO:tensorflow:global_step/sec: 0.0885389\n",
      "I1117 10:25:01.629618 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885389\n",
      "INFO:tensorflow:examples/sec: 2.83324\n",
      "I1117 10:25:01.629837 4595733952 tpu_estimator.py:2160] examples/sec: 2.83324\n",
      "INFO:tensorflow:global_step/sec: 0.0891128\n",
      "I1117 10:25:12.851438 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891128\n",
      "INFO:tensorflow:examples/sec: 2.85161\n",
      "I1117 10:25:12.851729 4595733952 tpu_estimator.py:2160] examples/sec: 2.85161\n",
      "INFO:tensorflow:global_step/sec: 0.0871484\n",
      "I1117 10:25:24.326022 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871484\n",
      "INFO:tensorflow:examples/sec: 2.78875\n",
      "I1117 10:25:24.326230 4595733952 tpu_estimator.py:2160] examples/sec: 2.78875\n",
      "INFO:tensorflow:global_step/sec: 0.0829472\n",
      "I1117 10:25:36.381875 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0829472\n",
      "INFO:tensorflow:examples/sec: 2.65431\n",
      "I1117 10:25:36.382064 4595733952 tpu_estimator.py:2160] examples/sec: 2.65431\n",
      "INFO:tensorflow:global_step/sec: 0.0846796\n",
      "I1117 10:25:48.191082 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0846796\n",
      "INFO:tensorflow:examples/sec: 2.70975\n",
      "I1117 10:25:48.191258 4595733952 tpu_estimator.py:2160] examples/sec: 2.70975\n",
      "INFO:tensorflow:global_step/sec: 0.0855935\n",
      "I1117 10:25:59.874241 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0855935\n",
      "INFO:tensorflow:examples/sec: 2.73899\n",
      "I1117 10:25:59.874450 4595733952 tpu_estimator.py:2160] examples/sec: 2.73899\n",
      "INFO:tensorflow:global_step/sec: 0.0824595\n",
      "I1117 10:26:12.001509 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824595\n",
      "INFO:tensorflow:examples/sec: 2.6387\n",
      "I1117 10:26:12.001925 4595733952 tpu_estimator.py:2160] examples/sec: 2.6387\n",
      "INFO:tensorflow:global_step/sec: 0.08228\n",
      "I1117 10:26:24.155082 4595733952 tpu_estimator.py:2159] global_step/sec: 0.08228\n",
      "INFO:tensorflow:examples/sec: 2.63296\n",
      "I1117 10:26:24.155370 4595733952 tpu_estimator.py:2160] examples/sec: 2.63296\n",
      "INFO:tensorflow:global_step/sec: 0.0856401\n",
      "I1117 10:26:35.831822 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0856401\n",
      "INFO:tensorflow:examples/sec: 2.74048\n",
      "I1117 10:26:35.832150 4595733952 tpu_estimator.py:2160] examples/sec: 2.74048\n",
      "INFO:tensorflow:global_step/sec: 0.0863862\n",
      "I1117 10:26:47.407737 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863862\n",
      "INFO:tensorflow:examples/sec: 2.76436\n",
      "I1117 10:26:47.407955 4595733952 tpu_estimator.py:2160] examples/sec: 2.76436\n",
      "INFO:tensorflow:global_step/sec: 0.0861773\n",
      "I1117 10:26:59.011720 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861773\n",
      "INFO:tensorflow:examples/sec: 2.75767\n",
      "I1117 10:26:59.011930 4595733952 tpu_estimator.py:2160] examples/sec: 2.75767\n",
      "INFO:tensorflow:global_step/sec: 0.0879381\n",
      "I1117 10:27:10.383361 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879381\n",
      "INFO:tensorflow:examples/sec: 2.81402\n",
      "I1117 10:27:10.383579 4595733952 tpu_estimator.py:2160] examples/sec: 2.81402\n",
      "INFO:tensorflow:global_step/sec: 0.0857505\n",
      "I1117 10:27:22.045088 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857505\n",
      "INFO:tensorflow:examples/sec: 2.74402\n",
      "I1117 10:27:22.045520 4595733952 tpu_estimator.py:2160] examples/sec: 2.74402\n",
      "INFO:tensorflow:global_step/sec: 0.0875597\n",
      "I1117 10:27:33.465863 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875597\n",
      "INFO:tensorflow:examples/sec: 2.80191\n",
      "I1117 10:27:33.466068 4595733952 tpu_estimator.py:2160] examples/sec: 2.80191\n",
      "INFO:tensorflow:global_step/sec: 0.0854106\n",
      "I1117 10:27:45.173989 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0854106\n",
      "INFO:tensorflow:examples/sec: 2.73314\n",
      "I1117 10:27:45.174179 4595733952 tpu_estimator.py:2160] examples/sec: 2.73314\n",
      "INFO:tensorflow:global_step/sec: 0.085063\n",
      "I1117 10:27:56.929999 4595733952 tpu_estimator.py:2159] global_step/sec: 0.085063\n",
      "INFO:tensorflow:examples/sec: 2.72202\n",
      "I1117 10:27:56.930213 4595733952 tpu_estimator.py:2160] examples/sec: 2.72202\n",
      "INFO:tensorflow:global_step/sec: 0.0869365\n",
      "I1117 10:28:08.432686 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869365\n",
      "INFO:tensorflow:examples/sec: 2.78197\n",
      "I1117 10:28:08.432938 4595733952 tpu_estimator.py:2160] examples/sec: 2.78197\n",
      "INFO:tensorflow:global_step/sec: 0.0883238\n",
      "I1117 10:28:19.754609 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883238\n",
      "INFO:tensorflow:examples/sec: 2.82636\n",
      "I1117 10:28:19.754809 4595733952 tpu_estimator.py:2160] examples/sec: 2.82636\n",
      "INFO:tensorflow:global_step/sec: 0.0878403\n",
      "I1117 10:28:31.138911 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878403\n",
      "INFO:tensorflow:examples/sec: 2.81089\n",
      "I1117 10:28:31.139116 4595733952 tpu_estimator.py:2160] examples/sec: 2.81089\n",
      "INFO:tensorflow:global_step/sec: 0.0889643\n",
      "I1117 10:28:42.379384 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889643\n",
      "INFO:tensorflow:examples/sec: 2.84686\n",
      "I1117 10:28:42.379606 4595733952 tpu_estimator.py:2160] examples/sec: 2.84686\n",
      "INFO:tensorflow:global_step/sec: 0.0874784\n",
      "I1117 10:28:53.810810 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874784\n",
      "INFO:tensorflow:examples/sec: 2.79931\n",
      "I1117 10:28:53.811014 4595733952 tpu_estimator.py:2160] examples/sec: 2.79931\n",
      "INFO:tensorflow:global_step/sec: 0.0857485\n",
      "I1117 10:29:05.472827 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857485\n",
      "INFO:tensorflow:examples/sec: 2.74395\n",
      "I1117 10:29:05.473050 4595733952 tpu_estimator.py:2160] examples/sec: 2.74395\n",
      "INFO:tensorflow:global_step/sec: 0.0866325\n",
      "I1117 10:29:17.015788 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866325\n",
      "INFO:tensorflow:examples/sec: 2.77224\n",
      "I1117 10:29:17.016005 4595733952 tpu_estimator.py:2160] examples/sec: 2.77224\n",
      "INFO:tensorflow:global_step/sec: 0.086585\n",
      "I1117 10:29:28.565160 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086585\n",
      "INFO:tensorflow:examples/sec: 2.77072\n",
      "I1117 10:29:28.565387 4595733952 tpu_estimator.py:2160] examples/sec: 2.77072\n",
      "INFO:tensorflow:global_step/sec: 0.0865395\n",
      "I1117 10:29:40.120608 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865395\n",
      "INFO:tensorflow:examples/sec: 2.76926\n",
      "I1117 10:29:40.120810 4595733952 tpu_estimator.py:2160] examples/sec: 2.76926\n",
      "INFO:tensorflow:global_step/sec: 0.0835572\n",
      "I1117 10:29:52.088410 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0835572\n",
      "INFO:tensorflow:examples/sec: 2.67383\n",
      "I1117 10:29:52.088614 4595733952 tpu_estimator.py:2160] examples/sec: 2.67383\n",
      "INFO:tensorflow:global_step/sec: 0.0872294\n",
      "I1117 10:30:03.552433 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872294\n",
      "INFO:tensorflow:examples/sec: 2.79134\n",
      "I1117 10:30:03.552634 4595733952 tpu_estimator.py:2160] examples/sec: 2.79134\n",
      "INFO:tensorflow:global_step/sec: 0.084548\n",
      "I1117 10:30:15.380044 4595733952 tpu_estimator.py:2159] global_step/sec: 0.084548\n",
      "INFO:tensorflow:examples/sec: 2.70554\n",
      "I1117 10:30:15.380244 4595733952 tpu_estimator.py:2160] examples/sec: 2.70554\n",
      "INFO:tensorflow:global_step/sec: 0.0747063\n",
      "I1117 10:30:28.765738 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0747063\n",
      "INFO:tensorflow:examples/sec: 2.3906\n",
      "I1117 10:30:28.765911 4595733952 tpu_estimator.py:2160] examples/sec: 2.3906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0843944\n",
      "I1117 10:30:40.614965 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843944\n",
      "INFO:tensorflow:examples/sec: 2.70062\n",
      "I1117 10:30:40.615170 4595733952 tpu_estimator.py:2160] examples/sec: 2.70062\n",
      "INFO:tensorflow:global_step/sec: 0.0884454\n",
      "I1117 10:30:51.921324 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884454\n",
      "INFO:tensorflow:examples/sec: 2.83025\n",
      "I1117 10:30:51.921542 4595733952 tpu_estimator.py:2160] examples/sec: 2.83025\n",
      "INFO:tensorflow:global_step/sec: 0.0890702\n",
      "I1117 10:31:03.148424 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890702\n",
      "INFO:tensorflow:examples/sec: 2.85025\n",
      "I1117 10:31:03.148663 4595733952 tpu_estimator.py:2160] examples/sec: 2.85025\n",
      "INFO:tensorflow:global_step/sec: 0.0895664\n",
      "I1117 10:31:14.313328 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895664\n",
      "INFO:tensorflow:examples/sec: 2.86613\n",
      "I1117 10:31:14.313541 4595733952 tpu_estimator.py:2160] examples/sec: 2.86613\n",
      "INFO:tensorflow:global_step/sec: 0.0892525\n",
      "I1117 10:31:25.517485 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892525\n",
      "INFO:tensorflow:examples/sec: 2.85608\n",
      "I1117 10:31:25.517719 4595733952 tpu_estimator.py:2160] examples/sec: 2.85608\n",
      "INFO:tensorflow:global_step/sec: 0.0879046\n",
      "I1117 10:31:36.893476 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879046\n",
      "INFO:tensorflow:examples/sec: 2.81295\n",
      "I1117 10:31:36.893693 4595733952 tpu_estimator.py:2160] examples/sec: 2.81295\n",
      "INFO:tensorflow:global_step/sec: 0.0846209\n",
      "I1117 10:31:48.710908 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0846209\n",
      "INFO:tensorflow:examples/sec: 2.70787\n",
      "I1117 10:31:48.711193 4595733952 tpu_estimator.py:2160] examples/sec: 2.70787\n",
      "INFO:tensorflow:global_step/sec: 0.0855053\n",
      "I1117 10:32:00.406023 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0855053\n",
      "INFO:tensorflow:examples/sec: 2.73617\n",
      "I1117 10:32:00.406225 4595733952 tpu_estimator.py:2160] examples/sec: 2.73617\n",
      "INFO:tensorflow:global_step/sec: 0.0872895\n",
      "I1117 10:32:11.862164 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872895\n",
      "INFO:tensorflow:examples/sec: 2.79326\n",
      "I1117 10:32:11.862375 4595733952 tpu_estimator.py:2160] examples/sec: 2.79326\n",
      "INFO:tensorflow:global_step/sec: 0.087073\n",
      "I1117 10:32:23.346806 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087073\n",
      "INFO:tensorflow:examples/sec: 2.78633\n",
      "I1117 10:32:23.348192 4595733952 tpu_estimator.py:2160] examples/sec: 2.78633\n",
      "INFO:tensorflow:global_step/sec: 0.0870637\n",
      "I1117 10:32:34.832631 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870637\n",
      "INFO:tensorflow:examples/sec: 2.78604\n",
      "I1117 10:32:34.832844 4595733952 tpu_estimator.py:2160] examples/sec: 2.78604\n",
      "INFO:tensorflow:global_step/sec: 0.0885257\n",
      "I1117 10:32:46.128783 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885257\n",
      "INFO:tensorflow:examples/sec: 2.83282\n",
      "I1117 10:32:46.129117 4595733952 tpu_estimator.py:2160] examples/sec: 2.83282\n",
      "INFO:tensorflow:global_step/sec: 0.083573\n",
      "I1117 10:32:58.094490 4595733952 tpu_estimator.py:2159] global_step/sec: 0.083573\n",
      "INFO:tensorflow:examples/sec: 2.67433\n",
      "I1117 10:32:58.094993 4595733952 tpu_estimator.py:2160] examples/sec: 2.67433\n",
      "INFO:tensorflow:global_step/sec: 0.0854701\n",
      "I1117 10:33:09.794385 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0854701\n",
      "INFO:tensorflow:examples/sec: 2.73504\n",
      "I1117 10:33:09.794603 4595733952 tpu_estimator.py:2160] examples/sec: 2.73504\n",
      "INFO:tensorflow:global_step/sec: 0.086582\n",
      "I1117 10:33:21.344105 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086582\n",
      "INFO:tensorflow:examples/sec: 2.77062\n",
      "I1117 10:33:21.344293 4595733952 tpu_estimator.py:2160] examples/sec: 2.77062\n",
      "INFO:tensorflow:global_step/sec: 0.0889218\n",
      "I1117 10:33:32.589961 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889218\n",
      "INFO:tensorflow:examples/sec: 2.8455\n",
      "I1117 10:33:32.590173 4595733952 tpu_estimator.py:2160] examples/sec: 2.8455\n",
      "INFO:tensorflow:global_step/sec: 0.089066\n",
      "I1117 10:33:43.817584 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089066\n",
      "INFO:tensorflow:examples/sec: 2.85011\n",
      "I1117 10:33:43.817800 4595733952 tpu_estimator.py:2160] examples/sec: 2.85011\n",
      "INFO:tensorflow:global_step/sec: 0.0888336\n",
      "I1117 10:33:55.074589 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888336\n",
      "INFO:tensorflow:examples/sec: 2.84268\n",
      "I1117 10:33:55.074809 4595733952 tpu_estimator.py:2160] examples/sec: 2.84268\n",
      "INFO:tensorflow:global_step/sec: 0.0875018\n",
      "I1117 10:34:06.502899 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875018\n",
      "INFO:tensorflow:examples/sec: 2.80006\n",
      "I1117 10:34:06.503100 4595733952 tpu_estimator.py:2160] examples/sec: 2.80006\n",
      "INFO:tensorflow:global_step/sec: 0.087421\n",
      "I1117 10:34:17.941823 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087421\n",
      "INFO:tensorflow:examples/sec: 2.79747\n",
      "I1117 10:34:17.942188 4595733952 tpu_estimator.py:2160] examples/sec: 2.79747\n",
      "INFO:tensorflow:global_step/sec: 0.087538\n",
      "I1117 10:34:29.365411 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087538\n",
      "INFO:tensorflow:examples/sec: 2.80122\n",
      "I1117 10:34:29.365611 4595733952 tpu_estimator.py:2160] examples/sec: 2.80122\n",
      "INFO:tensorflow:global_step/sec: 0.0821248\n",
      "I1117 10:34:41.542031 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0821248\n",
      "INFO:tensorflow:examples/sec: 2.62799\n",
      "I1117 10:34:41.542277 4595733952 tpu_estimator.py:2160] examples/sec: 2.62799\n",
      "INFO:tensorflow:global_step/sec: 0.0884899\n",
      "I1117 10:34:52.842714 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884899\n",
      "INFO:tensorflow:examples/sec: 2.83168\n",
      "I1117 10:34:52.842905 4595733952 tpu_estimator.py:2160] examples/sec: 2.83168\n",
      "INFO:tensorflow:global_step/sec: 0.0873031\n",
      "I1117 10:35:04.297156 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873031\n",
      "INFO:tensorflow:examples/sec: 2.7937\n",
      "I1117 10:35:04.297367 4595733952 tpu_estimator.py:2160] examples/sec: 2.7937\n",
      "INFO:tensorflow:global_step/sec: 0.0883119\n",
      "I1117 10:35:15.620594 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883119\n",
      "INFO:tensorflow:examples/sec: 2.82598\n",
      "I1117 10:35:15.620805 4595733952 tpu_estimator.py:2160] examples/sec: 2.82598\n",
      "INFO:tensorflow:global_step/sec: 0.0887029\n",
      "I1117 10:35:26.894180 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887029\n",
      "INFO:tensorflow:examples/sec: 2.83849\n",
      "I1117 10:35:26.894381 4595733952 tpu_estimator.py:2160] examples/sec: 2.83849\n",
      "INFO:tensorflow:global_step/sec: 0.08815\n",
      "I1117 10:35:38.238597 4595733952 tpu_estimator.py:2159] global_step/sec: 0.08815\n",
      "INFO:tensorflow:examples/sec: 2.8208\n",
      "I1117 10:35:38.239002 4595733952 tpu_estimator.py:2160] examples/sec: 2.8208\n",
      "INFO:tensorflow:global_step/sec: 0.0889131\n",
      "I1117 10:35:49.485404 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889131\n",
      "INFO:tensorflow:examples/sec: 2.84522\n",
      "I1117 10:35:49.485590 4595733952 tpu_estimator.py:2160] examples/sec: 2.84522\n",
      "INFO:tensorflow:global_step/sec: 0.0878427\n",
      "I1117 10:36:00.869420 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878427\n",
      "INFO:tensorflow:examples/sec: 2.81097\n",
      "I1117 10:36:00.870863 4595733952 tpu_estimator.py:2160] examples/sec: 2.81097\n",
      "INFO:tensorflow:global_step/sec: 0.0875588\n",
      "I1117 10:36:12.290323 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875588\n",
      "INFO:tensorflow:examples/sec: 2.80188\n",
      "I1117 10:36:12.290535 4595733952 tpu_estimator.py:2160] examples/sec: 2.80188\n",
      "INFO:tensorflow:global_step/sec: 0.0890088\n",
      "I1117 10:36:23.525156 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890088\n",
      "INFO:tensorflow:examples/sec: 2.84828\n",
      "I1117 10:36:23.525363 4595733952 tpu_estimator.py:2160] examples/sec: 2.84828\n",
      "INFO:tensorflow:global_step/sec: 0.0894606\n",
      "I1117 10:36:34.703288 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894606\n",
      "INFO:tensorflow:examples/sec: 2.86274\n",
      "I1117 10:36:34.703503 4595733952 tpu_estimator.py:2160] examples/sec: 2.86274\n",
      "INFO:tensorflow:global_step/sec: 0.0889512\n",
      "I1117 10:36:45.945392 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889512\n",
      "INFO:tensorflow:examples/sec: 2.84644\n",
      "I1117 10:36:45.945698 4595733952 tpu_estimator.py:2160] examples/sec: 2.84644\n",
      "INFO:tensorflow:global_step/sec: 0.0893426\n",
      "I1117 10:36:57.138265 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893426\n",
      "INFO:tensorflow:examples/sec: 2.85896\n",
      "I1117 10:36:57.138474 4595733952 tpu_estimator.py:2160] examples/sec: 2.85896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0881913\n",
      "I1117 10:37:08.477263 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881913\n",
      "INFO:tensorflow:examples/sec: 2.82212\n",
      "I1117 10:37:08.477527 4595733952 tpu_estimator.py:2160] examples/sec: 2.82212\n",
      "INFO:tensorflow:global_step/sec: 0.0888875\n",
      "I1117 10:37:19.727432 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888875\n",
      "INFO:tensorflow:examples/sec: 2.8444\n",
      "I1117 10:37:19.727665 4595733952 tpu_estimator.py:2160] examples/sec: 2.8444\n",
      "INFO:tensorflow:global_step/sec: 0.0877484\n",
      "I1117 10:37:31.123673 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877484\n",
      "INFO:tensorflow:examples/sec: 2.80795\n",
      "I1117 10:37:31.123893 4595733952 tpu_estimator.py:2160] examples/sec: 2.80795\n",
      "INFO:tensorflow:global_step/sec: 0.0863981\n",
      "I1117 10:37:42.697962 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863981\n",
      "INFO:tensorflow:examples/sec: 2.76474\n",
      "I1117 10:37:42.698328 4595733952 tpu_estimator.py:2160] examples/sec: 2.76474\n",
      "INFO:tensorflow:global_step/sec: 0.0880559\n",
      "I1117 10:37:54.054363 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880559\n",
      "INFO:tensorflow:examples/sec: 2.81779\n",
      "I1117 10:37:54.054559 4595733952 tpu_estimator.py:2160] examples/sec: 2.81779\n",
      "INFO:tensorflow:global_step/sec: 0.0878839\n",
      "I1117 10:38:05.433039 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878839\n",
      "INFO:tensorflow:examples/sec: 2.81228\n",
      "I1117 10:38:05.433238 4595733952 tpu_estimator.py:2160] examples/sec: 2.81228\n",
      "INFO:tensorflow:global_step/sec: 0.0874601\n",
      "I1117 10:38:16.866806 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874601\n",
      "INFO:tensorflow:examples/sec: 2.79872\n",
      "I1117 10:38:16.867126 4595733952 tpu_estimator.py:2160] examples/sec: 2.79872\n",
      "INFO:tensorflow:global_step/sec: 0.0881541\n",
      "I1117 10:38:28.210682 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881541\n",
      "INFO:tensorflow:examples/sec: 2.82093\n",
      "I1117 10:38:28.210895 4595733952 tpu_estimator.py:2160] examples/sec: 2.82093\n",
      "INFO:tensorflow:global_step/sec: 0.0891045\n",
      "I1117 10:38:39.433377 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891045\n",
      "INFO:tensorflow:examples/sec: 2.85134\n",
      "I1117 10:38:39.433598 4595733952 tpu_estimator.py:2160] examples/sec: 2.85134\n",
      "INFO:tensorflow:global_step/sec: 0.0891711\n",
      "I1117 10:38:50.647764 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891711\n",
      "INFO:tensorflow:examples/sec: 2.85348\n",
      "I1117 10:38:50.647978 4595733952 tpu_estimator.py:2160] examples/sec: 2.85348\n",
      "INFO:tensorflow:global_step/sec: 0.0875832\n",
      "I1117 10:39:02.065476 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875832\n",
      "INFO:tensorflow:examples/sec: 2.80266\n",
      "I1117 10:39:02.065680 4595733952 tpu_estimator.py:2160] examples/sec: 2.80266\n",
      "INFO:tensorflow:global_step/sec: 0.0870931\n",
      "I1117 10:39:13.547457 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870931\n",
      "INFO:tensorflow:examples/sec: 2.78698\n",
      "I1117 10:39:13.547662 4595733952 tpu_estimator.py:2160] examples/sec: 2.78698\n",
      "INFO:tensorflow:global_step/sec: 0.0837233\n",
      "I1117 10:39:25.491533 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837233\n",
      "INFO:tensorflow:examples/sec: 2.67914\n",
      "I1117 10:39:25.491713 4595733952 tpu_estimator.py:2160] examples/sec: 2.67914\n",
      "INFO:tensorflow:global_step/sec: 0.0883288\n",
      "I1117 10:39:36.812906 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883288\n",
      "INFO:tensorflow:examples/sec: 2.82652\n",
      "I1117 10:39:36.813126 4595733952 tpu_estimator.py:2160] examples/sec: 2.82652\n",
      "INFO:tensorflow:global_step/sec: 0.0888399\n",
      "I1117 10:39:48.069108 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888399\n",
      "INFO:tensorflow:examples/sec: 2.84288\n",
      "I1117 10:39:48.069473 4595733952 tpu_estimator.py:2160] examples/sec: 2.84288\n",
      "INFO:tensorflow:global_step/sec: 0.0891859\n",
      "I1117 10:39:59.281626 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891859\n",
      "INFO:tensorflow:examples/sec: 2.85395\n",
      "I1117 10:39:59.281829 4595733952 tpu_estimator.py:2160] examples/sec: 2.85395\n",
      "INFO:tensorflow:global_step/sec: 0.0888106\n",
      "I1117 10:40:10.541557 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888106\n",
      "INFO:tensorflow:examples/sec: 2.84194\n",
      "I1117 10:40:10.541785 4595733952 tpu_estimator.py:2160] examples/sec: 2.84194\n",
      "INFO:tensorflow:global_step/sec: 0.0874864\n",
      "I1117 10:40:21.971901 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874864\n",
      "INFO:tensorflow:examples/sec: 2.79956\n",
      "I1117 10:40:21.972120 4595733952 tpu_estimator.py:2160] examples/sec: 2.79956\n",
      "INFO:tensorflow:global_step/sec: 0.0886909\n",
      "I1117 10:40:33.247020 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886909\n",
      "INFO:tensorflow:examples/sec: 2.83811\n",
      "I1117 10:40:33.247217 4595733952 tpu_estimator.py:2160] examples/sec: 2.83811\n",
      "INFO:tensorflow:global_step/sec: 0.0872433\n",
      "I1117 10:40:44.709196 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872433\n",
      "INFO:tensorflow:examples/sec: 2.79179\n",
      "I1117 10:40:44.709394 4595733952 tpu_estimator.py:2160] examples/sec: 2.79179\n",
      "INFO:tensorflow:global_step/sec: 0.0857848\n",
      "I1117 10:40:56.366275 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857848\n",
      "INFO:tensorflow:examples/sec: 2.74511\n",
      "I1117 10:40:56.366497 4595733952 tpu_estimator.py:2160] examples/sec: 2.74511\n",
      "INFO:tensorflow:global_step/sec: 0.0848455\n",
      "I1117 10:41:08.152421 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0848455\n",
      "INFO:tensorflow:examples/sec: 2.71506\n",
      "I1117 10:41:08.152617 4595733952 tpu_estimator.py:2160] examples/sec: 2.71506\n",
      "INFO:tensorflow:global_step/sec: 0.0873915\n",
      "I1117 10:41:19.595180 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873915\n",
      "INFO:tensorflow:examples/sec: 2.79653\n",
      "I1117 10:41:19.595438 4595733952 tpu_estimator.py:2160] examples/sec: 2.79653\n",
      "INFO:tensorflow:global_step/sec: 0.0843514\n",
      "I1117 10:41:31.450324 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843514\n",
      "INFO:tensorflow:examples/sec: 2.69924\n",
      "I1117 10:41:31.450645 4595733952 tpu_estimator.py:2160] examples/sec: 2.69924\n",
      "INFO:tensorflow:global_step/sec: 0.0859428\n",
      "I1117 10:41:43.086014 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859428\n",
      "INFO:tensorflow:examples/sec: 2.75017\n",
      "I1117 10:41:43.086299 4595733952 tpu_estimator.py:2160] examples/sec: 2.75017\n",
      "INFO:tensorflow:global_step/sec: 0.087258\n",
      "I1117 10:41:54.546266 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087258\n",
      "INFO:tensorflow:examples/sec: 2.79226\n",
      "I1117 10:41:54.546484 4595733952 tpu_estimator.py:2160] examples/sec: 2.79226\n",
      "INFO:tensorflow:global_step/sec: 0.0884684\n",
      "I1117 10:42:05.849725 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884684\n",
      "INFO:tensorflow:examples/sec: 2.83099\n",
      "I1117 10:42:05.849928 4595733952 tpu_estimator.py:2160] examples/sec: 2.83099\n",
      "INFO:tensorflow:global_step/sec: 0.0795604\n",
      "I1117 10:42:18.418764 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0795604\n",
      "INFO:tensorflow:examples/sec: 2.54593\n",
      "I1117 10:42:18.418946 4595733952 tpu_estimator.py:2160] examples/sec: 2.54593\n",
      "INFO:tensorflow:global_step/sec: 0.086925\n",
      "I1117 10:42:29.922975 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086925\n",
      "INFO:tensorflow:examples/sec: 2.7816\n",
      "I1117 10:42:29.923177 4595733952 tpu_estimator.py:2160] examples/sec: 2.7816\n",
      "INFO:tensorflow:global_step/sec: 0.0837968\n",
      "I1117 10:42:41.856581 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837968\n",
      "INFO:tensorflow:examples/sec: 2.6815\n",
      "I1117 10:42:41.856782 4595733952 tpu_estimator.py:2160] examples/sec: 2.6815\n",
      "INFO:tensorflow:global_step/sec: 0.0842983\n",
      "I1117 10:42:53.719253 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842983\n",
      "INFO:tensorflow:examples/sec: 2.69755\n",
      "I1117 10:42:53.719470 4595733952 tpu_estimator.py:2160] examples/sec: 2.69755\n",
      "INFO:tensorflow:global_step/sec: 0.0832076\n",
      "I1117 10:43:05.737344 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832076\n",
      "INFO:tensorflow:examples/sec: 2.66264\n",
      "I1117 10:43:05.737541 4595733952 tpu_estimator.py:2160] examples/sec: 2.66264\n",
      "INFO:tensorflow:global_step/sec: 0.0853277\n",
      "I1117 10:43:17.456952 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853277\n",
      "INFO:tensorflow:examples/sec: 2.73048\n",
      "I1117 10:43:17.457387 4595733952 tpu_estimator.py:2160] examples/sec: 2.73048\n",
      "INFO:tensorflow:global_step/sec: 0.0864874\n",
      "I1117 10:43:29.019443 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0864874\n",
      "INFO:tensorflow:examples/sec: 2.7676\n",
      "I1117 10:43:29.019854 4595733952 tpu_estimator.py:2160] examples/sec: 2.7676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0837904\n",
      "I1117 10:43:40.953813 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837904\n",
      "INFO:tensorflow:examples/sec: 2.68129\n",
      "I1117 10:43:40.954179 4595733952 tpu_estimator.py:2160] examples/sec: 2.68129\n",
      "INFO:tensorflow:global_step/sec: 0.0863087\n",
      "I1117 10:43:52.540131 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863087\n",
      "INFO:tensorflow:examples/sec: 2.76188\n",
      "I1117 10:43:52.540346 4595733952 tpu_estimator.py:2160] examples/sec: 2.76188\n",
      "INFO:tensorflow:global_step/sec: 0.0842196\n",
      "I1117 10:44:04.413861 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842196\n",
      "INFO:tensorflow:examples/sec: 2.69503\n",
      "I1117 10:44:04.414107 4595733952 tpu_estimator.py:2160] examples/sec: 2.69503\n",
      "INFO:tensorflow:global_step/sec: 0.0813068\n",
      "I1117 10:44:16.712945 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0813068\n",
      "INFO:tensorflow:examples/sec: 2.60182\n",
      "I1117 10:44:16.713162 4595733952 tpu_estimator.py:2160] examples/sec: 2.60182\n",
      "INFO:tensorflow:global_step/sec: 0.0854577\n",
      "I1117 10:44:28.414599 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0854577\n",
      "INFO:tensorflow:examples/sec: 2.73465\n",
      "I1117 10:44:28.414779 4595733952 tpu_estimator.py:2160] examples/sec: 2.73465\n",
      "INFO:tensorflow:global_step/sec: 0.0819728\n",
      "I1117 10:44:40.613811 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0819728\n",
      "INFO:tensorflow:examples/sec: 2.62313\n",
      "I1117 10:44:40.614006 4595733952 tpu_estimator.py:2160] examples/sec: 2.62313\n",
      "INFO:tensorflow:global_step/sec: 0.0831211\n",
      "I1117 10:44:52.644442 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0831211\n",
      "INFO:tensorflow:examples/sec: 2.65987\n",
      "I1117 10:44:52.644824 4595733952 tpu_estimator.py:2160] examples/sec: 2.65987\n",
      "INFO:tensorflow:global_step/sec: 0.0784485\n",
      "I1117 10:45:05.391665 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0784485\n",
      "INFO:tensorflow:examples/sec: 2.51035\n",
      "I1117 10:45:05.391845 4595733952 tpu_estimator.py:2160] examples/sec: 2.51035\n",
      "INFO:tensorflow:global_step/sec: 0.0847852\n",
      "I1117 10:45:17.186223 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0847852\n",
      "INFO:tensorflow:examples/sec: 2.71313\n",
      "I1117 10:45:17.186514 4595733952 tpu_estimator.py:2160] examples/sec: 2.71313\n",
      "INFO:tensorflow:global_step/sec: 0.086473\n",
      "I1117 10:45:28.750501 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086473\n",
      "INFO:tensorflow:examples/sec: 2.76714\n",
      "I1117 10:45:28.750714 4595733952 tpu_estimator.py:2160] examples/sec: 2.76714\n",
      "INFO:tensorflow:global_step/sec: 0.0865315\n",
      "I1117 10:45:40.306956 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865315\n",
      "INFO:tensorflow:examples/sec: 2.76901\n",
      "I1117 10:45:40.307156 4595733952 tpu_estimator.py:2160] examples/sec: 2.76901\n",
      "INFO:tensorflow:global_step/sec: 0.0875989\n",
      "I1117 10:45:51.722632 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875989\n",
      "INFO:tensorflow:examples/sec: 2.80317\n",
      "I1117 10:45:51.722875 4595733952 tpu_estimator.py:2160] examples/sec: 2.80317\n",
      "INFO:tensorflow:global_step/sec: 0.0896795\n",
      "I1117 10:46:02.873445 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896795\n",
      "INFO:tensorflow:examples/sec: 2.86975\n",
      "I1117 10:46:02.873862 4595733952 tpu_estimator.py:2160] examples/sec: 2.86975\n",
      "INFO:tensorflow:global_step/sec: 0.0897932\n",
      "I1117 10:46:14.010149 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897932\n",
      "INFO:tensorflow:examples/sec: 2.87338\n",
      "I1117 10:46:14.010362 4595733952 tpu_estimator.py:2160] examples/sec: 2.87338\n",
      "INFO:tensorflow:global_step/sec: 0.0883616\n",
      "I1117 10:46:25.327289 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883616\n",
      "INFO:tensorflow:examples/sec: 2.82757\n",
      "I1117 10:46:25.327507 4595733952 tpu_estimator.py:2160] examples/sec: 2.82757\n",
      "INFO:tensorflow:global_step/sec: 0.0895442\n",
      "I1117 10:46:36.494960 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895442\n",
      "INFO:tensorflow:examples/sec: 2.86541\n",
      "I1117 10:46:36.495182 4595733952 tpu_estimator.py:2160] examples/sec: 2.86541\n",
      "INFO:tensorflow:global_step/sec: 0.0880288\n",
      "I1117 10:46:47.854856 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880288\n",
      "INFO:tensorflow:examples/sec: 2.81692\n",
      "I1117 10:46:47.855046 4595733952 tpu_estimator.py:2160] examples/sec: 2.81692\n",
      "INFO:tensorflow:global_step/sec: 0.0841898\n",
      "I1117 10:46:59.732771 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841898\n",
      "INFO:tensorflow:examples/sec: 2.69407\n",
      "I1117 10:46:59.732946 4595733952 tpu_estimator.py:2160] examples/sec: 2.69407\n",
      "INFO:tensorflow:global_step/sec: 0.0880816\n",
      "I1117 10:47:11.085911 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880816\n",
      "INFO:tensorflow:examples/sec: 2.81861\n",
      "I1117 10:47:11.086181 4595733952 tpu_estimator.py:2160] examples/sec: 2.81861\n",
      "INFO:tensorflow:global_step/sec: 0.0883081\n",
      "I1117 10:47:22.409885 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883081\n",
      "INFO:tensorflow:examples/sec: 2.82586\n",
      "I1117 10:47:22.410099 4595733952 tpu_estimator.py:2160] examples/sec: 2.82586\n",
      "INFO:tensorflow:global_step/sec: 0.0886522\n",
      "I1117 10:47:33.689926 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886522\n",
      "INFO:tensorflow:examples/sec: 2.83687\n",
      "I1117 10:47:33.690145 4595733952 tpu_estimator.py:2160] examples/sec: 2.83687\n",
      "INFO:tensorflow:global_step/sec: 0.0891937\n",
      "I1117 10:47:44.901493 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891937\n",
      "INFO:tensorflow:examples/sec: 2.8542\n",
      "I1117 10:47:44.901709 4595733952 tpu_estimator.py:2160] examples/sec: 2.8542\n",
      "INFO:tensorflow:global_step/sec: 0.0894234\n",
      "I1117 10:47:56.084229 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894234\n",
      "INFO:tensorflow:examples/sec: 2.86155\n",
      "I1117 10:47:56.084424 4595733952 tpu_estimator.py:2160] examples/sec: 2.86155\n",
      "INFO:tensorflow:global_step/sec: 0.0882325\n",
      "I1117 10:48:07.417927 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882325\n",
      "INFO:tensorflow:examples/sec: 2.82344\n",
      "I1117 10:48:07.418133 4595733952 tpu_estimator.py:2160] examples/sec: 2.82344\n",
      "INFO:tensorflow:global_step/sec: 0.0874115\n",
      "I1117 10:48:18.858074 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874115\n",
      "INFO:tensorflow:examples/sec: 2.79717\n",
      "I1117 10:48:18.858445 4595733952 tpu_estimator.py:2160] examples/sec: 2.79717\n",
      "INFO:tensorflow:global_step/sec: 0.0891711\n",
      "I1117 10:48:30.072485 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891711\n",
      "INFO:tensorflow:examples/sec: 2.85348\n",
      "I1117 10:48:30.072752 4595733952 tpu_estimator.py:2160] examples/sec: 2.85348\n",
      "INFO:tensorflow:global_step/sec: 0.0892196\n",
      "I1117 10:48:41.280771 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892196\n",
      "INFO:tensorflow:examples/sec: 2.85503\n",
      "I1117 10:48:41.280979 4595733952 tpu_estimator.py:2160] examples/sec: 2.85503\n",
      "INFO:tensorflow:global_step/sec: 0.0890288\n",
      "I1117 10:48:52.513104 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890288\n",
      "INFO:tensorflow:examples/sec: 2.84892\n",
      "I1117 10:48:52.513324 4595733952 tpu_estimator.py:2160] examples/sec: 2.84892\n",
      "INFO:tensorflow:global_step/sec: 0.0895481\n",
      "I1117 10:49:03.680293 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895481\n",
      "INFO:tensorflow:examples/sec: 2.86554\n",
      "I1117 10:49:03.680671 4595733952 tpu_estimator.py:2160] examples/sec: 2.86554\n",
      "INFO:tensorflow:global_step/sec: 0.0880729\n",
      "I1117 10:49:15.034520 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880729\n",
      "INFO:tensorflow:examples/sec: 2.81833\n",
      "I1117 10:49:15.034733 4595733952 tpu_estimator.py:2160] examples/sec: 2.81833\n",
      "INFO:tensorflow:global_step/sec: 0.0873873\n",
      "I1117 10:49:26.477865 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873873\n",
      "INFO:tensorflow:examples/sec: 2.79639\n",
      "I1117 10:49:26.478272 4595733952 tpu_estimator.py:2160] examples/sec: 2.79639\n",
      "INFO:tensorflow:global_step/sec: 0.0882937\n",
      "I1117 10:49:37.803678 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882937\n",
      "INFO:tensorflow:examples/sec: 2.8254\n",
      "I1117 10:49:37.804047 4595733952 tpu_estimator.py:2160] examples/sec: 2.8254\n",
      "INFO:tensorflow:global_step/sec: 0.088892\n",
      "I1117 10:49:49.053282 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088892\n",
      "INFO:tensorflow:examples/sec: 2.84454\n",
      "I1117 10:49:49.053490 4595733952 tpu_estimator.py:2160] examples/sec: 2.84454\n",
      "INFO:tensorflow:global_step/sec: 0.089348\n",
      "I1117 10:50:00.245484 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089348\n",
      "INFO:tensorflow:examples/sec: 2.85914\n",
      "I1117 10:50:00.245701 4595733952 tpu_estimator.py:2160] examples/sec: 2.85914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891218\n",
      "I1117 10:50:11.466075 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891218\n",
      "INFO:tensorflow:examples/sec: 2.8519\n",
      "I1117 10:50:11.466284 4595733952 tpu_estimator.py:2160] examples/sec: 2.8519\n",
      "INFO:tensorflow:global_step/sec: 0.0897285\n",
      "I1117 10:50:22.610812 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897285\n",
      "INFO:tensorflow:examples/sec: 2.87131\n",
      "I1117 10:50:22.611032 4595733952 tpu_estimator.py:2160] examples/sec: 2.87131\n",
      "INFO:tensorflow:global_step/sec: 0.0892504\n",
      "I1117 10:50:33.815233 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892504\n",
      "INFO:tensorflow:examples/sec: 2.85601\n",
      "I1117 10:50:33.815577 4595733952 tpu_estimator.py:2160] examples/sec: 2.85601\n",
      "INFO:tensorflow:global_step/sec: 0.0865053\n",
      "I1117 10:50:45.375206 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865053\n",
      "INFO:tensorflow:examples/sec: 2.76817\n",
      "I1117 10:50:45.375399 4595733952 tpu_estimator.py:2160] examples/sec: 2.76817\n",
      "INFO:tensorflow:global_step/sec: 0.0889264\n",
      "I1117 10:50:56.620490 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889264\n",
      "INFO:tensorflow:examples/sec: 2.84564\n",
      "I1117 10:50:56.620841 4595733952 tpu_estimator.py:2160] examples/sec: 2.84564\n",
      "INFO:tensorflow:global_step/sec: 0.089245\n",
      "I1117 10:51:07.825574 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089245\n",
      "INFO:tensorflow:examples/sec: 2.85584\n",
      "I1117 10:51:07.825772 4595733952 tpu_estimator.py:2160] examples/sec: 2.85584\n",
      "INFO:tensorflow:global_step/sec: 0.088119\n",
      "I1117 10:51:19.173874 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088119\n",
      "INFO:tensorflow:examples/sec: 2.81981\n",
      "I1117 10:51:19.174088 4595733952 tpu_estimator.py:2160] examples/sec: 2.81981\n",
      "INFO:tensorflow:global_step/sec: 0.0900745\n",
      "I1117 10:51:30.275819 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900745\n",
      "INFO:tensorflow:examples/sec: 2.88239\n",
      "I1117 10:51:30.276030 4595733952 tpu_estimator.py:2160] examples/sec: 2.88239\n",
      "INFO:tensorflow:global_step/sec: 0.0875074\n",
      "I1117 10:51:41.703398 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875074\n",
      "INFO:tensorflow:examples/sec: 2.80024\n",
      "I1117 10:51:41.703610 4595733952 tpu_estimator.py:2160] examples/sec: 2.80024\n",
      "INFO:tensorflow:global_step/sec: 0.0898662\n",
      "I1117 10:51:52.831045 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898662\n",
      "INFO:tensorflow:examples/sec: 2.87572\n",
      "I1117 10:51:52.831251 4595733952 tpu_estimator.py:2160] examples/sec: 2.87572\n",
      "INFO:tensorflow:global_step/sec: 0.0906668\n",
      "I1117 10:52:03.860446 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906668\n",
      "INFO:tensorflow:examples/sec: 2.90134\n",
      "I1117 10:52:03.860655 4595733952 tpu_estimator.py:2160] examples/sec: 2.90134\n",
      "INFO:tensorflow:global_step/sec: 0.0860796\n",
      "I1117 10:52:15.477581 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0860796\n",
      "INFO:tensorflow:examples/sec: 2.75455\n",
      "I1117 10:52:15.477765 4595733952 tpu_estimator.py:2160] examples/sec: 2.75455\n",
      "INFO:tensorflow:global_step/sec: 0.0855358\n",
      "I1117 10:52:27.168606 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0855358\n",
      "INFO:tensorflow:examples/sec: 2.73714\n",
      "I1117 10:52:27.168802 4595733952 tpu_estimator.py:2160] examples/sec: 2.73714\n",
      "INFO:tensorflow:global_step/sec: 0.0872494\n",
      "I1117 10:52:38.630003 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872494\n",
      "INFO:tensorflow:examples/sec: 2.79198\n",
      "I1117 10:52:38.630276 4595733952 tpu_estimator.py:2160] examples/sec: 2.79198\n",
      "INFO:tensorflow:global_step/sec: 0.087102\n",
      "I1117 10:52:50.110808 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087102\n",
      "INFO:tensorflow:examples/sec: 2.78726\n",
      "I1117 10:52:50.110999 4595733952 tpu_estimator.py:2160] examples/sec: 2.78726\n",
      "INFO:tensorflow:global_step/sec: 0.0901198\n",
      "I1117 10:53:01.207165 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901198\n",
      "INFO:tensorflow:examples/sec: 2.88383\n",
      "I1117 10:53:01.207375 4595733952 tpu_estimator.py:2160] examples/sec: 2.88383\n",
      "INFO:tensorflow:global_step/sec: 0.0848199\n",
      "I1117 10:53:12.996827 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0848199\n",
      "INFO:tensorflow:examples/sec: 2.71424\n",
      "I1117 10:53:12.997176 4595733952 tpu_estimator.py:2160] examples/sec: 2.71424\n",
      "INFO:tensorflow:global_step/sec: 0.0755927\n",
      "I1117 10:53:26.225679 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0755927\n",
      "INFO:tensorflow:examples/sec: 2.41897\n",
      "I1117 10:53:26.225975 4595733952 tpu_estimator.py:2160] examples/sec: 2.41897\n",
      "INFO:tensorflow:global_step/sec: 0.083079\n",
      "I1117 10:53:38.262387 4595733952 tpu_estimator.py:2159] global_step/sec: 0.083079\n",
      "INFO:tensorflow:examples/sec: 2.65853\n",
      "I1117 10:53:38.262642 4595733952 tpu_estimator.py:2160] examples/sec: 2.65853\n",
      "INFO:tensorflow:global_step/sec: 0.0886196\n",
      "I1117 10:53:49.546568 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886196\n",
      "INFO:tensorflow:examples/sec: 2.83583\n",
      "I1117 10:53:49.546790 4595733952 tpu_estimator.py:2160] examples/sec: 2.83583\n",
      "INFO:tensorflow:global_step/sec: 0.0890521\n",
      "I1117 10:54:00.775958 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890521\n",
      "INFO:tensorflow:examples/sec: 2.84967\n",
      "I1117 10:54:00.776162 4595733952 tpu_estimator.py:2160] examples/sec: 2.84967\n",
      "INFO:tensorflow:global_step/sec: 0.0901381\n",
      "I1117 10:54:11.870023 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901381\n",
      "INFO:tensorflow:examples/sec: 2.88442\n",
      "I1117 10:54:11.870259 4595733952 tpu_estimator.py:2160] examples/sec: 2.88442\n",
      "INFO:tensorflow:global_step/sec: 0.0896984\n",
      "I1117 10:54:23.018503 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896984\n",
      "INFO:tensorflow:examples/sec: 2.87035\n",
      "I1117 10:54:23.018707 4595733952 tpu_estimator.py:2160] examples/sec: 2.87035\n",
      "INFO:tensorflow:global_step/sec: 0.0903197\n",
      "I1117 10:54:34.090276 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903197\n",
      "INFO:tensorflow:examples/sec: 2.89023\n",
      "I1117 10:54:34.090489 4595733952 tpu_estimator.py:2160] examples/sec: 2.89023\n",
      "INFO:tensorflow:global_step/sec: 0.0893912\n",
      "I1117 10:54:45.277038 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893912\n",
      "INFO:tensorflow:examples/sec: 2.86052\n",
      "I1117 10:54:45.277239 4595733952 tpu_estimator.py:2160] examples/sec: 2.86052\n",
      "INFO:tensorflow:global_step/sec: 0.0892891\n",
      "I1117 10:54:56.476622 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892891\n",
      "INFO:tensorflow:examples/sec: 2.85725\n",
      "I1117 10:54:56.476836 4595733952 tpu_estimator.py:2160] examples/sec: 2.85725\n",
      "INFO:tensorflow:global_step/sec: 0.0898358\n",
      "I1117 10:55:07.608031 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898358\n",
      "INFO:tensorflow:examples/sec: 2.87475\n",
      "I1117 10:55:07.608239 4595733952 tpu_estimator.py:2160] examples/sec: 2.87475\n",
      "INFO:tensorflow:global_step/sec: 0.0900083\n",
      "I1117 10:55:18.718117 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900083\n",
      "INFO:tensorflow:examples/sec: 2.88027\n",
      "I1117 10:55:18.718320 4595733952 tpu_estimator.py:2160] examples/sec: 2.88027\n",
      "INFO:tensorflow:global_step/sec: 0.0884821\n",
      "I1117 10:55:30.019842 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884821\n",
      "INFO:tensorflow:examples/sec: 2.83143\n",
      "I1117 10:55:30.020058 4595733952 tpu_estimator.py:2160] examples/sec: 2.83143\n",
      "INFO:tensorflow:global_step/sec: 0.0883316\n",
      "I1117 10:55:41.340831 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883316\n",
      "INFO:tensorflow:examples/sec: 2.82661\n",
      "I1117 10:55:41.341058 4595733952 tpu_estimator.py:2160] examples/sec: 2.82661\n",
      "INFO:tensorflow:global_step/sec: 0.0895358\n",
      "I1117 10:55:52.509539 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895358\n",
      "INFO:tensorflow:examples/sec: 2.86515\n",
      "I1117 10:55:52.509752 4595733952 tpu_estimator.py:2160] examples/sec: 2.86515\n",
      "INFO:tensorflow:global_step/sec: 0.0907529\n",
      "I1117 10:56:03.528470 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907529\n",
      "INFO:tensorflow:examples/sec: 2.90409\n",
      "I1117 10:56:03.528679 4595733952 tpu_estimator.py:2160] examples/sec: 2.90409\n",
      "INFO:tensorflow:global_step/sec: 0.0906519\n",
      "I1117 10:56:14.559679 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906519\n",
      "INFO:tensorflow:examples/sec: 2.90086\n",
      "I1117 10:56:14.559892 4595733952 tpu_estimator.py:2160] examples/sec: 2.90086\n",
      "INFO:tensorflow:global_step/sec: 0.0902869\n",
      "I1117 10:56:25.635473 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902869\n",
      "INFO:tensorflow:examples/sec: 2.88918\n",
      "I1117 10:56:25.635685 4595733952 tpu_estimator.py:2160] examples/sec: 2.88918\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0895124\n",
      "I1117 10:56:36.807109 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895124\n",
      "INFO:tensorflow:examples/sec: 2.8644\n",
      "I1117 10:56:36.807320 4595733952 tpu_estimator.py:2160] examples/sec: 2.8644\n",
      "INFO:tensorflow:global_step/sec: 0.090694\n",
      "I1117 10:56:47.833200 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090694\n",
      "INFO:tensorflow:examples/sec: 2.90221\n",
      "I1117 10:56:47.833413 4595733952 tpu_estimator.py:2160] examples/sec: 2.90221\n",
      "INFO:tensorflow:global_step/sec: 0.0893543\n",
      "I1117 10:56:59.024624 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893543\n",
      "INFO:tensorflow:examples/sec: 2.85934\n",
      "I1117 10:56:59.026268 4595733952 tpu_estimator.py:2160] examples/sec: 2.85934\n",
      "INFO:tensorflow:global_step/sec: 0.0891137\n",
      "I1117 10:57:10.246284 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891137\n",
      "INFO:tensorflow:examples/sec: 2.85164\n",
      "I1117 10:57:10.246586 4595733952 tpu_estimator.py:2160] examples/sec: 2.85164\n",
      "INFO:tensorflow:global_step/sec: 0.0907048\n",
      "I1117 10:57:21.271008 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907048\n",
      "INFO:tensorflow:examples/sec: 2.90255\n",
      "I1117 10:57:21.271224 4595733952 tpu_estimator.py:2160] examples/sec: 2.90255\n",
      "INFO:tensorflow:global_step/sec: 0.0906812\n",
      "I1117 10:57:32.298651 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906812\n",
      "INFO:tensorflow:examples/sec: 2.9018\n",
      "I1117 10:57:32.298866 4595733952 tpu_estimator.py:2160] examples/sec: 2.9018\n",
      "INFO:tensorflow:global_step/sec: 0.0907587\n",
      "I1117 10:57:43.316890 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907587\n",
      "INFO:tensorflow:examples/sec: 2.90428\n",
      "I1117 10:57:43.317124 4595733952 tpu_estimator.py:2160] examples/sec: 2.90428\n",
      "INFO:tensorflow:global_step/sec: 0.0904917\n",
      "I1117 10:57:54.367619 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904917\n",
      "INFO:tensorflow:examples/sec: 2.89573\n",
      "I1117 10:57:54.367834 4595733952 tpu_estimator.py:2160] examples/sec: 2.89573\n",
      "INFO:tensorflow:global_step/sec: 0.0901305\n",
      "I1117 10:58:05.462603 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901305\n",
      "INFO:tensorflow:examples/sec: 2.88418\n",
      "I1117 10:58:05.462806 4595733952 tpu_estimator.py:2160] examples/sec: 2.88418\n",
      "INFO:tensorflow:global_step/sec: 0.090988\n",
      "I1117 10:58:16.453096 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090988\n",
      "INFO:tensorflow:examples/sec: 2.91162\n",
      "I1117 10:58:16.453310 4595733952 tpu_estimator.py:2160] examples/sec: 2.91162\n",
      "INFO:tensorflow:global_step/sec: 0.0903138\n",
      "I1117 10:58:27.525595 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903138\n",
      "INFO:tensorflow:examples/sec: 2.89004\n",
      "I1117 10:58:27.525825 4595733952 tpu_estimator.py:2160] examples/sec: 2.89004\n",
      "INFO:tensorflow:global_step/sec: 0.0907518\n",
      "I1117 10:58:38.544662 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907518\n",
      "INFO:tensorflow:examples/sec: 2.90406\n",
      "I1117 10:58:38.544881 4595733952 tpu_estimator.py:2160] examples/sec: 2.90406\n",
      "INFO:tensorflow:global_step/sec: 0.0902769\n",
      "I1117 10:58:49.621688 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902769\n",
      "INFO:tensorflow:examples/sec: 2.88886\n",
      "I1117 10:58:49.621903 4595733952 tpu_estimator.py:2160] examples/sec: 2.88886\n",
      "INFO:tensorflow:global_step/sec: 0.0905997\n",
      "I1117 10:59:00.659250 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905997\n",
      "INFO:tensorflow:examples/sec: 2.89919\n",
      "I1117 10:59:00.659465 4595733952 tpu_estimator.py:2160] examples/sec: 2.89919\n",
      "INFO:tensorflow:global_step/sec: 0.0903788\n",
      "I1117 10:59:11.723805 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903788\n",
      "INFO:tensorflow:examples/sec: 2.89212\n",
      "I1117 10:59:11.724009 4595733952 tpu_estimator.py:2160] examples/sec: 2.89212\n",
      "INFO:tensorflow:global_step/sec: 0.0905386\n",
      "I1117 10:59:22.768805 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905386\n",
      "INFO:tensorflow:examples/sec: 2.89724\n",
      "I1117 10:59:22.769021 4595733952 tpu_estimator.py:2160] examples/sec: 2.89724\n",
      "INFO:tensorflow:global_step/sec: 0.0900451\n",
      "I1117 10:59:33.874351 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900451\n",
      "INFO:tensorflow:examples/sec: 2.88144\n",
      "I1117 10:59:33.874565 4595733952 tpu_estimator.py:2160] examples/sec: 2.88144\n",
      "INFO:tensorflow:global_step/sec: 0.0906076\n",
      "I1117 10:59:44.910949 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906076\n",
      "INFO:tensorflow:examples/sec: 2.89944\n",
      "I1117 10:59:44.911166 4595733952 tpu_estimator.py:2160] examples/sec: 2.89944\n",
      "INFO:tensorflow:global_step/sec: 0.0898055\n",
      "I1117 10:59:56.046106 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898055\n",
      "INFO:tensorflow:examples/sec: 2.87378\n",
      "I1117 10:59:56.046535 4595733952 tpu_estimator.py:2160] examples/sec: 2.87378\n",
      "INFO:tensorflow:global_step/sec: 0.0795892\n",
      "I1117 11:00:08.610608 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0795892\n",
      "INFO:tensorflow:examples/sec: 2.54686\n",
      "I1117 11:00:08.610800 4595733952 tpu_estimator.py:2160] examples/sec: 2.54686\n",
      "INFO:tensorflow:global_step/sec: 0.0763461\n",
      "I1117 11:00:21.708883 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0763461\n",
      "INFO:tensorflow:examples/sec: 2.44308\n",
      "I1117 11:00:21.709114 4595733952 tpu_estimator.py:2160] examples/sec: 2.44308\n",
      "INFO:tensorflow:global_step/sec: 0.0888869\n",
      "I1117 11:00:32.959146 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888869\n",
      "INFO:tensorflow:examples/sec: 2.84438\n",
      "I1117 11:00:32.959372 4595733952 tpu_estimator.py:2160] examples/sec: 2.84438\n",
      "INFO:tensorflow:global_step/sec: 0.0891044\n",
      "I1117 11:00:44.181916 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891044\n",
      "INFO:tensorflow:examples/sec: 2.85134\n",
      "I1117 11:00:44.182112 4595733952 tpu_estimator.py:2160] examples/sec: 2.85134\n",
      "INFO:tensorflow:global_step/sec: 0.0893248\n",
      "I1117 11:00:55.377022 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893248\n",
      "INFO:tensorflow:examples/sec: 2.85839\n",
      "I1117 11:00:55.377233 4595733952 tpu_estimator.py:2160] examples/sec: 2.85839\n",
      "INFO:tensorflow:global_step/sec: 0.0881667\n",
      "I1117 11:01:06.719140 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881667\n",
      "INFO:tensorflow:examples/sec: 2.82133\n",
      "I1117 11:01:06.719316 4595733952 tpu_estimator.py:2160] examples/sec: 2.82133\n",
      "INFO:tensorflow:global_step/sec: 0.0877434\n",
      "I1117 11:01:18.116037 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877434\n",
      "INFO:tensorflow:examples/sec: 2.80779\n",
      "I1117 11:01:18.116246 4595733952 tpu_estimator.py:2160] examples/sec: 2.80779\n",
      "INFO:tensorflow:global_step/sec: 0.0880117\n",
      "I1117 11:01:29.478166 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880117\n",
      "INFO:tensorflow:examples/sec: 2.81637\n",
      "I1117 11:01:29.478392 4595733952 tpu_estimator.py:2160] examples/sec: 2.81637\n",
      "INFO:tensorflow:global_step/sec: 0.0883753\n",
      "I1117 11:01:40.793548 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883753\n",
      "INFO:tensorflow:examples/sec: 2.82801\n",
      "I1117 11:01:40.793758 4595733952 tpu_estimator.py:2160] examples/sec: 2.82801\n",
      "INFO:tensorflow:global_step/sec: 0.0874766\n",
      "I1117 11:01:52.225179 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874766\n",
      "INFO:tensorflow:examples/sec: 2.79925\n",
      "I1117 11:01:52.225395 4595733952 tpu_estimator.py:2160] examples/sec: 2.79925\n",
      "INFO:tensorflow:global_step/sec: 0.0850032\n",
      "I1117 11:02:03.989490 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0850032\n",
      "INFO:tensorflow:examples/sec: 2.7201\n",
      "I1117 11:02:03.989692 4595733952 tpu_estimator.py:2160] examples/sec: 2.7201\n",
      "INFO:tensorflow:global_step/sec: 0.0859849\n",
      "I1117 11:02:15.619385 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859849\n",
      "INFO:tensorflow:examples/sec: 2.75152\n",
      "I1117 11:02:15.619596 4595733952 tpu_estimator.py:2160] examples/sec: 2.75152\n",
      "INFO:tensorflow:global_step/sec: 0.0876126\n",
      "I1117 11:02:27.033252 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876126\n",
      "INFO:tensorflow:examples/sec: 2.8036\n",
      "I1117 11:02:27.033459 4595733952 tpu_estimator.py:2160] examples/sec: 2.8036\n",
      "INFO:tensorflow:global_step/sec: 0.0896918\n",
      "I1117 11:02:38.182565 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896918\n",
      "INFO:tensorflow:examples/sec: 2.87014\n",
      "I1117 11:02:38.182771 4595733952 tpu_estimator.py:2160] examples/sec: 2.87014\n",
      "INFO:tensorflow:global_step/sec: 0.0905057\n",
      "I1117 11:02:49.231611 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905057\n",
      "INFO:tensorflow:examples/sec: 2.89618\n",
      "I1117 11:02:49.231832 4595733952 tpu_estimator.py:2160] examples/sec: 2.89618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.088556\n",
      "I1117 11:03:00.523853 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088556\n",
      "INFO:tensorflow:examples/sec: 2.83379\n",
      "I1117 11:03:00.524064 4595733952 tpu_estimator.py:2160] examples/sec: 2.83379\n",
      "INFO:tensorflow:global_step/sec: 0.0870179\n",
      "I1117 11:03:12.015763 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870179\n",
      "INFO:tensorflow:examples/sec: 2.78457\n",
      "I1117 11:03:12.015994 4595733952 tpu_estimator.py:2160] examples/sec: 2.78457\n",
      "INFO:tensorflow:global_step/sec: 0.0888782\n",
      "I1117 11:03:23.267099 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888782\n",
      "INFO:tensorflow:examples/sec: 2.8441\n",
      "I1117 11:03:23.267308 4595733952 tpu_estimator.py:2160] examples/sec: 2.8441\n",
      "INFO:tensorflow:global_step/sec: 0.0887574\n",
      "I1117 11:03:34.533744 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887574\n",
      "INFO:tensorflow:examples/sec: 2.84024\n",
      "I1117 11:03:34.533937 4595733952 tpu_estimator.py:2160] examples/sec: 2.84024\n",
      "INFO:tensorflow:global_step/sec: 0.0883341\n",
      "I1117 11:03:45.854425 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883341\n",
      "INFO:tensorflow:examples/sec: 2.82669\n",
      "I1117 11:03:45.854637 4595733952 tpu_estimator.py:2160] examples/sec: 2.82669\n",
      "INFO:tensorflow:global_step/sec: 0.0873937\n",
      "I1117 11:03:57.296895 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873937\n",
      "INFO:tensorflow:examples/sec: 2.7966\n",
      "I1117 11:03:57.297114 4595733952 tpu_estimator.py:2160] examples/sec: 2.7966\n",
      "INFO:tensorflow:global_step/sec: 0.0881779\n",
      "I1117 11:04:08.637588 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881779\n",
      "INFO:tensorflow:examples/sec: 2.82169\n",
      "I1117 11:04:08.637793 4595733952 tpu_estimator.py:2160] examples/sec: 2.82169\n",
      "INFO:tensorflow:global_step/sec: 0.0875611\n",
      "I1117 11:04:20.058211 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875611\n",
      "INFO:tensorflow:examples/sec: 2.80195\n",
      "I1117 11:04:20.058454 4595733952 tpu_estimator.py:2160] examples/sec: 2.80195\n",
      "INFO:tensorflow:global_step/sec: 0.0870056\n",
      "I1117 11:04:31.551695 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870056\n",
      "INFO:tensorflow:examples/sec: 2.78418\n",
      "I1117 11:04:31.551888 4595733952 tpu_estimator.py:2160] examples/sec: 2.78418\n",
      "INFO:tensorflow:global_step/sec: 0.0875522\n",
      "I1117 11:04:42.973459 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875522\n",
      "INFO:tensorflow:examples/sec: 2.80167\n",
      "I1117 11:04:42.973657 4595733952 tpu_estimator.py:2160] examples/sec: 2.80167\n",
      "INFO:tensorflow:global_step/sec: 0.0872384\n",
      "I1117 11:04:54.436297 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872384\n",
      "INFO:tensorflow:examples/sec: 2.79163\n",
      "I1117 11:04:54.436506 4595733952 tpu_estimator.py:2160] examples/sec: 2.79163\n",
      "INFO:tensorflow:global_step/sec: 0.0881407\n",
      "I1117 11:05:05.781801 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881407\n",
      "INFO:tensorflow:examples/sec: 2.8205\n",
      "I1117 11:05:05.782000 4595733952 tpu_estimator.py:2160] examples/sec: 2.8205\n",
      "INFO:tensorflow:global_step/sec: 0.0892371\n",
      "I1117 11:05:16.987902 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892371\n",
      "INFO:tensorflow:examples/sec: 2.85559\n",
      "I1117 11:05:16.988096 4595733952 tpu_estimator.py:2160] examples/sec: 2.85559\n",
      "INFO:tensorflow:global_step/sec: 0.0864189\n",
      "I1117 11:05:28.559451 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0864189\n",
      "INFO:tensorflow:examples/sec: 2.7654\n",
      "I1117 11:05:28.559653 4595733952 tpu_estimator.py:2160] examples/sec: 2.7654\n",
      "INFO:tensorflow:global_step/sec: 0.0881734\n",
      "I1117 11:05:39.900747 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881734\n",
      "INFO:tensorflow:examples/sec: 2.82155\n",
      "I1117 11:05:39.900963 4595733952 tpu_estimator.py:2160] examples/sec: 2.82155\n",
      "INFO:tensorflow:global_step/sec: 0.086979\n",
      "I1117 11:05:51.397765 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086979\n",
      "INFO:tensorflow:examples/sec: 2.78333\n",
      "I1117 11:05:51.397953 4595733952 tpu_estimator.py:2160] examples/sec: 2.78333\n",
      "INFO:tensorflow:global_step/sec: 0.0867504\n",
      "I1117 11:06:02.925128 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867504\n",
      "INFO:tensorflow:examples/sec: 2.77601\n",
      "I1117 11:06:02.925360 4595733952 tpu_estimator.py:2160] examples/sec: 2.77601\n",
      "INFO:tensorflow:global_step/sec: 0.0865814\n",
      "I1117 11:06:14.474948 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865814\n",
      "INFO:tensorflow:examples/sec: 2.7706\n",
      "I1117 11:06:14.475158 4595733952 tpu_estimator.py:2160] examples/sec: 2.7706\n",
      "INFO:tensorflow:global_step/sec: 0.0878726\n",
      "I1117 11:06:25.855065 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878726\n",
      "INFO:tensorflow:examples/sec: 2.81192\n",
      "I1117 11:06:25.856406 4595733952 tpu_estimator.py:2160] examples/sec: 2.81192\n",
      "INFO:tensorflow:global_step/sec: 0.0871275\n",
      "I1117 11:06:37.332493 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871275\n",
      "INFO:tensorflow:examples/sec: 2.78808\n",
      "I1117 11:06:37.332715 4595733952 tpu_estimator.py:2160] examples/sec: 2.78808\n",
      "INFO:tensorflow:global_step/sec: 0.0857878\n",
      "I1117 11:06:48.989144 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857878\n",
      "INFO:tensorflow:examples/sec: 2.74521\n",
      "I1117 11:06:48.989350 4595733952 tpu_estimator.py:2160] examples/sec: 2.74521\n",
      "INFO:tensorflow:global_step/sec: 0.078859\n",
      "I1117 11:07:01.670027 4595733952 tpu_estimator.py:2159] global_step/sec: 0.078859\n",
      "INFO:tensorflow:examples/sec: 2.52349\n",
      "I1117 11:07:01.670258 4595733952 tpu_estimator.py:2160] examples/sec: 2.52349\n",
      "INFO:tensorflow:global_step/sec: 0.0844183\n",
      "I1117 11:07:13.515799 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844183\n",
      "INFO:tensorflow:examples/sec: 2.70139\n",
      "I1117 11:07:13.516032 4595733952 tpu_estimator.py:2160] examples/sec: 2.70139\n",
      "INFO:tensorflow:global_step/sec: 0.0839207\n",
      "I1117 11:07:25.431838 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839207\n",
      "INFO:tensorflow:examples/sec: 2.68546\n",
      "I1117 11:07:25.432057 4595733952 tpu_estimator.py:2160] examples/sec: 2.68546\n",
      "INFO:tensorflow:global_step/sec: 0.0809628\n",
      "I1117 11:07:37.783133 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0809628\n",
      "INFO:tensorflow:examples/sec: 2.59081\n",
      "I1117 11:07:37.783493 4595733952 tpu_estimator.py:2160] examples/sec: 2.59081\n",
      "INFO:tensorflow:global_step/sec: 0.0850113\n",
      "I1117 11:07:49.546348 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0850113\n",
      "INFO:tensorflow:examples/sec: 2.72036\n",
      "I1117 11:07:49.546585 4595733952 tpu_estimator.py:2160] examples/sec: 2.72036\n",
      "INFO:tensorflow:global_step/sec: 0.0883627\n",
      "I1117 11:08:00.863299 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883627\n",
      "INFO:tensorflow:examples/sec: 2.82761\n",
      "I1117 11:08:00.863528 4595733952 tpu_estimator.py:2160] examples/sec: 2.82761\n",
      "INFO:tensorflow:global_step/sec: 0.0890559\n",
      "I1117 11:08:12.092190 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890559\n",
      "INFO:tensorflow:examples/sec: 2.84979\n",
      "I1117 11:08:12.092395 4595733952 tpu_estimator.py:2160] examples/sec: 2.84979\n",
      "INFO:tensorflow:global_step/sec: 0.0881587\n",
      "I1117 11:08:23.435404 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881587\n",
      "INFO:tensorflow:examples/sec: 2.82108\n",
      "I1117 11:08:23.435811 4595733952 tpu_estimator.py:2160] examples/sec: 2.82108\n",
      "INFO:tensorflow:global_step/sec: 0.0857023\n",
      "I1117 11:08:35.103720 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857023\n",
      "INFO:tensorflow:examples/sec: 2.74247\n",
      "I1117 11:08:35.105129 4595733952 tpu_estimator.py:2160] examples/sec: 2.74247\n",
      "INFO:tensorflow:global_step/sec: 0.0896916\n",
      "I1117 11:08:46.253040 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896916\n",
      "INFO:tensorflow:examples/sec: 2.87013\n",
      "I1117 11:08:46.253271 4595733952 tpu_estimator.py:2160] examples/sec: 2.87013\n",
      "INFO:tensorflow:global_step/sec: 0.0911165\n",
      "I1117 11:08:57.227967 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911165\n",
      "INFO:tensorflow:examples/sec: 2.91573\n",
      "I1117 11:08:57.228193 4595733952 tpu_estimator.py:2160] examples/sec: 2.91573\n",
      "INFO:tensorflow:global_step/sec: 0.0914058\n",
      "I1117 11:09:08.168173 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0914058\n",
      "INFO:tensorflow:examples/sec: 2.92499\n",
      "I1117 11:09:08.168367 4595733952 tpu_estimator.py:2160] examples/sec: 2.92499\n",
      "INFO:tensorflow:global_step/sec: 0.0900004\n",
      "I1117 11:09:19.279247 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900004\n",
      "INFO:tensorflow:examples/sec: 2.88001\n",
      "I1117 11:09:19.279483 4595733952 tpu_estimator.py:2160] examples/sec: 2.88001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.00315799\n",
      "I1117 11:14:35.936250 4595733952 tpu_estimator.py:2159] global_step/sec: 0.00315799\n",
      "INFO:tensorflow:examples/sec: 0.101056\n",
      "I1117 11:14:35.937087 4595733952 tpu_estimator.py:2160] examples/sec: 0.101056\n",
      "INFO:tensorflow:global_step/sec: 0.0638944\n",
      "I1117 11:14:51.587126 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0638944\n",
      "INFO:tensorflow:examples/sec: 2.04462\n",
      "I1117 11:14:51.587372 4595733952 tpu_estimator.py:2160] examples/sec: 2.04462\n",
      "INFO:tensorflow:global_step/sec: 0.0862425\n",
      "I1117 11:15:03.183120 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862425\n",
      "INFO:tensorflow:examples/sec: 2.75976\n",
      "I1117 11:15:03.185297 4595733952 tpu_estimator.py:2160] examples/sec: 2.75976\n",
      "INFO:tensorflow:global_step/sec: 0.0894495\n",
      "I1117 11:15:14.361838 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894495\n",
      "INFO:tensorflow:examples/sec: 2.86238\n",
      "I1117 11:15:14.362040 4595733952 tpu_estimator.py:2160] examples/sec: 2.86238\n",
      "INFO:tensorflow:global_step/sec: 0.0869176\n",
      "I1117 11:15:25.866979 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869176\n",
      "INFO:tensorflow:examples/sec: 2.78136\n",
      "I1117 11:15:25.867197 4595733952 tpu_estimator.py:2160] examples/sec: 2.78136\n",
      "INFO:tensorflow:global_step/sec: 0.085619\n",
      "I1117 11:15:37.546613 4595733952 tpu_estimator.py:2159] global_step/sec: 0.085619\n",
      "INFO:tensorflow:examples/sec: 2.73981\n",
      "I1117 11:15:37.546830 4595733952 tpu_estimator.py:2160] examples/sec: 2.73981\n",
      "INFO:tensorflow:global_step/sec: 0.0865228\n",
      "I1117 11:15:49.104259 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865228\n",
      "INFO:tensorflow:examples/sec: 2.76873\n",
      "I1117 11:15:49.104472 4595733952 tpu_estimator.py:2160] examples/sec: 2.76873\n",
      "INFO:tensorflow:global_step/sec: 0.0788184\n",
      "I1117 11:16:01.791626 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0788184\n",
      "INFO:tensorflow:examples/sec: 2.52219\n",
      "I1117 11:16:01.791823 4595733952 tpu_estimator.py:2160] examples/sec: 2.52219\n",
      "INFO:tensorflow:global_step/sec: 0.0866728\n",
      "I1117 11:16:13.329279 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866728\n",
      "INFO:tensorflow:examples/sec: 2.77353\n",
      "I1117 11:16:13.329472 4595733952 tpu_estimator.py:2160] examples/sec: 2.77353\n",
      "INFO:tensorflow:global_step/sec: 0.0874802\n",
      "I1117 11:16:24.760453 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874802\n",
      "INFO:tensorflow:examples/sec: 2.79937\n",
      "I1117 11:16:24.760666 4595733952 tpu_estimator.py:2160] examples/sec: 2.79937\n",
      "INFO:tensorflow:global_step/sec: 0.0858829\n",
      "I1117 11:16:36.404197 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0858829\n",
      "INFO:tensorflow:examples/sec: 2.74825\n",
      "I1117 11:16:36.404378 4595733952 tpu_estimator.py:2160] examples/sec: 2.74825\n",
      "INFO:tensorflow:global_step/sec: 0.086781\n",
      "I1117 11:16:47.927478 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086781\n",
      "INFO:tensorflow:examples/sec: 2.77699\n",
      "I1117 11:16:47.927679 4595733952 tpu_estimator.py:2160] examples/sec: 2.77699\n",
      "INFO:tensorflow:global_step/sec: 0.0860201\n",
      "I1117 11:16:59.552668 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0860201\n",
      "INFO:tensorflow:examples/sec: 2.75264\n",
      "I1117 11:16:59.552878 4595733952 tpu_estimator.py:2160] examples/sec: 2.75264\n",
      "INFO:tensorflow:global_step/sec: 0.0887638\n",
      "I1117 11:17:10.818490 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887638\n",
      "INFO:tensorflow:examples/sec: 2.84044\n",
      "I1117 11:17:10.818866 4595733952 tpu_estimator.py:2160] examples/sec: 2.84044\n",
      "INFO:tensorflow:global_step/sec: 0.0790023\n",
      "I1117 11:17:23.477079 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0790023\n",
      "INFO:tensorflow:examples/sec: 2.52807\n",
      "I1117 11:17:23.477447 4595733952 tpu_estimator.py:2160] examples/sec: 2.52807\n",
      "INFO:tensorflow:global_step/sec: 0.0860097\n",
      "I1117 11:17:35.102962 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0860097\n",
      "INFO:tensorflow:examples/sec: 2.75231\n",
      "I1117 11:17:35.103165 4595733952 tpu_estimator.py:2160] examples/sec: 2.75231\n",
      "INFO:tensorflow:global_step/sec: 0.0867824\n",
      "I1117 11:17:46.626055 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867824\n",
      "INFO:tensorflow:examples/sec: 2.77704\n",
      "I1117 11:17:46.626266 4595733952 tpu_estimator.py:2160] examples/sec: 2.77704\n",
      "INFO:tensorflow:global_step/sec: 0.0860193\n",
      "I1117 11:17:58.251330 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0860193\n",
      "INFO:tensorflow:examples/sec: 2.75262\n",
      "I1117 11:17:58.251529 4595733952 tpu_estimator.py:2160] examples/sec: 2.75262\n",
      "INFO:tensorflow:global_step/sec: 0.0899045\n",
      "I1117 11:18:09.374262 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899045\n",
      "INFO:tensorflow:examples/sec: 2.87694\n",
      "I1117 11:18:09.374633 4595733952 tpu_estimator.py:2160] examples/sec: 2.87694\n",
      "INFO:tensorflow:global_step/sec: 0.0842733\n",
      "I1117 11:18:21.240443 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842733\n",
      "INFO:tensorflow:examples/sec: 2.69674\n",
      "I1117 11:18:21.240627 4595733952 tpu_estimator.py:2160] examples/sec: 2.69674\n",
      "INFO:tensorflow:global_step/sec: 0.0819958\n",
      "I1117 11:18:33.436154 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0819958\n",
      "INFO:tensorflow:examples/sec: 2.62387\n",
      "I1117 11:18:33.436356 4595733952 tpu_estimator.py:2160] examples/sec: 2.62387\n",
      "INFO:tensorflow:global_step/sec: 0.0863625\n",
      "I1117 11:18:45.015229 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863625\n",
      "INFO:tensorflow:examples/sec: 2.7636\n",
      "I1117 11:18:45.015406 4595733952 tpu_estimator.py:2160] examples/sec: 2.7636\n",
      "INFO:tensorflow:global_step/sec: 0.0842892\n",
      "I1117 11:18:56.879162 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842892\n",
      "INFO:tensorflow:examples/sec: 2.69726\n",
      "I1117 11:18:56.879369 4595733952 tpu_estimator.py:2160] examples/sec: 2.69726\n",
      "INFO:tensorflow:global_step/sec: 0.0851632\n",
      "I1117 11:19:08.621332 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0851632\n",
      "INFO:tensorflow:examples/sec: 2.72522\n",
      "I1117 11:19:08.621554 4595733952 tpu_estimator.py:2160] examples/sec: 2.72522\n",
      "INFO:tensorflow:global_step/sec: 0.0879709\n",
      "I1117 11:19:19.988724 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879709\n",
      "INFO:tensorflow:examples/sec: 2.81507\n",
      "I1117 11:19:19.988950 4595733952 tpu_estimator.py:2160] examples/sec: 2.81507\n",
      "INFO:tensorflow:global_step/sec: 0.0861724\n",
      "I1117 11:19:31.593370 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861724\n",
      "INFO:tensorflow:examples/sec: 2.75752\n",
      "I1117 11:19:31.593582 4595733952 tpu_estimator.py:2160] examples/sec: 2.75752\n",
      "INFO:tensorflow:global_step/sec: 0.0859316\n",
      "I1117 11:19:43.230530 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859316\n",
      "INFO:tensorflow:examples/sec: 2.74981\n",
      "I1117 11:19:43.230712 4595733952 tpu_estimator.py:2160] examples/sec: 2.74981\n",
      "INFO:tensorflow:global_step/sec: 0.0865797\n",
      "I1117 11:19:54.780562 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865797\n",
      "INFO:tensorflow:examples/sec: 2.77055\n",
      "I1117 11:19:54.780786 4595733952 tpu_estimator.py:2160] examples/sec: 2.77055\n",
      "INFO:tensorflow:global_step/sec: 0.0791537\n",
      "I1117 11:20:07.414245 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0791537\n",
      "INFO:tensorflow:examples/sec: 2.53292\n",
      "I1117 11:20:07.414499 4595733952 tpu_estimator.py:2160] examples/sec: 2.53292\n",
      "INFO:tensorflow:global_step/sec: 0.0822688\n",
      "I1117 11:20:19.569477 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0822688\n",
      "INFO:tensorflow:examples/sec: 2.6326\n",
      "I1117 11:20:19.569803 4595733952 tpu_estimator.py:2160] examples/sec: 2.6326\n",
      "INFO:tensorflow:global_step/sec: 0.0622093\n",
      "I1117 11:20:35.644232 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0622093\n",
      "INFO:tensorflow:examples/sec: 1.9907\n",
      "I1117 11:20:35.644404 4595733952 tpu_estimator.py:2160] examples/sec: 1.9907\n",
      "INFO:tensorflow:global_step/sec: 0.0717876\n",
      "I1117 11:20:49.574263 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0717876\n",
      "INFO:tensorflow:examples/sec: 2.2972\n",
      "I1117 11:20:49.575757 4595733952 tpu_estimator.py:2160] examples/sec: 2.2972\n",
      "INFO:tensorflow:global_step/sec: 0.0646307\n",
      "I1117 11:21:05.046765 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0646307\n",
      "INFO:tensorflow:examples/sec: 2.06818\n",
      "I1117 11:21:05.046947 4595733952 tpu_estimator.py:2160] examples/sec: 2.06818\n",
      "INFO:tensorflow:global_step/sec: 0.0560487\n",
      "I1117 11:21:22.888569 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0560487\n",
      "INFO:tensorflow:examples/sec: 1.79356\n",
      "I1117 11:21:22.889144 4595733952 tpu_estimator.py:2160] examples/sec: 1.79356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0664227\n",
      "I1117 11:21:37.943507 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0664227\n",
      "INFO:tensorflow:examples/sec: 2.12553\n",
      "I1117 11:21:37.943717 4595733952 tpu_estimator.py:2160] examples/sec: 2.12553\n",
      "INFO:tensorflow:global_step/sec: 0.068989\n",
      "I1117 11:21:52.438574 4595733952 tpu_estimator.py:2159] global_step/sec: 0.068989\n",
      "INFO:tensorflow:examples/sec: 2.20765\n",
      "I1117 11:21:52.438799 4595733952 tpu_estimator.py:2160] examples/sec: 2.20765\n",
      "INFO:tensorflow:global_step/sec: 0.0689257\n",
      "I1117 11:22:06.946937 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0689257\n",
      "INFO:tensorflow:examples/sec: 2.20562\n",
      "I1117 11:22:06.947131 4595733952 tpu_estimator.py:2160] examples/sec: 2.20562\n",
      "INFO:tensorflow:global_step/sec: 0.0574751\n",
      "I1117 11:22:24.345772 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0574751\n",
      "INFO:tensorflow:examples/sec: 1.8392\n",
      "I1117 11:22:24.345962 4595733952 tpu_estimator.py:2160] examples/sec: 1.8392\n",
      "INFO:tensorflow:global_step/sec: 0.062151\n",
      "I1117 11:22:40.435652 4595733952 tpu_estimator.py:2159] global_step/sec: 0.062151\n",
      "INFO:tensorflow:examples/sec: 1.98883\n",
      "I1117 11:22:40.435855 4595733952 tpu_estimator.py:2160] examples/sec: 1.98883\n",
      "INFO:tensorflow:global_step/sec: 0.0653339\n",
      "I1117 11:22:55.741636 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0653339\n",
      "INFO:tensorflow:examples/sec: 2.09069\n",
      "I1117 11:22:55.741894 4595733952 tpu_estimator.py:2160] examples/sec: 2.09069\n",
      "INFO:tensorflow:global_step/sec: 0.0665658\n",
      "I1117 11:23:10.764369 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0665658\n",
      "INFO:tensorflow:examples/sec: 2.1301\n",
      "I1117 11:23:10.764575 4595733952 tpu_estimator.py:2160] examples/sec: 2.1301\n",
      "INFO:tensorflow:global_step/sec: 0.0515358\n",
      "I1117 11:23:30.168357 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0515358\n",
      "INFO:tensorflow:examples/sec: 1.64915\n",
      "I1117 11:23:30.168572 4595733952 tpu_estimator.py:2160] examples/sec: 1.64915\n",
      "INFO:tensorflow:global_step/sec: 0.0468948\n",
      "I1117 11:23:51.492695 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0468948\n",
      "INFO:tensorflow:examples/sec: 1.50063\n",
      "I1117 11:23:51.492923 4595733952 tpu_estimator.py:2160] examples/sec: 1.50063\n",
      "INFO:tensorflow:global_step/sec: 0.0506103\n",
      "I1117 11:24:11.251536 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0506103\n",
      "INFO:tensorflow:examples/sec: 1.61953\n",
      "I1117 11:24:11.251754 4595733952 tpu_estimator.py:2160] examples/sec: 1.61953\n",
      "INFO:tensorflow:global_step/sec: 0.0547412\n",
      "I1117 11:24:29.519370 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0547412\n",
      "INFO:tensorflow:examples/sec: 1.75172\n",
      "I1117 11:24:29.519853 4595733952 tpu_estimator.py:2160] examples/sec: 1.75172\n",
      "INFO:tensorflow:global_step/sec: 0.0551365\n",
      "I1117 11:24:47.656122 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0551365\n",
      "INFO:tensorflow:examples/sec: 1.76437\n",
      "I1117 11:24:47.656542 4595733952 tpu_estimator.py:2160] examples/sec: 1.76437\n",
      "INFO:tensorflow:global_step/sec: 0.0410082\n",
      "I1117 11:25:12.041550 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0410082\n",
      "INFO:tensorflow:examples/sec: 1.31226\n",
      "I1117 11:25:12.041803 4595733952 tpu_estimator.py:2160] examples/sec: 1.31226\n",
      "INFO:tensorflow:global_step/sec: 0.0474824\n",
      "I1117 11:25:33.101872 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0474824\n",
      "INFO:tensorflow:examples/sec: 1.51944\n",
      "I1117 11:25:33.102063 4595733952 tpu_estimator.py:2160] examples/sec: 1.51944\n",
      "INFO:tensorflow:global_step/sec: 0.0624551\n",
      "I1117 11:25:49.113434 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0624551\n",
      "INFO:tensorflow:examples/sec: 1.99856\n",
      "I1117 11:25:49.113631 4595733952 tpu_estimator.py:2160] examples/sec: 1.99856\n",
      "INFO:tensorflow:global_step/sec: 0.0556033\n",
      "I1117 11:26:07.097953 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0556033\n",
      "INFO:tensorflow:examples/sec: 1.77931\n",
      "I1117 11:26:07.098202 4595733952 tpu_estimator.py:2160] examples/sec: 1.77931\n",
      "INFO:tensorflow:global_step/sec: 0.0457802\n",
      "I1117 11:26:28.941485 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0457802\n",
      "INFO:tensorflow:examples/sec: 1.46497\n",
      "I1117 11:26:28.941770 4595733952 tpu_estimator.py:2160] examples/sec: 1.46497\n",
      "INFO:tensorflow:global_step/sec: 0.0410259\n",
      "I1117 11:26:53.316426 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0410259\n",
      "INFO:tensorflow:examples/sec: 1.31283\n",
      "I1117 11:26:53.316823 4595733952 tpu_estimator.py:2160] examples/sec: 1.31283\n",
      "INFO:tensorflow:global_step/sec: 0.0468106\n",
      "I1117 11:27:14.679008 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0468106\n",
      "INFO:tensorflow:examples/sec: 1.49794\n",
      "I1117 11:27:14.679238 4595733952 tpu_estimator.py:2160] examples/sec: 1.49794\n",
      "INFO:tensorflow:global_step/sec: 0.0535023\n",
      "I1117 11:27:33.369784 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0535023\n",
      "INFO:tensorflow:examples/sec: 1.71207\n",
      "I1117 11:27:33.370017 4595733952 tpu_estimator.py:2160] examples/sec: 1.71207\n",
      "INFO:tensorflow:global_step/sec: 0.0467645\n",
      "I1117 11:27:54.753546 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0467645\n",
      "INFO:tensorflow:examples/sec: 1.49646\n",
      "I1117 11:27:54.753789 4595733952 tpu_estimator.py:2160] examples/sec: 1.49646\n",
      "INFO:tensorflow:global_step/sec: 0.0433662\n",
      "I1117 11:28:17.813055 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0433662\n",
      "INFO:tensorflow:examples/sec: 1.38772\n",
      "I1117 11:28:17.813544 4595733952 tpu_estimator.py:2160] examples/sec: 1.38772\n",
      "INFO:tensorflow:global_step/sec: 0.0420259\n",
      "I1117 11:28:41.607908 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0420259\n",
      "INFO:tensorflow:examples/sec: 1.34483\n",
      "I1117 11:28:41.608184 4595733952 tpu_estimator.py:2160] examples/sec: 1.34483\n",
      "INFO:tensorflow:global_step/sec: 0.0414562\n",
      "I1117 11:29:05.729773 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0414562\n",
      "INFO:tensorflow:examples/sec: 1.3266\n",
      "I1117 11:29:05.730429 4595733952 tpu_estimator.py:2160] examples/sec: 1.3266\n",
      "INFO:tensorflow:global_step/sec: 0.0422194\n",
      "I1117 11:29:29.415518 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0422194\n",
      "INFO:tensorflow:examples/sec: 1.35102\n",
      "I1117 11:29:29.415814 4595733952 tpu_estimator.py:2160] examples/sec: 1.35102\n",
      "INFO:tensorflow:global_step/sec: 0.0440886\n",
      "I1117 11:29:52.097107 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0440886\n",
      "INFO:tensorflow:examples/sec: 1.41084\n",
      "I1117 11:29:52.097797 4595733952 tpu_estimator.py:2160] examples/sec: 1.41084\n",
      "INFO:tensorflow:global_step/sec: 0.0413996\n",
      "I1117 11:30:16.251924 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0413996\n",
      "INFO:tensorflow:examples/sec: 1.32479\n",
      "I1117 11:30:16.252212 4595733952 tpu_estimator.py:2160] examples/sec: 1.32479\n",
      "INFO:tensorflow:global_step/sec: 0.0455113\n",
      "I1117 11:30:38.224544 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0455113\n",
      "INFO:tensorflow:examples/sec: 1.45636\n",
      "I1117 11:30:38.224832 4595733952 tpu_estimator.py:2160] examples/sec: 1.45636\n",
      "INFO:tensorflow:global_step/sec: 0.0413333\n",
      "I1117 11:31:02.418064 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0413333\n",
      "INFO:tensorflow:examples/sec: 1.32267\n",
      "I1117 11:31:02.418606 4595733952 tpu_estimator.py:2160] examples/sec: 1.32267\n",
      "INFO:tensorflow:global_step/sec: 0.0416762\n",
      "I1117 11:31:26.412595 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0416762\n",
      "INFO:tensorflow:examples/sec: 1.33364\n",
      "I1117 11:31:26.412888 4595733952 tpu_estimator.py:2160] examples/sec: 1.33364\n",
      "INFO:tensorflow:global_step/sec: 0.0400352\n",
      "I1117 11:31:51.390603 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0400352\n",
      "INFO:tensorflow:examples/sec: 1.28113\n",
      "I1117 11:31:51.390991 4595733952 tpu_estimator.py:2160] examples/sec: 1.28113\n",
      "INFO:tensorflow:global_step/sec: 0.0463229\n",
      "I1117 11:32:12.978181 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0463229\n",
      "INFO:tensorflow:examples/sec: 1.48233\n",
      "I1117 11:32:12.978466 4595733952 tpu_estimator.py:2160] examples/sec: 1.48233\n",
      "INFO:tensorflow:global_step/sec: 0.0418598\n",
      "I1117 11:32:36.867424 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0418598\n",
      "INFO:tensorflow:examples/sec: 1.33951\n",
      "I1117 11:32:36.867934 4595733952 tpu_estimator.py:2160] examples/sec: 1.33951\n",
      "INFO:tensorflow:global_step/sec: 0.0467893\n",
      "I1117 11:32:58.239834 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0467893\n",
      "INFO:tensorflow:examples/sec: 1.49726\n",
      "I1117 11:32:58.240107 4595733952 tpu_estimator.py:2160] examples/sec: 1.49726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0414407\n",
      "I1117 11:33:22.370715 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0414407\n",
      "INFO:tensorflow:examples/sec: 1.3261\n",
      "I1117 11:33:22.371028 4595733952 tpu_estimator.py:2160] examples/sec: 1.3261\n",
      "INFO:tensorflow:global_step/sec: 0.0460943\n",
      "I1117 11:33:44.065341 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0460943\n",
      "INFO:tensorflow:examples/sec: 1.47502\n",
      "I1117 11:33:44.065588 4595733952 tpu_estimator.py:2160] examples/sec: 1.47502\n",
      "INFO:tensorflow:global_step/sec: 0.0427184\n",
      "I1117 11:34:07.474469 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0427184\n",
      "INFO:tensorflow:examples/sec: 1.36699\n",
      "I1117 11:34:07.474778 4595733952 tpu_estimator.py:2160] examples/sec: 1.36699\n",
      "INFO:tensorflow:global_step/sec: 0.0390444\n",
      "I1117 11:34:33.086341 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0390444\n",
      "INFO:tensorflow:examples/sec: 1.24942\n",
      "I1117 11:34:33.086606 4595733952 tpu_estimator.py:2160] examples/sec: 1.24942\n",
      "INFO:tensorflow:global_step/sec: 0.0445926\n",
      "I1117 11:34:55.511602 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0445926\n",
      "INFO:tensorflow:examples/sec: 1.42696\n",
      "I1117 11:34:55.511909 4595733952 tpu_estimator.py:2160] examples/sec: 1.42696\n",
      "INFO:tensorflow:global_step/sec: 0.042352\n",
      "I1117 11:35:19.123404 4595733952 tpu_estimator.py:2159] global_step/sec: 0.042352\n",
      "INFO:tensorflow:examples/sec: 1.35526\n",
      "I1117 11:35:19.125180 4595733952 tpu_estimator.py:2160] examples/sec: 1.35526\n",
      "INFO:tensorflow:global_step/sec: 0.0410689\n",
      "I1117 11:35:43.472611 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0410689\n",
      "INFO:tensorflow:examples/sec: 1.3142\n",
      "I1117 11:35:43.472910 4595733952 tpu_estimator.py:2160] examples/sec: 1.3142\n",
      "INFO:tensorflow:global_step/sec: 0.0416663\n",
      "I1117 11:36:07.472772 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0416663\n",
      "INFO:tensorflow:examples/sec: 1.33332\n",
      "I1117 11:36:07.473011 4595733952 tpu_estimator.py:2160] examples/sec: 1.33332\n",
      "INFO:tensorflow:global_step/sec: 0.0683216\n",
      "I1117 11:36:22.109393 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0683216\n",
      "INFO:tensorflow:examples/sec: 2.18629\n",
      "I1117 11:36:22.109591 4595733952 tpu_estimator.py:2160] examples/sec: 2.18629\n",
      "INFO:tensorflow:global_step/sec: 0.0704355\n",
      "I1117 11:36:36.306787 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0704355\n",
      "INFO:tensorflow:examples/sec: 2.25394\n",
      "I1117 11:36:36.307000 4595733952 tpu_estimator.py:2160] examples/sec: 2.25394\n",
      "INFO:tensorflow:global_step/sec: 0.0607513\n",
      "I1117 11:36:52.767398 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0607513\n",
      "INFO:tensorflow:examples/sec: 1.94404\n",
      "I1117 11:36:52.767664 4595733952 tpu_estimator.py:2160] examples/sec: 1.94404\n",
      "INFO:tensorflow:global_step/sec: 0.000158549\n",
      "I1117 13:21:59.968907 4595733952 tpu_estimator.py:2159] global_step/sec: 0.000158549\n",
      "INFO:tensorflow:examples/sec: 0.00507357\n",
      "I1117 13:21:59.969070 4595733952 tpu_estimator.py:2160] examples/sec: 0.00507357\n",
      "INFO:tensorflow:global_step/sec: 0.0495925\n",
      "I1117 13:22:20.133505 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0495925\n",
      "INFO:tensorflow:examples/sec: 1.58696\n",
      "I1117 13:22:20.133916 4595733952 tpu_estimator.py:2160] examples/sec: 1.58696\n",
      "INFO:tensorflow:global_step/sec: 0.0189903\n",
      "I1117 13:23:12.791862 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0189903\n",
      "INFO:tensorflow:examples/sec: 0.607689\n",
      "I1117 13:23:12.792080 4595733952 tpu_estimator.py:2160] examples/sec: 0.607689\n",
      "INFO:tensorflow:global_step/sec: 0.0182435\n",
      "I1117 13:24:07.606106 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0182435\n",
      "INFO:tensorflow:examples/sec: 0.583791\n",
      "I1117 13:24:07.606681 4595733952 tpu_estimator.py:2160] examples/sec: 0.583791\n",
      "INFO:tensorflow:global_step/sec: 0.065124\n",
      "I1117 13:24:22.961254 4595733952 tpu_estimator.py:2159] global_step/sec: 0.065124\n",
      "INFO:tensorflow:examples/sec: 2.08397\n",
      "I1117 13:24:22.961407 4595733952 tpu_estimator.py:2160] examples/sec: 2.08397\n",
      "INFO:tensorflow:global_step/sec: 0.0861001\n",
      "I1117 13:24:34.575644 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861001\n",
      "INFO:tensorflow:examples/sec: 2.7552\n",
      "I1117 13:24:34.577611 4595733952 tpu_estimator.py:2160] examples/sec: 2.7552\n",
      "INFO:tensorflow:global_step/sec: 0.0886778\n",
      "I1117 13:24:45.852505 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886778\n",
      "INFO:tensorflow:examples/sec: 2.83769\n",
      "I1117 13:24:45.854095 4595733952 tpu_estimator.py:2160] examples/sec: 2.83769\n",
      "INFO:tensorflow:global_step/sec: 0.0887414\n",
      "I1117 13:24:57.121186 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887414\n",
      "INFO:tensorflow:examples/sec: 2.83972\n",
      "I1117 13:24:57.121428 4595733952 tpu_estimator.py:2160] examples/sec: 2.83972\n",
      "INFO:tensorflow:global_step/sec: 0.0890222\n",
      "I1117 13:25:08.354361 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890222\n",
      "INFO:tensorflow:examples/sec: 2.84871\n",
      "I1117 13:25:08.354575 4595733952 tpu_estimator.py:2160] examples/sec: 2.84871\n",
      "INFO:tensorflow:global_step/sec: 0.0908913\n",
      "I1117 13:25:19.356485 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908913\n",
      "INFO:tensorflow:examples/sec: 2.90852\n",
      "I1117 13:25:19.356707 4595733952 tpu_estimator.py:2160] examples/sec: 2.90852\n",
      "INFO:tensorflow:global_step/sec: 0.0908322\n",
      "I1117 13:25:30.365793 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908322\n",
      "INFO:tensorflow:examples/sec: 2.90663\n",
      "I1117 13:25:30.366011 4595733952 tpu_estimator.py:2160] examples/sec: 2.90663\n",
      "INFO:tensorflow:global_step/sec: 0.0910828\n",
      "I1117 13:25:41.344823 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910828\n",
      "INFO:tensorflow:examples/sec: 2.91465\n",
      "I1117 13:25:41.345050 4595733952 tpu_estimator.py:2160] examples/sec: 2.91465\n",
      "INFO:tensorflow:global_step/sec: 0.0908687\n",
      "I1117 13:25:52.349700 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908687\n",
      "INFO:tensorflow:examples/sec: 2.9078\n",
      "I1117 13:25:52.351236 4595733952 tpu_estimator.py:2160] examples/sec: 2.9078\n",
      "INFO:tensorflow:global_step/sec: 0.0906794\n",
      "I1117 13:26:03.377560 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906794\n",
      "INFO:tensorflow:examples/sec: 2.90174\n",
      "I1117 13:26:03.377772 4595733952 tpu_estimator.py:2160] examples/sec: 2.90174\n",
      "INFO:tensorflow:global_step/sec: 0.0906436\n",
      "I1117 13:26:14.409784 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906436\n",
      "INFO:tensorflow:examples/sec: 2.90059\n",
      "I1117 13:26:14.409992 4595733952 tpu_estimator.py:2160] examples/sec: 2.90059\n",
      "INFO:tensorflow:global_step/sec: 0.0906741\n",
      "I1117 13:26:25.438292 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906741\n",
      "INFO:tensorflow:examples/sec: 2.90157\n",
      "I1117 13:26:25.438511 4595733952 tpu_estimator.py:2160] examples/sec: 2.90157\n",
      "INFO:tensorflow:global_step/sec: 0.0903297\n",
      "I1117 13:26:36.508859 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903297\n",
      "INFO:tensorflow:examples/sec: 2.89055\n",
      "I1117 13:26:36.511290 4595733952 tpu_estimator.py:2160] examples/sec: 2.89055\n",
      "INFO:tensorflow:global_step/sec: 0.0908799\n",
      "I1117 13:26:47.512376 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908799\n",
      "INFO:tensorflow:examples/sec: 2.90816\n",
      "I1117 13:26:47.512588 4595733952 tpu_estimator.py:2160] examples/sec: 2.90816\n",
      "INFO:tensorflow:global_step/sec: 0.090406\n",
      "I1117 13:26:58.573597 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090406\n",
      "INFO:tensorflow:examples/sec: 2.89299\n",
      "I1117 13:26:58.573961 4595733952 tpu_estimator.py:2160] examples/sec: 2.89299\n",
      "INFO:tensorflow:global_step/sec: 0.0904575\n",
      "I1117 13:27:09.628511 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904575\n",
      "INFO:tensorflow:examples/sec: 2.89464\n",
      "I1117 13:27:09.628722 4595733952 tpu_estimator.py:2160] examples/sec: 2.89464\n",
      "INFO:tensorflow:global_step/sec: 0.0896775\n",
      "I1117 13:27:20.779652 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896775\n",
      "INFO:tensorflow:examples/sec: 2.86968\n",
      "I1117 13:27:20.780045 4595733952 tpu_estimator.py:2160] examples/sec: 2.86968\n",
      "INFO:tensorflow:global_step/sec: 0.0845523\n",
      "I1117 13:27:32.606560 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0845523\n",
      "INFO:tensorflow:examples/sec: 2.70567\n",
      "I1117 13:27:32.606751 4595733952 tpu_estimator.py:2160] examples/sec: 2.70567\n",
      "INFO:tensorflow:global_step/sec: 0.0826724\n",
      "I1117 13:27:44.702490 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0826724\n",
      "INFO:tensorflow:examples/sec: 2.64552\n",
      "I1117 13:27:44.702676 4595733952 tpu_estimator.py:2160] examples/sec: 2.64552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0824702\n",
      "I1117 13:27:56.828068 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824702\n",
      "INFO:tensorflow:examples/sec: 2.63905\n",
      "I1117 13:27:56.828248 4595733952 tpu_estimator.py:2160] examples/sec: 2.63905\n",
      "INFO:tensorflow:global_step/sec: 0.0791165\n",
      "I1117 13:28:09.467694 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0791165\n",
      "INFO:tensorflow:examples/sec: 2.53173\n",
      "I1117 13:28:09.467925 4595733952 tpu_estimator.py:2160] examples/sec: 2.53173\n",
      "INFO:tensorflow:global_step/sec: 0.0819046\n",
      "I1117 13:28:21.677092 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0819046\n",
      "INFO:tensorflow:examples/sec: 2.62095\n",
      "I1117 13:28:21.677366 4595733952 tpu_estimator.py:2160] examples/sec: 2.62095\n",
      "INFO:tensorflow:global_step/sec: 0.079762\n",
      "I1117 13:28:34.214313 4595733952 tpu_estimator.py:2159] global_step/sec: 0.079762\n",
      "INFO:tensorflow:examples/sec: 2.55238\n",
      "I1117 13:28:34.214502 4595733952 tpu_estimator.py:2160] examples/sec: 2.55238\n",
      "INFO:tensorflow:global_step/sec: 0.0791922\n",
      "I1117 13:28:46.841803 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0791922\n",
      "INFO:tensorflow:examples/sec: 2.53415\n",
      "I1117 13:28:46.842002 4595733952 tpu_estimator.py:2160] examples/sec: 2.53415\n",
      "INFO:tensorflow:global_step/sec: 0.0783385\n",
      "I1117 13:28:59.606906 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0783385\n",
      "INFO:tensorflow:examples/sec: 2.50683\n",
      "I1117 13:28:59.607090 4595733952 tpu_estimator.py:2160] examples/sec: 2.50683\n",
      "INFO:tensorflow:global_step/sec: 0.0778533\n",
      "I1117 13:29:12.451588 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0778533\n",
      "INFO:tensorflow:examples/sec: 2.49131\n",
      "I1117 13:29:12.451784 4595733952 tpu_estimator.py:2160] examples/sec: 2.49131\n",
      "INFO:tensorflow:global_step/sec: 0.072845\n",
      "I1117 13:29:26.179354 4595733952 tpu_estimator.py:2159] global_step/sec: 0.072845\n",
      "INFO:tensorflow:examples/sec: 2.33104\n",
      "I1117 13:29:26.179543 4595733952 tpu_estimator.py:2160] examples/sec: 2.33104\n",
      "INFO:tensorflow:global_step/sec: 0.0708061\n",
      "I1117 13:29:40.302437 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0708061\n",
      "INFO:tensorflow:examples/sec: 2.2658\n",
      "I1117 13:29:40.302636 4595733952 tpu_estimator.py:2160] examples/sec: 2.2658\n",
      "INFO:tensorflow:global_step/sec: 0.0724829\n",
      "I1117 13:29:54.098835 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0724829\n",
      "INFO:tensorflow:examples/sec: 2.31945\n",
      "I1117 13:29:54.099205 4595733952 tpu_estimator.py:2160] examples/sec: 2.31945\n",
      "INFO:tensorflow:global_step/sec: 0.0727998\n",
      "I1117 13:30:07.835109 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0727998\n",
      "INFO:tensorflow:examples/sec: 2.32959\n",
      "I1117 13:30:07.835307 4595733952 tpu_estimator.py:2160] examples/sec: 2.32959\n",
      "INFO:tensorflow:global_step/sec: 0.0749135\n",
      "I1117 13:30:21.183833 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0749135\n",
      "INFO:tensorflow:examples/sec: 2.39723\n",
      "I1117 13:30:21.184016 4595733952 tpu_estimator.py:2160] examples/sec: 2.39723\n",
      "INFO:tensorflow:global_step/sec: 0.0740023\n",
      "I1117 13:30:34.696938 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0740023\n",
      "INFO:tensorflow:examples/sec: 2.36807\n",
      "I1117 13:30:34.697133 4595733952 tpu_estimator.py:2160] examples/sec: 2.36807\n",
      "INFO:tensorflow:global_step/sec: 0.0736726\n",
      "I1117 13:30:48.270576 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0736726\n",
      "INFO:tensorflow:examples/sec: 2.35752\n",
      "I1117 13:30:48.270781 4595733952 tpu_estimator.py:2160] examples/sec: 2.35752\n",
      "INFO:tensorflow:global_step/sec: 0.0724916\n",
      "I1117 13:31:02.065231 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0724916\n",
      "INFO:tensorflow:examples/sec: 2.31973\n",
      "I1117 13:31:02.065424 4595733952 tpu_estimator.py:2160] examples/sec: 2.31973\n",
      "INFO:tensorflow:global_step/sec: 0.0725617\n",
      "I1117 13:31:15.846574 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0725617\n",
      "INFO:tensorflow:examples/sec: 2.32198\n",
      "I1117 13:31:15.846771 4595733952 tpu_estimator.py:2160] examples/sec: 2.32198\n",
      "INFO:tensorflow:global_step/sec: 0.0722715\n",
      "I1117 13:31:29.683290 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0722715\n",
      "INFO:tensorflow:examples/sec: 2.31269\n",
      "I1117 13:31:29.683504 4595733952 tpu_estimator.py:2160] examples/sec: 2.31269\n",
      "INFO:tensorflow:global_step/sec: 0.0711136\n",
      "I1117 13:31:43.745301 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0711136\n",
      "INFO:tensorflow:examples/sec: 2.27563\n",
      "I1117 13:31:43.745487 4595733952 tpu_estimator.py:2160] examples/sec: 2.27563\n",
      "INFO:tensorflow:global_step/sec: 0.0717301\n",
      "I1117 13:31:57.686458 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0717301\n",
      "INFO:tensorflow:examples/sec: 2.29536\n",
      "I1117 13:31:57.686821 4595733952 tpu_estimator.py:2160] examples/sec: 2.29536\n",
      "INFO:tensorflow:global_step/sec: 0.0703178\n",
      "I1117 13:32:11.907610 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0703178\n",
      "INFO:tensorflow:examples/sec: 2.25017\n",
      "I1117 13:32:11.907816 4595733952 tpu_estimator.py:2160] examples/sec: 2.25017\n",
      "INFO:tensorflow:global_step/sec: 0.0705212\n",
      "I1117 13:32:26.087754 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0705212\n",
      "INFO:tensorflow:examples/sec: 2.25668\n",
      "I1117 13:32:26.087961 4595733952 tpu_estimator.py:2160] examples/sec: 2.25668\n",
      "INFO:tensorflow:global_step/sec: 0.070842\n",
      "I1117 13:32:40.203638 4595733952 tpu_estimator.py:2159] global_step/sec: 0.070842\n",
      "INFO:tensorflow:examples/sec: 2.26695\n",
      "I1117 13:32:40.203812 4595733952 tpu_estimator.py:2160] examples/sec: 2.26695\n",
      "INFO:tensorflow:global_step/sec: 0.0700701\n",
      "I1117 13:32:54.475088 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0700701\n",
      "INFO:tensorflow:examples/sec: 2.24224\n",
      "I1117 13:32:54.475335 4595733952 tpu_estimator.py:2160] examples/sec: 2.24224\n",
      "INFO:tensorflow:global_step/sec: 0.0714764\n",
      "I1117 13:33:08.465698 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0714764\n",
      "INFO:tensorflow:examples/sec: 2.28724\n",
      "I1117 13:33:08.465887 4595733952 tpu_estimator.py:2160] examples/sec: 2.28724\n",
      "INFO:tensorflow:global_step/sec: 0.0716369\n",
      "I1117 13:33:22.424979 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0716369\n",
      "INFO:tensorflow:examples/sec: 2.29238\n",
      "I1117 13:33:22.425160 4595733952 tpu_estimator.py:2160] examples/sec: 2.29238\n",
      "INFO:tensorflow:global_step/sec: 0.0608723\n",
      "I1117 13:33:38.852853 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0608723\n",
      "INFO:tensorflow:examples/sec: 1.94791\n",
      "I1117 13:33:38.853416 4595733952 tpu_estimator.py:2160] examples/sec: 1.94791\n",
      "INFO:tensorflow:global_step/sec: 0.0659918\n",
      "I1117 13:33:54.006213 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0659918\n",
      "INFO:tensorflow:examples/sec: 2.11174\n",
      "I1117 13:33:54.006411 4595733952 tpu_estimator.py:2160] examples/sec: 2.11174\n",
      "INFO:tensorflow:global_step/sec: 0.0668272\n",
      "I1117 13:34:08.970206 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0668272\n",
      "INFO:tensorflow:examples/sec: 2.13847\n",
      "I1117 13:34:08.970407 4595733952 tpu_estimator.py:2160] examples/sec: 2.13847\n",
      "INFO:tensorflow:global_step/sec: 0.0710086\n",
      "I1117 13:34:23.053003 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0710086\n",
      "INFO:tensorflow:examples/sec: 2.27227\n",
      "I1117 13:34:23.053215 4595733952 tpu_estimator.py:2160] examples/sec: 2.27227\n",
      "INFO:tensorflow:global_step/sec: 0.0703674\n",
      "I1117 13:34:37.264112 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0703674\n",
      "INFO:tensorflow:examples/sec: 2.25176\n",
      "I1117 13:34:37.264442 4595733952 tpu_estimator.py:2160] examples/sec: 2.25176\n",
      "INFO:tensorflow:global_step/sec: 0.0711998\n",
      "I1117 13:34:51.309125 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0711998\n",
      "INFO:tensorflow:examples/sec: 2.27839\n",
      "I1117 13:34:51.309324 4595733952 tpu_estimator.py:2160] examples/sec: 2.27839\n",
      "INFO:tensorflow:global_step/sec: 0.0720837\n",
      "I1117 13:35:05.181902 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0720837\n",
      "INFO:tensorflow:examples/sec: 2.30668\n",
      "I1117 13:35:05.182145 4595733952 tpu_estimator.py:2160] examples/sec: 2.30668\n",
      "INFO:tensorflow:global_step/sec: 0.0703125\n",
      "I1117 13:35:19.404117 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0703125\n",
      "INFO:tensorflow:examples/sec: 2.25\n",
      "I1117 13:35:19.404302 4595733952 tpu_estimator.py:2160] examples/sec: 2.25\n",
      "INFO:tensorflow:global_step/sec: 0.0690347\n",
      "I1117 13:35:33.889589 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0690347\n",
      "INFO:tensorflow:examples/sec: 2.20911\n",
      "I1117 13:35:33.890004 4595733952 tpu_estimator.py:2160] examples/sec: 2.20911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0734733\n",
      "I1117 13:35:47.500028 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0734733\n",
      "INFO:tensorflow:examples/sec: 2.35114\n",
      "I1117 13:35:47.500249 4595733952 tpu_estimator.py:2160] examples/sec: 2.35114\n",
      "INFO:tensorflow:global_step/sec: 0.073337\n",
      "I1117 13:36:01.135654 4595733952 tpu_estimator.py:2159] global_step/sec: 0.073337\n",
      "INFO:tensorflow:examples/sec: 2.34678\n",
      "I1117 13:36:01.135846 4595733952 tpu_estimator.py:2160] examples/sec: 2.34678\n",
      "INFO:tensorflow:global_step/sec: 0.0684888\n",
      "I1117 13:36:15.736581 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0684888\n",
      "INFO:tensorflow:examples/sec: 2.19164\n",
      "I1117 13:36:15.736781 4595733952 tpu_estimator.py:2160] examples/sec: 2.19164\n",
      "INFO:tensorflow:global_step/sec: 0.0761104\n",
      "I1117 13:36:28.875482 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0761104\n",
      "INFO:tensorflow:examples/sec: 2.43553\n",
      "I1117 13:36:28.875692 4595733952 tpu_estimator.py:2160] examples/sec: 2.43553\n",
      "INFO:tensorflow:global_step/sec: 0.0830283\n",
      "I1117 13:36:40.919508 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0830283\n",
      "INFO:tensorflow:examples/sec: 2.65691\n",
      "I1117 13:36:40.919889 4595733952 tpu_estimator.py:2160] examples/sec: 2.65691\n",
      "INFO:tensorflow:global_step/sec: 0.0858545\n",
      "I1117 13:36:52.567088 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0858545\n",
      "INFO:tensorflow:examples/sec: 2.74735\n",
      "I1117 13:36:52.567290 4595733952 tpu_estimator.py:2160] examples/sec: 2.74735\n",
      "INFO:tensorflow:global_step/sec: 0.0875407\n",
      "I1117 13:37:03.990353 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875407\n",
      "INFO:tensorflow:examples/sec: 2.8013\n",
      "I1117 13:37:03.990561 4595733952 tpu_estimator.py:2160] examples/sec: 2.8013\n",
      "INFO:tensorflow:global_step/sec: 0.0882113\n",
      "I1117 13:37:15.326766 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882113\n",
      "INFO:tensorflow:examples/sec: 2.82276\n",
      "I1117 13:37:15.326972 4595733952 tpu_estimator.py:2160] examples/sec: 2.82276\n",
      "INFO:tensorflow:global_step/sec: 0.0879148\n",
      "I1117 13:37:26.701421 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879148\n",
      "INFO:tensorflow:examples/sec: 2.81327\n",
      "I1117 13:37:26.701627 4595733952 tpu_estimator.py:2160] examples/sec: 2.81327\n",
      "INFO:tensorflow:global_step/sec: 0.088185\n",
      "I1117 13:37:38.041234 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088185\n",
      "INFO:tensorflow:examples/sec: 2.82192\n",
      "I1117 13:37:38.041461 4595733952 tpu_estimator.py:2160] examples/sec: 2.82192\n",
      "INFO:tensorflow:global_step/sec: 0.0881003\n",
      "I1117 13:37:49.391932 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881003\n",
      "INFO:tensorflow:examples/sec: 2.81921\n",
      "I1117 13:37:49.392183 4595733952 tpu_estimator.py:2160] examples/sec: 2.81921\n",
      "INFO:tensorflow:global_step/sec: 0.0879799\n",
      "I1117 13:38:00.758167 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879799\n",
      "INFO:tensorflow:examples/sec: 2.81536\n",
      "I1117 13:38:00.758391 4595733952 tpu_estimator.py:2160] examples/sec: 2.81536\n",
      "INFO:tensorflow:global_step/sec: 0.0877661\n",
      "I1117 13:38:12.152065 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877661\n",
      "INFO:tensorflow:examples/sec: 2.80852\n",
      "I1117 13:38:12.152270 4595733952 tpu_estimator.py:2160] examples/sec: 2.80852\n",
      "INFO:tensorflow:global_step/sec: 0.0880704\n",
      "I1117 13:38:23.506633 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880704\n",
      "INFO:tensorflow:examples/sec: 2.81825\n",
      "I1117 13:38:23.506841 4595733952 tpu_estimator.py:2160] examples/sec: 2.81825\n",
      "INFO:tensorflow:global_step/sec: 0.0881828\n",
      "I1117 13:38:34.846701 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881828\n",
      "INFO:tensorflow:examples/sec: 2.82185\n",
      "I1117 13:38:34.846907 4595733952 tpu_estimator.py:2160] examples/sec: 2.82185\n",
      "INFO:tensorflow:global_step/sec: 0.0891752\n",
      "I1117 13:38:46.060590 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891752\n",
      "INFO:tensorflow:examples/sec: 2.85361\n",
      "I1117 13:38:46.060798 4595733952 tpu_estimator.py:2160] examples/sec: 2.85361\n",
      "INFO:tensorflow:global_step/sec: 0.0885267\n",
      "I1117 13:38:57.356623 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885267\n",
      "INFO:tensorflow:examples/sec: 2.83285\n",
      "I1117 13:38:57.356831 4595733952 tpu_estimator.py:2160] examples/sec: 2.83285\n",
      "INFO:tensorflow:global_step/sec: 0.0880939\n",
      "I1117 13:39:08.708144 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880939\n",
      "INFO:tensorflow:examples/sec: 2.819\n",
      "I1117 13:39:08.708353 4595733952 tpu_estimator.py:2160] examples/sec: 2.819\n",
      "INFO:tensorflow:global_step/sec: 0.0872929\n",
      "I1117 13:39:20.163830 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872929\n",
      "INFO:tensorflow:examples/sec: 2.79337\n",
      "I1117 13:39:20.164039 4595733952 tpu_estimator.py:2160] examples/sec: 2.79337\n",
      "INFO:tensorflow:global_step/sec: 0.0825393\n",
      "I1117 13:39:32.279271 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0825393\n",
      "INFO:tensorflow:examples/sec: 2.64126\n",
      "I1117 13:39:32.279484 4595733952 tpu_estimator.py:2160] examples/sec: 2.64126\n",
      "INFO:tensorflow:global_step/sec: 0.0870639\n",
      "I1117 13:39:43.765095 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870639\n",
      "INFO:tensorflow:examples/sec: 2.78604\n",
      "I1117 13:39:43.765306 4595733952 tpu_estimator.py:2160] examples/sec: 2.78604\n",
      "INFO:tensorflow:global_step/sec: 0.0882224\n",
      "I1117 13:39:55.100075 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882224\n",
      "INFO:tensorflow:examples/sec: 2.82312\n",
      "I1117 13:39:55.100276 4595733952 tpu_estimator.py:2160] examples/sec: 2.82312\n",
      "INFO:tensorflow:global_step/sec: 0.0883599\n",
      "I1117 13:40:06.417423 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883599\n",
      "INFO:tensorflow:examples/sec: 2.82752\n",
      "I1117 13:40:06.417627 4595733952 tpu_estimator.py:2160] examples/sec: 2.82752\n",
      "INFO:tensorflow:global_step/sec: 0.0881491\n",
      "I1117 13:40:17.761866 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881491\n",
      "INFO:tensorflow:examples/sec: 2.82077\n",
      "I1117 13:40:17.762075 4595733952 tpu_estimator.py:2160] examples/sec: 2.82077\n",
      "INFO:tensorflow:global_step/sec: 0.0882711\n",
      "I1117 13:40:29.090587 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882711\n",
      "INFO:tensorflow:examples/sec: 2.82468\n",
      "I1117 13:40:29.091032 4595733952 tpu_estimator.py:2160] examples/sec: 2.82468\n",
      "INFO:tensorflow:global_step/sec: 0.0879224\n",
      "I1117 13:40:40.464231 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879224\n",
      "INFO:tensorflow:examples/sec: 2.81352\n",
      "I1117 13:40:40.464441 4595733952 tpu_estimator.py:2160] examples/sec: 2.81352\n",
      "INFO:tensorflow:global_step/sec: 0.0878237\n",
      "I1117 13:40:51.850688 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878237\n",
      "INFO:tensorflow:examples/sec: 2.81036\n",
      "I1117 13:40:51.850902 4595733952 tpu_estimator.py:2160] examples/sec: 2.81036\n",
      "INFO:tensorflow:global_step/sec: 0.0882469\n",
      "I1117 13:41:03.182533 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882469\n",
      "INFO:tensorflow:examples/sec: 2.8239\n",
      "I1117 13:41:03.182744 4595733952 tpu_estimator.py:2160] examples/sec: 2.8239\n",
      "INFO:tensorflow:global_step/sec: 0.088362\n",
      "I1117 13:41:14.499625 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088362\n",
      "INFO:tensorflow:examples/sec: 2.82758\n",
      "I1117 13:41:14.499845 4595733952 tpu_estimator.py:2160] examples/sec: 2.82758\n",
      "INFO:tensorflow:global_step/sec: 0.088439\n",
      "I1117 13:41:25.806852 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088439\n",
      "INFO:tensorflow:examples/sec: 2.83005\n",
      "I1117 13:41:25.807144 4595733952 tpu_estimator.py:2160] examples/sec: 2.83005\n",
      "INFO:tensorflow:global_step/sec: 0.08926\n",
      "I1117 13:41:37.010066 4595733952 tpu_estimator.py:2159] global_step/sec: 0.08926\n",
      "INFO:tensorflow:examples/sec: 2.85632\n",
      "I1117 13:41:37.010282 4595733952 tpu_estimator.py:2160] examples/sec: 2.85632\n",
      "INFO:tensorflow:global_step/sec: 0.0886849\n",
      "I1117 13:41:48.285933 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886849\n",
      "INFO:tensorflow:examples/sec: 2.83792\n",
      "I1117 13:41:48.286154 4595733952 tpu_estimator.py:2160] examples/sec: 2.83792\n",
      "INFO:tensorflow:global_step/sec: 0.0874885\n",
      "I1117 13:41:59.716018 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874885\n",
      "INFO:tensorflow:examples/sec: 2.79963\n",
      "I1117 13:41:59.716233 4595733952 tpu_estimator.py:2160] examples/sec: 2.79963\n",
      "INFO:tensorflow:global_step/sec: 0.0869667\n",
      "I1117 13:42:11.214653 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869667\n",
      "INFO:tensorflow:examples/sec: 2.78294\n",
      "I1117 13:42:11.214859 4595733952 tpu_estimator.py:2160] examples/sec: 2.78294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0880592\n",
      "I1117 13:42:22.570661 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880592\n",
      "INFO:tensorflow:examples/sec: 2.81789\n",
      "I1117 13:42:22.570873 4595733952 tpu_estimator.py:2160] examples/sec: 2.81789\n",
      "INFO:tensorflow:global_step/sec: 0.0875815\n",
      "I1117 13:42:33.988598 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875815\n",
      "INFO:tensorflow:examples/sec: 2.80261\n",
      "I1117 13:42:33.988824 4595733952 tpu_estimator.py:2160] examples/sec: 2.80261\n",
      "INFO:tensorflow:global_step/sec: 0.0882405\n",
      "I1117 13:42:45.321233 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882405\n",
      "INFO:tensorflow:examples/sec: 2.8237\n",
      "I1117 13:42:45.321437 4595733952 tpu_estimator.py:2160] examples/sec: 2.8237\n",
      "INFO:tensorflow:global_step/sec: 0.0878855\n",
      "I1117 13:42:56.699695 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878855\n",
      "INFO:tensorflow:examples/sec: 2.81234\n",
      "I1117 13:42:56.699911 4595733952 tpu_estimator.py:2160] examples/sec: 2.81234\n",
      "INFO:tensorflow:global_step/sec: 0.0880994\n",
      "I1117 13:43:08.050501 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880994\n",
      "INFO:tensorflow:examples/sec: 2.81918\n",
      "I1117 13:43:08.050713 4595733952 tpu_estimator.py:2160] examples/sec: 2.81918\n",
      "INFO:tensorflow:global_step/sec: 0.08698\n",
      "I1117 13:43:19.547405 4595733952 tpu_estimator.py:2159] global_step/sec: 0.08698\n",
      "INFO:tensorflow:examples/sec: 2.78336\n",
      "I1117 13:43:19.547621 4595733952 tpu_estimator.py:2160] examples/sec: 2.78336\n",
      "INFO:tensorflow:global_step/sec: 0.0871957\n",
      "I1117 13:43:31.015856 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871957\n",
      "INFO:tensorflow:examples/sec: 2.79026\n",
      "I1117 13:43:31.016069 4595733952 tpu_estimator.py:2160] examples/sec: 2.79026\n",
      "INFO:tensorflow:global_step/sec: 0.0873565\n",
      "I1117 13:43:42.463205 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873565\n",
      "INFO:tensorflow:examples/sec: 2.79541\n",
      "I1117 13:43:42.463418 4595733952 tpu_estimator.py:2160] examples/sec: 2.79541\n",
      "INFO:tensorflow:global_step/sec: 0.0878256\n",
      "I1117 13:43:53.849410 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878256\n",
      "INFO:tensorflow:examples/sec: 2.81042\n",
      "I1117 13:43:53.849624 4595733952 tpu_estimator.py:2160] examples/sec: 2.81042\n",
      "INFO:tensorflow:global_step/sec: 0.0883929\n",
      "I1117 13:44:05.162539 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883929\n",
      "INFO:tensorflow:examples/sec: 2.82857\n",
      "I1117 13:44:05.162754 4595733952 tpu_estimator.py:2160] examples/sec: 2.82857\n",
      "INFO:tensorflow:global_step/sec: 0.0888296\n",
      "I1117 13:44:16.420063 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888296\n",
      "INFO:tensorflow:examples/sec: 2.84255\n",
      "I1117 13:44:16.420432 4595733952 tpu_estimator.py:2160] examples/sec: 2.84255\n",
      "INFO:tensorflow:global_step/sec: 0.0886758\n",
      "I1117 13:44:27.697082 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886758\n",
      "INFO:tensorflow:examples/sec: 2.83762\n",
      "I1117 13:44:27.697295 4595733952 tpu_estimator.py:2160] examples/sec: 2.83762\n",
      "INFO:tensorflow:global_step/sec: 0.0886311\n",
      "I1117 13:44:38.979789 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886311\n",
      "INFO:tensorflow:examples/sec: 2.8362\n",
      "I1117 13:44:38.979992 4595733952 tpu_estimator.py:2160] examples/sec: 2.8362\n",
      "INFO:tensorflow:global_step/sec: 0.088829\n",
      "I1117 13:44:50.237390 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088829\n",
      "INFO:tensorflow:examples/sec: 2.84253\n",
      "I1117 13:44:50.237603 4595733952 tpu_estimator.py:2160] examples/sec: 2.84253\n",
      "INFO:tensorflow:global_step/sec: 0.0886272\n",
      "I1117 13:45:01.520597 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0886272\n",
      "INFO:tensorflow:examples/sec: 2.83607\n",
      "I1117 13:45:01.520807 4595733952 tpu_estimator.py:2160] examples/sec: 2.83607\n",
      "INFO:tensorflow:global_step/sec: 0.0881907\n",
      "I1117 13:45:12.859668 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881907\n",
      "INFO:tensorflow:examples/sec: 2.8221\n",
      "I1117 13:45:12.859889 4595733952 tpu_estimator.py:2160] examples/sec: 2.8221\n",
      "INFO:tensorflow:global_step/sec: 0.0882721\n",
      "I1117 13:45:24.188277 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882721\n",
      "INFO:tensorflow:examples/sec: 2.82471\n",
      "I1117 13:45:24.188493 4595733952 tpu_estimator.py:2160] examples/sec: 2.82471\n",
      "INFO:tensorflow:global_step/sec: 0.0868946\n",
      "I1117 13:45:35.696449 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0868946\n",
      "INFO:tensorflow:examples/sec: 2.78063\n",
      "I1117 13:45:35.696651 4595733952 tpu_estimator.py:2160] examples/sec: 2.78063\n",
      "INFO:tensorflow:global_step/sec: 0.0875752\n",
      "I1117 13:45:47.115237 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875752\n",
      "INFO:tensorflow:examples/sec: 2.80241\n",
      "I1117 13:45:47.115449 4595733952 tpu_estimator.py:2160] examples/sec: 2.80241\n",
      "INFO:tensorflow:global_step/sec: 0.0871738\n",
      "I1117 13:45:58.586557 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871738\n",
      "INFO:tensorflow:examples/sec: 2.78956\n",
      "I1117 13:45:58.586923 4595733952 tpu_estimator.py:2160] examples/sec: 2.78956\n",
      "INFO:tensorflow:global_step/sec: 0.0882256\n",
      "I1117 13:46:09.921123 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882256\n",
      "INFO:tensorflow:examples/sec: 2.82322\n",
      "I1117 13:46:09.921326 4595733952 tpu_estimator.py:2160] examples/sec: 2.82322\n",
      "INFO:tensorflow:global_step/sec: 0.0873922\n",
      "I1117 13:46:21.363888 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873922\n",
      "INFO:tensorflow:examples/sec: 2.79655\n",
      "I1117 13:46:21.364106 4595733952 tpu_estimator.py:2160] examples/sec: 2.79655\n",
      "INFO:tensorflow:global_step/sec: 0.0878721\n",
      "I1117 13:46:32.743962 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878721\n",
      "INFO:tensorflow:examples/sec: 2.81191\n",
      "I1117 13:46:32.744220 4595733952 tpu_estimator.py:2160] examples/sec: 2.81191\n",
      "INFO:tensorflow:global_step/sec: 0.0882709\n",
      "I1117 13:46:44.072746 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882709\n",
      "INFO:tensorflow:examples/sec: 2.82467\n",
      "I1117 13:46:44.072968 4595733952 tpu_estimator.py:2160] examples/sec: 2.82467\n",
      "INFO:tensorflow:global_step/sec: 0.0879102\n",
      "I1117 13:46:55.447940 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879102\n",
      "INFO:tensorflow:examples/sec: 2.81313\n",
      "I1117 13:46:55.448116 4595733952 tpu_estimator.py:2160] examples/sec: 2.81313\n",
      "INFO:tensorflow:global_step/sec: 0.0816488\n",
      "I1117 13:47:07.695601 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0816488\n",
      "INFO:tensorflow:examples/sec: 2.61276\n",
      "I1117 13:47:07.696012 4595733952 tpu_estimator.py:2160] examples/sec: 2.61276\n",
      "INFO:tensorflow:global_step/sec: 0.0836171\n",
      "I1117 13:47:19.654798 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836171\n",
      "INFO:tensorflow:examples/sec: 2.67575\n",
      "I1117 13:47:19.655554 4595733952 tpu_estimator.py:2160] examples/sec: 2.67575\n",
      "INFO:tensorflow:global_step/sec: 0.0827291\n",
      "I1117 13:47:31.742480 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0827291\n",
      "INFO:tensorflow:examples/sec: 2.64733\n",
      "I1117 13:47:31.742736 4595733952 tpu_estimator.py:2160] examples/sec: 2.64733\n",
      "INFO:tensorflow:global_step/sec: 0.0792771\n",
      "I1117 13:47:44.356451 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0792771\n",
      "INFO:tensorflow:examples/sec: 2.53687\n",
      "I1117 13:47:44.356682 4595733952 tpu_estimator.py:2160] examples/sec: 2.53687\n",
      "INFO:tensorflow:global_step/sec: 0.0867322\n",
      "I1117 13:47:55.886223 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867322\n",
      "INFO:tensorflow:examples/sec: 2.77543\n",
      "I1117 13:47:55.886433 4595733952 tpu_estimator.py:2160] examples/sec: 2.77543\n",
      "INFO:tensorflow:global_step/sec: 0.088151\n",
      "I1117 13:48:07.230426 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088151\n",
      "INFO:tensorflow:examples/sec: 2.82083\n",
      "I1117 13:48:07.230634 4595733952 tpu_estimator.py:2160] examples/sec: 2.82083\n",
      "INFO:tensorflow:global_step/sec: 0.0874238\n",
      "I1117 13:48:18.668902 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874238\n",
      "INFO:tensorflow:examples/sec: 2.79756\n",
      "I1117 13:48:18.669114 4595733952 tpu_estimator.py:2160] examples/sec: 2.79756\n",
      "INFO:tensorflow:global_step/sec: 0.0875401\n",
      "I1117 13:48:30.092249 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875401\n",
      "INFO:tensorflow:examples/sec: 2.80128\n",
      "I1117 13:48:30.092458 4595733952 tpu_estimator.py:2160] examples/sec: 2.80128\n",
      "INFO:tensorflow:global_step/sec: 0.0876351\n",
      "I1117 13:48:41.503201 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876351\n",
      "INFO:tensorflow:examples/sec: 2.80432\n",
      "I1117 13:48:41.503411 4595733952 tpu_estimator.py:2160] examples/sec: 2.80432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0870825\n",
      "I1117 13:48:52.986564 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870825\n",
      "INFO:tensorflow:examples/sec: 2.78664\n",
      "I1117 13:48:52.986774 4595733952 tpu_estimator.py:2160] examples/sec: 2.78664\n",
      "INFO:tensorflow:global_step/sec: 0.0871824\n",
      "I1117 13:49:04.456768 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871824\n",
      "INFO:tensorflow:examples/sec: 2.78984\n",
      "I1117 13:49:04.457022 4595733952 tpu_estimator.py:2160] examples/sec: 2.78984\n",
      "INFO:tensorflow:global_step/sec: 0.0881203\n",
      "I1117 13:49:15.804886 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881203\n",
      "INFO:tensorflow:examples/sec: 2.81985\n",
      "I1117 13:49:15.805099 4595733952 tpu_estimator.py:2160] examples/sec: 2.81985\n",
      "INFO:tensorflow:global_step/sec: 0.0890937\n",
      "I1117 13:49:27.029019 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890937\n",
      "INFO:tensorflow:examples/sec: 2.851\n",
      "I1117 13:49:27.029231 4595733952 tpu_estimator.py:2160] examples/sec: 2.851\n",
      "INFO:tensorflow:global_step/sec: 0.087544\n",
      "I1117 13:49:38.451843 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087544\n",
      "INFO:tensorflow:examples/sec: 2.80141\n",
      "I1117 13:49:38.452051 4595733952 tpu_estimator.py:2160] examples/sec: 2.80141\n",
      "INFO:tensorflow:global_step/sec: 0.0864474\n",
      "I1117 13:49:50.019582 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0864474\n",
      "INFO:tensorflow:examples/sec: 2.76632\n",
      "I1117 13:49:50.019814 4595733952 tpu_estimator.py:2160] examples/sec: 2.76632\n",
      "INFO:tensorflow:global_step/sec: 0.0824726\n",
      "I1117 13:50:02.144806 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824726\n",
      "INFO:tensorflow:examples/sec: 2.63912\n",
      "I1117 13:50:02.144989 4595733952 tpu_estimator.py:2160] examples/sec: 2.63912\n",
      "INFO:tensorflow:global_step/sec: 0.080824\n",
      "I1117 13:50:14.517424 4595733952 tpu_estimator.py:2159] global_step/sec: 0.080824\n",
      "INFO:tensorflow:examples/sec: 2.58637\n",
      "I1117 13:50:14.517617 4595733952 tpu_estimator.py:2160] examples/sec: 2.58637\n",
      "INFO:tensorflow:global_step/sec: 0.0817856\n",
      "I1117 13:50:26.744468 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0817856\n",
      "INFO:tensorflow:examples/sec: 2.61714\n",
      "I1117 13:50:26.744678 4595733952 tpu_estimator.py:2160] examples/sec: 2.61714\n",
      "INFO:tensorflow:global_step/sec: 0.0817844\n",
      "I1117 13:50:38.971728 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0817844\n",
      "INFO:tensorflow:examples/sec: 2.6171\n",
      "I1117 13:50:38.971956 4595733952 tpu_estimator.py:2160] examples/sec: 2.6171\n",
      "INFO:tensorflow:global_step/sec: 0.0795866\n",
      "I1117 13:50:51.536672 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0795866\n",
      "INFO:tensorflow:examples/sec: 2.54677\n",
      "I1117 13:50:51.536934 4595733952 tpu_estimator.py:2160] examples/sec: 2.54677\n",
      "INFO:tensorflow:global_step/sec: 0.0831912\n",
      "I1117 13:51:03.557174 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0831912\n",
      "INFO:tensorflow:examples/sec: 2.66212\n",
      "I1117 13:51:03.557423 4595733952 tpu_estimator.py:2160] examples/sec: 2.66212\n",
      "INFO:tensorflow:global_step/sec: 0.0841645\n",
      "I1117 13:51:15.438667 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841645\n",
      "INFO:tensorflow:examples/sec: 2.69327\n",
      "I1117 13:51:15.438862 4595733952 tpu_estimator.py:2160] examples/sec: 2.69327\n",
      "INFO:tensorflow:global_step/sec: 0.0748611\n",
      "I1117 13:51:28.796724 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0748611\n",
      "INFO:tensorflow:examples/sec: 2.39555\n",
      "I1117 13:51:28.797860 4595733952 tpu_estimator.py:2160] examples/sec: 2.39555\n",
      "INFO:tensorflow:global_step/sec: 0.0740624\n",
      "I1117 13:51:42.298818 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0740624\n",
      "INFO:tensorflow:examples/sec: 2.37\n",
      "I1117 13:51:42.298992 4595733952 tpu_estimator.py:2160] examples/sec: 2.37\n",
      "INFO:tensorflow:global_step/sec: 0.0554898\n",
      "I1117 13:52:00.320204 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0554898\n",
      "INFO:tensorflow:examples/sec: 1.77567\n",
      "I1117 13:52:00.320410 4595733952 tpu_estimator.py:2160] examples/sec: 1.77567\n",
      "INFO:tensorflow:global_step/sec: 0.0796197\n",
      "I1117 13:52:12.879898 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0796197\n",
      "INFO:tensorflow:examples/sec: 2.54783\n",
      "I1117 13:52:12.880224 4595733952 tpu_estimator.py:2160] examples/sec: 2.54783\n",
      "INFO:tensorflow:global_step/sec: 0.0821586\n",
      "I1117 13:52:25.051542 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0821586\n",
      "INFO:tensorflow:examples/sec: 2.62908\n",
      "I1117 13:52:25.051824 4595733952 tpu_estimator.py:2160] examples/sec: 2.62908\n",
      "INFO:tensorflow:global_step/sec: 0.0814101\n",
      "I1117 13:52:37.334968 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0814101\n",
      "INFO:tensorflow:examples/sec: 2.60512\n",
      "I1117 13:52:37.335174 4595733952 tpu_estimator.py:2160] examples/sec: 2.60512\n",
      "INFO:tensorflow:global_step/sec: 0.0669661\n",
      "I1117 13:52:52.268033 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0669661\n",
      "INFO:tensorflow:examples/sec: 2.14291\n",
      "I1117 13:52:52.268301 4595733952 tpu_estimator.py:2160] examples/sec: 2.14291\n",
      "INFO:tensorflow:global_step/sec: 0.0824017\n",
      "I1117 13:53:04.403593 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824017\n",
      "INFO:tensorflow:examples/sec: 2.63685\n",
      "I1117 13:53:04.403798 4595733952 tpu_estimator.py:2160] examples/sec: 2.63685\n",
      "INFO:tensorflow:global_step/sec: 0.0839425\n",
      "I1117 13:53:16.316506 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839425\n",
      "INFO:tensorflow:examples/sec: 2.68616\n",
      "I1117 13:53:16.316784 4595733952 tpu_estimator.py:2160] examples/sec: 2.68616\n",
      "INFO:tensorflow:global_step/sec: 0.0836069\n",
      "I1117 13:53:28.277238 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836069\n",
      "INFO:tensorflow:examples/sec: 2.67542\n",
      "I1117 13:53:28.277450 4595733952 tpu_estimator.py:2160] examples/sec: 2.67542\n",
      "INFO:tensorflow:global_step/sec: 0.0834729\n",
      "I1117 13:53:40.257173 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0834729\n",
      "INFO:tensorflow:examples/sec: 2.67113\n",
      "I1117 13:53:40.257383 4595733952 tpu_estimator.py:2160] examples/sec: 2.67113\n",
      "INFO:tensorflow:global_step/sec: 0.0839057\n",
      "I1117 13:53:52.175323 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839057\n",
      "INFO:tensorflow:examples/sec: 2.68498\n",
      "I1117 13:53:52.175568 4595733952 tpu_estimator.py:2160] examples/sec: 2.68498\n",
      "INFO:tensorflow:global_step/sec: 0.0849084\n",
      "I1117 13:54:03.952716 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0849084\n",
      "INFO:tensorflow:examples/sec: 2.71707\n",
      "I1117 13:54:03.952924 4595733952 tpu_estimator.py:2160] examples/sec: 2.71707\n",
      "INFO:tensorflow:global_step/sec: 0.085026\n",
      "I1117 13:54:15.713827 4595733952 tpu_estimator.py:2159] global_step/sec: 0.085026\n",
      "INFO:tensorflow:examples/sec: 2.72083\n",
      "I1117 13:54:15.714032 4595733952 tpu_estimator.py:2160] examples/sec: 2.72083\n",
      "INFO:tensorflow:global_step/sec: 0.0851863\n",
      "I1117 13:54:27.452803 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0851863\n",
      "INFO:tensorflow:examples/sec: 2.72596\n",
      "I1117 13:54:27.453015 4595733952 tpu_estimator.py:2160] examples/sec: 2.72596\n",
      "INFO:tensorflow:global_step/sec: 0.0800699\n",
      "I1117 13:54:39.941861 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0800699\n",
      "INFO:tensorflow:examples/sec: 2.56224\n",
      "I1117 13:54:39.942206 4595733952 tpu_estimator.py:2160] examples/sec: 2.56224\n",
      "INFO:tensorflow:global_step/sec: 0.0818019\n",
      "I1117 13:54:52.166540 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0818019\n",
      "INFO:tensorflow:examples/sec: 2.61766\n",
      "I1117 13:54:52.166739 4595733952 tpu_estimator.py:2160] examples/sec: 2.61766\n",
      "INFO:tensorflow:global_step/sec: 0.0674431\n",
      "I1117 13:55:06.993854 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0674431\n",
      "INFO:tensorflow:examples/sec: 2.15818\n",
      "I1117 13:55:06.994065 4595733952 tpu_estimator.py:2160] examples/sec: 2.15818\n",
      "INFO:tensorflow:global_step/sec: 0.0741813\n",
      "I1117 13:55:20.474337 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0741813\n",
      "INFO:tensorflow:examples/sec: 2.3738\n",
      "I1117 13:55:20.474563 4595733952 tpu_estimator.py:2160] examples/sec: 2.3738\n",
      "INFO:tensorflow:global_step/sec: 0.0758991\n",
      "I1117 13:55:33.649729 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0758991\n",
      "INFO:tensorflow:examples/sec: 2.42877\n",
      "I1117 13:55:33.649924 4595733952 tpu_estimator.py:2160] examples/sec: 2.42877\n",
      "INFO:tensorflow:global_step/sec: 0.0688509\n",
      "I1117 13:55:48.173845 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0688509\n",
      "INFO:tensorflow:examples/sec: 2.20323\n",
      "I1117 13:55:48.174023 4595733952 tpu_estimator.py:2160] examples/sec: 2.20323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0772377\n",
      "I1117 13:56:01.120905 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0772377\n",
      "INFO:tensorflow:examples/sec: 2.47161\n",
      "I1117 13:56:01.121109 4595733952 tpu_estimator.py:2160] examples/sec: 2.47161\n",
      "INFO:tensorflow:global_step/sec: 0.0761712\n",
      "I1117 13:56:14.249198 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0761712\n",
      "INFO:tensorflow:examples/sec: 2.43748\n",
      "I1117 13:56:14.249404 4595733952 tpu_estimator.py:2160] examples/sec: 2.43748\n",
      "INFO:tensorflow:global_step/sec: 0.0763914\n",
      "I1117 13:56:27.339696 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0763914\n",
      "INFO:tensorflow:examples/sec: 2.44452\n",
      "I1117 13:56:27.340026 4595733952 tpu_estimator.py:2160] examples/sec: 2.44452\n",
      "INFO:tensorflow:global_step/sec: 0.0763411\n",
      "I1117 13:56:40.438799 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0763411\n",
      "INFO:tensorflow:examples/sec: 2.44292\n",
      "I1117 13:56:40.438994 4595733952 tpu_estimator.py:2160] examples/sec: 2.44292\n",
      "INFO:tensorflow:global_step/sec: 0.0701304\n",
      "I1117 13:56:54.697955 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0701304\n",
      "INFO:tensorflow:examples/sec: 2.24417\n",
      "I1117 13:56:54.698153 4595733952 tpu_estimator.py:2160] examples/sec: 2.24417\n",
      "INFO:tensorflow:global_step/sec: 0.073984\n",
      "I1117 13:57:08.214431 4595733952 tpu_estimator.py:2159] global_step/sec: 0.073984\n",
      "INFO:tensorflow:examples/sec: 2.36749\n",
      "I1117 13:57:08.214623 4595733952 tpu_estimator.py:2160] examples/sec: 2.36749\n",
      "INFO:tensorflow:global_step/sec: 0.076832\n",
      "I1117 13:57:21.229813 4595733952 tpu_estimator.py:2159] global_step/sec: 0.076832\n",
      "INFO:tensorflow:examples/sec: 2.45862\n",
      "I1117 13:57:21.230018 4595733952 tpu_estimator.py:2160] examples/sec: 2.45862\n",
      "INFO:tensorflow:global_step/sec: 0.0767491\n",
      "I1117 13:57:34.259361 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0767491\n",
      "INFO:tensorflow:examples/sec: 2.45597\n",
      "I1117 13:57:34.259656 4595733952 tpu_estimator.py:2160] examples/sec: 2.45597\n",
      "INFO:tensorflow:global_step/sec: 0.0763995\n",
      "I1117 13:57:47.348375 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0763995\n",
      "INFO:tensorflow:examples/sec: 2.44478\n",
      "I1117 13:57:47.348581 4595733952 tpu_estimator.py:2160] examples/sec: 2.44478\n",
      "INFO:tensorflow:global_step/sec: 0.0768724\n",
      "I1117 13:58:00.356968 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0768724\n",
      "INFO:tensorflow:examples/sec: 2.45992\n",
      "I1117 13:58:00.357187 4595733952 tpu_estimator.py:2160] examples/sec: 2.45992\n",
      "INFO:tensorflow:global_step/sec: 0.0743541\n",
      "I1117 13:58:13.806115 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0743541\n",
      "INFO:tensorflow:examples/sec: 2.37933\n",
      "I1117 13:58:13.806316 4595733952 tpu_estimator.py:2160] examples/sec: 2.37933\n",
      "INFO:tensorflow:global_step/sec: 0.0749425\n",
      "I1117 13:58:27.149656 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0749425\n",
      "INFO:tensorflow:examples/sec: 2.39816\n",
      "I1117 13:58:27.149854 4595733952 tpu_estimator.py:2160] examples/sec: 2.39816\n",
      "INFO:tensorflow:global_step/sec: 0.0769349\n",
      "I1117 13:58:40.147660 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0769349\n",
      "INFO:tensorflow:examples/sec: 2.46192\n",
      "I1117 13:58:40.148002 4595733952 tpu_estimator.py:2160] examples/sec: 2.46192\n",
      "INFO:tensorflow:global_step/sec: 0.076513\n",
      "I1117 13:58:53.217319 4595733952 tpu_estimator.py:2159] global_step/sec: 0.076513\n",
      "INFO:tensorflow:examples/sec: 2.44842\n",
      "I1117 13:58:53.217692 4595733952 tpu_estimator.py:2160] examples/sec: 2.44842\n",
      "INFO:tensorflow:global_step/sec: 0.0753705\n",
      "I1117 13:59:06.485117 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0753705\n",
      "INFO:tensorflow:examples/sec: 2.41186\n",
      "I1117 13:59:06.485315 4595733952 tpu_estimator.py:2160] examples/sec: 2.41186\n",
      "INFO:tensorflow:global_step/sec: 0.0763239\n",
      "I1117 13:59:19.587180 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0763239\n",
      "INFO:tensorflow:examples/sec: 2.44236\n",
      "I1117 13:59:19.587393 4595733952 tpu_estimator.py:2160] examples/sec: 2.44236\n",
      "INFO:tensorflow:global_step/sec: 0.0764116\n",
      "I1117 13:59:32.674313 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0764116\n",
      "INFO:tensorflow:examples/sec: 2.44517\n",
      "I1117 13:59:32.674859 4595733952 tpu_estimator.py:2160] examples/sec: 2.44517\n",
      "INFO:tensorflow:global_step/sec: 0.0697254\n",
      "I1117 13:59:47.016122 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0697254\n",
      "INFO:tensorflow:examples/sec: 2.23121\n",
      "I1117 13:59:47.016304 4595733952 tpu_estimator.py:2160] examples/sec: 2.23121\n",
      "INFO:tensorflow:global_step/sec: 0.0751198\n",
      "I1117 14:00:00.328243 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0751198\n",
      "INFO:tensorflow:examples/sec: 2.40383\n",
      "I1117 14:00:00.328799 4595733952 tpu_estimator.py:2160] examples/sec: 2.40383\n",
      "INFO:tensorflow:global_step/sec: 0.0771219\n",
      "I1117 14:00:13.294714 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0771219\n",
      "INFO:tensorflow:examples/sec: 2.4679\n",
      "I1117 14:00:13.294903 4595733952 tpu_estimator.py:2160] examples/sec: 2.4679\n",
      "INFO:tensorflow:global_step/sec: 0.0717326\n",
      "I1117 14:00:27.235418 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0717326\n",
      "INFO:tensorflow:examples/sec: 2.29544\n",
      "I1117 14:00:27.235612 4595733952 tpu_estimator.py:2160] examples/sec: 2.29544\n",
      "INFO:tensorflow:global_step/sec: 0.0709123\n",
      "I1117 14:00:41.337339 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0709123\n",
      "INFO:tensorflow:examples/sec: 2.26919\n",
      "I1117 14:00:41.337567 4595733952 tpu_estimator.py:2160] examples/sec: 2.26919\n",
      "INFO:tensorflow:global_step/sec: 0.0708656\n",
      "I1117 14:00:55.448550 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0708656\n",
      "INFO:tensorflow:examples/sec: 2.2677\n",
      "I1117 14:00:55.448756 4595733952 tpu_estimator.py:2160] examples/sec: 2.2677\n",
      "INFO:tensorflow:global_step/sec: 0.0816612\n",
      "I1117 14:01:07.694256 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0816612\n",
      "INFO:tensorflow:examples/sec: 2.61316\n",
      "I1117 14:01:07.694449 4595733952 tpu_estimator.py:2160] examples/sec: 2.61316\n",
      "INFO:tensorflow:global_step/sec: 0.0819916\n",
      "I1117 14:01:19.890644 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0819916\n",
      "INFO:tensorflow:examples/sec: 2.62373\n",
      "I1117 14:01:19.890854 4595733952 tpu_estimator.py:2160] examples/sec: 2.62373\n",
      "INFO:tensorflow:global_step/sec: 0.0836797\n",
      "I1117 14:01:31.840965 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836797\n",
      "INFO:tensorflow:examples/sec: 2.67775\n",
      "I1117 14:01:31.841161 4595733952 tpu_estimator.py:2160] examples/sec: 2.67775\n",
      "INFO:tensorflow:global_step/sec: 0.0828296\n",
      "I1117 14:01:43.913952 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0828296\n",
      "INFO:tensorflow:examples/sec: 2.65055\n",
      "I1117 14:01:43.914155 4595733952 tpu_estimator.py:2160] examples/sec: 2.65055\n",
      "INFO:tensorflow:global_step/sec: 0.0827662\n",
      "I1117 14:01:55.996270 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0827662\n",
      "INFO:tensorflow:examples/sec: 2.64852\n",
      "I1117 14:01:55.996485 4595733952 tpu_estimator.py:2160] examples/sec: 2.64852\n",
      "INFO:tensorflow:global_step/sec: 0.0816698\n",
      "I1117 14:02:08.240611 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0816698\n",
      "INFO:tensorflow:examples/sec: 2.61343\n",
      "I1117 14:02:08.240823 4595733952 tpu_estimator.py:2160] examples/sec: 2.61343\n",
      "INFO:tensorflow:global_step/sec: 0.0832888\n",
      "I1117 14:02:20.247009 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832888\n",
      "INFO:tensorflow:examples/sec: 2.66524\n",
      "I1117 14:02:20.247209 4595733952 tpu_estimator.py:2160] examples/sec: 2.66524\n",
      "INFO:tensorflow:global_step/sec: 0.0832509\n",
      "I1117 14:02:32.258902 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832509\n",
      "INFO:tensorflow:examples/sec: 2.66403\n",
      "I1117 14:02:32.259108 4595733952 tpu_estimator.py:2160] examples/sec: 2.66403\n",
      "INFO:tensorflow:global_step/sec: 0.0799441\n",
      "I1117 14:02:44.767637 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0799441\n",
      "INFO:tensorflow:examples/sec: 2.55821\n",
      "I1117 14:02:44.767835 4595733952 tpu_estimator.py:2160] examples/sec: 2.55821\n",
      "INFO:tensorflow:global_step/sec: 0.0725564\n",
      "I1117 14:02:58.550024 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0725564\n",
      "INFO:tensorflow:examples/sec: 2.3218\n",
      "I1117 14:02:58.550211 4595733952 tpu_estimator.py:2160] examples/sec: 2.3218\n",
      "INFO:tensorflow:global_step/sec: 0.0754256\n",
      "I1117 14:03:11.808122 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0754256\n",
      "INFO:tensorflow:examples/sec: 2.41362\n",
      "I1117 14:03:11.808316 4595733952 tpu_estimator.py:2160] examples/sec: 2.41362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0812678\n",
      "I1117 14:03:24.113142 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0812678\n",
      "INFO:tensorflow:examples/sec: 2.60057\n",
      "I1117 14:03:24.113552 4595733952 tpu_estimator.py:2160] examples/sec: 2.60057\n",
      "INFO:tensorflow:global_step/sec: 0.0849034\n",
      "I1117 14:03:35.891274 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0849034\n",
      "INFO:tensorflow:examples/sec: 2.71691\n",
      "I1117 14:03:35.891628 4595733952 tpu_estimator.py:2160] examples/sec: 2.71691\n",
      "INFO:tensorflow:global_step/sec: 0.0732312\n",
      "I1117 14:03:49.546574 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0732312\n",
      "INFO:tensorflow:examples/sec: 2.3434\n",
      "I1117 14:03:49.547053 4595733952 tpu_estimator.py:2160] examples/sec: 2.3434\n",
      "INFO:tensorflow:global_step/sec: 0.0627082\n",
      "I1117 14:04:05.493514 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0627082\n",
      "INFO:tensorflow:examples/sec: 2.00666\n",
      "I1117 14:04:05.493813 4595733952 tpu_estimator.py:2160] examples/sec: 2.00666\n",
      "INFO:tensorflow:global_step/sec: 0.0680121\n",
      "I1117 14:04:20.196730 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0680121\n",
      "INFO:tensorflow:examples/sec: 2.17639\n",
      "I1117 14:04:20.196933 4595733952 tpu_estimator.py:2160] examples/sec: 2.17639\n",
      "INFO:tensorflow:global_step/sec: 0.0765282\n",
      "I1117 14:04:33.263839 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0765282\n",
      "INFO:tensorflow:examples/sec: 2.4489\n",
      "I1117 14:04:33.264061 4595733952 tpu_estimator.py:2160] examples/sec: 2.4489\n",
      "INFO:tensorflow:global_step/sec: 0.0841508\n",
      "I1117 14:04:45.147254 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841508\n",
      "INFO:tensorflow:examples/sec: 2.69283\n",
      "I1117 14:04:45.147462 4595733952 tpu_estimator.py:2160] examples/sec: 2.69283\n",
      "INFO:tensorflow:global_step/sec: 0.0882437\n",
      "I1117 14:04:56.479516 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882437\n",
      "INFO:tensorflow:examples/sec: 2.8238\n",
      "I1117 14:04:56.479725 4595733952 tpu_estimator.py:2160] examples/sec: 2.8238\n",
      "INFO:tensorflow:global_step/sec: 0.089565\n",
      "I1117 14:05:07.644598 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089565\n",
      "INFO:tensorflow:examples/sec: 2.86608\n",
      "I1117 14:05:07.644814 4595733952 tpu_estimator.py:2160] examples/sec: 2.86608\n",
      "INFO:tensorflow:global_step/sec: 0.0891557\n",
      "I1117 14:05:18.860931 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891557\n",
      "INFO:tensorflow:examples/sec: 2.85298\n",
      "I1117 14:05:18.861181 4595733952 tpu_estimator.py:2160] examples/sec: 2.85298\n",
      "INFO:tensorflow:global_step/sec: 0.0895747\n",
      "I1117 14:05:30.024786 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895747\n",
      "INFO:tensorflow:examples/sec: 2.86639\n",
      "I1117 14:05:30.025005 4595733952 tpu_estimator.py:2160] examples/sec: 2.86639\n",
      "INFO:tensorflow:global_step/sec: 0.0897688\n",
      "I1117 14:05:41.164523 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897688\n",
      "INFO:tensorflow:examples/sec: 2.8726\n",
      "I1117 14:05:41.164741 4595733952 tpu_estimator.py:2160] examples/sec: 2.8726\n",
      "INFO:tensorflow:global_step/sec: 0.0899501\n",
      "I1117 14:05:52.281793 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899501\n",
      "INFO:tensorflow:examples/sec: 2.8784\n",
      "I1117 14:05:52.282006 4595733952 tpu_estimator.py:2160] examples/sec: 2.8784\n",
      "INFO:tensorflow:global_step/sec: 0.0898144\n",
      "I1117 14:06:03.415874 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898144\n",
      "INFO:tensorflow:examples/sec: 2.87406\n",
      "I1117 14:06:03.416143 4595733952 tpu_estimator.py:2160] examples/sec: 2.87406\n",
      "INFO:tensorflow:global_step/sec: 0.0898288\n",
      "I1117 14:06:14.548181 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898288\n",
      "INFO:tensorflow:examples/sec: 2.87452\n",
      "I1117 14:06:14.548403 4595733952 tpu_estimator.py:2160] examples/sec: 2.87452\n",
      "INFO:tensorflow:global_step/sec: 0.0899529\n",
      "I1117 14:06:25.665073 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899529\n",
      "INFO:tensorflow:examples/sec: 2.87849\n",
      "I1117 14:06:25.665288 4595733952 tpu_estimator.py:2160] examples/sec: 2.87849\n",
      "INFO:tensorflow:global_step/sec: 0.0907252\n",
      "I1117 14:06:36.687354 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907252\n",
      "INFO:tensorflow:examples/sec: 2.90321\n",
      "I1117 14:06:36.687551 4595733952 tpu_estimator.py:2160] examples/sec: 2.90321\n",
      "INFO:tensorflow:global_step/sec: 0.0895433\n",
      "I1117 14:06:47.855158 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895433\n",
      "INFO:tensorflow:examples/sec: 2.86539\n",
      "I1117 14:06:47.855371 4595733952 tpu_estimator.py:2160] examples/sec: 2.86539\n",
      "INFO:tensorflow:global_step/sec: 0.0901689\n",
      "I1117 14:06:58.945453 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901689\n",
      "INFO:tensorflow:examples/sec: 2.8854\n",
      "I1117 14:06:58.945660 4595733952 tpu_estimator.py:2160] examples/sec: 2.8854\n",
      "INFO:tensorflow:global_step/sec: 0.0902017\n",
      "I1117 14:07:10.031687 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902017\n",
      "INFO:tensorflow:examples/sec: 2.88646\n",
      "I1117 14:07:10.031872 4595733952 tpu_estimator.py:2160] examples/sec: 2.88646\n",
      "INFO:tensorflow:global_step/sec: 0.0897689\n",
      "I1117 14:07:21.171442 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897689\n",
      "INFO:tensorflow:examples/sec: 2.8726\n",
      "I1117 14:07:21.171665 4595733952 tpu_estimator.py:2160] examples/sec: 2.8726\n",
      "INFO:tensorflow:global_step/sec: 0.0906119\n",
      "I1117 14:07:32.207514 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906119\n",
      "INFO:tensorflow:examples/sec: 2.89958\n",
      "I1117 14:07:32.207725 4595733952 tpu_estimator.py:2160] examples/sec: 2.89958\n",
      "INFO:tensorflow:global_step/sec: 0.0900007\n",
      "I1117 14:07:43.318536 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900007\n",
      "INFO:tensorflow:examples/sec: 2.88002\n",
      "I1117 14:07:43.318746 4595733952 tpu_estimator.py:2160] examples/sec: 2.88002\n",
      "INFO:tensorflow:global_step/sec: 0.0907401\n",
      "I1117 14:07:54.339027 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907401\n",
      "INFO:tensorflow:examples/sec: 2.90368\n",
      "I1117 14:07:54.339248 4595733952 tpu_estimator.py:2160] examples/sec: 2.90368\n",
      "INFO:tensorflow:global_step/sec: 0.0906181\n",
      "I1117 14:08:05.374360 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906181\n",
      "INFO:tensorflow:examples/sec: 2.89978\n",
      "I1117 14:08:05.374589 4595733952 tpu_estimator.py:2160] examples/sec: 2.89978\n",
      "INFO:tensorflow:global_step/sec: 0.0895487\n",
      "I1117 14:08:16.541450 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895487\n",
      "INFO:tensorflow:examples/sec: 2.86556\n",
      "I1117 14:08:16.541666 4595733952 tpu_estimator.py:2160] examples/sec: 2.86556\n",
      "INFO:tensorflow:global_step/sec: 0.0906362\n",
      "I1117 14:08:27.574558 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906362\n",
      "INFO:tensorflow:examples/sec: 2.90036\n",
      "I1117 14:08:27.574768 4595733952 tpu_estimator.py:2160] examples/sec: 2.90036\n",
      "INFO:tensorflow:global_step/sec: 0.0902386\n",
      "I1117 14:08:38.656299 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902386\n",
      "INFO:tensorflow:examples/sec: 2.88764\n",
      "I1117 14:08:38.656513 4595733952 tpu_estimator.py:2160] examples/sec: 2.88764\n",
      "INFO:tensorflow:global_step/sec: 0.0904724\n",
      "I1117 14:08:49.709388 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904724\n",
      "INFO:tensorflow:examples/sec: 2.89512\n",
      "I1117 14:08:49.710886 4595733952 tpu_estimator.py:2160] examples/sec: 2.89512\n",
      "INFO:tensorflow:global_step/sec: 0.0900306\n",
      "I1117 14:09:00.816730 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900306\n",
      "INFO:tensorflow:examples/sec: 2.88098\n",
      "I1117 14:09:00.816944 4595733952 tpu_estimator.py:2160] examples/sec: 2.88098\n",
      "INFO:tensorflow:global_step/sec: 0.0902744\n",
      "I1117 14:09:11.894069 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902744\n",
      "INFO:tensorflow:examples/sec: 2.88878\n",
      "I1117 14:09:11.894287 4595733952 tpu_estimator.py:2160] examples/sec: 2.88878\n",
      "INFO:tensorflow:global_step/sec: 0.090355\n",
      "I1117 14:09:22.961520 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090355\n",
      "INFO:tensorflow:examples/sec: 2.89136\n",
      "I1117 14:09:22.961729 4595733952 tpu_estimator.py:2160] examples/sec: 2.89136\n",
      "INFO:tensorflow:global_step/sec: 0.090111\n",
      "I1117 14:09:34.058941 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090111\n",
      "INFO:tensorflow:examples/sec: 2.88355\n",
      "I1117 14:09:34.059154 4595733952 tpu_estimator.py:2160] examples/sec: 2.88355\n",
      "INFO:tensorflow:global_step/sec: 0.0900983\n",
      "I1117 14:09:45.157935 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900983\n",
      "INFO:tensorflow:examples/sec: 2.88315\n",
      "I1117 14:09:45.158148 4595733952 tpu_estimator.py:2160] examples/sec: 2.88315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0902562\n",
      "I1117 14:09:56.237515 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902562\n",
      "INFO:tensorflow:examples/sec: 2.8882\n",
      "I1117 14:09:56.237743 4595733952 tpu_estimator.py:2160] examples/sec: 2.8882\n",
      "INFO:tensorflow:global_step/sec: 0.0902737\n",
      "I1117 14:10:07.314931 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902737\n",
      "INFO:tensorflow:examples/sec: 2.88876\n",
      "I1117 14:10:07.315159 4595733952 tpu_estimator.py:2160] examples/sec: 2.88876\n",
      "INFO:tensorflow:global_step/sec: 0.0902504\n",
      "I1117 14:10:18.395214 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902504\n",
      "INFO:tensorflow:examples/sec: 2.88801\n",
      "I1117 14:10:18.395429 4595733952 tpu_estimator.py:2160] examples/sec: 2.88801\n",
      "INFO:tensorflow:global_step/sec: 0.0902101\n",
      "I1117 14:10:29.480450 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902101\n",
      "INFO:tensorflow:examples/sec: 2.88672\n",
      "I1117 14:10:29.480674 4595733952 tpu_estimator.py:2160] examples/sec: 2.88672\n",
      "INFO:tensorflow:global_step/sec: 0.0901033\n",
      "I1117 14:10:40.578874 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901033\n",
      "INFO:tensorflow:examples/sec: 2.88331\n",
      "I1117 14:10:40.579098 4595733952 tpu_estimator.py:2160] examples/sec: 2.88331\n",
      "INFO:tensorflow:global_step/sec: 0.0902578\n",
      "I1117 14:10:51.658188 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902578\n",
      "INFO:tensorflow:examples/sec: 2.88825\n",
      "I1117 14:10:51.658405 4595733952 tpu_estimator.py:2160] examples/sec: 2.88825\n",
      "INFO:tensorflow:global_step/sec: 0.0903387\n",
      "I1117 14:11:02.727658 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903387\n",
      "INFO:tensorflow:examples/sec: 2.89084\n",
      "I1117 14:11:02.727883 4595733952 tpu_estimator.py:2160] examples/sec: 2.89084\n",
      "INFO:tensorflow:global_step/sec: 0.0900378\n",
      "I1117 14:11:13.834104 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900378\n",
      "INFO:tensorflow:examples/sec: 2.88121\n",
      "I1117 14:11:13.834323 4595733952 tpu_estimator.py:2160] examples/sec: 2.88121\n",
      "INFO:tensorflow:global_step/sec: 0.0903201\n",
      "I1117 14:11:24.905834 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903201\n",
      "INFO:tensorflow:examples/sec: 2.89024\n",
      "I1117 14:11:24.906063 4595733952 tpu_estimator.py:2160] examples/sec: 2.89024\n",
      "INFO:tensorflow:global_step/sec: 0.0902729\n",
      "I1117 14:11:35.983351 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902729\n",
      "INFO:tensorflow:examples/sec: 2.88873\n",
      "I1117 14:11:35.983571 4595733952 tpu_estimator.py:2160] examples/sec: 2.88873\n",
      "INFO:tensorflow:global_step/sec: 0.0899074\n",
      "I1117 14:11:47.105925 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899074\n",
      "INFO:tensorflow:examples/sec: 2.87704\n",
      "I1117 14:11:47.106153 4595733952 tpu_estimator.py:2160] examples/sec: 2.87704\n",
      "INFO:tensorflow:global_step/sec: 0.0902725\n",
      "I1117 14:11:58.183486 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902725\n",
      "INFO:tensorflow:examples/sec: 2.88872\n",
      "I1117 14:11:58.183857 4595733952 tpu_estimator.py:2160] examples/sec: 2.88872\n",
      "INFO:tensorflow:global_step/sec: 0.0898966\n",
      "I1117 14:12:09.307376 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898966\n",
      "INFO:tensorflow:examples/sec: 2.87669\n",
      "I1117 14:12:09.307591 4595733952 tpu_estimator.py:2160] examples/sec: 2.87669\n",
      "INFO:tensorflow:global_step/sec: 0.089791\n",
      "I1117 14:12:20.444355 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089791\n",
      "INFO:tensorflow:examples/sec: 2.87331\n",
      "I1117 14:12:20.444633 4595733952 tpu_estimator.py:2160] examples/sec: 2.87331\n",
      "INFO:tensorflow:global_step/sec: 0.090524\n",
      "I1117 14:12:31.491137 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090524\n",
      "INFO:tensorflow:examples/sec: 2.89677\n",
      "I1117 14:12:31.491354 4595733952 tpu_estimator.py:2160] examples/sec: 2.89677\n",
      "INFO:tensorflow:global_step/sec: 0.0902883\n",
      "I1117 14:12:42.566766 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902883\n",
      "INFO:tensorflow:examples/sec: 2.88923\n",
      "I1117 14:12:42.566979 4595733952 tpu_estimator.py:2160] examples/sec: 2.88923\n",
      "INFO:tensorflow:global_step/sec: 0.0904734\n",
      "I1117 14:12:53.619751 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904734\n",
      "INFO:tensorflow:examples/sec: 2.89515\n",
      "I1117 14:12:53.619963 4595733952 tpu_estimator.py:2160] examples/sec: 2.89515\n",
      "INFO:tensorflow:global_step/sec: 0.0903956\n",
      "I1117 14:13:04.682227 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903956\n",
      "INFO:tensorflow:examples/sec: 2.89266\n",
      "I1117 14:13:04.682445 4595733952 tpu_estimator.py:2160] examples/sec: 2.89266\n",
      "INFO:tensorflow:global_step/sec: 0.0901793\n",
      "I1117 14:13:15.771246 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901793\n",
      "INFO:tensorflow:examples/sec: 2.88574\n",
      "I1117 14:13:15.771461 4595733952 tpu_estimator.py:2160] examples/sec: 2.88574\n",
      "INFO:tensorflow:global_step/sec: 0.0900985\n",
      "I1117 14:13:26.870218 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900985\n",
      "INFO:tensorflow:examples/sec: 2.88315\n",
      "I1117 14:13:26.870437 4595733952 tpu_estimator.py:2160] examples/sec: 2.88315\n",
      "INFO:tensorflow:global_step/sec: 0.0905112\n",
      "I1117 14:13:37.918576 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905112\n",
      "INFO:tensorflow:examples/sec: 2.89636\n",
      "I1117 14:13:37.918799 4595733952 tpu_estimator.py:2160] examples/sec: 2.89636\n",
      "INFO:tensorflow:global_step/sec: 0.0897286\n",
      "I1117 14:13:49.063287 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897286\n",
      "INFO:tensorflow:examples/sec: 2.87131\n",
      "I1117 14:13:49.063505 4595733952 tpu_estimator.py:2160] examples/sec: 2.87131\n",
      "INFO:tensorflow:global_step/sec: 0.0901932\n",
      "I1117 14:14:00.150590 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901932\n",
      "INFO:tensorflow:examples/sec: 2.88618\n",
      "I1117 14:14:00.150805 4595733952 tpu_estimator.py:2160] examples/sec: 2.88618\n",
      "INFO:tensorflow:global_step/sec: 0.090094\n",
      "I1117 14:14:11.250117 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090094\n",
      "INFO:tensorflow:examples/sec: 2.88301\n",
      "I1117 14:14:11.250332 4595733952 tpu_estimator.py:2160] examples/sec: 2.88301\n",
      "INFO:tensorflow:global_step/sec: 0.0902475\n",
      "I1117 14:14:22.330757 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902475\n",
      "INFO:tensorflow:examples/sec: 2.88792\n",
      "I1117 14:14:22.330987 4595733952 tpu_estimator.py:2160] examples/sec: 2.88792\n",
      "INFO:tensorflow:global_step/sec: 0.0903836\n",
      "I1117 14:14:33.394704 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903836\n",
      "INFO:tensorflow:examples/sec: 2.89228\n",
      "I1117 14:14:33.394919 4595733952 tpu_estimator.py:2160] examples/sec: 2.89228\n",
      "INFO:tensorflow:global_step/sec: 0.0901435\n",
      "I1117 14:14:44.488154 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901435\n",
      "INFO:tensorflow:examples/sec: 2.88459\n",
      "I1117 14:14:44.488378 4595733952 tpu_estimator.py:2160] examples/sec: 2.88459\n",
      "INFO:tensorflow:global_step/sec: 0.0902747\n",
      "I1117 14:14:55.565426 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902747\n",
      "INFO:tensorflow:examples/sec: 2.88879\n",
      "I1117 14:14:55.565641 4595733952 tpu_estimator.py:2160] examples/sec: 2.88879\n",
      "INFO:tensorflow:global_step/sec: 0.0899482\n",
      "I1117 14:15:06.682927 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899482\n",
      "INFO:tensorflow:examples/sec: 2.87834\n",
      "I1117 14:15:06.683176 4595733952 tpu_estimator.py:2160] examples/sec: 2.87834\n",
      "INFO:tensorflow:global_step/sec: 0.0899182\n",
      "I1117 14:15:17.804165 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899182\n",
      "INFO:tensorflow:examples/sec: 2.87738\n",
      "I1117 14:15:17.804389 4595733952 tpu_estimator.py:2160] examples/sec: 2.87738\n",
      "INFO:tensorflow:global_step/sec: 0.0903105\n",
      "I1117 14:15:28.877064 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903105\n",
      "INFO:tensorflow:examples/sec: 2.88994\n",
      "I1117 14:15:28.877280 4595733952 tpu_estimator.py:2160] examples/sec: 2.88994\n",
      "INFO:tensorflow:global_step/sec: 0.0901517\n",
      "I1117 14:15:39.969475 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901517\n",
      "INFO:tensorflow:examples/sec: 2.88486\n",
      "I1117 14:15:39.969689 4595733952 tpu_estimator.py:2160] examples/sec: 2.88486\n",
      "INFO:tensorflow:global_step/sec: 0.0900029\n",
      "I1117 14:15:51.080231 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900029\n",
      "INFO:tensorflow:examples/sec: 2.88009\n",
      "I1117 14:15:51.080453 4595733952 tpu_estimator.py:2160] examples/sec: 2.88009\n",
      "INFO:tensorflow:global_step/sec: 0.090103\n",
      "I1117 14:16:02.178623 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090103\n",
      "INFO:tensorflow:examples/sec: 2.88329\n",
      "I1117 14:16:02.178837 4595733952 tpu_estimator.py:2160] examples/sec: 2.88329\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0899328\n",
      "I1117 14:16:13.298066 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899328\n",
      "INFO:tensorflow:examples/sec: 2.87785\n",
      "I1117 14:16:13.298285 4595733952 tpu_estimator.py:2160] examples/sec: 2.87785\n",
      "INFO:tensorflow:global_step/sec: 0.0894477\n",
      "I1117 14:16:24.477786 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894477\n",
      "INFO:tensorflow:examples/sec: 2.86233\n",
      "I1117 14:16:24.477996 4595733952 tpu_estimator.py:2160] examples/sec: 2.86233\n",
      "INFO:tensorflow:global_step/sec: 0.0899663\n",
      "I1117 14:16:35.593065 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899663\n",
      "INFO:tensorflow:examples/sec: 2.87892\n",
      "I1117 14:16:35.593290 4595733952 tpu_estimator.py:2160] examples/sec: 2.87892\n",
      "INFO:tensorflow:global_step/sec: 0.0902095\n",
      "I1117 14:16:46.678371 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902095\n",
      "INFO:tensorflow:examples/sec: 2.8867\n",
      "I1117 14:16:46.678618 4595733952 tpu_estimator.py:2160] examples/sec: 2.8867\n",
      "INFO:tensorflow:global_step/sec: 0.0897069\n",
      "I1117 14:16:57.825786 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897069\n",
      "INFO:tensorflow:examples/sec: 2.87062\n",
      "I1117 14:16:57.825987 4595733952 tpu_estimator.py:2160] examples/sec: 2.87062\n",
      "INFO:tensorflow:global_step/sec: 0.0901691\n",
      "I1117 14:17:08.916044 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901691\n",
      "INFO:tensorflow:examples/sec: 2.88541\n",
      "I1117 14:17:08.916268 4595733952 tpu_estimator.py:2160] examples/sec: 2.88541\n",
      "INFO:tensorflow:global_step/sec: 0.0901998\n",
      "I1117 14:17:20.002538 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901998\n",
      "INFO:tensorflow:examples/sec: 2.88639\n",
      "I1117 14:17:20.002749 4595733952 tpu_estimator.py:2160] examples/sec: 2.88639\n",
      "INFO:tensorflow:global_step/sec: 0.0904143\n",
      "I1117 14:17:31.062744 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904143\n",
      "INFO:tensorflow:examples/sec: 2.89326\n",
      "I1117 14:17:31.062958 4595733952 tpu_estimator.py:2160] examples/sec: 2.89326\n",
      "INFO:tensorflow:global_step/sec: 0.0902715\n",
      "I1117 14:17:42.140438 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902715\n",
      "INFO:tensorflow:examples/sec: 2.88869\n",
      "I1117 14:17:42.140674 4595733952 tpu_estimator.py:2160] examples/sec: 2.88869\n",
      "INFO:tensorflow:global_step/sec: 0.0902457\n",
      "I1117 14:17:53.221296 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902457\n",
      "INFO:tensorflow:examples/sec: 2.88786\n",
      "I1117 14:17:53.221523 4595733952 tpu_estimator.py:2160] examples/sec: 2.88786\n",
      "INFO:tensorflow:global_step/sec: 0.0902716\n",
      "I1117 14:18:04.298965 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902716\n",
      "INFO:tensorflow:examples/sec: 2.88869\n",
      "I1117 14:18:04.299180 4595733952 tpu_estimator.py:2160] examples/sec: 2.88869\n",
      "INFO:tensorflow:global_step/sec: 0.0902002\n",
      "I1117 14:18:15.385432 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902002\n",
      "INFO:tensorflow:examples/sec: 2.88641\n",
      "I1117 14:18:15.385649 4595733952 tpu_estimator.py:2160] examples/sec: 2.88641\n",
      "INFO:tensorflow:global_step/sec: 0.0907201\n",
      "I1117 14:18:26.408365 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907201\n",
      "INFO:tensorflow:examples/sec: 2.90304\n",
      "I1117 14:18:26.408591 4595733952 tpu_estimator.py:2160] examples/sec: 2.90304\n",
      "INFO:tensorflow:global_step/sec: 0.0900216\n",
      "I1117 14:18:37.516788 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900216\n",
      "INFO:tensorflow:examples/sec: 2.88069\n",
      "I1117 14:18:37.517008 4595733952 tpu_estimator.py:2160] examples/sec: 2.88069\n",
      "INFO:tensorflow:global_step/sec: 0.0902469\n",
      "I1117 14:18:48.597488 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902469\n",
      "INFO:tensorflow:examples/sec: 2.8879\n",
      "I1117 14:18:48.597700 4595733952 tpu_estimator.py:2160] examples/sec: 2.8879\n",
      "INFO:tensorflow:global_step/sec: 0.0899432\n",
      "I1117 14:18:59.715610 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899432\n",
      "INFO:tensorflow:examples/sec: 2.87818\n",
      "I1117 14:18:59.715823 4595733952 tpu_estimator.py:2160] examples/sec: 2.87818\n",
      "INFO:tensorflow:global_step/sec: 0.0902163\n",
      "I1117 14:19:10.800090 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902163\n",
      "INFO:tensorflow:examples/sec: 2.88692\n",
      "I1117 14:19:10.800328 4595733952 tpu_estimator.py:2160] examples/sec: 2.88692\n",
      "INFO:tensorflow:global_step/sec: 0.0902072\n",
      "I1117 14:19:21.885683 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902072\n",
      "INFO:tensorflow:examples/sec: 2.88663\n",
      "I1117 14:19:21.885904 4595733952 tpu_estimator.py:2160] examples/sec: 2.88663\n",
      "INFO:tensorflow:global_step/sec: 0.0902739\n",
      "I1117 14:19:32.964379 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902739\n",
      "INFO:tensorflow:examples/sec: 2.88877\n",
      "I1117 14:19:32.964627 4595733952 tpu_estimator.py:2160] examples/sec: 2.88877\n",
      "INFO:tensorflow:global_step/sec: 0.0906047\n",
      "I1117 14:19:44.000030 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906047\n",
      "INFO:tensorflow:examples/sec: 2.89935\n",
      "I1117 14:19:44.000249 4595733952 tpu_estimator.py:2160] examples/sec: 2.89935\n",
      "INFO:tensorflow:global_step/sec: 0.0902064\n",
      "I1117 14:19:55.085711 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902064\n",
      "INFO:tensorflow:examples/sec: 2.8866\n",
      "I1117 14:19:55.085924 4595733952 tpu_estimator.py:2160] examples/sec: 2.8866\n",
      "INFO:tensorflow:global_step/sec: 0.0892888\n",
      "I1117 14:20:06.285327 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892888\n",
      "INFO:tensorflow:examples/sec: 2.85724\n",
      "I1117 14:20:06.285554 4595733952 tpu_estimator.py:2160] examples/sec: 2.85724\n",
      "INFO:tensorflow:global_step/sec: 0.0890836\n",
      "I1117 14:20:17.510700 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890836\n",
      "INFO:tensorflow:examples/sec: 2.85068\n",
      "I1117 14:20:17.510895 4595733952 tpu_estimator.py:2160] examples/sec: 2.85068\n",
      "INFO:tensorflow:global_step/sec: 0.0895677\n",
      "I1117 14:20:28.675469 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895677\n",
      "INFO:tensorflow:examples/sec: 2.86617\n",
      "I1117 14:20:28.675706 4595733952 tpu_estimator.py:2160] examples/sec: 2.86617\n",
      "INFO:tensorflow:global_step/sec: 0.0897751\n",
      "I1117 14:20:39.814409 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897751\n",
      "INFO:tensorflow:examples/sec: 2.8728\n",
      "I1117 14:20:39.814618 4595733952 tpu_estimator.py:2160] examples/sec: 2.8728\n",
      "INFO:tensorflow:global_step/sec: 0.0901126\n",
      "I1117 14:20:50.911647 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901126\n",
      "INFO:tensorflow:examples/sec: 2.8836\n",
      "I1117 14:20:50.911870 4595733952 tpu_estimator.py:2160] examples/sec: 2.8836\n",
      "INFO:tensorflow:global_step/sec: 0.0902578\n",
      "I1117 14:21:01.991026 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902578\n",
      "INFO:tensorflow:examples/sec: 2.88825\n",
      "I1117 14:21:01.991250 4595733952 tpu_estimator.py:2160] examples/sec: 2.88825\n",
      "INFO:tensorflow:global_step/sec: 0.0897706\n",
      "I1117 14:21:13.130544 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897706\n",
      "INFO:tensorflow:examples/sec: 2.87266\n",
      "I1117 14:21:13.130759 4595733952 tpu_estimator.py:2160] examples/sec: 2.87266\n",
      "INFO:tensorflow:global_step/sec: 0.089953\n",
      "I1117 14:21:24.247431 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089953\n",
      "INFO:tensorflow:examples/sec: 2.8785\n",
      "I1117 14:21:24.247658 4595733952 tpu_estimator.py:2160] examples/sec: 2.8785\n",
      "INFO:tensorflow:global_step/sec: 0.0899378\n",
      "I1117 14:21:35.366220 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899378\n",
      "INFO:tensorflow:examples/sec: 2.87801\n",
      "I1117 14:21:35.366432 4595733952 tpu_estimator.py:2160] examples/sec: 2.87801\n",
      "INFO:tensorflow:global_step/sec: 0.0899726\n",
      "I1117 14:21:46.480725 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899726\n",
      "INFO:tensorflow:examples/sec: 2.87912\n",
      "I1117 14:21:46.480944 4595733952 tpu_estimator.py:2160] examples/sec: 2.87912\n",
      "INFO:tensorflow:global_step/sec: 0.0903854\n",
      "I1117 14:21:57.544466 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903854\n",
      "INFO:tensorflow:examples/sec: 2.89233\n",
      "I1117 14:21:57.544681 4595733952 tpu_estimator.py:2160] examples/sec: 2.89233\n",
      "INFO:tensorflow:global_step/sec: 0.0903392\n",
      "I1117 14:22:08.613853 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903392\n",
      "INFO:tensorflow:examples/sec: 2.89085\n",
      "I1117 14:22:08.614078 4595733952 tpu_estimator.py:2160] examples/sec: 2.89085\n",
      "INFO:tensorflow:global_step/sec: 0.0875967\n",
      "I1117 14:22:20.029795 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875967\n",
      "INFO:tensorflow:examples/sec: 2.8031\n",
      "I1117 14:22:20.030007 4595733952 tpu_estimator.py:2160] examples/sec: 2.8031\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0856031\n",
      "I1117 14:22:31.711642 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0856031\n",
      "INFO:tensorflow:examples/sec: 2.7393\n",
      "I1117 14:22:31.711855 4595733952 tpu_estimator.py:2160] examples/sec: 2.7393\n",
      "INFO:tensorflow:global_step/sec: 0.0903368\n",
      "I1117 14:22:42.781329 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903368\n",
      "INFO:tensorflow:examples/sec: 2.89078\n",
      "I1117 14:22:42.781551 4595733952 tpu_estimator.py:2160] examples/sec: 2.89078\n",
      "INFO:tensorflow:global_step/sec: 0.0904003\n",
      "I1117 14:22:53.843223 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904003\n",
      "INFO:tensorflow:examples/sec: 2.89281\n",
      "I1117 14:22:53.843453 4595733952 tpu_estimator.py:2160] examples/sec: 2.89281\n",
      "INFO:tensorflow:global_step/sec: 0.0898673\n",
      "I1117 14:23:04.970768 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898673\n",
      "INFO:tensorflow:examples/sec: 2.87575\n",
      "I1117 14:23:04.970978 4595733952 tpu_estimator.py:2160] examples/sec: 2.87575\n",
      "INFO:tensorflow:global_step/sec: 0.0895984\n",
      "I1117 14:23:16.131676 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895984\n",
      "INFO:tensorflow:examples/sec: 2.86715\n",
      "I1117 14:23:16.131920 4595733952 tpu_estimator.py:2160] examples/sec: 2.86715\n",
      "INFO:tensorflow:global_step/sec: 0.0888735\n",
      "I1117 14:23:27.383614 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888735\n",
      "INFO:tensorflow:examples/sec: 2.84395\n",
      "I1117 14:23:27.383834 4595733952 tpu_estimator.py:2160] examples/sec: 2.84395\n",
      "INFO:tensorflow:global_step/sec: 0.0899901\n",
      "I1117 14:23:38.495934 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899901\n",
      "INFO:tensorflow:examples/sec: 2.87968\n",
      "I1117 14:23:38.496140 4595733952 tpu_estimator.py:2160] examples/sec: 2.87968\n",
      "INFO:tensorflow:global_step/sec: 0.0898924\n",
      "I1117 14:23:49.620359 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898924\n",
      "INFO:tensorflow:examples/sec: 2.87656\n",
      "I1117 14:23:49.620635 4595733952 tpu_estimator.py:2160] examples/sec: 2.87656\n",
      "INFO:tensorflow:global_step/sec: 0.0902066\n",
      "I1117 14:24:00.706096 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902066\n",
      "INFO:tensorflow:examples/sec: 2.88661\n",
      "I1117 14:24:00.706426 4595733952 tpu_estimator.py:2160] examples/sec: 2.88661\n",
      "INFO:tensorflow:global_step/sec: 0.0900584\n",
      "I1117 14:24:11.809928 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900584\n",
      "INFO:tensorflow:examples/sec: 2.88187\n",
      "I1117 14:24:11.810157 4595733952 tpu_estimator.py:2160] examples/sec: 2.88187\n",
      "INFO:tensorflow:global_step/sec: 0.0907525\n",
      "I1117 14:24:22.828911 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907525\n",
      "INFO:tensorflow:examples/sec: 2.90408\n",
      "I1117 14:24:22.829142 4595733952 tpu_estimator.py:2160] examples/sec: 2.90408\n",
      "INFO:tensorflow:global_step/sec: 0.0903384\n",
      "I1117 14:24:33.898425 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903384\n",
      "INFO:tensorflow:examples/sec: 2.89083\n",
      "I1117 14:24:33.898660 4595733952 tpu_estimator.py:2160] examples/sec: 2.89083\n",
      "INFO:tensorflow:global_step/sec: 0.0904046\n",
      "I1117 14:24:44.959775 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904046\n",
      "INFO:tensorflow:examples/sec: 2.89295\n",
      "I1117 14:24:44.959982 4595733952 tpu_estimator.py:2160] examples/sec: 2.89295\n",
      "INFO:tensorflow:global_step/sec: 0.0903918\n",
      "I1117 14:24:56.022722 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903918\n",
      "INFO:tensorflow:examples/sec: 2.89254\n",
      "I1117 14:24:56.022932 4595733952 tpu_estimator.py:2160] examples/sec: 2.89254\n",
      "INFO:tensorflow:global_step/sec: 0.0897039\n",
      "I1117 14:25:07.170524 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897039\n",
      "INFO:tensorflow:examples/sec: 2.87052\n",
      "I1117 14:25:07.170749 4595733952 tpu_estimator.py:2160] examples/sec: 2.87052\n",
      "INFO:tensorflow:global_step/sec: 0.0895879\n",
      "I1117 14:25:18.332730 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895879\n",
      "INFO:tensorflow:examples/sec: 2.86681\n",
      "I1117 14:25:18.333129 4595733952 tpu_estimator.py:2160] examples/sec: 2.86681\n",
      "INFO:tensorflow:global_step/sec: 0.0904998\n",
      "I1117 14:25:29.382494 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904998\n",
      "INFO:tensorflow:examples/sec: 2.89599\n",
      "I1117 14:25:29.382705 4595733952 tpu_estimator.py:2160] examples/sec: 2.89599\n",
      "INFO:tensorflow:global_step/sec: 0.0902972\n",
      "I1117 14:25:40.457015 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902972\n",
      "INFO:tensorflow:examples/sec: 2.88951\n",
      "I1117 14:25:40.457227 4595733952 tpu_estimator.py:2160] examples/sec: 2.88951\n",
      "INFO:tensorflow:global_step/sec: 0.0903969\n",
      "I1117 14:25:51.519333 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903969\n",
      "INFO:tensorflow:examples/sec: 2.8927\n",
      "I1117 14:25:51.519544 4595733952 tpu_estimator.py:2160] examples/sec: 2.8927\n",
      "INFO:tensorflow:global_step/sec: 0.0904714\n",
      "I1117 14:26:02.572551 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904714\n",
      "INFO:tensorflow:examples/sec: 2.89509\n",
      "I1117 14:26:02.572762 4595733952 tpu_estimator.py:2160] examples/sec: 2.89509\n",
      "INFO:tensorflow:global_step/sec: 0.0901906\n",
      "I1117 14:26:13.660182 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901906\n",
      "INFO:tensorflow:examples/sec: 2.8861\n",
      "I1117 14:26:13.660406 4595733952 tpu_estimator.py:2160] examples/sec: 2.8861\n",
      "INFO:tensorflow:global_step/sec: 0.0903821\n",
      "I1117 14:26:24.724325 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903821\n",
      "INFO:tensorflow:examples/sec: 2.89223\n",
      "I1117 14:26:24.724552 4595733952 tpu_estimator.py:2160] examples/sec: 2.89223\n",
      "INFO:tensorflow:global_step/sec: 0.0903272\n",
      "I1117 14:26:35.795176 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903272\n",
      "INFO:tensorflow:examples/sec: 2.89047\n",
      "I1117 14:26:35.795389 4595733952 tpu_estimator.py:2160] examples/sec: 2.89047\n",
      "INFO:tensorflow:global_step/sec: 0.0902973\n",
      "I1117 14:26:46.869707 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902973\n",
      "INFO:tensorflow:examples/sec: 2.88951\n",
      "I1117 14:26:46.869937 4595733952 tpu_estimator.py:2160] examples/sec: 2.88951\n",
      "INFO:tensorflow:global_step/sec: 0.0903373\n",
      "I1117 14:26:57.939326 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903373\n",
      "INFO:tensorflow:examples/sec: 2.89079\n",
      "I1117 14:26:57.939538 4595733952 tpu_estimator.py:2160] examples/sec: 2.89079\n",
      "INFO:tensorflow:global_step/sec: 0.0901397\n",
      "I1117 14:27:09.033244 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901397\n",
      "INFO:tensorflow:examples/sec: 2.88447\n",
      "I1117 14:27:09.033468 4595733952 tpu_estimator.py:2160] examples/sec: 2.88447\n",
      "INFO:tensorflow:global_step/sec: 0.0903779\n",
      "I1117 14:27:20.097862 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903779\n",
      "INFO:tensorflow:examples/sec: 2.89209\n",
      "I1117 14:27:20.098081 4595733952 tpu_estimator.py:2160] examples/sec: 2.89209\n",
      "INFO:tensorflow:global_step/sec: 0.0904758\n",
      "I1117 14:27:31.150558 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904758\n",
      "INFO:tensorflow:examples/sec: 2.89522\n",
      "I1117 14:27:31.150764 4595733952 tpu_estimator.py:2160] examples/sec: 2.89522\n",
      "INFO:tensorflow:global_step/sec: 0.0905086\n",
      "I1117 14:27:42.199216 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905086\n",
      "INFO:tensorflow:examples/sec: 2.89627\n",
      "I1117 14:27:42.199427 4595733952 tpu_estimator.py:2160] examples/sec: 2.89627\n",
      "INFO:tensorflow:global_step/sec: 0.0903369\n",
      "I1117 14:27:53.268890 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903369\n",
      "INFO:tensorflow:examples/sec: 2.89078\n",
      "I1117 14:27:53.269104 4595733952 tpu_estimator.py:2160] examples/sec: 2.89078\n",
      "INFO:tensorflow:global_step/sec: 0.0899517\n",
      "I1117 14:28:04.385980 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899517\n",
      "INFO:tensorflow:examples/sec: 2.87846\n",
      "I1117 14:28:04.386232 4595733952 tpu_estimator.py:2160] examples/sec: 2.87846\n",
      "INFO:tensorflow:global_step/sec: 0.090087\n",
      "I1117 14:28:15.486351 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090087\n",
      "INFO:tensorflow:examples/sec: 2.88278\n",
      "I1117 14:28:15.486567 4595733952 tpu_estimator.py:2160] examples/sec: 2.88278\n",
      "INFO:tensorflow:global_step/sec: 0.0902195\n",
      "I1117 14:28:26.570425 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902195\n",
      "INFO:tensorflow:examples/sec: 2.88703\n",
      "I1117 14:28:26.570636 4595733952 tpu_estimator.py:2160] examples/sec: 2.88703\n",
      "INFO:tensorflow:global_step/sec: 0.089727\n",
      "I1117 14:28:37.715337 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089727\n",
      "INFO:tensorflow:examples/sec: 2.87126\n",
      "I1117 14:28:37.715552 4595733952 tpu_estimator.py:2160] examples/sec: 2.87126\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0901016\n",
      "I1117 14:28:48.813920 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901016\n",
      "INFO:tensorflow:examples/sec: 2.88325\n",
      "I1117 14:28:48.814129 4595733952 tpu_estimator.py:2160] examples/sec: 2.88325\n",
      "INFO:tensorflow:global_step/sec: 0.0900996\n",
      "I1117 14:28:59.912759 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900996\n",
      "INFO:tensorflow:examples/sec: 2.88319\n",
      "I1117 14:28:59.913002 4595733952 tpu_estimator.py:2160] examples/sec: 2.88319\n",
      "INFO:tensorflow:global_step/sec: 0.0903536\n",
      "I1117 14:29:10.980398 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903536\n",
      "INFO:tensorflow:examples/sec: 2.89131\n",
      "I1117 14:29:10.980611 4595733952 tpu_estimator.py:2160] examples/sec: 2.89131\n",
      "INFO:tensorflow:global_step/sec: 0.0902442\n",
      "I1117 14:29:22.061485 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902442\n",
      "INFO:tensorflow:examples/sec: 2.88782\n",
      "I1117 14:29:22.061717 4595733952 tpu_estimator.py:2160] examples/sec: 2.88782\n",
      "INFO:tensorflow:global_step/sec: 0.0904031\n",
      "I1117 14:29:33.122982 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904031\n",
      "INFO:tensorflow:examples/sec: 2.8929\n",
      "I1117 14:29:33.123199 4595733952 tpu_estimator.py:2160] examples/sec: 2.8929\n",
      "INFO:tensorflow:global_step/sec: 0.0895809\n",
      "I1117 14:29:44.286092 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895809\n",
      "INFO:tensorflow:examples/sec: 2.86659\n",
      "I1117 14:29:44.286318 4595733952 tpu_estimator.py:2160] examples/sec: 2.86659\n",
      "INFO:tensorflow:global_step/sec: 0.0906661\n",
      "I1117 14:29:55.315568 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906661\n",
      "INFO:tensorflow:examples/sec: 2.90132\n",
      "I1117 14:29:55.315794 4595733952 tpu_estimator.py:2160] examples/sec: 2.90132\n",
      "INFO:tensorflow:global_step/sec: 0.0905435\n",
      "I1117 14:30:06.359984 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905435\n",
      "INFO:tensorflow:examples/sec: 2.89739\n",
      "I1117 14:30:06.360207 4595733952 tpu_estimator.py:2160] examples/sec: 2.89739\n",
      "INFO:tensorflow:global_step/sec: 0.0904739\n",
      "I1117 14:30:17.412890 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904739\n",
      "INFO:tensorflow:examples/sec: 2.89517\n",
      "I1117 14:30:17.413106 4595733952 tpu_estimator.py:2160] examples/sec: 2.89517\n",
      "INFO:tensorflow:global_step/sec: 0.0903647\n",
      "I1117 14:30:28.479159 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903647\n",
      "INFO:tensorflow:examples/sec: 2.89167\n",
      "I1117 14:30:28.479365 4595733952 tpu_estimator.py:2160] examples/sec: 2.89167\n",
      "INFO:tensorflow:global_step/sec: 0.0904141\n",
      "I1117 14:30:39.539385 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904141\n",
      "INFO:tensorflow:examples/sec: 2.89325\n",
      "I1117 14:30:39.539599 4595733952 tpu_estimator.py:2160] examples/sec: 2.89325\n",
      "INFO:tensorflow:global_step/sec: 0.0903008\n",
      "I1117 14:30:50.613479 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903008\n",
      "INFO:tensorflow:examples/sec: 2.88963\n",
      "I1117 14:30:50.613698 4595733952 tpu_estimator.py:2160] examples/sec: 2.88963\n",
      "INFO:tensorflow:global_step/sec: 0.0904686\n",
      "I1117 14:31:01.667054 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904686\n",
      "INFO:tensorflow:examples/sec: 2.895\n",
      "I1117 14:31:01.667280 4595733952 tpu_estimator.py:2160] examples/sec: 2.895\n",
      "INFO:tensorflow:global_step/sec: 0.0902571\n",
      "I1117 14:31:12.746515 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902571\n",
      "INFO:tensorflow:examples/sec: 2.88823\n",
      "I1117 14:31:12.746755 4595733952 tpu_estimator.py:2160] examples/sec: 2.88823\n",
      "INFO:tensorflow:global_step/sec: 0.0904896\n",
      "I1117 14:31:23.797502 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904896\n",
      "INFO:tensorflow:examples/sec: 2.89567\n",
      "I1117 14:31:23.797719 4595733952 tpu_estimator.py:2160] examples/sec: 2.89567\n",
      "INFO:tensorflow:global_step/sec: 0.0901588\n",
      "I1117 14:31:34.889050 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901588\n",
      "INFO:tensorflow:examples/sec: 2.88508\n",
      "I1117 14:31:34.889266 4595733952 tpu_estimator.py:2160] examples/sec: 2.88508\n",
      "INFO:tensorflow:global_step/sec: 0.090148\n",
      "I1117 14:31:45.981914 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090148\n",
      "INFO:tensorflow:examples/sec: 2.88474\n",
      "I1117 14:31:45.982129 4595733952 tpu_estimator.py:2160] examples/sec: 2.88474\n",
      "INFO:tensorflow:global_step/sec: 0.0903018\n",
      "I1117 14:31:57.055889 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903018\n",
      "INFO:tensorflow:examples/sec: 2.88966\n",
      "I1117 14:31:57.056101 4595733952 tpu_estimator.py:2160] examples/sec: 2.88966\n",
      "INFO:tensorflow:global_step/sec: 0.0903427\n",
      "I1117 14:32:08.124886 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903427\n",
      "INFO:tensorflow:examples/sec: 2.89097\n",
      "I1117 14:32:08.125198 4595733952 tpu_estimator.py:2160] examples/sec: 2.89097\n",
      "INFO:tensorflow:global_step/sec: 0.0904453\n",
      "I1117 14:32:19.181260 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904453\n",
      "INFO:tensorflow:examples/sec: 2.89425\n",
      "I1117 14:32:19.181465 4595733952 tpu_estimator.py:2160] examples/sec: 2.89425\n",
      "INFO:tensorflow:global_step/sec: 0.0902424\n",
      "I1117 14:32:30.262526 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902424\n",
      "INFO:tensorflow:examples/sec: 2.88776\n",
      "I1117 14:32:30.262739 4595733952 tpu_estimator.py:2160] examples/sec: 2.88776\n",
      "INFO:tensorflow:global_step/sec: 0.0907643\n",
      "I1117 14:32:41.280076 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907643\n",
      "INFO:tensorflow:examples/sec: 2.90446\n",
      "I1117 14:32:41.280287 4595733952 tpu_estimator.py:2160] examples/sec: 2.90446\n",
      "INFO:tensorflow:global_step/sec: 0.0907117\n",
      "I1117 14:32:52.304013 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907117\n",
      "INFO:tensorflow:examples/sec: 2.90277\n",
      "I1117 14:32:52.304230 4595733952 tpu_estimator.py:2160] examples/sec: 2.90277\n",
      "INFO:tensorflow:global_step/sec: 0.0906722\n",
      "I1117 14:33:03.332766 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906722\n",
      "INFO:tensorflow:examples/sec: 2.90151\n",
      "I1117 14:33:03.333002 4595733952 tpu_estimator.py:2160] examples/sec: 2.90151\n",
      "INFO:tensorflow:global_step/sec: 0.090055\n",
      "I1117 14:33:14.437097 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090055\n",
      "INFO:tensorflow:examples/sec: 2.88176\n",
      "I1117 14:33:14.437313 4595733952 tpu_estimator.py:2160] examples/sec: 2.88176\n",
      "INFO:tensorflow:global_step/sec: 0.0906874\n",
      "I1117 14:33:25.463979 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906874\n",
      "INFO:tensorflow:examples/sec: 2.902\n",
      "I1117 14:33:25.464218 4595733952 tpu_estimator.py:2160] examples/sec: 2.902\n",
      "INFO:tensorflow:global_step/sec: 0.0905068\n",
      "I1117 14:33:36.512856 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905068\n",
      "INFO:tensorflow:examples/sec: 2.89622\n",
      "I1117 14:33:36.513071 4595733952 tpu_estimator.py:2160] examples/sec: 2.89622\n",
      "INFO:tensorflow:global_step/sec: 0.090506\n",
      "I1117 14:33:47.561853 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090506\n",
      "INFO:tensorflow:examples/sec: 2.89619\n",
      "I1117 14:33:47.562064 4595733952 tpu_estimator.py:2160] examples/sec: 2.89619\n",
      "INFO:tensorflow:global_step/sec: 0.0902951\n",
      "I1117 14:33:58.636661 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902951\n",
      "INFO:tensorflow:examples/sec: 2.88944\n",
      "I1117 14:33:58.636894 4595733952 tpu_estimator.py:2160] examples/sec: 2.88944\n",
      "INFO:tensorflow:global_step/sec: 0.0903603\n",
      "I1117 14:34:09.703450 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903603\n",
      "INFO:tensorflow:examples/sec: 2.89153\n",
      "I1117 14:34:09.703666 4595733952 tpu_estimator.py:2160] examples/sec: 2.89153\n",
      "INFO:tensorflow:global_step/sec: 0.0898859\n",
      "I1117 14:34:20.828637 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898859\n",
      "INFO:tensorflow:examples/sec: 2.87635\n",
      "I1117 14:34:20.828847 4595733952 tpu_estimator.py:2160] examples/sec: 2.87635\n",
      "INFO:tensorflow:global_step/sec: 0.0900436\n",
      "I1117 14:34:31.934406 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900436\n",
      "INFO:tensorflow:examples/sec: 2.88139\n",
      "I1117 14:34:31.934620 4595733952 tpu_estimator.py:2160] examples/sec: 2.88139\n",
      "INFO:tensorflow:global_step/sec: 0.0903941\n",
      "I1117 14:34:42.997077 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903941\n",
      "INFO:tensorflow:examples/sec: 2.89261\n",
      "I1117 14:34:42.997291 4595733952 tpu_estimator.py:2160] examples/sec: 2.89261\n",
      "INFO:tensorflow:global_step/sec: 0.0895722\n",
      "I1117 14:34:54.161268 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895722\n",
      "INFO:tensorflow:examples/sec: 2.86631\n",
      "I1117 14:34:54.161494 4595733952 tpu_estimator.py:2160] examples/sec: 2.86631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.090225\n",
      "I1117 14:35:05.244673 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090225\n",
      "INFO:tensorflow:examples/sec: 2.8872\n",
      "I1117 14:35:05.244904 4595733952 tpu_estimator.py:2160] examples/sec: 2.8872\n",
      "INFO:tensorflow:global_step/sec: 0.0898957\n",
      "I1117 14:35:16.368670 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898957\n",
      "INFO:tensorflow:examples/sec: 2.87666\n",
      "I1117 14:35:16.368893 4595733952 tpu_estimator.py:2160] examples/sec: 2.87666\n",
      "INFO:tensorflow:global_step/sec: 0.0901816\n",
      "I1117 14:35:27.457396 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901816\n",
      "INFO:tensorflow:examples/sec: 2.88581\n",
      "I1117 14:35:27.457615 4595733952 tpu_estimator.py:2160] examples/sec: 2.88581\n",
      "INFO:tensorflow:global_step/sec: 0.0901402\n",
      "I1117 14:35:38.551234 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901402\n",
      "INFO:tensorflow:examples/sec: 2.88449\n",
      "I1117 14:35:38.551481 4595733952 tpu_estimator.py:2160] examples/sec: 2.88449\n",
      "INFO:tensorflow:global_step/sec: 0.0904446\n",
      "I1117 14:35:49.607733 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904446\n",
      "INFO:tensorflow:examples/sec: 2.89423\n",
      "I1117 14:35:49.607994 4595733952 tpu_estimator.py:2160] examples/sec: 2.89423\n",
      "INFO:tensorflow:global_step/sec: 0.0900787\n",
      "I1117 14:36:00.709151 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900787\n",
      "INFO:tensorflow:examples/sec: 2.88252\n",
      "I1117 14:36:00.709377 4595733952 tpu_estimator.py:2160] examples/sec: 2.88252\n",
      "INFO:tensorflow:global_step/sec: 0.0901695\n",
      "I1117 14:36:11.799360 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901695\n",
      "INFO:tensorflow:examples/sec: 2.88542\n",
      "I1117 14:36:11.799582 4595733952 tpu_estimator.py:2160] examples/sec: 2.88542\n",
      "INFO:tensorflow:global_step/sec: 0.0904543\n",
      "I1117 14:36:22.854663 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904543\n",
      "INFO:tensorflow:examples/sec: 2.89454\n",
      "I1117 14:36:22.854949 4595733952 tpu_estimator.py:2160] examples/sec: 2.89454\n",
      "INFO:tensorflow:global_step/sec: 0.0905373\n",
      "I1117 14:36:33.899837 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905373\n",
      "INFO:tensorflow:examples/sec: 2.89719\n",
      "I1117 14:36:33.900067 4595733952 tpu_estimator.py:2160] examples/sec: 2.89719\n",
      "INFO:tensorflow:global_step/sec: 0.0900319\n",
      "I1117 14:36:45.007004 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900319\n",
      "INFO:tensorflow:examples/sec: 2.88102\n",
      "I1117 14:36:45.007234 4595733952 tpu_estimator.py:2160] examples/sec: 2.88102\n",
      "INFO:tensorflow:global_step/sec: 0.0905371\n",
      "I1117 14:36:56.052216 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905371\n",
      "INFO:tensorflow:examples/sec: 2.89719\n",
      "I1117 14:36:56.052431 4595733952 tpu_estimator.py:2160] examples/sec: 2.89719\n",
      "INFO:tensorflow:global_step/sec: 0.0905431\n",
      "I1117 14:37:07.096666 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905431\n",
      "INFO:tensorflow:examples/sec: 2.89738\n",
      "I1117 14:37:07.096892 4595733952 tpu_estimator.py:2160] examples/sec: 2.89738\n",
      "INFO:tensorflow:global_step/sec: 0.090064\n",
      "I1117 14:37:18.199878 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090064\n",
      "INFO:tensorflow:examples/sec: 2.88205\n",
      "I1117 14:37:18.200257 4595733952 tpu_estimator.py:2160] examples/sec: 2.88205\n",
      "INFO:tensorflow:global_step/sec: 0.0904588\n",
      "I1117 14:37:29.254657 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904588\n",
      "INFO:tensorflow:examples/sec: 2.89468\n",
      "I1117 14:37:29.254869 4595733952 tpu_estimator.py:2160] examples/sec: 2.89468\n",
      "INFO:tensorflow:global_step/sec: 0.0902989\n",
      "I1117 14:37:40.328986 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902989\n",
      "INFO:tensorflow:examples/sec: 2.88956\n",
      "I1117 14:37:40.329201 4595733952 tpu_estimator.py:2160] examples/sec: 2.88956\n",
      "INFO:tensorflow:global_step/sec: 0.0903738\n",
      "I1117 14:37:51.394109 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903738\n",
      "INFO:tensorflow:examples/sec: 2.89196\n",
      "I1117 14:37:51.394312 4595733952 tpu_estimator.py:2160] examples/sec: 2.89196\n",
      "INFO:tensorflow:global_step/sec: 0.0900838\n",
      "I1117 14:38:02.494892 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900838\n",
      "INFO:tensorflow:examples/sec: 2.88268\n",
      "I1117 14:38:02.495100 4595733952 tpu_estimator.py:2160] examples/sec: 2.88268\n",
      "INFO:tensorflow:global_step/sec: 0.0901828\n",
      "I1117 14:38:13.583509 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901828\n",
      "INFO:tensorflow:examples/sec: 2.88585\n",
      "I1117 14:38:13.583733 4595733952 tpu_estimator.py:2160] examples/sec: 2.88585\n",
      "INFO:tensorflow:global_step/sec: 0.0905082\n",
      "I1117 14:38:24.632207 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905082\n",
      "INFO:tensorflow:examples/sec: 2.89626\n",
      "I1117 14:38:24.632426 4595733952 tpu_estimator.py:2160] examples/sec: 2.89626\n",
      "INFO:tensorflow:global_step/sec: 0.0900575\n",
      "I1117 14:38:35.736217 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900575\n",
      "INFO:tensorflow:examples/sec: 2.88184\n",
      "I1117 14:38:35.736437 4595733952 tpu_estimator.py:2160] examples/sec: 2.88184\n",
      "INFO:tensorflow:global_step/sec: 0.0904028\n",
      "I1117 14:38:46.797827 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904028\n",
      "INFO:tensorflow:examples/sec: 2.89289\n",
      "I1117 14:38:46.798068 4595733952 tpu_estimator.py:2160] examples/sec: 2.89289\n",
      "INFO:tensorflow:global_step/sec: 0.0902712\n",
      "I1117 14:38:57.875543 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902712\n",
      "INFO:tensorflow:examples/sec: 2.88868\n",
      "I1117 14:38:57.875753 4595733952 tpu_estimator.py:2160] examples/sec: 2.88868\n",
      "INFO:tensorflow:global_step/sec: 0.0902349\n",
      "I1117 14:39:08.957751 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902349\n",
      "INFO:tensorflow:examples/sec: 2.88752\n",
      "I1117 14:39:08.957973 4595733952 tpu_estimator.py:2160] examples/sec: 2.88752\n",
      "INFO:tensorflow:global_step/sec: 0.0902726\n",
      "I1117 14:39:20.035296 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902726\n",
      "INFO:tensorflow:examples/sec: 2.88872\n",
      "I1117 14:39:20.035506 4595733952 tpu_estimator.py:2160] examples/sec: 2.88872\n",
      "INFO:tensorflow:global_step/sec: 0.0900528\n",
      "I1117 14:39:31.139897 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900528\n",
      "INFO:tensorflow:examples/sec: 2.88169\n",
      "I1117 14:39:31.140119 4595733952 tpu_estimator.py:2160] examples/sec: 2.88169\n",
      "INFO:tensorflow:global_step/sec: 0.0905226\n",
      "I1117 14:39:42.186868 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905226\n",
      "INFO:tensorflow:examples/sec: 2.89672\n",
      "I1117 14:39:42.187113 4595733952 tpu_estimator.py:2160] examples/sec: 2.89672\n",
      "INFO:tensorflow:global_step/sec: 0.0904873\n",
      "I1117 14:39:53.238142 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904873\n",
      "INFO:tensorflow:examples/sec: 2.89559\n",
      "I1117 14:39:53.238386 4595733952 tpu_estimator.py:2160] examples/sec: 2.89559\n",
      "INFO:tensorflow:global_step/sec: 0.0901283\n",
      "I1117 14:40:04.333434 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901283\n",
      "INFO:tensorflow:examples/sec: 2.88411\n",
      "I1117 14:40:04.333663 4595733952 tpu_estimator.py:2160] examples/sec: 2.88411\n",
      "INFO:tensorflow:global_step/sec: 0.0902901\n",
      "I1117 14:40:15.408837 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902901\n",
      "INFO:tensorflow:examples/sec: 2.88928\n",
      "I1117 14:40:15.409052 4595733952 tpu_estimator.py:2160] examples/sec: 2.88928\n",
      "INFO:tensorflow:global_step/sec: 0.0898424\n",
      "I1117 14:40:26.539427 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898424\n",
      "INFO:tensorflow:examples/sec: 2.87496\n",
      "I1117 14:40:26.539639 4595733952 tpu_estimator.py:2160] examples/sec: 2.87496\n",
      "INFO:tensorflow:global_step/sec: 0.0906692\n",
      "I1117 14:40:37.568543 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906692\n",
      "INFO:tensorflow:examples/sec: 2.90141\n",
      "I1117 14:40:37.568767 4595733952 tpu_estimator.py:2160] examples/sec: 2.90141\n",
      "INFO:tensorflow:global_step/sec: 0.0903738\n",
      "I1117 14:40:48.633704 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903738\n",
      "INFO:tensorflow:examples/sec: 2.89196\n",
      "I1117 14:40:48.633926 4595733952 tpu_estimator.py:2160] examples/sec: 2.89196\n",
      "INFO:tensorflow:global_step/sec: 0.0902387\n",
      "I1117 14:40:59.715418 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902387\n",
      "INFO:tensorflow:examples/sec: 2.88764\n",
      "I1117 14:40:59.715632 4595733952 tpu_estimator.py:2160] examples/sec: 2.88764\n",
      "INFO:tensorflow:global_step/sec: 0.0901797\n",
      "I1117 14:41:10.804401 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901797\n",
      "INFO:tensorflow:examples/sec: 2.88575\n",
      "I1117 14:41:10.804617 4595733952 tpu_estimator.py:2160] examples/sec: 2.88575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0906549\n",
      "I1117 14:41:21.835252 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906549\n",
      "INFO:tensorflow:examples/sec: 2.90096\n",
      "I1117 14:41:21.835479 4595733952 tpu_estimator.py:2160] examples/sec: 2.90096\n",
      "INFO:tensorflow:global_step/sec: 0.0899695\n",
      "I1117 14:41:32.950114 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899695\n",
      "INFO:tensorflow:examples/sec: 2.87902\n",
      "I1117 14:41:32.950322 4595733952 tpu_estimator.py:2160] examples/sec: 2.87902\n",
      "INFO:tensorflow:global_step/sec: 0.0904399\n",
      "I1117 14:41:44.007192 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904399\n",
      "INFO:tensorflow:examples/sec: 2.89408\n",
      "I1117 14:41:44.007428 4595733952 tpu_estimator.py:2160] examples/sec: 2.89408\n",
      "INFO:tensorflow:global_step/sec: 0.0903874\n",
      "I1117 14:41:55.070694 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903874\n",
      "INFO:tensorflow:examples/sec: 2.8924\n",
      "I1117 14:41:55.070917 4595733952 tpu_estimator.py:2160] examples/sec: 2.8924\n",
      "INFO:tensorflow:global_step/sec: 0.0906146\n",
      "I1117 14:42:06.106434 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906146\n",
      "INFO:tensorflow:examples/sec: 2.89967\n",
      "I1117 14:42:06.106654 4595733952 tpu_estimator.py:2160] examples/sec: 2.89967\n",
      "INFO:tensorflow:global_step/sec: 0.0902579\n",
      "I1117 14:42:17.185788 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902579\n",
      "INFO:tensorflow:examples/sec: 2.88825\n",
      "I1117 14:42:17.185999 4595733952 tpu_estimator.py:2160] examples/sec: 2.88825\n",
      "INFO:tensorflow:global_step/sec: 0.0903772\n",
      "I1117 14:42:28.250528 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903772\n",
      "INFO:tensorflow:examples/sec: 2.89207\n",
      "I1117 14:42:28.250928 4595733952 tpu_estimator.py:2160] examples/sec: 2.89207\n",
      "INFO:tensorflow:global_step/sec: 0.0903963\n",
      "I1117 14:42:39.312943 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903963\n",
      "INFO:tensorflow:examples/sec: 2.89268\n",
      "I1117 14:42:39.313163 4595733952 tpu_estimator.py:2160] examples/sec: 2.89268\n",
      "INFO:tensorflow:global_step/sec: 0.0906623\n",
      "I1117 14:42:50.342889 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906623\n",
      "INFO:tensorflow:examples/sec: 2.90119\n",
      "I1117 14:42:50.343116 4595733952 tpu_estimator.py:2160] examples/sec: 2.90119\n",
      "INFO:tensorflow:global_step/sec: 0.0904949\n",
      "I1117 14:43:01.393229 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904949\n",
      "INFO:tensorflow:examples/sec: 2.89584\n",
      "I1117 14:43:01.393446 4595733952 tpu_estimator.py:2160] examples/sec: 2.89584\n",
      "INFO:tensorflow:global_step/sec: 0.0906931\n",
      "I1117 14:43:12.419434 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906931\n",
      "INFO:tensorflow:examples/sec: 2.90218\n",
      "I1117 14:43:12.419660 4595733952 tpu_estimator.py:2160] examples/sec: 2.90218\n",
      "INFO:tensorflow:global_step/sec: 0.090549\n",
      "I1117 14:43:23.463165 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090549\n",
      "INFO:tensorflow:examples/sec: 2.89757\n",
      "I1117 14:43:23.463375 4595733952 tpu_estimator.py:2160] examples/sec: 2.89757\n",
      "INFO:tensorflow:global_step/sec: 0.0901667\n",
      "I1117 14:43:34.553733 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901667\n",
      "INFO:tensorflow:examples/sec: 2.88533\n",
      "I1117 14:43:34.553952 4595733952 tpu_estimator.py:2160] examples/sec: 2.88533\n",
      "INFO:tensorflow:global_step/sec: 0.0900771\n",
      "I1117 14:43:45.655348 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900771\n",
      "INFO:tensorflow:examples/sec: 2.88247\n",
      "I1117 14:43:45.655561 4595733952 tpu_estimator.py:2160] examples/sec: 2.88247\n",
      "INFO:tensorflow:global_step/sec: 0.0905443\n",
      "I1117 14:43:56.699671 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905443\n",
      "INFO:tensorflow:examples/sec: 2.89742\n",
      "I1117 14:43:56.699893 4595733952 tpu_estimator.py:2160] examples/sec: 2.89742\n",
      "INFO:tensorflow:global_step/sec: 0.0906493\n",
      "I1117 14:44:07.731186 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906493\n",
      "INFO:tensorflow:examples/sec: 2.90078\n",
      "I1117 14:44:07.731401 4595733952 tpu_estimator.py:2160] examples/sec: 2.90078\n",
      "INFO:tensorflow:global_step/sec: 0.0902884\n",
      "I1117 14:44:18.806804 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902884\n",
      "INFO:tensorflow:examples/sec: 2.88923\n",
      "I1117 14:44:18.807016 4595733952 tpu_estimator.py:2160] examples/sec: 2.88923\n",
      "INFO:tensorflow:global_step/sec: 0.0905074\n",
      "I1117 14:44:29.855624 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905074\n",
      "INFO:tensorflow:examples/sec: 2.89624\n",
      "I1117 14:44:29.855839 4595733952 tpu_estimator.py:2160] examples/sec: 2.89624\n",
      "INFO:tensorflow:global_step/sec: 0.0905017\n",
      "I1117 14:44:40.905146 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905017\n",
      "INFO:tensorflow:examples/sec: 2.89605\n",
      "I1117 14:44:40.905357 4595733952 tpu_estimator.py:2160] examples/sec: 2.89605\n",
      "INFO:tensorflow:global_step/sec: 0.0901712\n",
      "I1117 14:44:51.995159 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901712\n",
      "INFO:tensorflow:examples/sec: 2.88548\n",
      "I1117 14:44:51.995375 4595733952 tpu_estimator.py:2160] examples/sec: 2.88548\n",
      "INFO:tensorflow:global_step/sec: 0.0903458\n",
      "I1117 14:45:03.063754 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903458\n",
      "INFO:tensorflow:examples/sec: 2.89107\n",
      "I1117 14:45:03.063987 4595733952 tpu_estimator.py:2160] examples/sec: 2.89107\n",
      "INFO:tensorflow:global_step/sec: 0.0900045\n",
      "I1117 14:45:14.174298 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900045\n",
      "INFO:tensorflow:examples/sec: 2.88015\n",
      "I1117 14:45:14.174516 4595733952 tpu_estimator.py:2160] examples/sec: 2.88015\n",
      "INFO:tensorflow:global_step/sec: 0.0902163\n",
      "I1117 14:45:25.258778 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902163\n",
      "INFO:tensorflow:examples/sec: 2.88692\n",
      "I1117 14:45:25.259003 4595733952 tpu_estimator.py:2160] examples/sec: 2.88692\n",
      "INFO:tensorflow:global_step/sec: 0.0906004\n",
      "I1117 14:45:36.296257 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906004\n",
      "INFO:tensorflow:examples/sec: 2.89921\n",
      "I1117 14:45:36.296473 4595733952 tpu_estimator.py:2160] examples/sec: 2.89921\n",
      "INFO:tensorflow:global_step/sec: 0.0905395\n",
      "I1117 14:45:47.341157 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905395\n",
      "INFO:tensorflow:examples/sec: 2.89727\n",
      "I1117 14:45:47.341373 4595733952 tpu_estimator.py:2160] examples/sec: 2.89727\n",
      "INFO:tensorflow:global_step/sec: 0.0906046\n",
      "I1117 14:45:58.378130 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906046\n",
      "INFO:tensorflow:examples/sec: 2.89935\n",
      "I1117 14:45:58.378355 4595733952 tpu_estimator.py:2160] examples/sec: 2.89935\n",
      "INFO:tensorflow:global_step/sec: 0.0906846\n",
      "I1117 14:46:09.405354 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906846\n",
      "INFO:tensorflow:examples/sec: 2.90191\n",
      "I1117 14:46:09.405570 4595733952 tpu_estimator.py:2160] examples/sec: 2.90191\n",
      "INFO:tensorflow:global_step/sec: 0.0904982\n",
      "I1117 14:46:20.455307 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904982\n",
      "INFO:tensorflow:examples/sec: 2.89594\n",
      "I1117 14:46:20.455542 4595733952 tpu_estimator.py:2160] examples/sec: 2.89594\n",
      "INFO:tensorflow:global_step/sec: 0.0896886\n",
      "I1117 14:46:31.604995 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896886\n",
      "INFO:tensorflow:examples/sec: 2.87003\n",
      "I1117 14:46:31.605208 4595733952 tpu_estimator.py:2160] examples/sec: 2.87003\n",
      "INFO:tensorflow:global_step/sec: 0.0899034\n",
      "I1117 14:46:42.728044 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899034\n",
      "INFO:tensorflow:examples/sec: 2.87691\n",
      "I1117 14:46:42.728266 4595733952 tpu_estimator.py:2160] examples/sec: 2.87691\n",
      "INFO:tensorflow:global_step/sec: 0.0903793\n",
      "I1117 14:46:53.792521 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903793\n",
      "INFO:tensorflow:examples/sec: 2.89214\n",
      "I1117 14:46:53.792737 4595733952 tpu_estimator.py:2160] examples/sec: 2.89214\n",
      "INFO:tensorflow:global_step/sec: 0.090539\n",
      "I1117 14:47:04.837512 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090539\n",
      "INFO:tensorflow:examples/sec: 2.89725\n",
      "I1117 14:47:04.837890 4595733952 tpu_estimator.py:2160] examples/sec: 2.89725\n",
      "INFO:tensorflow:global_step/sec: 0.0909289\n",
      "I1117 14:47:15.835083 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909289\n",
      "INFO:tensorflow:examples/sec: 2.90973\n",
      "I1117 14:47:15.835300 4595733952 tpu_estimator.py:2160] examples/sec: 2.90973\n",
      "INFO:tensorflow:global_step/sec: 0.0905039\n",
      "I1117 14:47:26.884348 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905039\n",
      "INFO:tensorflow:examples/sec: 2.89612\n",
      "I1117 14:47:26.884573 4595733952 tpu_estimator.py:2160] examples/sec: 2.89612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0903026\n",
      "I1117 14:47:37.958223 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903026\n",
      "INFO:tensorflow:examples/sec: 2.88968\n",
      "I1117 14:47:37.958448 4595733952 tpu_estimator.py:2160] examples/sec: 2.88968\n",
      "INFO:tensorflow:global_step/sec: 0.0905855\n",
      "I1117 14:47:48.997530 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905855\n",
      "INFO:tensorflow:examples/sec: 2.89874\n",
      "I1117 14:47:48.997743 4595733952 tpu_estimator.py:2160] examples/sec: 2.89874\n",
      "INFO:tensorflow:global_step/sec: 0.0905597\n",
      "I1117 14:48:00.039944 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905597\n",
      "INFO:tensorflow:examples/sec: 2.89791\n",
      "I1117 14:48:00.040156 4595733952 tpu_estimator.py:2160] examples/sec: 2.89791\n",
      "INFO:tensorflow:global_step/sec: 0.0902051\n",
      "I1117 14:48:11.125792 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902051\n",
      "INFO:tensorflow:examples/sec: 2.88656\n",
      "I1117 14:48:11.126004 4595733952 tpu_estimator.py:2160] examples/sec: 2.88656\n",
      "INFO:tensorflow:global_step/sec: 0.0896894\n",
      "I1117 14:48:22.275416 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896894\n",
      "INFO:tensorflow:examples/sec: 2.87006\n",
      "I1117 14:48:22.275622 4595733952 tpu_estimator.py:2160] examples/sec: 2.87006\n",
      "INFO:tensorflow:global_step/sec: 0.0904495\n",
      "I1117 14:48:33.331278 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904495\n",
      "INFO:tensorflow:examples/sec: 2.89438\n",
      "I1117 14:48:33.331506 4595733952 tpu_estimator.py:2160] examples/sec: 2.89438\n",
      "INFO:tensorflow:global_step/sec: 0.0905134\n",
      "I1117 14:48:44.379365 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905134\n",
      "INFO:tensorflow:examples/sec: 2.89643\n",
      "I1117 14:48:44.379585 4595733952 tpu_estimator.py:2160] examples/sec: 2.89643\n",
      "INFO:tensorflow:global_step/sec: 0.0904481\n",
      "I1117 14:48:55.435428 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904481\n",
      "INFO:tensorflow:examples/sec: 2.89434\n",
      "I1117 14:48:55.435648 4595733952 tpu_estimator.py:2160] examples/sec: 2.89434\n",
      "INFO:tensorflow:global_step/sec: 0.0906018\n",
      "I1117 14:49:06.472727 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906018\n",
      "INFO:tensorflow:examples/sec: 2.89926\n",
      "I1117 14:49:06.472939 4595733952 tpu_estimator.py:2160] examples/sec: 2.89926\n",
      "INFO:tensorflow:global_step/sec: 0.0904106\n",
      "I1117 14:49:17.533396 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904106\n",
      "INFO:tensorflow:examples/sec: 2.89314\n",
      "I1117 14:49:17.534855 4595733952 tpu_estimator.py:2160] examples/sec: 2.89314\n",
      "INFO:tensorflow:global_step/sec: 0.0905988\n",
      "I1117 14:49:28.571036 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905988\n",
      "INFO:tensorflow:examples/sec: 2.89916\n",
      "I1117 14:49:28.571246 4595733952 tpu_estimator.py:2160] examples/sec: 2.89916\n",
      "INFO:tensorflow:global_step/sec: 0.0907626\n",
      "I1117 14:49:39.588785 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907626\n",
      "INFO:tensorflow:examples/sec: 2.9044\n",
      "I1117 14:49:39.588989 4595733952 tpu_estimator.py:2160] examples/sec: 2.9044\n",
      "INFO:tensorflow:global_step/sec: 0.0902902\n",
      "I1117 14:49:50.664207 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902902\n",
      "INFO:tensorflow:examples/sec: 2.88929\n",
      "I1117 14:49:50.664432 4595733952 tpu_estimator.py:2160] examples/sec: 2.88929\n",
      "INFO:tensorflow:global_step/sec: 0.0905495\n",
      "I1117 14:50:01.707894 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905495\n",
      "INFO:tensorflow:examples/sec: 2.89758\n",
      "I1117 14:50:01.708120 4595733952 tpu_estimator.py:2160] examples/sec: 2.89758\n",
      "INFO:tensorflow:global_step/sec: 0.0901144\n",
      "I1117 14:50:12.804886 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901144\n",
      "INFO:tensorflow:examples/sec: 2.88366\n",
      "I1117 14:50:12.805098 4595733952 tpu_estimator.py:2160] examples/sec: 2.88366\n",
      "INFO:tensorflow:global_step/sec: 0.0895143\n",
      "I1117 14:50:23.976289 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895143\n",
      "INFO:tensorflow:examples/sec: 2.86446\n",
      "I1117 14:50:23.976505 4595733952 tpu_estimator.py:2160] examples/sec: 2.86446\n",
      "INFO:tensorflow:global_step/sec: 0.0903666\n",
      "I1117 14:50:35.042330 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903666\n",
      "INFO:tensorflow:examples/sec: 2.89173\n",
      "I1117 14:50:35.042541 4595733952 tpu_estimator.py:2160] examples/sec: 2.89173\n",
      "INFO:tensorflow:global_step/sec: 0.0902057\n",
      "I1117 14:50:46.128098 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902057\n",
      "INFO:tensorflow:examples/sec: 2.88658\n",
      "I1117 14:50:46.128320 4595733952 tpu_estimator.py:2160] examples/sec: 2.88658\n",
      "INFO:tensorflow:global_step/sec: 0.0902474\n",
      "I1117 14:50:57.208765 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902474\n",
      "INFO:tensorflow:examples/sec: 2.88792\n",
      "I1117 14:50:57.209001 4595733952 tpu_estimator.py:2160] examples/sec: 2.88792\n",
      "INFO:tensorflow:global_step/sec: 0.0905178\n",
      "I1117 14:51:08.256316 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905178\n",
      "INFO:tensorflow:examples/sec: 2.89657\n",
      "I1117 14:51:08.256541 4595733952 tpu_estimator.py:2160] examples/sec: 2.89657\n",
      "INFO:tensorflow:global_step/sec: 0.0905764\n",
      "I1117 14:51:19.296709 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905764\n",
      "INFO:tensorflow:examples/sec: 2.89845\n",
      "I1117 14:51:19.297971 4595733952 tpu_estimator.py:2160] examples/sec: 2.89845\n",
      "INFO:tensorflow:global_step/sec: 0.0896646\n",
      "I1117 14:51:30.449394 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896646\n",
      "INFO:tensorflow:examples/sec: 2.86927\n",
      "I1117 14:51:30.449627 4595733952 tpu_estimator.py:2160] examples/sec: 2.86927\n",
      "INFO:tensorflow:global_step/sec: 0.0906537\n",
      "I1117 14:51:41.480478 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906537\n",
      "INFO:tensorflow:examples/sec: 2.90092\n",
      "I1117 14:51:41.480794 4595733952 tpu_estimator.py:2160] examples/sec: 2.90092\n",
      "INFO:tensorflow:global_step/sec: 0.0903916\n",
      "I1117 14:51:52.543353 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903916\n",
      "INFO:tensorflow:examples/sec: 2.89253\n",
      "I1117 14:51:52.543627 4595733952 tpu_estimator.py:2160] examples/sec: 2.89253\n",
      "INFO:tensorflow:global_step/sec: 0.0905247\n",
      "I1117 14:52:03.590145 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905247\n",
      "INFO:tensorflow:examples/sec: 2.89679\n",
      "I1117 14:52:03.590471 4595733952 tpu_estimator.py:2160] examples/sec: 2.89679\n",
      "INFO:tensorflow:global_step/sec: 0.0902504\n",
      "I1117 14:52:14.670363 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902504\n",
      "INFO:tensorflow:examples/sec: 2.88801\n",
      "I1117 14:52:14.670589 4595733952 tpu_estimator.py:2160] examples/sec: 2.88801\n",
      "INFO:tensorflow:global_step/sec: 0.0902999\n",
      "I1117 14:52:25.744554 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902999\n",
      "INFO:tensorflow:examples/sec: 2.8896\n",
      "I1117 14:52:25.744783 4595733952 tpu_estimator.py:2160] examples/sec: 2.8896\n",
      "INFO:tensorflow:global_step/sec: 0.0890019\n",
      "I1117 14:52:36.980243 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0890019\n",
      "INFO:tensorflow:examples/sec: 2.84806\n",
      "I1117 14:52:36.980474 4595733952 tpu_estimator.py:2160] examples/sec: 2.84806\n",
      "INFO:tensorflow:global_step/sec: 0.0870057\n",
      "I1117 14:52:48.473716 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870057\n",
      "INFO:tensorflow:examples/sec: 2.78418\n",
      "I1117 14:52:48.473886 4595733952 tpu_estimator.py:2160] examples/sec: 2.78418\n",
      "INFO:tensorflow:global_step/sec: 0.0906292\n",
      "I1117 14:52:59.507730 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906292\n",
      "INFO:tensorflow:examples/sec: 2.90013\n",
      "I1117 14:52:59.507941 4595733952 tpu_estimator.py:2160] examples/sec: 2.90013\n",
      "INFO:tensorflow:global_step/sec: 0.0907102\n",
      "I1117 14:53:10.531852 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907102\n",
      "INFO:tensorflow:examples/sec: 2.90273\n",
      "I1117 14:53:10.532236 4595733952 tpu_estimator.py:2160] examples/sec: 2.90273\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./bert_output/model.ckpt.\n",
      "I1117 14:53:21.528387 4595733952 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into ./bert_output/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0652792\n",
      "I1117 14:53:25.850610 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0652792\n",
      "INFO:tensorflow:examples/sec: 2.08893\n",
      "I1117 14:53:25.850796 4595733952 tpu_estimator.py:2160] examples/sec: 2.08893\n",
      "INFO:tensorflow:global_step/sec: 0.0857899\n",
      "I1117 14:53:37.507047 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857899\n",
      "INFO:tensorflow:examples/sec: 2.74528\n",
      "I1117 14:53:37.507256 4595733952 tpu_estimator.py:2160] examples/sec: 2.74528\n",
      "INFO:tensorflow:global_step/sec: 0.0904662\n",
      "I1117 14:53:48.560920 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904662\n",
      "INFO:tensorflow:examples/sec: 2.89492\n",
      "I1117 14:53:48.561142 4595733952 tpu_estimator.py:2160] examples/sec: 2.89492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0900019\n",
      "I1117 14:53:59.671791 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900019\n",
      "INFO:tensorflow:examples/sec: 2.88006\n",
      "I1117 14:53:59.672017 4595733952 tpu_estimator.py:2160] examples/sec: 2.88006\n",
      "INFO:tensorflow:global_step/sec: 0.090282\n",
      "I1117 14:54:10.748189 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090282\n",
      "INFO:tensorflow:examples/sec: 2.88902\n",
      "I1117 14:54:10.748404 4595733952 tpu_estimator.py:2160] examples/sec: 2.88902\n",
      "INFO:tensorflow:global_step/sec: 0.0902668\n",
      "I1117 14:54:21.826462 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902668\n",
      "INFO:tensorflow:examples/sec: 2.88854\n",
      "I1117 14:54:21.826683 4595733952 tpu_estimator.py:2160] examples/sec: 2.88854\n",
      "INFO:tensorflow:global_step/sec: 0.090982\n",
      "I1117 14:54:32.817651 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090982\n",
      "INFO:tensorflow:examples/sec: 2.91142\n",
      "I1117 14:54:32.817874 4595733952 tpu_estimator.py:2160] examples/sec: 2.91142\n",
      "INFO:tensorflow:global_step/sec: 0.0907711\n",
      "I1117 14:54:43.834380 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907711\n",
      "INFO:tensorflow:examples/sec: 2.90467\n",
      "I1117 14:54:43.834604 4595733952 tpu_estimator.py:2160] examples/sec: 2.90467\n",
      "INFO:tensorflow:global_step/sec: 0.0903264\n",
      "I1117 14:54:54.905329 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903264\n",
      "INFO:tensorflow:examples/sec: 2.89045\n",
      "I1117 14:54:54.905552 4595733952 tpu_estimator.py:2160] examples/sec: 2.89045\n",
      "INFO:tensorflow:global_step/sec: 0.089602\n",
      "I1117 14:55:06.065797 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089602\n",
      "INFO:tensorflow:examples/sec: 2.86726\n",
      "I1117 14:55:06.066035 4595733952 tpu_estimator.py:2160] examples/sec: 2.86726\n",
      "INFO:tensorflow:global_step/sec: 0.0902468\n",
      "I1117 14:55:17.146528 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902468\n",
      "INFO:tensorflow:examples/sec: 2.8879\n",
      "I1117 14:55:17.146750 4595733952 tpu_estimator.py:2160] examples/sec: 2.8879\n",
      "INFO:tensorflow:global_step/sec: 0.0904825\n",
      "I1117 14:55:28.198385 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904825\n",
      "INFO:tensorflow:examples/sec: 2.89544\n",
      "I1117 14:55:28.198604 4595733952 tpu_estimator.py:2160] examples/sec: 2.89544\n",
      "INFO:tensorflow:global_step/sec: 0.0903541\n",
      "I1117 14:55:39.265946 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903541\n",
      "INFO:tensorflow:examples/sec: 2.89133\n",
      "I1117 14:55:39.266159 4595733952 tpu_estimator.py:2160] examples/sec: 2.89133\n",
      "INFO:tensorflow:global_step/sec: 0.0906097\n",
      "I1117 14:55:50.302325 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906097\n",
      "INFO:tensorflow:examples/sec: 2.89951\n",
      "I1117 14:55:50.302618 4595733952 tpu_estimator.py:2160] examples/sec: 2.89951\n",
      "INFO:tensorflow:global_step/sec: 0.0906831\n",
      "I1117 14:56:01.329720 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906831\n",
      "INFO:tensorflow:examples/sec: 2.90186\n",
      "I1117 14:56:01.329946 4595733952 tpu_estimator.py:2160] examples/sec: 2.90186\n",
      "INFO:tensorflow:global_step/sec: 0.0906663\n",
      "I1117 14:56:12.359162 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906663\n",
      "INFO:tensorflow:examples/sec: 2.90132\n",
      "I1117 14:56:12.359370 4595733952 tpu_estimator.py:2160] examples/sec: 2.90132\n",
      "INFO:tensorflow:global_step/sec: 0.0910105\n",
      "I1117 14:56:23.346916 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910105\n",
      "INFO:tensorflow:examples/sec: 2.91234\n",
      "I1117 14:56:23.347141 4595733952 tpu_estimator.py:2160] examples/sec: 2.91234\n",
      "INFO:tensorflow:global_step/sec: 0.0908446\n",
      "I1117 14:56:34.354722 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908446\n",
      "INFO:tensorflow:examples/sec: 2.90703\n",
      "I1117 14:56:34.354949 4595733952 tpu_estimator.py:2160] examples/sec: 2.90703\n",
      "INFO:tensorflow:global_step/sec: 0.0906542\n",
      "I1117 14:56:45.385660 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906542\n",
      "INFO:tensorflow:examples/sec: 2.90094\n",
      "I1117 14:56:45.385883 4595733952 tpu_estimator.py:2160] examples/sec: 2.90094\n",
      "INFO:tensorflow:global_step/sec: 0.0906801\n",
      "I1117 14:56:56.413430 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906801\n",
      "INFO:tensorflow:examples/sec: 2.90176\n",
      "I1117 14:56:56.413657 4595733952 tpu_estimator.py:2160] examples/sec: 2.90176\n",
      "INFO:tensorflow:global_step/sec: 0.089717\n",
      "I1117 14:57:07.559623 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089717\n",
      "INFO:tensorflow:examples/sec: 2.87094\n",
      "I1117 14:57:07.559838 4595733952 tpu_estimator.py:2160] examples/sec: 2.87094\n",
      "INFO:tensorflow:global_step/sec: 0.090248\n",
      "I1117 14:57:18.640166 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090248\n",
      "INFO:tensorflow:examples/sec: 2.88793\n",
      "I1117 14:57:18.640391 4595733952 tpu_estimator.py:2160] examples/sec: 2.88793\n",
      "INFO:tensorflow:global_step/sec: 0.0905559\n",
      "I1117 14:57:29.683068 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905559\n",
      "INFO:tensorflow:examples/sec: 2.89779\n",
      "I1117 14:57:29.683292 4595733952 tpu_estimator.py:2160] examples/sec: 2.89779\n",
      "INFO:tensorflow:global_step/sec: 0.0907073\n",
      "I1117 14:57:40.707541 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907073\n",
      "INFO:tensorflow:examples/sec: 2.90263\n",
      "I1117 14:57:40.707762 4595733952 tpu_estimator.py:2160] examples/sec: 2.90263\n",
      "INFO:tensorflow:global_step/sec: 0.0909788\n",
      "I1117 14:57:51.699121 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909788\n",
      "INFO:tensorflow:examples/sec: 2.91132\n",
      "I1117 14:57:51.699350 4595733952 tpu_estimator.py:2160] examples/sec: 2.91132\n",
      "INFO:tensorflow:global_step/sec: 0.0910861\n",
      "I1117 14:58:02.677744 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910861\n",
      "INFO:tensorflow:examples/sec: 2.91476\n",
      "I1117 14:58:02.677973 4595733952 tpu_estimator.py:2160] examples/sec: 2.91476\n",
      "INFO:tensorflow:global_step/sec: 0.0904651\n",
      "I1117 14:58:13.731734 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904651\n",
      "INFO:tensorflow:examples/sec: 2.89488\n",
      "I1117 14:58:13.731966 4595733952 tpu_estimator.py:2160] examples/sec: 2.89488\n",
      "INFO:tensorflow:global_step/sec: 0.0907586\n",
      "I1117 14:58:24.749971 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907586\n",
      "INFO:tensorflow:examples/sec: 2.90427\n",
      "I1117 14:58:24.750200 4595733952 tpu_estimator.py:2160] examples/sec: 2.90427\n",
      "INFO:tensorflow:global_step/sec: 0.0908403\n",
      "I1117 14:58:35.758305 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908403\n",
      "INFO:tensorflow:examples/sec: 2.90689\n",
      "I1117 14:58:35.758554 4595733952 tpu_estimator.py:2160] examples/sec: 2.90689\n",
      "INFO:tensorflow:global_step/sec: 0.0908441\n",
      "I1117 14:58:46.766179 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908441\n",
      "INFO:tensorflow:examples/sec: 2.90701\n",
      "I1117 14:58:46.766408 4595733952 tpu_estimator.py:2160] examples/sec: 2.90701\n",
      "INFO:tensorflow:global_step/sec: 0.0906916\n",
      "I1117 14:58:57.792543 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906916\n",
      "INFO:tensorflow:examples/sec: 2.90213\n",
      "I1117 14:58:57.792760 4595733952 tpu_estimator.py:2160] examples/sec: 2.90213\n",
      "INFO:tensorflow:global_step/sec: 0.0896627\n",
      "I1117 14:59:08.945514 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896627\n",
      "INFO:tensorflow:examples/sec: 2.86921\n",
      "I1117 14:59:08.945840 4595733952 tpu_estimator.py:2160] examples/sec: 2.86921\n",
      "INFO:tensorflow:global_step/sec: 0.0895261\n",
      "I1117 14:59:20.115390 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895261\n",
      "INFO:tensorflow:examples/sec: 2.86483\n",
      "I1117 14:59:20.115602 4595733952 tpu_estimator.py:2160] examples/sec: 2.86483\n",
      "INFO:tensorflow:global_step/sec: 0.0907771\n",
      "I1117 14:59:31.131378 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907771\n",
      "INFO:tensorflow:examples/sec: 2.90487\n",
      "I1117 14:59:31.131589 4595733952 tpu_estimator.py:2160] examples/sec: 2.90487\n",
      "INFO:tensorflow:global_step/sec: 0.0903311\n",
      "I1117 14:59:42.201771 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903311\n",
      "INFO:tensorflow:examples/sec: 2.8906\n",
      "I1117 14:59:42.201997 4595733952 tpu_estimator.py:2160] examples/sec: 2.8906\n",
      "INFO:tensorflow:global_step/sec: 0.0906545\n",
      "I1117 14:59:53.232671 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906545\n",
      "INFO:tensorflow:examples/sec: 2.90094\n",
      "I1117 14:59:53.232893 4595733952 tpu_estimator.py:2160] examples/sec: 2.90094\n",
      "INFO:tensorflow:global_step/sec: 0.0907073\n",
      "I1117 15:00:04.257143 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907073\n",
      "INFO:tensorflow:examples/sec: 2.90263\n",
      "I1117 15:00:04.257368 4595733952 tpu_estimator.py:2160] examples/sec: 2.90263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0905444\n",
      "I1117 15:00:15.301445 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905444\n",
      "INFO:tensorflow:examples/sec: 2.89742\n",
      "I1117 15:00:15.301673 4595733952 tpu_estimator.py:2160] examples/sec: 2.89742\n",
      "INFO:tensorflow:global_step/sec: 0.0901953\n",
      "I1117 15:00:26.388500 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901953\n",
      "INFO:tensorflow:examples/sec: 2.88625\n",
      "I1117 15:00:26.388725 4595733952 tpu_estimator.py:2160] examples/sec: 2.88625\n",
      "INFO:tensorflow:global_step/sec: 0.0905636\n",
      "I1117 15:00:37.430459 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905636\n",
      "INFO:tensorflow:examples/sec: 2.89804\n",
      "I1117 15:00:37.430676 4595733952 tpu_estimator.py:2160] examples/sec: 2.89804\n",
      "INFO:tensorflow:global_step/sec: 0.0907275\n",
      "I1117 15:00:48.452488 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907275\n",
      "INFO:tensorflow:examples/sec: 2.90328\n",
      "I1117 15:00:48.452702 4595733952 tpu_estimator.py:2160] examples/sec: 2.90328\n",
      "INFO:tensorflow:global_step/sec: 0.0910342\n",
      "I1117 15:00:59.437361 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910342\n",
      "INFO:tensorflow:examples/sec: 2.91309\n",
      "I1117 15:00:59.437582 4595733952 tpu_estimator.py:2160] examples/sec: 2.91309\n",
      "INFO:tensorflow:global_step/sec: 0.0906507\n",
      "I1117 15:01:10.468713 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906507\n",
      "INFO:tensorflow:examples/sec: 2.90082\n",
      "I1117 15:01:10.468963 4595733952 tpu_estimator.py:2160] examples/sec: 2.90082\n",
      "INFO:tensorflow:global_step/sec: 0.0907457\n",
      "I1117 15:01:21.488512 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907457\n",
      "INFO:tensorflow:examples/sec: 2.90386\n",
      "I1117 15:01:21.488741 4595733952 tpu_estimator.py:2160] examples/sec: 2.90386\n",
      "INFO:tensorflow:global_step/sec: 0.0911642\n",
      "I1117 15:01:32.457716 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911642\n",
      "INFO:tensorflow:examples/sec: 2.91725\n",
      "I1117 15:01:32.457923 4595733952 tpu_estimator.py:2160] examples/sec: 2.91725\n",
      "INFO:tensorflow:global_step/sec: 0.0898476\n",
      "I1117 15:01:43.587702 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898476\n",
      "INFO:tensorflow:examples/sec: 2.87512\n",
      "I1117 15:01:43.587959 4595733952 tpu_estimator.py:2160] examples/sec: 2.87512\n",
      "INFO:tensorflow:global_step/sec: 0.0908349\n",
      "I1117 15:01:54.596678 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908349\n",
      "INFO:tensorflow:examples/sec: 2.90672\n",
      "I1117 15:01:54.596908 4595733952 tpu_estimator.py:2160] examples/sec: 2.90672\n",
      "INFO:tensorflow:global_step/sec: 0.0908492\n",
      "I1117 15:02:05.603915 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908492\n",
      "INFO:tensorflow:examples/sec: 2.90717\n",
      "I1117 15:02:05.604140 4595733952 tpu_estimator.py:2160] examples/sec: 2.90717\n",
      "INFO:tensorflow:global_step/sec: 0.0901445\n",
      "I1117 15:02:16.697230 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901445\n",
      "INFO:tensorflow:examples/sec: 2.88462\n",
      "I1117 15:02:16.697463 4595733952 tpu_estimator.py:2160] examples/sec: 2.88462\n",
      "INFO:tensorflow:global_step/sec: 0.090499\n",
      "I1117 15:02:27.747079 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090499\n",
      "INFO:tensorflow:examples/sec: 2.89597\n",
      "I1117 15:02:27.747298 4595733952 tpu_estimator.py:2160] examples/sec: 2.89597\n",
      "INFO:tensorflow:global_step/sec: 0.0907202\n",
      "I1117 15:02:38.769983 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907202\n",
      "INFO:tensorflow:examples/sec: 2.90305\n",
      "I1117 15:02:38.770205 4595733952 tpu_estimator.py:2160] examples/sec: 2.90305\n",
      "INFO:tensorflow:global_step/sec: 0.0908432\n",
      "I1117 15:02:49.777965 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908432\n",
      "INFO:tensorflow:examples/sec: 2.90698\n",
      "I1117 15:02:49.778191 4595733952 tpu_estimator.py:2160] examples/sec: 2.90698\n",
      "INFO:tensorflow:global_step/sec: 0.0904486\n",
      "I1117 15:03:00.833956 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904486\n",
      "INFO:tensorflow:examples/sec: 2.89436\n",
      "I1117 15:03:00.834172 4595733952 tpu_estimator.py:2160] examples/sec: 2.89436\n",
      "INFO:tensorflow:global_step/sec: 0.0908283\n",
      "I1117 15:03:11.843744 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908283\n",
      "INFO:tensorflow:examples/sec: 2.90651\n",
      "I1117 15:03:11.843973 4595733952 tpu_estimator.py:2160] examples/sec: 2.90651\n",
      "INFO:tensorflow:global_step/sec: 0.0907483\n",
      "I1117 15:03:22.863237 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907483\n",
      "INFO:tensorflow:examples/sec: 2.90395\n",
      "I1117 15:03:22.863473 4595733952 tpu_estimator.py:2160] examples/sec: 2.90395\n",
      "INFO:tensorflow:global_step/sec: 0.0909133\n",
      "I1117 15:03:33.862709 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909133\n",
      "INFO:tensorflow:examples/sec: 2.90923\n",
      "I1117 15:03:33.862931 4595733952 tpu_estimator.py:2160] examples/sec: 2.90923\n",
      "INFO:tensorflow:global_step/sec: 0.0901399\n",
      "I1117 15:03:44.956585 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901399\n",
      "INFO:tensorflow:examples/sec: 2.88448\n",
      "I1117 15:03:44.956799 4595733952 tpu_estimator.py:2160] examples/sec: 2.88448\n",
      "INFO:tensorflow:global_step/sec: 0.0893696\n",
      "I1117 15:03:56.146069 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893696\n",
      "INFO:tensorflow:examples/sec: 2.85983\n",
      "I1117 15:03:56.146284 4595733952 tpu_estimator.py:2160] examples/sec: 2.85983\n",
      "INFO:tensorflow:global_step/sec: 0.0904366\n",
      "I1117 15:04:07.203556 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904366\n",
      "INFO:tensorflow:examples/sec: 2.89397\n",
      "I1117 15:04:07.203795 4595733952 tpu_estimator.py:2160] examples/sec: 2.89397\n",
      "INFO:tensorflow:global_step/sec: 0.0909353\n",
      "I1117 15:04:18.200371 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909353\n",
      "INFO:tensorflow:examples/sec: 2.90993\n",
      "I1117 15:04:18.200695 4595733952 tpu_estimator.py:2160] examples/sec: 2.90993\n",
      "INFO:tensorflow:global_step/sec: 0.090249\n",
      "I1117 15:04:29.280814 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090249\n",
      "INFO:tensorflow:examples/sec: 2.88797\n",
      "I1117 15:04:29.281045 4595733952 tpu_estimator.py:2160] examples/sec: 2.88797\n",
      "INFO:tensorflow:global_step/sec: 0.0904899\n",
      "I1117 15:04:40.331775 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904899\n",
      "INFO:tensorflow:examples/sec: 2.89568\n",
      "I1117 15:04:40.332002 4595733952 tpu_estimator.py:2160] examples/sec: 2.89568\n",
      "INFO:tensorflow:global_step/sec: 0.0905126\n",
      "I1117 15:04:51.379966 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905126\n",
      "INFO:tensorflow:examples/sec: 2.8964\n",
      "I1117 15:04:51.380459 4595733952 tpu_estimator.py:2160] examples/sec: 2.8964\n",
      "INFO:tensorflow:global_step/sec: 0.091039\n",
      "I1117 15:05:02.364262 4595733952 tpu_estimator.py:2159] global_step/sec: 0.091039\n",
      "INFO:tensorflow:examples/sec: 2.91325\n",
      "I1117 15:05:02.364486 4595733952 tpu_estimator.py:2160] examples/sec: 2.91325\n",
      "INFO:tensorflow:global_step/sec: 0.0907577\n",
      "I1117 15:05:13.382612 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907577\n",
      "INFO:tensorflow:examples/sec: 2.90425\n",
      "I1117 15:05:13.382838 4595733952 tpu_estimator.py:2160] examples/sec: 2.90425\n",
      "INFO:tensorflow:global_step/sec: 0.0905212\n",
      "I1117 15:05:24.429818 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905212\n",
      "INFO:tensorflow:examples/sec: 2.89668\n",
      "I1117 15:05:24.430037 4595733952 tpu_estimator.py:2160] examples/sec: 2.89668\n",
      "INFO:tensorflow:global_step/sec: 0.0902477\n",
      "I1117 15:05:35.510366 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902477\n",
      "INFO:tensorflow:examples/sec: 2.88793\n",
      "I1117 15:05:35.510595 4595733952 tpu_estimator.py:2160] examples/sec: 2.88793\n",
      "INFO:tensorflow:global_step/sec: 0.0908409\n",
      "I1117 15:05:46.518627 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908409\n",
      "INFO:tensorflow:examples/sec: 2.90691\n",
      "I1117 15:05:46.518853 4595733952 tpu_estimator.py:2160] examples/sec: 2.90691\n",
      "INFO:tensorflow:global_step/sec: 0.0906988\n",
      "I1117 15:05:57.544126 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906988\n",
      "INFO:tensorflow:examples/sec: 2.90236\n",
      "I1117 15:05:57.544346 4595733952 tpu_estimator.py:2160] examples/sec: 2.90236\n",
      "INFO:tensorflow:global_step/sec: 0.0902734\n",
      "I1117 15:06:08.621639 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902734\n",
      "INFO:tensorflow:examples/sec: 2.88875\n",
      "I1117 15:06:08.621865 4595733952 tpu_estimator.py:2160] examples/sec: 2.88875\n",
      "INFO:tensorflow:global_step/sec: 0.0903845\n",
      "I1117 15:06:19.685438 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903845\n",
      "INFO:tensorflow:examples/sec: 2.8923\n",
      "I1117 15:06:19.685662 4595733952 tpu_estimator.py:2160] examples/sec: 2.8923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0900348\n",
      "I1117 15:06:30.792309 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900348\n",
      "INFO:tensorflow:examples/sec: 2.88111\n",
      "I1117 15:06:30.792634 4595733952 tpu_estimator.py:2160] examples/sec: 2.88111\n",
      "INFO:tensorflow:global_step/sec: 0.0907295\n",
      "I1117 15:06:41.814025 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907295\n",
      "INFO:tensorflow:examples/sec: 2.90334\n",
      "I1117 15:06:41.814254 4595733952 tpu_estimator.py:2160] examples/sec: 2.90334\n",
      "INFO:tensorflow:global_step/sec: 0.0907962\n",
      "I1117 15:06:52.827699 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907962\n",
      "INFO:tensorflow:examples/sec: 2.90548\n",
      "I1117 15:06:52.827920 4595733952 tpu_estimator.py:2160] examples/sec: 2.90548\n",
      "INFO:tensorflow:global_step/sec: 0.0906274\n",
      "I1117 15:07:03.861893 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906274\n",
      "INFO:tensorflow:examples/sec: 2.90008\n",
      "I1117 15:07:03.862117 4595733952 tpu_estimator.py:2160] examples/sec: 2.90008\n",
      "INFO:tensorflow:global_step/sec: 0.0902981\n",
      "I1117 15:07:14.936320 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902981\n",
      "INFO:tensorflow:examples/sec: 2.88954\n",
      "I1117 15:07:14.936549 4595733952 tpu_estimator.py:2160] examples/sec: 2.88954\n",
      "INFO:tensorflow:global_step/sec: 0.0907183\n",
      "I1117 15:07:25.959460 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907183\n",
      "INFO:tensorflow:examples/sec: 2.90299\n",
      "I1117 15:07:25.959685 4595733952 tpu_estimator.py:2160] examples/sec: 2.90299\n",
      "INFO:tensorflow:global_step/sec: 0.0910766\n",
      "I1117 15:07:36.939237 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910766\n",
      "INFO:tensorflow:examples/sec: 2.91445\n",
      "I1117 15:07:36.939486 4595733952 tpu_estimator.py:2160] examples/sec: 2.91445\n",
      "INFO:tensorflow:global_step/sec: 0.0906057\n",
      "I1117 15:07:47.976062 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906057\n",
      "INFO:tensorflow:examples/sec: 2.89938\n",
      "I1117 15:07:47.976289 4595733952 tpu_estimator.py:2160] examples/sec: 2.89938\n",
      "INFO:tensorflow:global_step/sec: 0.0903722\n",
      "I1117 15:07:59.041406 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903722\n",
      "INFO:tensorflow:examples/sec: 2.89191\n",
      "I1117 15:07:59.041637 4595733952 tpu_estimator.py:2160] examples/sec: 2.89191\n",
      "INFO:tensorflow:global_step/sec: 0.0908233\n",
      "I1117 15:08:10.051851 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908233\n",
      "INFO:tensorflow:examples/sec: 2.90635\n",
      "I1117 15:08:10.052358 4595733952 tpu_estimator.py:2160] examples/sec: 2.90635\n",
      "INFO:tensorflow:global_step/sec: 0.0902371\n",
      "I1117 15:08:21.133701 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902371\n",
      "INFO:tensorflow:examples/sec: 2.88759\n",
      "I1117 15:08:21.133975 4595733952 tpu_estimator.py:2160] examples/sec: 2.88759\n",
      "INFO:tensorflow:global_step/sec: 0.09069\n",
      "I1117 15:08:32.160282 4595733952 tpu_estimator.py:2159] global_step/sec: 0.09069\n",
      "INFO:tensorflow:examples/sec: 2.90208\n",
      "I1117 15:08:32.160510 4595733952 tpu_estimator.py:2160] examples/sec: 2.90208\n",
      "INFO:tensorflow:global_step/sec: 0.0913606\n",
      "I1117 15:08:43.105914 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0913606\n",
      "INFO:tensorflow:examples/sec: 2.92354\n",
      "I1117 15:08:43.106136 4595733952 tpu_estimator.py:2160] examples/sec: 2.92354\n",
      "INFO:tensorflow:global_step/sec: 0.09045\n",
      "I1117 15:08:54.161732 4595733952 tpu_estimator.py:2159] global_step/sec: 0.09045\n",
      "INFO:tensorflow:examples/sec: 2.8944\n",
      "I1117 15:08:54.161949 4595733952 tpu_estimator.py:2160] examples/sec: 2.8944\n",
      "INFO:tensorflow:global_step/sec: 0.0904817\n",
      "I1117 15:09:05.213696 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904817\n",
      "INFO:tensorflow:examples/sec: 2.89541\n",
      "I1117 15:09:05.213922 4595733952 tpu_estimator.py:2160] examples/sec: 2.89541\n",
      "INFO:tensorflow:global_step/sec: 0.090421\n",
      "I1117 15:09:16.273079 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090421\n",
      "INFO:tensorflow:examples/sec: 2.89347\n",
      "I1117 15:09:16.273302 4595733952 tpu_estimator.py:2160] examples/sec: 2.89347\n",
      "INFO:tensorflow:global_step/sec: 0.0906205\n",
      "I1117 15:09:27.308109 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906205\n",
      "INFO:tensorflow:examples/sec: 2.89986\n",
      "I1117 15:09:27.308317 4595733952 tpu_estimator.py:2160] examples/sec: 2.89986\n",
      "INFO:tensorflow:global_step/sec: 0.0905475\n",
      "I1117 15:09:38.352045 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905475\n",
      "INFO:tensorflow:examples/sec: 2.89752\n",
      "I1117 15:09:38.352274 4595733952 tpu_estimator.py:2160] examples/sec: 2.89752\n",
      "INFO:tensorflow:global_step/sec: 0.0911968\n",
      "I1117 15:09:49.317338 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911968\n",
      "INFO:tensorflow:examples/sec: 2.9183\n",
      "I1117 15:09:49.317569 4595733952 tpu_estimator.py:2160] examples/sec: 2.9183\n",
      "INFO:tensorflow:global_step/sec: 0.0905278\n",
      "I1117 15:10:00.363668 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905278\n",
      "INFO:tensorflow:examples/sec: 2.89689\n",
      "I1117 15:10:00.363893 4595733952 tpu_estimator.py:2160] examples/sec: 2.89689\n",
      "INFO:tensorflow:global_step/sec: 0.0906481\n",
      "I1117 15:10:11.395338 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906481\n",
      "INFO:tensorflow:examples/sec: 2.90074\n",
      "I1117 15:10:11.395564 4595733952 tpu_estimator.py:2160] examples/sec: 2.90074\n",
      "INFO:tensorflow:global_step/sec: 0.0904841\n",
      "I1117 15:10:22.447003 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904841\n",
      "INFO:tensorflow:examples/sec: 2.89549\n",
      "I1117 15:10:22.447228 4595733952 tpu_estimator.py:2160] examples/sec: 2.89549\n",
      "INFO:tensorflow:global_step/sec: 0.0912743\n",
      "I1117 15:10:33.403007 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0912743\n",
      "INFO:tensorflow:examples/sec: 2.92078\n",
      "I1117 15:10:33.403258 4595733952 tpu_estimator.py:2160] examples/sec: 2.92078\n",
      "INFO:tensorflow:global_step/sec: 0.0911109\n",
      "I1117 15:10:44.378629 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911109\n",
      "INFO:tensorflow:examples/sec: 2.91555\n",
      "I1117 15:10:44.378854 4595733952 tpu_estimator.py:2160] examples/sec: 2.91555\n",
      "INFO:tensorflow:global_step/sec: 0.0902317\n",
      "I1117 15:10:55.461184 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902317\n",
      "INFO:tensorflow:examples/sec: 2.88742\n",
      "I1117 15:10:55.461534 4595733952 tpu_estimator.py:2160] examples/sec: 2.88742\n",
      "INFO:tensorflow:global_step/sec: 0.0904744\n",
      "I1117 15:11:06.514060 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904744\n",
      "INFO:tensorflow:examples/sec: 2.89518\n",
      "I1117 15:11:06.514289 4595733952 tpu_estimator.py:2160] examples/sec: 2.89518\n",
      "INFO:tensorflow:global_step/sec: 0.0908729\n",
      "I1117 15:11:17.518445 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908729\n",
      "INFO:tensorflow:examples/sec: 2.90793\n",
      "I1117 15:11:17.518832 4595733952 tpu_estimator.py:2160] examples/sec: 2.90793\n",
      "INFO:tensorflow:global_step/sec: 0.0911104\n",
      "I1117 15:11:28.494132 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911104\n",
      "INFO:tensorflow:examples/sec: 2.91553\n",
      "I1117 15:11:28.494385 4595733952 tpu_estimator.py:2160] examples/sec: 2.91553\n",
      "INFO:tensorflow:global_step/sec: 0.0909015\n",
      "I1117 15:11:39.495053 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909015\n",
      "INFO:tensorflow:examples/sec: 2.90885\n",
      "I1117 15:11:39.495279 4595733952 tpu_estimator.py:2160] examples/sec: 2.90885\n",
      "INFO:tensorflow:global_step/sec: 0.0908342\n",
      "I1117 15:11:50.504119 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908342\n",
      "INFO:tensorflow:examples/sec: 2.9067\n",
      "I1117 15:11:50.504385 4595733952 tpu_estimator.py:2160] examples/sec: 2.9067\n",
      "INFO:tensorflow:global_step/sec: 0.0904824\n",
      "I1117 15:12:01.556002 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904824\n",
      "INFO:tensorflow:examples/sec: 2.89544\n",
      "I1117 15:12:01.556236 4595733952 tpu_estimator.py:2160] examples/sec: 2.89544\n",
      "INFO:tensorflow:global_step/sec: 0.090779\n",
      "I1117 15:12:12.571760 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090779\n",
      "INFO:tensorflow:examples/sec: 2.90493\n",
      "I1117 15:12:12.571979 4595733952 tpu_estimator.py:2160] examples/sec: 2.90493\n",
      "INFO:tensorflow:global_step/sec: 0.0909711\n",
      "I1117 15:12:23.564263 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909711\n",
      "INFO:tensorflow:examples/sec: 2.91107\n",
      "I1117 15:12:23.564485 4595733952 tpu_estimator.py:2160] examples/sec: 2.91107\n",
      "INFO:tensorflow:global_step/sec: 0.0908705\n",
      "I1117 15:12:34.568944 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908705\n",
      "INFO:tensorflow:examples/sec: 2.90786\n",
      "I1117 15:12:34.569168 4595733952 tpu_estimator.py:2160] examples/sec: 2.90786\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0910139\n",
      "I1117 15:12:45.556285 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910139\n",
      "INFO:tensorflow:examples/sec: 2.91244\n",
      "I1117 15:12:45.556513 4595733952 tpu_estimator.py:2160] examples/sec: 2.91244\n",
      "INFO:tensorflow:global_step/sec: 0.0904853\n",
      "I1117 15:12:56.607789 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904853\n",
      "INFO:tensorflow:examples/sec: 2.89553\n",
      "I1117 15:12:56.608015 4595733952 tpu_estimator.py:2160] examples/sec: 2.89553\n",
      "INFO:tensorflow:global_step/sec: 0.0904578\n",
      "I1117 15:13:07.662702 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904578\n",
      "INFO:tensorflow:examples/sec: 2.89465\n",
      "I1117 15:13:07.662924 4595733952 tpu_estimator.py:2160] examples/sec: 2.89465\n",
      "INFO:tensorflow:global_step/sec: 0.0910639\n",
      "I1117 15:13:18.644003 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910639\n",
      "INFO:tensorflow:examples/sec: 2.91404\n",
      "I1117 15:13:18.644226 4595733952 tpu_estimator.py:2160] examples/sec: 2.91404\n",
      "INFO:tensorflow:global_step/sec: 0.0908254\n",
      "I1117 15:13:29.654120 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908254\n",
      "INFO:tensorflow:examples/sec: 2.90641\n",
      "I1117 15:13:29.654361 4595733952 tpu_estimator.py:2160] examples/sec: 2.90641\n",
      "INFO:tensorflow:global_step/sec: 0.0910371\n",
      "I1117 15:13:40.638648 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910371\n",
      "INFO:tensorflow:examples/sec: 2.91319\n",
      "I1117 15:13:40.638866 4595733952 tpu_estimator.py:2160] examples/sec: 2.91319\n",
      "INFO:tensorflow:global_step/sec: 0.0901113\n",
      "I1117 15:13:51.736063 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901113\n",
      "INFO:tensorflow:examples/sec: 2.88356\n",
      "I1117 15:13:51.736291 4595733952 tpu_estimator.py:2160] examples/sec: 2.88356\n",
      "INFO:tensorflow:global_step/sec: 0.0909874\n",
      "I1117 15:14:02.726567 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909874\n",
      "INFO:tensorflow:examples/sec: 2.9116\n",
      "I1117 15:14:02.726789 4595733952 tpu_estimator.py:2160] examples/sec: 2.9116\n",
      "INFO:tensorflow:global_step/sec: 0.0909531\n",
      "I1117 15:14:13.721254 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909531\n",
      "INFO:tensorflow:examples/sec: 2.9105\n",
      "I1117 15:14:13.721469 4595733952 tpu_estimator.py:2160] examples/sec: 2.9105\n",
      "INFO:tensorflow:global_step/sec: 0.0903493\n",
      "I1117 15:14:24.789400 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903493\n",
      "INFO:tensorflow:examples/sec: 2.89118\n",
      "I1117 15:14:24.789622 4595733952 tpu_estimator.py:2160] examples/sec: 2.89118\n",
      "INFO:tensorflow:global_step/sec: 0.0900281\n",
      "I1117 15:14:35.897047 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900281\n",
      "INFO:tensorflow:examples/sec: 2.8809\n",
      "I1117 15:14:35.897289 4595733952 tpu_estimator.py:2160] examples/sec: 2.8809\n",
      "INFO:tensorflow:global_step/sec: 0.0910667\n",
      "I1117 15:14:46.878010 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910667\n",
      "INFO:tensorflow:examples/sec: 2.91413\n",
      "I1117 15:14:46.878231 4595733952 tpu_estimator.py:2160] examples/sec: 2.91413\n",
      "INFO:tensorflow:global_step/sec: 0.0909544\n",
      "I1117 15:14:57.872534 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909544\n",
      "INFO:tensorflow:examples/sec: 2.91054\n",
      "I1117 15:14:57.872760 4595733952 tpu_estimator.py:2160] examples/sec: 2.91054\n",
      "INFO:tensorflow:global_step/sec: 0.0909984\n",
      "I1117 15:15:08.861734 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909984\n",
      "INFO:tensorflow:examples/sec: 2.91195\n",
      "I1117 15:15:08.861959 4595733952 tpu_estimator.py:2160] examples/sec: 2.91195\n",
      "INFO:tensorflow:global_step/sec: 0.090947\n",
      "I1117 15:15:19.857146 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090947\n",
      "INFO:tensorflow:examples/sec: 2.9103\n",
      "I1117 15:15:19.857372 4595733952 tpu_estimator.py:2160] examples/sec: 2.9103\n",
      "INFO:tensorflow:global_step/sec: 0.0909302\n",
      "I1117 15:15:30.854607 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909302\n",
      "INFO:tensorflow:examples/sec: 2.90977\n",
      "I1117 15:15:30.854820 4595733952 tpu_estimator.py:2160] examples/sec: 2.90977\n",
      "INFO:tensorflow:global_step/sec: 0.0908075\n",
      "I1117 15:15:41.866914 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908075\n",
      "INFO:tensorflow:examples/sec: 2.90584\n",
      "I1117 15:15:41.867138 4595733952 tpu_estimator.py:2160] examples/sec: 2.90584\n",
      "INFO:tensorflow:global_step/sec: 0.0909401\n",
      "I1117 15:15:52.863297 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909401\n",
      "INFO:tensorflow:examples/sec: 2.91008\n",
      "I1117 15:15:52.863525 4595733952 tpu_estimator.py:2160] examples/sec: 2.91008\n",
      "INFO:tensorflow:global_step/sec: 0.0909528\n",
      "I1117 15:16:03.857861 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909528\n",
      "INFO:tensorflow:examples/sec: 2.91049\n",
      "I1117 15:16:03.858096 4595733952 tpu_estimator.py:2160] examples/sec: 2.91049\n",
      "INFO:tensorflow:global_step/sec: 0.08884\n",
      "I1117 15:16:15.114027 4595733952 tpu_estimator.py:2159] global_step/sec: 0.08884\n",
      "INFO:tensorflow:examples/sec: 2.84288\n",
      "I1117 15:16:15.114229 4595733952 tpu_estimator.py:2160] examples/sec: 2.84288\n",
      "INFO:tensorflow:global_step/sec: 0.0903222\n",
      "I1117 15:16:26.185530 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903222\n",
      "INFO:tensorflow:examples/sec: 2.89031\n",
      "I1117 15:16:26.185766 4595733952 tpu_estimator.py:2160] examples/sec: 2.89031\n",
      "INFO:tensorflow:global_step/sec: 0.0908962\n",
      "I1117 15:16:37.187098 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908962\n",
      "INFO:tensorflow:examples/sec: 2.90868\n",
      "I1117 15:16:37.187328 4595733952 tpu_estimator.py:2160] examples/sec: 2.90868\n",
      "INFO:tensorflow:global_step/sec: 0.0904208\n",
      "I1117 15:16:48.246499 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904208\n",
      "INFO:tensorflow:examples/sec: 2.89347\n",
      "I1117 15:16:48.246725 4595733952 tpu_estimator.py:2160] examples/sec: 2.89347\n",
      "INFO:tensorflow:global_step/sec: 0.0908459\n",
      "I1117 15:16:59.254149 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908459\n",
      "INFO:tensorflow:examples/sec: 2.90707\n",
      "I1117 15:16:59.254405 4595733952 tpu_estimator.py:2160] examples/sec: 2.90707\n",
      "INFO:tensorflow:global_step/sec: 0.0907765\n",
      "I1117 15:17:10.270222 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907765\n",
      "INFO:tensorflow:examples/sec: 2.90485\n",
      "I1117 15:17:10.270448 4595733952 tpu_estimator.py:2160] examples/sec: 2.90485\n",
      "INFO:tensorflow:global_step/sec: 0.0906033\n",
      "I1117 15:17:21.307373 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906033\n",
      "INFO:tensorflow:examples/sec: 2.8993\n",
      "I1117 15:17:21.307613 4595733952 tpu_estimator.py:2160] examples/sec: 2.8993\n",
      "INFO:tensorflow:global_step/sec: 0.0902755\n",
      "I1117 15:17:32.384562 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902755\n",
      "INFO:tensorflow:examples/sec: 2.88882\n",
      "I1117 15:17:32.384788 4595733952 tpu_estimator.py:2160] examples/sec: 2.88882\n",
      "INFO:tensorflow:global_step/sec: 0.0912775\n",
      "I1117 15:17:43.340176 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0912775\n",
      "INFO:tensorflow:examples/sec: 2.92088\n",
      "I1117 15:17:43.340418 4595733952 tpu_estimator.py:2160] examples/sec: 2.92088\n",
      "INFO:tensorflow:global_step/sec: 0.0905799\n",
      "I1117 15:17:54.380144 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905799\n",
      "INFO:tensorflow:examples/sec: 2.89856\n",
      "I1117 15:17:54.380370 4595733952 tpu_estimator.py:2160] examples/sec: 2.89856\n",
      "INFO:tensorflow:global_step/sec: 0.0909699\n",
      "I1117 15:18:05.372784 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909699\n",
      "INFO:tensorflow:examples/sec: 2.91104\n",
      "I1117 15:18:05.373007 4595733952 tpu_estimator.py:2160] examples/sec: 2.91104\n",
      "INFO:tensorflow:global_step/sec: 0.0904458\n",
      "I1117 15:18:16.429146 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904458\n",
      "INFO:tensorflow:examples/sec: 2.89426\n",
      "I1117 15:18:16.429370 4595733952 tpu_estimator.py:2160] examples/sec: 2.89426\n",
      "INFO:tensorflow:global_step/sec: 0.0907227\n",
      "I1117 15:18:27.451745 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907227\n",
      "INFO:tensorflow:examples/sec: 2.90313\n",
      "I1117 15:18:27.451975 4595733952 tpu_estimator.py:2160] examples/sec: 2.90313\n",
      "INFO:tensorflow:global_step/sec: 0.0905945\n",
      "I1117 15:18:38.489939 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905945\n",
      "INFO:tensorflow:examples/sec: 2.89902\n",
      "I1117 15:18:38.490160 4595733952 tpu_estimator.py:2160] examples/sec: 2.89902\n",
      "INFO:tensorflow:global_step/sec: 0.0907714\n",
      "I1117 15:18:49.506650 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907714\n",
      "INFO:tensorflow:examples/sec: 2.90469\n",
      "I1117 15:18:49.506874 4595733952 tpu_estimator.py:2160] examples/sec: 2.90469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0910369\n",
      "I1117 15:19:00.491181 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910369\n",
      "INFO:tensorflow:examples/sec: 2.91318\n",
      "I1117 15:19:00.491403 4595733952 tpu_estimator.py:2160] examples/sec: 2.91318\n",
      "INFO:tensorflow:global_step/sec: 0.0907801\n",
      "I1117 15:19:11.506814 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907801\n",
      "INFO:tensorflow:examples/sec: 2.90496\n",
      "I1117 15:19:11.507047 4595733952 tpu_estimator.py:2160] examples/sec: 2.90496\n",
      "INFO:tensorflow:global_step/sec: 0.0905673\n",
      "I1117 15:19:22.548316 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905673\n",
      "INFO:tensorflow:examples/sec: 2.89815\n",
      "I1117 15:19:22.548543 4595733952 tpu_estimator.py:2160] examples/sec: 2.89815\n",
      "INFO:tensorflow:global_step/sec: 0.0906037\n",
      "I1117 15:19:33.585393 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906037\n",
      "INFO:tensorflow:examples/sec: 2.89932\n",
      "I1117 15:19:33.585619 4595733952 tpu_estimator.py:2160] examples/sec: 2.89932\n",
      "INFO:tensorflow:global_step/sec: 0.0912394\n",
      "I1117 15:19:44.545570 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0912394\n",
      "INFO:tensorflow:examples/sec: 2.91966\n",
      "I1117 15:19:44.545794 4595733952 tpu_estimator.py:2160] examples/sec: 2.91966\n",
      "INFO:tensorflow:global_step/sec: 0.0908851\n",
      "I1117 15:19:55.548476 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908851\n",
      "INFO:tensorflow:examples/sec: 2.90832\n",
      "I1117 15:19:55.548698 4595733952 tpu_estimator.py:2160] examples/sec: 2.90832\n",
      "INFO:tensorflow:global_step/sec: 0.0900483\n",
      "I1117 15:20:06.653614 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900483\n",
      "INFO:tensorflow:examples/sec: 2.88155\n",
      "I1117 15:20:06.653825 4595733952 tpu_estimator.py:2160] examples/sec: 2.88155\n",
      "INFO:tensorflow:global_step/sec: 0.0904159\n",
      "I1117 15:20:17.713633 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904159\n",
      "INFO:tensorflow:examples/sec: 2.89331\n",
      "I1117 15:20:17.713860 4595733952 tpu_estimator.py:2160] examples/sec: 2.89331\n",
      "INFO:tensorflow:global_step/sec: 0.0900941\n",
      "I1117 15:20:28.813141 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900941\n",
      "INFO:tensorflow:examples/sec: 2.88301\n",
      "I1117 15:20:28.813362 4595733952 tpu_estimator.py:2160] examples/sec: 2.88301\n",
      "INFO:tensorflow:global_step/sec: 0.0906854\n",
      "I1117 15:20:39.840269 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906854\n",
      "INFO:tensorflow:examples/sec: 2.90193\n",
      "I1117 15:20:39.840488 4595733952 tpu_estimator.py:2160] examples/sec: 2.90193\n",
      "INFO:tensorflow:global_step/sec: 0.0909501\n",
      "I1117 15:20:50.835319 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909501\n",
      "INFO:tensorflow:examples/sec: 2.9104\n",
      "I1117 15:20:50.835547 4595733952 tpu_estimator.py:2160] examples/sec: 2.9104\n",
      "INFO:tensorflow:global_step/sec: 0.0909523\n",
      "I1117 15:21:01.830098 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909523\n",
      "INFO:tensorflow:examples/sec: 2.91048\n",
      "I1117 15:21:01.830345 4595733952 tpu_estimator.py:2160] examples/sec: 2.91048\n",
      "INFO:tensorflow:global_step/sec: 0.0905025\n",
      "I1117 15:21:12.879503 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905025\n",
      "INFO:tensorflow:examples/sec: 2.89608\n",
      "I1117 15:21:12.879727 4595733952 tpu_estimator.py:2160] examples/sec: 2.89608\n",
      "INFO:tensorflow:global_step/sec: 0.0910631\n",
      "I1117 15:21:23.860893 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910631\n",
      "INFO:tensorflow:examples/sec: 2.91402\n",
      "I1117 15:21:23.861110 4595733952 tpu_estimator.py:2160] examples/sec: 2.91402\n",
      "INFO:tensorflow:global_step/sec: 0.0904496\n",
      "I1117 15:21:34.916783 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904496\n",
      "INFO:tensorflow:examples/sec: 2.89439\n",
      "I1117 15:21:34.917008 4595733952 tpu_estimator.py:2160] examples/sec: 2.89439\n",
      "INFO:tensorflow:global_step/sec: 0.090392\n",
      "I1117 15:21:45.979707 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090392\n",
      "INFO:tensorflow:examples/sec: 2.89255\n",
      "I1117 15:21:45.979930 4595733952 tpu_estimator.py:2160] examples/sec: 2.89255\n",
      "INFO:tensorflow:global_step/sec: 0.0905953\n",
      "I1117 15:21:57.017816 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905953\n",
      "INFO:tensorflow:examples/sec: 2.89905\n",
      "I1117 15:21:57.018042 4595733952 tpu_estimator.py:2160] examples/sec: 2.89905\n",
      "INFO:tensorflow:global_step/sec: 0.090385\n",
      "I1117 15:22:08.081588 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090385\n",
      "INFO:tensorflow:examples/sec: 2.89232\n",
      "I1117 15:22:08.081813 4595733952 tpu_estimator.py:2160] examples/sec: 2.89232\n",
      "INFO:tensorflow:global_step/sec: 0.0909561\n",
      "I1117 15:22:19.075909 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909561\n",
      "INFO:tensorflow:examples/sec: 2.91059\n",
      "I1117 15:22:19.076131 4595733952 tpu_estimator.py:2160] examples/sec: 2.91059\n",
      "INFO:tensorflow:global_step/sec: 0.0903911\n",
      "I1117 15:22:30.138945 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903911\n",
      "INFO:tensorflow:examples/sec: 2.89252\n",
      "I1117 15:22:30.139192 4595733952 tpu_estimator.py:2160] examples/sec: 2.89252\n",
      "INFO:tensorflow:global_step/sec: 0.090531\n",
      "I1117 15:22:41.184878 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090531\n",
      "INFO:tensorflow:examples/sec: 2.89699\n",
      "I1117 15:22:41.185096 4595733952 tpu_estimator.py:2160] examples/sec: 2.89699\n",
      "INFO:tensorflow:global_step/sec: 0.0909262\n",
      "I1117 15:22:52.182822 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909262\n",
      "INFO:tensorflow:examples/sec: 2.90964\n",
      "I1117 15:22:52.183308 4595733952 tpu_estimator.py:2160] examples/sec: 2.90964\n",
      "INFO:tensorflow:global_step/sec: 0.0907776\n",
      "I1117 15:23:03.198720 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907776\n",
      "INFO:tensorflow:examples/sec: 2.90488\n",
      "I1117 15:23:03.198931 4595733952 tpu_estimator.py:2160] examples/sec: 2.90488\n",
      "INFO:tensorflow:global_step/sec: 0.0903306\n",
      "I1117 15:23:14.269168 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903306\n",
      "INFO:tensorflow:examples/sec: 2.89058\n",
      "I1117 15:23:14.269380 4595733952 tpu_estimator.py:2160] examples/sec: 2.89058\n",
      "INFO:tensorflow:global_step/sec: 0.0911071\n",
      "I1117 15:23:25.245244 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911071\n",
      "INFO:tensorflow:examples/sec: 2.91543\n",
      "I1117 15:23:25.245440 4595733952 tpu_estimator.py:2160] examples/sec: 2.91543\n",
      "INFO:tensorflow:global_step/sec: 0.0907727\n",
      "I1117 15:23:36.261797 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907727\n",
      "INFO:tensorflow:examples/sec: 2.90473\n",
      "I1117 15:23:36.262034 4595733952 tpu_estimator.py:2160] examples/sec: 2.90473\n",
      "INFO:tensorflow:global_step/sec: 0.0906171\n",
      "I1117 15:23:47.297232 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906171\n",
      "INFO:tensorflow:examples/sec: 2.89975\n",
      "I1117 15:23:47.297451 4595733952 tpu_estimator.py:2160] examples/sec: 2.89975\n",
      "INFO:tensorflow:global_step/sec: 0.0906448\n",
      "I1117 15:23:58.329316 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906448\n",
      "INFO:tensorflow:examples/sec: 2.90063\n",
      "I1117 15:23:58.329538 4595733952 tpu_estimator.py:2160] examples/sec: 2.90063\n",
      "INFO:tensorflow:global_step/sec: 0.0904292\n",
      "I1117 15:24:09.387674 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904292\n",
      "INFO:tensorflow:examples/sec: 2.89374\n",
      "I1117 15:24:09.387885 4595733952 tpu_estimator.py:2160] examples/sec: 2.89374\n",
      "INFO:tensorflow:global_step/sec: 0.0908405\n",
      "I1117 15:24:20.395999 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908405\n",
      "INFO:tensorflow:examples/sec: 2.90689\n",
      "I1117 15:24:20.396224 4595733952 tpu_estimator.py:2160] examples/sec: 2.90689\n",
      "INFO:tensorflow:global_step/sec: 0.0905758\n",
      "I1117 15:24:31.436498 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905758\n",
      "INFO:tensorflow:examples/sec: 2.89843\n",
      "I1117 15:24:31.436710 4595733952 tpu_estimator.py:2160] examples/sec: 2.89843\n",
      "INFO:tensorflow:global_step/sec: 0.0908332\n",
      "I1117 15:24:42.445670 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908332\n",
      "INFO:tensorflow:examples/sec: 2.90666\n",
      "I1117 15:24:42.445914 4595733952 tpu_estimator.py:2160] examples/sec: 2.90666\n",
      "INFO:tensorflow:global_step/sec: 0.0911624\n",
      "I1117 15:24:53.415100 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911624\n",
      "INFO:tensorflow:examples/sec: 2.9172\n",
      "I1117 15:24:53.415328 4595733952 tpu_estimator.py:2160] examples/sec: 2.9172\n",
      "INFO:tensorflow:global_step/sec: 0.090586\n",
      "I1117 15:25:04.454334 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090586\n",
      "INFO:tensorflow:examples/sec: 2.89875\n",
      "I1117 15:25:04.454554 4595733952 tpu_estimator.py:2160] examples/sec: 2.89875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0903761\n",
      "I1117 15:25:15.519204 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903761\n",
      "INFO:tensorflow:examples/sec: 2.89204\n",
      "I1117 15:25:15.519429 4595733952 tpu_estimator.py:2160] examples/sec: 2.89204\n",
      "INFO:tensorflow:global_step/sec: 0.0909245\n",
      "I1117 15:25:26.517336 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909245\n",
      "INFO:tensorflow:examples/sec: 2.90959\n",
      "I1117 15:25:26.517558 4595733952 tpu_estimator.py:2160] examples/sec: 2.90959\n",
      "INFO:tensorflow:global_step/sec: 0.0907391\n",
      "I1117 15:25:37.537951 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907391\n",
      "INFO:tensorflow:examples/sec: 2.90365\n",
      "I1117 15:25:37.538179 4595733952 tpu_estimator.py:2160] examples/sec: 2.90365\n",
      "INFO:tensorflow:global_step/sec: 0.0907458\n",
      "I1117 15:25:48.557749 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907458\n",
      "INFO:tensorflow:examples/sec: 2.90387\n",
      "I1117 15:25:48.557967 4595733952 tpu_estimator.py:2160] examples/sec: 2.90387\n",
      "INFO:tensorflow:global_step/sec: 0.0904159\n",
      "I1117 15:25:59.617739 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904159\n",
      "INFO:tensorflow:examples/sec: 2.89331\n",
      "I1117 15:25:59.617967 4595733952 tpu_estimator.py:2160] examples/sec: 2.89331\n",
      "INFO:tensorflow:global_step/sec: 0.0903337\n",
      "I1117 15:26:10.687839 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903337\n",
      "INFO:tensorflow:examples/sec: 2.89068\n",
      "I1117 15:26:10.688065 4595733952 tpu_estimator.py:2160] examples/sec: 2.89068\n",
      "INFO:tensorflow:global_step/sec: 0.0904755\n",
      "I1117 15:26:21.740528 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904755\n",
      "INFO:tensorflow:examples/sec: 2.89522\n",
      "I1117 15:26:21.740771 4595733952 tpu_estimator.py:2160] examples/sec: 2.89522\n",
      "INFO:tensorflow:global_step/sec: 0.0905846\n",
      "I1117 15:26:32.779927 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905846\n",
      "INFO:tensorflow:examples/sec: 2.89871\n",
      "I1117 15:26:32.780152 4595733952 tpu_estimator.py:2160] examples/sec: 2.89871\n",
      "INFO:tensorflow:global_step/sec: 0.0906189\n",
      "I1117 15:26:43.815194 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906189\n",
      "INFO:tensorflow:examples/sec: 2.8998\n",
      "I1117 15:26:43.815524 4595733952 tpu_estimator.py:2160] examples/sec: 2.8998\n",
      "INFO:tensorflow:global_step/sec: 0.0908491\n",
      "I1117 15:26:54.822415 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908491\n",
      "INFO:tensorflow:examples/sec: 2.90717\n",
      "I1117 15:26:54.822803 4595733952 tpu_estimator.py:2160] examples/sec: 2.90717\n",
      "INFO:tensorflow:global_step/sec: 0.0906038\n",
      "I1117 15:27:05.859488 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906038\n",
      "INFO:tensorflow:examples/sec: 2.89932\n",
      "I1117 15:27:05.859714 4595733952 tpu_estimator.py:2160] examples/sec: 2.89932\n",
      "INFO:tensorflow:global_step/sec: 0.0905118\n",
      "I1117 15:27:16.907768 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905118\n",
      "INFO:tensorflow:examples/sec: 2.89638\n",
      "I1117 15:27:16.907993 4595733952 tpu_estimator.py:2160] examples/sec: 2.89638\n",
      "INFO:tensorflow:global_step/sec: 0.0908748\n",
      "I1117 15:27:27.911931 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908748\n",
      "INFO:tensorflow:examples/sec: 2.90799\n",
      "I1117 15:27:27.912173 4595733952 tpu_estimator.py:2160] examples/sec: 2.90799\n",
      "INFO:tensorflow:global_step/sec: 0.0907192\n",
      "I1117 15:27:38.934956 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907192\n",
      "INFO:tensorflow:examples/sec: 2.90301\n",
      "I1117 15:27:38.935341 4595733952 tpu_estimator.py:2160] examples/sec: 2.90301\n",
      "INFO:tensorflow:global_step/sec: 0.0910061\n",
      "I1117 15:27:49.923233 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0910061\n",
      "INFO:tensorflow:examples/sec: 2.91219\n",
      "I1117 15:27:49.923479 4595733952 tpu_estimator.py:2160] examples/sec: 2.91219\n",
      "INFO:tensorflow:global_step/sec: 0.0906843\n",
      "I1117 15:28:00.950510 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906843\n",
      "INFO:tensorflow:examples/sec: 2.9019\n",
      "I1117 15:28:00.950757 4595733952 tpu_estimator.py:2160] examples/sec: 2.9019\n",
      "INFO:tensorflow:global_step/sec: 0.0908524\n",
      "I1117 15:28:11.957362 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908524\n",
      "INFO:tensorflow:examples/sec: 2.90728\n",
      "I1117 15:28:11.957590 4595733952 tpu_estimator.py:2160] examples/sec: 2.90728\n",
      "INFO:tensorflow:global_step/sec: 0.0911625\n",
      "I1117 15:28:22.926778 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0911625\n",
      "INFO:tensorflow:examples/sec: 2.9172\n",
      "I1117 15:28:22.926999 4595733952 tpu_estimator.py:2160] examples/sec: 2.9172\n",
      "INFO:tensorflow:global_step/sec: 0.0905369\n",
      "I1117 15:28:33.971987 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905369\n",
      "INFO:tensorflow:examples/sec: 2.89718\n",
      "I1117 15:28:33.972212 4595733952 tpu_estimator.py:2160] examples/sec: 2.89718\n",
      "INFO:tensorflow:global_step/sec: 0.0907246\n",
      "I1117 15:28:44.994357 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907246\n",
      "INFO:tensorflow:examples/sec: 2.90319\n",
      "I1117 15:28:44.994583 4595733952 tpu_estimator.py:2160] examples/sec: 2.90319\n",
      "INFO:tensorflow:global_step/sec: 0.090436\n",
      "I1117 15:28:56.051897 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090436\n",
      "INFO:tensorflow:examples/sec: 2.89395\n",
      "I1117 15:28:56.052122 4595733952 tpu_estimator.py:2160] examples/sec: 2.89395\n",
      "INFO:tensorflow:global_step/sec: 0.0899999\n",
      "I1117 15:29:07.163012 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899999\n",
      "INFO:tensorflow:examples/sec: 2.88\n",
      "I1117 15:29:07.163225 4595733952 tpu_estimator.py:2160] examples/sec: 2.88\n",
      "INFO:tensorflow:global_step/sec: 0.0909178\n",
      "I1117 15:29:18.161987 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909178\n",
      "INFO:tensorflow:examples/sec: 2.90937\n",
      "I1117 15:29:18.162255 4595733952 tpu_estimator.py:2160] examples/sec: 2.90937\n",
      "INFO:tensorflow:global_step/sec: 0.0905866\n",
      "I1117 15:29:29.201132 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905866\n",
      "INFO:tensorflow:examples/sec: 2.89877\n",
      "I1117 15:29:29.201370 4595733952 tpu_estimator.py:2160] examples/sec: 2.89877\n",
      "INFO:tensorflow:global_step/sec: 0.0905269\n",
      "I1117 15:29:40.247572 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905269\n",
      "INFO:tensorflow:examples/sec: 2.89686\n",
      "I1117 15:29:40.247956 4595733952 tpu_estimator.py:2160] examples/sec: 2.89686\n",
      "INFO:tensorflow:global_step/sec: 0.0907863\n",
      "I1117 15:29:51.262544 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907863\n",
      "INFO:tensorflow:examples/sec: 2.90516\n",
      "I1117 15:29:51.262768 4595733952 tpu_estimator.py:2160] examples/sec: 2.90516\n",
      "INFO:tensorflow:global_step/sec: 0.090998\n",
      "I1117 15:30:02.251698 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090998\n",
      "INFO:tensorflow:examples/sec: 2.91194\n",
      "I1117 15:30:02.251924 4595733952 tpu_estimator.py:2160] examples/sec: 2.91194\n",
      "INFO:tensorflow:global_step/sec: 0.0882062\n",
      "I1117 15:30:13.588770 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882062\n",
      "INFO:tensorflow:examples/sec: 2.8226\n",
      "I1117 15:30:13.588998 4595733952 tpu_estimator.py:2160] examples/sec: 2.8226\n",
      "INFO:tensorflow:global_step/sec: 0.0822829\n",
      "I1117 15:30:25.741908 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0822829\n",
      "INFO:tensorflow:examples/sec: 2.63305\n",
      "I1117 15:30:25.742115 4595733952 tpu_estimator.py:2160] examples/sec: 2.63305\n",
      "INFO:tensorflow:global_step/sec: 0.0899508\n",
      "I1117 15:30:36.859136 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899508\n",
      "INFO:tensorflow:examples/sec: 2.87842\n",
      "I1117 15:30:36.859343 4595733952 tpu_estimator.py:2160] examples/sec: 2.87842\n",
      "INFO:tensorflow:global_step/sec: 0.0902821\n",
      "I1117 15:30:47.935542 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902821\n",
      "INFO:tensorflow:examples/sec: 2.88903\n",
      "I1117 15:30:47.935759 4595733952 tpu_estimator.py:2160] examples/sec: 2.88903\n",
      "INFO:tensorflow:global_step/sec: 0.0898654\n",
      "I1117 15:30:59.063333 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898654\n",
      "INFO:tensorflow:examples/sec: 2.87569\n",
      "I1117 15:30:59.063557 4595733952 tpu_estimator.py:2160] examples/sec: 2.87569\n",
      "INFO:tensorflow:global_step/sec: 0.0900515\n",
      "I1117 15:31:10.168044 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900515\n",
      "INFO:tensorflow:examples/sec: 2.88165\n",
      "I1117 15:31:10.168271 4595733952 tpu_estimator.py:2160] examples/sec: 2.88165\n",
      "INFO:tensorflow:global_step/sec: 0.0901587\n",
      "I1117 15:31:21.259599 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901587\n",
      "INFO:tensorflow:examples/sec: 2.88508\n",
      "I1117 15:31:21.259827 4595733952 tpu_estimator.py:2160] examples/sec: 2.88508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0903608\n",
      "I1117 15:31:32.326344 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903608\n",
      "INFO:tensorflow:examples/sec: 2.89155\n",
      "I1117 15:31:32.326563 4595733952 tpu_estimator.py:2160] examples/sec: 2.89155\n",
      "INFO:tensorflow:global_step/sec: 0.0895387\n",
      "I1117 15:31:43.494696 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895387\n",
      "INFO:tensorflow:examples/sec: 2.86524\n",
      "I1117 15:31:43.494915 4595733952 tpu_estimator.py:2160] examples/sec: 2.86524\n",
      "INFO:tensorflow:global_step/sec: 0.0901089\n",
      "I1117 15:31:54.592381 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901089\n",
      "INFO:tensorflow:examples/sec: 2.88348\n",
      "I1117 15:31:54.592627 4595733952 tpu_estimator.py:2160] examples/sec: 2.88348\n",
      "INFO:tensorflow:global_step/sec: 0.089905\n",
      "I1117 15:32:05.715229 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089905\n",
      "INFO:tensorflow:examples/sec: 2.87696\n",
      "I1117 15:32:05.715462 4595733952 tpu_estimator.py:2160] examples/sec: 2.87696\n",
      "INFO:tensorflow:global_step/sec: 0.0900291\n",
      "I1117 15:32:16.822768 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900291\n",
      "INFO:tensorflow:examples/sec: 2.88093\n",
      "I1117 15:32:16.823012 4595733952 tpu_estimator.py:2160] examples/sec: 2.88093\n",
      "INFO:tensorflow:global_step/sec: 0.0902066\n",
      "I1117 15:32:27.908424 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902066\n",
      "INFO:tensorflow:examples/sec: 2.88661\n",
      "I1117 15:32:27.908656 4595733952 tpu_estimator.py:2160] examples/sec: 2.88661\n",
      "INFO:tensorflow:global_step/sec: 0.090055\n",
      "I1117 15:32:39.012747 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090055\n",
      "INFO:tensorflow:examples/sec: 2.88176\n",
      "I1117 15:32:39.012972 4595733952 tpu_estimator.py:2160] examples/sec: 2.88176\n",
      "INFO:tensorflow:global_step/sec: 0.0902692\n",
      "I1117 15:32:50.090733 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902692\n",
      "INFO:tensorflow:examples/sec: 2.88862\n",
      "I1117 15:32:50.090997 4595733952 tpu_estimator.py:2160] examples/sec: 2.88862\n",
      "INFO:tensorflow:global_step/sec: 0.0904849\n",
      "I1117 15:33:01.142287 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904849\n",
      "INFO:tensorflow:examples/sec: 2.89552\n",
      "I1117 15:33:01.142693 4595733952 tpu_estimator.py:2160] examples/sec: 2.89552\n",
      "INFO:tensorflow:global_step/sec: 0.0908117\n",
      "I1117 15:33:12.154103 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908117\n",
      "INFO:tensorflow:examples/sec: 2.90597\n",
      "I1117 15:33:12.154330 4595733952 tpu_estimator.py:2160] examples/sec: 2.90597\n",
      "INFO:tensorflow:global_step/sec: 0.0906104\n",
      "I1117 15:33:23.190343 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906104\n",
      "INFO:tensorflow:examples/sec: 2.89953\n",
      "I1117 15:33:23.190572 4595733952 tpu_estimator.py:2160] examples/sec: 2.89953\n",
      "INFO:tensorflow:global_step/sec: 0.0897707\n",
      "I1117 15:33:34.329829 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897707\n",
      "INFO:tensorflow:examples/sec: 2.87266\n",
      "I1117 15:33:34.330049 4595733952 tpu_estimator.py:2160] examples/sec: 2.87266\n",
      "INFO:tensorflow:global_step/sec: 0.0900055\n",
      "I1117 15:33:45.440286 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900055\n",
      "INFO:tensorflow:examples/sec: 2.88018\n",
      "I1117 15:33:45.440532 4595733952 tpu_estimator.py:2160] examples/sec: 2.88018\n",
      "INFO:tensorflow:global_step/sec: 0.0897048\n",
      "I1117 15:33:56.587985 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897048\n",
      "INFO:tensorflow:examples/sec: 2.87055\n",
      "I1117 15:33:56.588213 4595733952 tpu_estimator.py:2160] examples/sec: 2.87055\n",
      "INFO:tensorflow:global_step/sec: 0.0897744\n",
      "I1117 15:34:07.726989 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897744\n",
      "INFO:tensorflow:examples/sec: 2.87278\n",
      "I1117 15:34:07.727226 4595733952 tpu_estimator.py:2160] examples/sec: 2.87278\n",
      "INFO:tensorflow:global_step/sec: 0.0898882\n",
      "I1117 15:34:18.851927 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898882\n",
      "INFO:tensorflow:examples/sec: 2.87642\n",
      "I1117 15:34:18.852163 4595733952 tpu_estimator.py:2160] examples/sec: 2.87642\n",
      "INFO:tensorflow:global_step/sec: 0.0903872\n",
      "I1117 15:34:29.915428 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903872\n",
      "INFO:tensorflow:examples/sec: 2.89239\n",
      "I1117 15:34:29.915653 4595733952 tpu_estimator.py:2160] examples/sec: 2.89239\n",
      "INFO:tensorflow:global_step/sec: 0.0905982\n",
      "I1117 15:34:40.953178 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905982\n",
      "INFO:tensorflow:examples/sec: 2.89914\n",
      "I1117 15:34:40.953398 4595733952 tpu_estimator.py:2160] examples/sec: 2.89914\n",
      "INFO:tensorflow:global_step/sec: 0.090444\n",
      "I1117 15:34:52.009737 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090444\n",
      "INFO:tensorflow:examples/sec: 2.89421\n",
      "I1117 15:34:52.009957 4595733952 tpu_estimator.py:2160] examples/sec: 2.89421\n",
      "INFO:tensorflow:global_step/sec: 0.0904291\n",
      "I1117 15:35:03.068156 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904291\n",
      "INFO:tensorflow:examples/sec: 2.89373\n",
      "I1117 15:35:03.068393 4595733952 tpu_estimator.py:2160] examples/sec: 2.89373\n",
      "INFO:tensorflow:global_step/sec: 0.0906368\n",
      "I1117 15:35:14.101202 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906368\n",
      "INFO:tensorflow:examples/sec: 2.90038\n",
      "I1117 15:35:14.101490 4595733952 tpu_estimator.py:2160] examples/sec: 2.90038\n",
      "INFO:tensorflow:global_step/sec: 0.0900183\n",
      "I1117 15:35:25.210047 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900183\n",
      "INFO:tensorflow:examples/sec: 2.88059\n",
      "I1117 15:35:25.210298 4595733952 tpu_estimator.py:2160] examples/sec: 2.88059\n",
      "INFO:tensorflow:global_step/sec: 0.0897172\n",
      "I1117 15:35:36.356165 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897172\n",
      "INFO:tensorflow:examples/sec: 2.87095\n",
      "I1117 15:35:36.356391 4595733952 tpu_estimator.py:2160] examples/sec: 2.87095\n",
      "INFO:tensorflow:global_step/sec: 0.0903842\n",
      "I1117 15:35:47.420049 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903842\n",
      "INFO:tensorflow:examples/sec: 2.89229\n",
      "I1117 15:35:47.420272 4595733952 tpu_estimator.py:2160] examples/sec: 2.89229\n",
      "INFO:tensorflow:global_step/sec: 0.0898651\n",
      "I1117 15:35:58.547834 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898651\n",
      "INFO:tensorflow:examples/sec: 2.87568\n",
      "I1117 15:35:58.548054 4595733952 tpu_estimator.py:2160] examples/sec: 2.87568\n",
      "INFO:tensorflow:global_step/sec: 0.0900313\n",
      "I1117 15:36:09.655071 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900313\n",
      "INFO:tensorflow:examples/sec: 2.881\n",
      "I1117 15:36:09.655289 4595733952 tpu_estimator.py:2160] examples/sec: 2.881\n",
      "INFO:tensorflow:global_step/sec: 0.0904245\n",
      "I1117 15:36:20.714036 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904245\n",
      "INFO:tensorflow:examples/sec: 2.89358\n",
      "I1117 15:36:20.714262 4595733952 tpu_estimator.py:2160] examples/sec: 2.89358\n",
      "INFO:tensorflow:global_step/sec: 0.0896246\n",
      "I1117 15:36:31.871684 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896246\n",
      "INFO:tensorflow:examples/sec: 2.86799\n",
      "I1117 15:36:31.871908 4595733952 tpu_estimator.py:2160] examples/sec: 2.86799\n",
      "INFO:tensorflow:global_step/sec: 0.0898809\n",
      "I1117 15:36:42.997545 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898809\n",
      "INFO:tensorflow:examples/sec: 2.87619\n",
      "I1117 15:36:42.997767 4595733952 tpu_estimator.py:2160] examples/sec: 2.87619\n",
      "INFO:tensorflow:global_step/sec: 0.0905905\n",
      "I1117 15:36:54.036218 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905905\n",
      "INFO:tensorflow:examples/sec: 2.89889\n",
      "I1117 15:36:54.036455 4595733952 tpu_estimator.py:2160] examples/sec: 2.89889\n",
      "INFO:tensorflow:global_step/sec: 0.0905115\n",
      "I1117 15:37:05.084540 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905115\n",
      "INFO:tensorflow:examples/sec: 2.89637\n",
      "I1117 15:37:05.084786 4595733952 tpu_estimator.py:2160] examples/sec: 2.89637\n",
      "INFO:tensorflow:global_step/sec: 0.0900407\n",
      "I1117 15:37:16.190640 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900407\n",
      "INFO:tensorflow:examples/sec: 2.8813\n",
      "I1117 15:37:16.190891 4595733952 tpu_estimator.py:2160] examples/sec: 2.8813\n",
      "INFO:tensorflow:global_step/sec: 0.0908383\n",
      "I1117 15:37:27.199201 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908383\n",
      "INFO:tensorflow:examples/sec: 2.90683\n",
      "I1117 15:37:27.199430 4595733952 tpu_estimator.py:2160] examples/sec: 2.90683\n",
      "INFO:tensorflow:global_step/sec: 0.0908293\n",
      "I1117 15:37:38.208862 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908293\n",
      "INFO:tensorflow:examples/sec: 2.90654\n",
      "I1117 15:37:38.209089 4595733952 tpu_estimator.py:2160] examples/sec: 2.90654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.090131\n",
      "I1117 15:37:49.303815 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090131\n",
      "INFO:tensorflow:examples/sec: 2.88419\n",
      "I1117 15:37:49.304039 4595733952 tpu_estimator.py:2160] examples/sec: 2.88419\n",
      "INFO:tensorflow:global_step/sec: 0.0901286\n",
      "I1117 15:38:00.399078 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901286\n",
      "INFO:tensorflow:examples/sec: 2.88411\n",
      "I1117 15:38:00.399307 4595733952 tpu_estimator.py:2160] examples/sec: 2.88411\n",
      "INFO:tensorflow:global_step/sec: 0.0903023\n",
      "I1117 15:38:11.473015 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903023\n",
      "INFO:tensorflow:examples/sec: 2.88967\n",
      "I1117 15:38:11.473240 4595733952 tpu_estimator.py:2160] examples/sec: 2.88967\n",
      "INFO:tensorflow:global_step/sec: 0.0906074\n",
      "I1117 15:38:22.509612 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906074\n",
      "INFO:tensorflow:examples/sec: 2.89944\n",
      "I1117 15:38:22.509831 4595733952 tpu_estimator.py:2160] examples/sec: 2.89944\n",
      "INFO:tensorflow:global_step/sec: 0.0902558\n",
      "I1117 15:38:33.589240 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902558\n",
      "INFO:tensorflow:examples/sec: 2.88819\n",
      "I1117 15:38:33.589485 4595733952 tpu_estimator.py:2160] examples/sec: 2.88819\n",
      "INFO:tensorflow:global_step/sec: 0.0897015\n",
      "I1117 15:38:44.737313 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897015\n",
      "INFO:tensorflow:examples/sec: 2.87045\n",
      "I1117 15:38:44.737539 4595733952 tpu_estimator.py:2160] examples/sec: 2.87045\n",
      "INFO:tensorflow:global_step/sec: 0.0908119\n",
      "I1117 15:38:55.749106 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908119\n",
      "INFO:tensorflow:examples/sec: 2.90598\n",
      "I1117 15:38:55.749331 4595733952 tpu_estimator.py:2160] examples/sec: 2.90598\n",
      "INFO:tensorflow:global_step/sec: 0.0901699\n",
      "I1117 15:39:06.839279 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901699\n",
      "INFO:tensorflow:examples/sec: 2.88544\n",
      "I1117 15:39:06.839501 4595733952 tpu_estimator.py:2160] examples/sec: 2.88544\n",
      "INFO:tensorflow:global_step/sec: 0.0900623\n",
      "I1117 15:39:17.942687 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900623\n",
      "INFO:tensorflow:examples/sec: 2.88199\n",
      "I1117 15:39:17.943088 4595733952 tpu_estimator.py:2160] examples/sec: 2.88199\n",
      "INFO:tensorflow:global_step/sec: 0.0900905\n",
      "I1117 15:39:29.042634 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900905\n",
      "INFO:tensorflow:examples/sec: 2.8829\n",
      "I1117 15:39:29.042862 4595733952 tpu_estimator.py:2160] examples/sec: 2.8829\n",
      "INFO:tensorflow:global_step/sec: 0.0904875\n",
      "I1117 15:39:40.093884 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904875\n",
      "INFO:tensorflow:examples/sec: 2.8956\n",
      "I1117 15:39:40.094106 4595733952 tpu_estimator.py:2160] examples/sec: 2.8956\n",
      "INFO:tensorflow:global_step/sec: 0.0901754\n",
      "I1117 15:39:51.183389 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901754\n",
      "INFO:tensorflow:examples/sec: 2.88561\n",
      "I1117 15:39:51.183614 4595733952 tpu_estimator.py:2160] examples/sec: 2.88561\n",
      "INFO:tensorflow:global_step/sec: 0.0904536\n",
      "I1117 15:40:02.238781 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904536\n",
      "INFO:tensorflow:examples/sec: 2.89452\n",
      "I1117 15:40:02.239011 4595733952 tpu_estimator.py:2160] examples/sec: 2.89452\n",
      "INFO:tensorflow:global_step/sec: 0.0902823\n",
      "I1117 15:40:13.315151 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902823\n",
      "INFO:tensorflow:examples/sec: 2.88903\n",
      "I1117 15:40:13.315389 4595733952 tpu_estimator.py:2160] examples/sec: 2.88903\n",
      "INFO:tensorflow:global_step/sec: 0.0903656\n",
      "I1117 15:40:24.381314 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903656\n",
      "INFO:tensorflow:examples/sec: 2.8917\n",
      "I1117 15:40:24.381536 4595733952 tpu_estimator.py:2160] examples/sec: 2.8917\n",
      "INFO:tensorflow:global_step/sec: 0.0904308\n",
      "I1117 15:40:35.439493 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904308\n",
      "INFO:tensorflow:examples/sec: 2.89379\n",
      "I1117 15:40:35.439723 4595733952 tpu_estimator.py:2160] examples/sec: 2.89379\n",
      "INFO:tensorflow:global_step/sec: 0.0903931\n",
      "I1117 15:40:46.502297 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903931\n",
      "INFO:tensorflow:examples/sec: 2.89258\n",
      "I1117 15:40:46.502513 4595733952 tpu_estimator.py:2160] examples/sec: 2.89258\n",
      "INFO:tensorflow:global_step/sec: 0.0902091\n",
      "I1117 15:40:57.587643 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902091\n",
      "INFO:tensorflow:examples/sec: 2.88669\n",
      "I1117 15:40:57.587867 4595733952 tpu_estimator.py:2160] examples/sec: 2.88669\n",
      "INFO:tensorflow:global_step/sec: 0.0903371\n",
      "I1117 15:41:08.657289 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903371\n",
      "INFO:tensorflow:examples/sec: 2.89079\n",
      "I1117 15:41:08.657507 4595733952 tpu_estimator.py:2160] examples/sec: 2.89079\n",
      "INFO:tensorflow:global_step/sec: 0.0903143\n",
      "I1117 15:41:19.729748 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903143\n",
      "INFO:tensorflow:examples/sec: 2.89006\n",
      "I1117 15:41:19.729973 4595733952 tpu_estimator.py:2160] examples/sec: 2.89006\n",
      "INFO:tensorflow:global_step/sec: 0.090219\n",
      "I1117 15:41:30.813886 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090219\n",
      "INFO:tensorflow:examples/sec: 2.88701\n",
      "I1117 15:41:30.814110 4595733952 tpu_estimator.py:2160] examples/sec: 2.88701\n",
      "INFO:tensorflow:global_step/sec: 0.0901405\n",
      "I1117 15:41:41.907676 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901405\n",
      "INFO:tensorflow:examples/sec: 2.8845\n",
      "I1117 15:41:41.907898 4595733952 tpu_estimator.py:2160] examples/sec: 2.8845\n",
      "INFO:tensorflow:global_step/sec: 0.0909433\n",
      "I1117 15:41:52.903565 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909433\n",
      "INFO:tensorflow:examples/sec: 2.91018\n",
      "I1117 15:41:52.903789 4595733952 tpu_estimator.py:2160] examples/sec: 2.91018\n",
      "INFO:tensorflow:global_step/sec: 0.0901846\n",
      "I1117 15:42:03.991924 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901846\n",
      "INFO:tensorflow:examples/sec: 2.88591\n",
      "I1117 15:42:03.992141 4595733952 tpu_estimator.py:2160] examples/sec: 2.88591\n",
      "INFO:tensorflow:global_step/sec: 0.0898337\n",
      "I1117 15:42:15.123628 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898337\n",
      "INFO:tensorflow:examples/sec: 2.87468\n",
      "I1117 15:42:15.123878 4595733952 tpu_estimator.py:2160] examples/sec: 2.87468\n",
      "INFO:tensorflow:global_step/sec: 0.0904187\n",
      "I1117 15:42:26.183264 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904187\n",
      "INFO:tensorflow:examples/sec: 2.8934\n",
      "I1117 15:42:26.183486 4595733952 tpu_estimator.py:2160] examples/sec: 2.8934\n",
      "INFO:tensorflow:global_step/sec: 0.0905864\n",
      "I1117 15:42:37.222470 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905864\n",
      "INFO:tensorflow:examples/sec: 2.89877\n",
      "I1117 15:42:37.222946 4595733952 tpu_estimator.py:2160] examples/sec: 2.89877\n",
      "INFO:tensorflow:global_step/sec: 0.0904085\n",
      "I1117 15:42:48.283357 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904085\n",
      "INFO:tensorflow:examples/sec: 2.89307\n",
      "I1117 15:42:48.283596 4595733952 tpu_estimator.py:2160] examples/sec: 2.89307\n",
      "INFO:tensorflow:global_step/sec: 0.0902942\n",
      "I1117 15:42:59.358266 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902942\n",
      "INFO:tensorflow:examples/sec: 2.88941\n",
      "I1117 15:42:59.358495 4595733952 tpu_estimator.py:2160] examples/sec: 2.88941\n",
      "INFO:tensorflow:global_step/sec: 0.0904182\n",
      "I1117 15:43:10.418001 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904182\n",
      "INFO:tensorflow:examples/sec: 2.89338\n",
      "I1117 15:43:10.418230 4595733952 tpu_estimator.py:2160] examples/sec: 2.89338\n",
      "INFO:tensorflow:global_step/sec: 0.0902344\n",
      "I1117 15:43:21.500246 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902344\n",
      "INFO:tensorflow:examples/sec: 2.8875\n",
      "I1117 15:43:21.500475 4595733952 tpu_estimator.py:2160] examples/sec: 2.8875\n",
      "INFO:tensorflow:global_step/sec: 0.0905138\n",
      "I1117 15:43:32.548269 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905138\n",
      "INFO:tensorflow:examples/sec: 2.89644\n",
      "I1117 15:43:32.548490 4595733952 tpu_estimator.py:2160] examples/sec: 2.89644\n",
      "INFO:tensorflow:global_step/sec: 0.0905913\n",
      "I1117 15:43:43.586864 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905913\n",
      "INFO:tensorflow:examples/sec: 2.89892\n",
      "I1117 15:43:43.587090 4595733952 tpu_estimator.py:2160] examples/sec: 2.89892\n",
      "INFO:tensorflow:global_step/sec: 0.0905175\n",
      "I1117 15:43:54.634451 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905175\n",
      "INFO:tensorflow:examples/sec: 2.89656\n",
      "I1117 15:43:54.634676 4595733952 tpu_estimator.py:2160] examples/sec: 2.89656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0904009\n",
      "I1117 15:44:05.696290 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904009\n",
      "INFO:tensorflow:examples/sec: 2.89283\n",
      "I1117 15:44:05.696521 4595733952 tpu_estimator.py:2160] examples/sec: 2.89283\n",
      "INFO:tensorflow:global_step/sec: 0.0904139\n",
      "I1117 15:44:16.756538 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904139\n",
      "INFO:tensorflow:examples/sec: 2.89324\n",
      "I1117 15:44:16.756762 4595733952 tpu_estimator.py:2160] examples/sec: 2.89324\n",
      "INFO:tensorflow:global_step/sec: 0.0902444\n",
      "I1117 15:44:27.837582 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902444\n",
      "INFO:tensorflow:examples/sec: 2.88782\n",
      "I1117 15:44:27.837808 4595733952 tpu_estimator.py:2160] examples/sec: 2.88782\n",
      "INFO:tensorflow:global_step/sec: 0.0908879\n",
      "I1117 15:44:38.840120 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908879\n",
      "INFO:tensorflow:examples/sec: 2.90841\n",
      "I1117 15:44:38.840494 4595733952 tpu_estimator.py:2160] examples/sec: 2.90841\n",
      "INFO:tensorflow:global_step/sec: 0.0907595\n",
      "I1117 15:44:49.858238 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907595\n",
      "INFO:tensorflow:examples/sec: 2.9043\n",
      "I1117 15:44:49.858458 4595733952 tpu_estimator.py:2160] examples/sec: 2.9043\n",
      "INFO:tensorflow:global_step/sec: 0.0903618\n",
      "I1117 15:45:00.924875 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903618\n",
      "INFO:tensorflow:examples/sec: 2.89158\n",
      "I1117 15:45:00.925103 4595733952 tpu_estimator.py:2160] examples/sec: 2.89158\n",
      "INFO:tensorflow:global_step/sec: 0.0900238\n",
      "I1117 15:45:12.033051 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900238\n",
      "INFO:tensorflow:examples/sec: 2.88076\n",
      "I1117 15:45:12.033278 4595733952 tpu_estimator.py:2160] examples/sec: 2.88076\n",
      "INFO:tensorflow:global_step/sec: 0.0904872\n",
      "I1117 15:45:23.084339 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904872\n",
      "INFO:tensorflow:examples/sec: 2.89559\n",
      "I1117 15:45:23.084561 4595733952 tpu_estimator.py:2160] examples/sec: 2.89559\n",
      "INFO:tensorflow:global_step/sec: 0.0903299\n",
      "I1117 15:45:34.154864 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903299\n",
      "INFO:tensorflow:examples/sec: 2.89056\n",
      "I1117 15:45:34.155081 4595733952 tpu_estimator.py:2160] examples/sec: 2.89056\n",
      "INFO:tensorflow:global_step/sec: 0.0903698\n",
      "I1117 15:45:45.220525 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903698\n",
      "INFO:tensorflow:examples/sec: 2.89183\n",
      "I1117 15:45:45.220735 4595733952 tpu_estimator.py:2160] examples/sec: 2.89183\n",
      "INFO:tensorflow:global_step/sec: 0.0906407\n",
      "I1117 15:45:56.253078 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906407\n",
      "INFO:tensorflow:examples/sec: 2.9005\n",
      "I1117 15:45:56.253289 4595733952 tpu_estimator.py:2160] examples/sec: 2.9005\n",
      "INFO:tensorflow:global_step/sec: 0.0907659\n",
      "I1117 15:46:07.270443 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907659\n",
      "INFO:tensorflow:examples/sec: 2.90451\n",
      "I1117 15:46:07.270670 4595733952 tpu_estimator.py:2160] examples/sec: 2.90451\n",
      "INFO:tensorflow:global_step/sec: 0.0905402\n",
      "I1117 15:46:18.315263 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905402\n",
      "INFO:tensorflow:examples/sec: 2.89729\n",
      "I1117 15:46:18.315482 4595733952 tpu_estimator.py:2160] examples/sec: 2.89729\n",
      "INFO:tensorflow:global_step/sec: 0.090378\n",
      "I1117 15:46:29.379939 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090378\n",
      "INFO:tensorflow:examples/sec: 2.8921\n",
      "I1117 15:46:29.380172 4595733952 tpu_estimator.py:2160] examples/sec: 2.8921\n",
      "INFO:tensorflow:global_step/sec: 0.0903809\n",
      "I1117 15:46:40.444194 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903809\n",
      "INFO:tensorflow:examples/sec: 2.89219\n",
      "I1117 15:46:40.444441 4595733952 tpu_estimator.py:2160] examples/sec: 2.89219\n",
      "INFO:tensorflow:global_step/sec: 0.0904883\n",
      "I1117 15:46:51.495368 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904883\n",
      "INFO:tensorflow:examples/sec: 2.89562\n",
      "I1117 15:46:51.495798 4595733952 tpu_estimator.py:2160] examples/sec: 2.89562\n",
      "INFO:tensorflow:global_step/sec: 0.0905288\n",
      "I1117 15:47:02.541596 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905288\n",
      "INFO:tensorflow:examples/sec: 2.89692\n",
      "I1117 15:47:02.541845 4595733952 tpu_estimator.py:2160] examples/sec: 2.89692\n",
      "INFO:tensorflow:global_step/sec: 0.0907624\n",
      "I1117 15:47:13.559328 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907624\n",
      "INFO:tensorflow:examples/sec: 2.9044\n",
      "I1117 15:47:13.559545 4595733952 tpu_estimator.py:2160] examples/sec: 2.9044\n",
      "INFO:tensorflow:global_step/sec: 0.0902879\n",
      "I1117 15:47:24.634999 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902879\n",
      "INFO:tensorflow:examples/sec: 2.88921\n",
      "I1117 15:47:24.635436 4595733952 tpu_estimator.py:2160] examples/sec: 2.88921\n",
      "INFO:tensorflow:global_step/sec: 0.0903414\n",
      "I1117 15:47:35.704134 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903414\n",
      "INFO:tensorflow:examples/sec: 2.89093\n",
      "I1117 15:47:35.704358 4595733952 tpu_estimator.py:2160] examples/sec: 2.89093\n",
      "INFO:tensorflow:global_step/sec: 0.0906517\n",
      "I1117 15:47:46.735369 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906517\n",
      "INFO:tensorflow:examples/sec: 2.90085\n",
      "I1117 15:47:46.735601 4595733952 tpu_estimator.py:2160] examples/sec: 2.90085\n",
      "INFO:tensorflow:global_step/sec: 0.0902176\n",
      "I1117 15:47:57.819678 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902176\n",
      "INFO:tensorflow:examples/sec: 2.88696\n",
      "I1117 15:47:57.819901 4595733952 tpu_estimator.py:2160] examples/sec: 2.88696\n",
      "INFO:tensorflow:global_step/sec: 0.0907132\n",
      "I1117 15:48:08.843425 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907132\n",
      "INFO:tensorflow:examples/sec: 2.90282\n",
      "I1117 15:48:08.843647 4595733952 tpu_estimator.py:2160] examples/sec: 2.90282\n",
      "INFO:tensorflow:global_step/sec: 0.090461\n",
      "I1117 15:48:19.897911 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090461\n",
      "INFO:tensorflow:examples/sec: 2.89475\n",
      "I1117 15:48:19.898133 4595733952 tpu_estimator.py:2160] examples/sec: 2.89475\n",
      "INFO:tensorflow:global_step/sec: 0.0907163\n",
      "I1117 15:48:30.921306 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907163\n",
      "INFO:tensorflow:examples/sec: 2.90292\n",
      "I1117 15:48:30.921538 4595733952 tpu_estimator.py:2160] examples/sec: 2.90292\n",
      "INFO:tensorflow:global_step/sec: 0.0906245\n",
      "I1117 15:48:41.955837 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906245\n",
      "INFO:tensorflow:examples/sec: 2.89998\n",
      "I1117 15:48:41.956063 4595733952 tpu_estimator.py:2160] examples/sec: 2.89998\n",
      "INFO:tensorflow:global_step/sec: 0.0908672\n",
      "I1117 15:48:52.960923 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908672\n",
      "INFO:tensorflow:examples/sec: 2.90775\n",
      "I1117 15:48:52.961226 4595733952 tpu_estimator.py:2160] examples/sec: 2.90775\n",
      "INFO:tensorflow:global_step/sec: 0.090577\n",
      "I1117 15:49:04.001259 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090577\n",
      "INFO:tensorflow:examples/sec: 2.89846\n",
      "I1117 15:49:04.001519 4595733952 tpu_estimator.py:2160] examples/sec: 2.89846\n",
      "INFO:tensorflow:global_step/sec: 0.0904403\n",
      "I1117 15:49:15.058290 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904403\n",
      "INFO:tensorflow:examples/sec: 2.89409\n",
      "I1117 15:49:15.058521 4595733952 tpu_estimator.py:2160] examples/sec: 2.89409\n",
      "INFO:tensorflow:global_step/sec: 0.0906712\n",
      "I1117 15:49:26.087130 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906712\n",
      "INFO:tensorflow:examples/sec: 2.90148\n",
      "I1117 15:49:26.087355 4595733952 tpu_estimator.py:2160] examples/sec: 2.90148\n",
      "INFO:tensorflow:global_step/sec: 0.0906678\n",
      "I1117 15:49:37.116408 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906678\n",
      "INFO:tensorflow:examples/sec: 2.90137\n",
      "I1117 15:49:37.116631 4595733952 tpu_estimator.py:2160] examples/sec: 2.90137\n",
      "INFO:tensorflow:global_step/sec: 0.0901609\n",
      "I1117 15:49:48.207715 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901609\n",
      "INFO:tensorflow:examples/sec: 2.88515\n",
      "I1117 15:49:48.207965 4595733952 tpu_estimator.py:2160] examples/sec: 2.88515\n",
      "INFO:tensorflow:global_step/sec: 0.0905206\n",
      "I1117 15:49:59.254909 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905206\n",
      "INFO:tensorflow:examples/sec: 2.89666\n",
      "I1117 15:49:59.255131 4595733952 tpu_estimator.py:2160] examples/sec: 2.89666\n",
      "INFO:tensorflow:global_step/sec: 0.0904013\n",
      "I1117 15:50:10.316689 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904013\n",
      "INFO:tensorflow:examples/sec: 2.89284\n",
      "I1117 15:50:10.316912 4595733952 tpu_estimator.py:2160] examples/sec: 2.89284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0906528\n",
      "I1117 15:50:21.347781 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906528\n",
      "INFO:tensorflow:examples/sec: 2.90089\n",
      "I1117 15:50:21.347993 4595733952 tpu_estimator.py:2160] examples/sec: 2.90089\n",
      "INFO:tensorflow:global_step/sec: 0.0900874\n",
      "I1117 15:50:32.448117 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900874\n",
      "INFO:tensorflow:examples/sec: 2.8828\n",
      "I1117 15:50:32.448338 4595733952 tpu_estimator.py:2160] examples/sec: 2.8828\n",
      "INFO:tensorflow:global_step/sec: 0.0904181\n",
      "I1117 15:50:43.507852 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904181\n",
      "INFO:tensorflow:examples/sec: 2.89338\n",
      "I1117 15:50:43.508077 4595733952 tpu_estimator.py:2160] examples/sec: 2.89338\n",
      "INFO:tensorflow:global_step/sec: 0.0898166\n",
      "I1117 15:50:54.641659 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898166\n",
      "INFO:tensorflow:examples/sec: 2.87413\n",
      "I1117 15:50:54.642088 4595733952 tpu_estimator.py:2160] examples/sec: 2.87413\n",
      "INFO:tensorflow:global_step/sec: 0.0905206\n",
      "I1117 15:51:05.688864 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905206\n",
      "INFO:tensorflow:examples/sec: 2.89666\n",
      "I1117 15:51:05.689090 4595733952 tpu_estimator.py:2160] examples/sec: 2.89666\n",
      "INFO:tensorflow:global_step/sec: 0.0905696\n",
      "I1117 15:51:16.730108 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905696\n",
      "INFO:tensorflow:examples/sec: 2.89823\n",
      "I1117 15:51:16.730339 4595733952 tpu_estimator.py:2160] examples/sec: 2.89823\n",
      "INFO:tensorflow:global_step/sec: 0.0905506\n",
      "I1117 15:51:27.773653 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905506\n",
      "INFO:tensorflow:examples/sec: 2.89762\n",
      "I1117 15:51:27.773879 4595733952 tpu_estimator.py:2160] examples/sec: 2.89762\n",
      "INFO:tensorflow:global_step/sec: 0.0898891\n",
      "I1117 15:51:38.898490 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898891\n",
      "INFO:tensorflow:examples/sec: 2.87645\n",
      "I1117 15:51:38.898727 4595733952 tpu_estimator.py:2160] examples/sec: 2.87645\n",
      "INFO:tensorflow:global_step/sec: 0.0903247\n",
      "I1117 15:51:49.969640 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903247\n",
      "INFO:tensorflow:examples/sec: 2.89039\n",
      "I1117 15:51:49.969863 4595733952 tpu_estimator.py:2160] examples/sec: 2.89039\n",
      "INFO:tensorflow:global_step/sec: 0.0908435\n",
      "I1117 15:52:00.977591 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908435\n",
      "INFO:tensorflow:examples/sec: 2.90699\n",
      "I1117 15:52:00.977815 4595733952 tpu_estimator.py:2160] examples/sec: 2.90699\n",
      "INFO:tensorflow:global_step/sec: 0.0901838\n",
      "I1117 15:52:12.066059 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901838\n",
      "INFO:tensorflow:examples/sec: 2.88588\n",
      "I1117 15:52:12.066286 4595733952 tpu_estimator.py:2160] examples/sec: 2.88588\n",
      "INFO:tensorflow:global_step/sec: 0.0901715\n",
      "I1117 15:52:23.156039 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901715\n",
      "INFO:tensorflow:examples/sec: 2.88549\n",
      "I1117 15:52:23.156266 4595733952 tpu_estimator.py:2160] examples/sec: 2.88549\n",
      "INFO:tensorflow:global_step/sec: 0.0899098\n",
      "I1117 15:52:34.278285 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899098\n",
      "INFO:tensorflow:examples/sec: 2.87711\n",
      "I1117 15:52:34.278500 4595733952 tpu_estimator.py:2160] examples/sec: 2.87711\n",
      "INFO:tensorflow:global_step/sec: 0.0902487\n",
      "I1117 15:52:45.358793 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902487\n",
      "INFO:tensorflow:examples/sec: 2.88796\n",
      "I1117 15:52:45.359025 4595733952 tpu_estimator.py:2160] examples/sec: 2.88796\n",
      "INFO:tensorflow:global_step/sec: 0.0905499\n",
      "I1117 15:52:56.402451 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905499\n",
      "INFO:tensorflow:examples/sec: 2.8976\n",
      "I1117 15:52:56.402705 4595733952 tpu_estimator.py:2160] examples/sec: 2.8976\n",
      "INFO:tensorflow:global_step/sec: 0.0901818\n",
      "I1117 15:53:07.491122 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901818\n",
      "INFO:tensorflow:examples/sec: 2.88582\n",
      "I1117 15:53:07.491386 4595733952 tpu_estimator.py:2160] examples/sec: 2.88582\n",
      "INFO:tensorflow:global_step/sec: 0.0905941\n",
      "I1117 15:53:18.529387 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905941\n",
      "INFO:tensorflow:examples/sec: 2.89901\n",
      "I1117 15:53:18.529612 4595733952 tpu_estimator.py:2160] examples/sec: 2.89901\n",
      "INFO:tensorflow:global_step/sec: 0.0904348\n",
      "I1117 15:53:29.587092 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904348\n",
      "INFO:tensorflow:examples/sec: 2.89391\n",
      "I1117 15:53:29.587319 4595733952 tpu_estimator.py:2160] examples/sec: 2.89391\n",
      "INFO:tensorflow:global_step/sec: 0.0904551\n",
      "I1117 15:53:40.642283 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904551\n",
      "INFO:tensorflow:examples/sec: 2.89456\n",
      "I1117 15:53:40.642501 4595733952 tpu_estimator.py:2160] examples/sec: 2.89456\n",
      "INFO:tensorflow:global_step/sec: 0.0898015\n",
      "I1117 15:53:51.777955 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898015\n",
      "INFO:tensorflow:examples/sec: 2.87365\n",
      "I1117 15:53:51.778181 4595733952 tpu_estimator.py:2160] examples/sec: 2.87365\n",
      "INFO:tensorflow:global_step/sec: 0.0904259\n",
      "I1117 15:54:02.836766 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904259\n",
      "INFO:tensorflow:examples/sec: 2.89363\n",
      "I1117 15:54:02.837140 4595733952 tpu_estimator.py:2160] examples/sec: 2.89363\n",
      "INFO:tensorflow:global_step/sec: 0.0902328\n",
      "I1117 15:54:13.919178 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902328\n",
      "INFO:tensorflow:examples/sec: 2.88745\n",
      "I1117 15:54:13.919400 4595733952 tpu_estimator.py:2160] examples/sec: 2.88745\n",
      "INFO:tensorflow:global_step/sec: 0.090728\n",
      "I1117 15:54:24.941133 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090728\n",
      "INFO:tensorflow:examples/sec: 2.9033\n",
      "I1117 15:54:24.941364 4595733952 tpu_estimator.py:2160] examples/sec: 2.9033\n",
      "INFO:tensorflow:global_step/sec: 0.0900912\n",
      "I1117 15:54:36.040968 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900912\n",
      "INFO:tensorflow:examples/sec: 2.88292\n",
      "I1117 15:54:36.041187 4595733952 tpu_estimator.py:2160] examples/sec: 2.88292\n",
      "INFO:tensorflow:global_step/sec: 0.0905613\n",
      "I1117 15:54:47.083235 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905613\n",
      "INFO:tensorflow:examples/sec: 2.89796\n",
      "I1117 15:54:47.083455 4595733952 tpu_estimator.py:2160] examples/sec: 2.89796\n",
      "INFO:tensorflow:global_step/sec: 0.0901602\n",
      "I1117 15:54:58.174616 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901602\n",
      "INFO:tensorflow:examples/sec: 2.88513\n",
      "I1117 15:54:58.174855 4595733952 tpu_estimator.py:2160] examples/sec: 2.88513\n",
      "INFO:tensorflow:global_step/sec: 0.0902529\n",
      "I1117 15:55:09.254585 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902529\n",
      "INFO:tensorflow:examples/sec: 2.88809\n",
      "I1117 15:55:09.254812 4595733952 tpu_estimator.py:2160] examples/sec: 2.88809\n",
      "INFO:tensorflow:global_step/sec: 0.0904172\n",
      "I1117 15:55:20.314429 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904172\n",
      "INFO:tensorflow:examples/sec: 2.89335\n",
      "I1117 15:55:20.314650 4595733952 tpu_estimator.py:2160] examples/sec: 2.89335\n",
      "INFO:tensorflow:global_step/sec: 0.0901001\n",
      "I1117 15:55:31.413199 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901001\n",
      "INFO:tensorflow:examples/sec: 2.8832\n",
      "I1117 15:55:31.413425 4595733952 tpu_estimator.py:2160] examples/sec: 2.8832\n",
      "INFO:tensorflow:global_step/sec: 0.0898788\n",
      "I1117 15:55:42.539290 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898788\n",
      "INFO:tensorflow:examples/sec: 2.87612\n",
      "I1117 15:55:42.539690 4595733952 tpu_estimator.py:2160] examples/sec: 2.87612\n",
      "INFO:tensorflow:global_step/sec: 0.0904277\n",
      "I1117 15:55:53.597845 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904277\n",
      "INFO:tensorflow:examples/sec: 2.89369\n",
      "I1117 15:55:53.598064 4595733952 tpu_estimator.py:2160] examples/sec: 2.89369\n",
      "INFO:tensorflow:global_step/sec: 0.0904811\n",
      "I1117 15:56:04.649893 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904811\n",
      "INFO:tensorflow:examples/sec: 2.89539\n",
      "I1117 15:56:04.650110 4595733952 tpu_estimator.py:2160] examples/sec: 2.89539\n",
      "INFO:tensorflow:global_step/sec: 0.0894859\n",
      "I1117 15:56:15.824858 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894859\n",
      "INFO:tensorflow:examples/sec: 2.86355\n",
      "I1117 15:56:15.825077 4595733952 tpu_estimator.py:2160] examples/sec: 2.86355\n",
      "INFO:tensorflow:global_step/sec: 0.0900211\n",
      "I1117 15:56:26.933341 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900211\n",
      "INFO:tensorflow:examples/sec: 2.88068\n",
      "I1117 15:56:26.933565 4595733952 tpu_estimator.py:2160] examples/sec: 2.88068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0901419\n",
      "I1117 15:56:38.026957 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901419\n",
      "INFO:tensorflow:examples/sec: 2.88454\n",
      "I1117 15:56:38.027179 4595733952 tpu_estimator.py:2160] examples/sec: 2.88454\n",
      "INFO:tensorflow:global_step/sec: 0.0899211\n",
      "I1117 15:56:49.147848 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899211\n",
      "INFO:tensorflow:examples/sec: 2.87747\n",
      "I1117 15:56:49.148160 4595733952 tpu_estimator.py:2160] examples/sec: 2.87747\n",
      "INFO:tensorflow:global_step/sec: 0.090459\n",
      "I1117 15:57:00.202553 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090459\n",
      "INFO:tensorflow:examples/sec: 2.89469\n",
      "I1117 15:57:00.202789 4595733952 tpu_estimator.py:2160] examples/sec: 2.89469\n",
      "INFO:tensorflow:global_step/sec: 0.0903115\n",
      "I1117 15:57:11.275362 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903115\n",
      "INFO:tensorflow:examples/sec: 2.88997\n",
      "I1117 15:57:11.275583 4595733952 tpu_estimator.py:2160] examples/sec: 2.88997\n",
      "INFO:tensorflow:global_step/sec: 0.0906972\n",
      "I1117 15:57:22.301057 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906972\n",
      "INFO:tensorflow:examples/sec: 2.90231\n",
      "I1117 15:57:22.301284 4595733952 tpu_estimator.py:2160] examples/sec: 2.90231\n",
      "INFO:tensorflow:global_step/sec: 0.0901815\n",
      "I1117 15:57:33.389765 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901815\n",
      "INFO:tensorflow:examples/sec: 2.88581\n",
      "I1117 15:57:33.389961 4595733952 tpu_estimator.py:2160] examples/sec: 2.88581\n",
      "INFO:tensorflow:global_step/sec: 0.0901802\n",
      "I1117 15:57:44.478714 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901802\n",
      "INFO:tensorflow:examples/sec: 2.88577\n",
      "I1117 15:57:44.478937 4595733952 tpu_estimator.py:2160] examples/sec: 2.88577\n",
      "INFO:tensorflow:global_step/sec: 0.0906115\n",
      "I1117 15:57:55.514826 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906115\n",
      "INFO:tensorflow:examples/sec: 2.89957\n",
      "I1117 15:57:55.515051 4595733952 tpu_estimator.py:2160] examples/sec: 2.89957\n",
      "INFO:tensorflow:global_step/sec: 0.0905202\n",
      "I1117 15:58:06.562084 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905202\n",
      "INFO:tensorflow:examples/sec: 2.89665\n",
      "I1117 15:58:06.562309 4595733952 tpu_estimator.py:2160] examples/sec: 2.89665\n",
      "INFO:tensorflow:global_step/sec: 0.0904853\n",
      "I1117 15:58:17.613600 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904853\n",
      "INFO:tensorflow:examples/sec: 2.89553\n",
      "I1117 15:58:17.613826 4595733952 tpu_estimator.py:2160] examples/sec: 2.89553\n",
      "INFO:tensorflow:global_step/sec: 0.0897147\n",
      "I1117 15:58:28.760040 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897147\n",
      "INFO:tensorflow:examples/sec: 2.87087\n",
      "I1117 15:58:28.760261 4595733952 tpu_estimator.py:2160] examples/sec: 2.87087\n",
      "INFO:tensorflow:global_step/sec: 0.0891457\n",
      "I1117 15:58:39.977642 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891457\n",
      "INFO:tensorflow:examples/sec: 2.85266\n",
      "I1117 15:58:39.977864 4595733952 tpu_estimator.py:2160] examples/sec: 2.85266\n",
      "INFO:tensorflow:global_step/sec: 0.0903795\n",
      "I1117 15:58:51.042104 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903795\n",
      "INFO:tensorflow:examples/sec: 2.89214\n",
      "I1117 15:58:51.042335 4595733952 tpu_estimator.py:2160] examples/sec: 2.89214\n",
      "INFO:tensorflow:global_step/sec: 0.0900481\n",
      "I1117 15:59:02.147295 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900481\n",
      "INFO:tensorflow:examples/sec: 2.88154\n",
      "I1117 15:59:02.147524 4595733952 tpu_estimator.py:2160] examples/sec: 2.88154\n",
      "INFO:tensorflow:global_step/sec: 0.0897613\n",
      "I1117 15:59:13.287942 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897613\n",
      "INFO:tensorflow:examples/sec: 2.87236\n",
      "I1117 15:59:13.288172 4595733952 tpu_estimator.py:2160] examples/sec: 2.87236\n",
      "INFO:tensorflow:global_step/sec: 0.0908324\n",
      "I1117 15:59:24.297228 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908324\n",
      "INFO:tensorflow:examples/sec: 2.90664\n",
      "I1117 15:59:24.297451 4595733952 tpu_estimator.py:2160] examples/sec: 2.90664\n",
      "INFO:tensorflow:global_step/sec: 0.0904629\n",
      "I1117 15:59:35.351481 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904629\n",
      "INFO:tensorflow:examples/sec: 2.89481\n",
      "I1117 15:59:35.351704 4595733952 tpu_estimator.py:2160] examples/sec: 2.89481\n",
      "INFO:tensorflow:global_step/sec: 0.0903767\n",
      "I1117 15:59:46.416272 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903767\n",
      "INFO:tensorflow:examples/sec: 2.89206\n",
      "I1117 15:59:46.416498 4595733952 tpu_estimator.py:2160] examples/sec: 2.89206\n",
      "INFO:tensorflow:global_step/sec: 0.0901715\n",
      "I1117 15:59:57.506262 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901715\n",
      "INFO:tensorflow:examples/sec: 2.88549\n",
      "I1117 15:59:57.506495 4595733952 tpu_estimator.py:2160] examples/sec: 2.88549\n",
      "INFO:tensorflow:global_step/sec: 0.0902815\n",
      "I1117 16:00:08.582728 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902815\n",
      "INFO:tensorflow:examples/sec: 2.88901\n",
      "I1117 16:00:08.582964 4595733952 tpu_estimator.py:2160] examples/sec: 2.88901\n",
      "INFO:tensorflow:global_step/sec: 0.0900176\n",
      "I1117 16:00:19.691657 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900176\n",
      "INFO:tensorflow:examples/sec: 2.88056\n",
      "I1117 16:00:19.691879 4595733952 tpu_estimator.py:2160] examples/sec: 2.88056\n",
      "INFO:tensorflow:global_step/sec: 0.0895991\n",
      "I1117 16:00:30.852488 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895991\n",
      "INFO:tensorflow:examples/sec: 2.86717\n",
      "I1117 16:00:30.852712 4595733952 tpu_estimator.py:2160] examples/sec: 2.86717\n",
      "INFO:tensorflow:global_step/sec: 0.0885568\n",
      "I1117 16:00:42.144639 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885568\n",
      "INFO:tensorflow:examples/sec: 2.83382\n",
      "I1117 16:00:42.144829 4595733952 tpu_estimator.py:2160] examples/sec: 2.83382\n",
      "INFO:tensorflow:global_step/sec: 0.0906803\n",
      "I1117 16:00:53.172418 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906803\n",
      "INFO:tensorflow:examples/sec: 2.90177\n",
      "I1117 16:00:53.172638 4595733952 tpu_estimator.py:2160] examples/sec: 2.90177\n",
      "INFO:tensorflow:global_step/sec: 0.0904915\n",
      "I1117 16:01:04.223190 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904915\n",
      "INFO:tensorflow:examples/sec: 2.89573\n",
      "I1117 16:01:04.223589 4595733952 tpu_estimator.py:2160] examples/sec: 2.89573\n",
      "INFO:tensorflow:global_step/sec: 0.0907563\n",
      "I1117 16:01:15.241705 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907563\n",
      "INFO:tensorflow:examples/sec: 2.9042\n",
      "I1117 16:01:15.241940 4595733952 tpu_estimator.py:2160] examples/sec: 2.9042\n",
      "INFO:tensorflow:global_step/sec: 0.0904942\n",
      "I1117 16:01:26.292144 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904942\n",
      "INFO:tensorflow:examples/sec: 2.89582\n",
      "I1117 16:01:26.292371 4595733952 tpu_estimator.py:2160] examples/sec: 2.89582\n",
      "INFO:tensorflow:global_step/sec: 0.0906107\n",
      "I1117 16:01:37.328345 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0906107\n",
      "INFO:tensorflow:examples/sec: 2.89954\n",
      "I1117 16:01:37.328560 4595733952 tpu_estimator.py:2160] examples/sec: 2.89954\n",
      "INFO:tensorflow:global_step/sec: 0.0892247\n",
      "I1117 16:01:48.536026 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892247\n",
      "INFO:tensorflow:examples/sec: 2.85519\n",
      "I1117 16:01:48.536243 4595733952 tpu_estimator.py:2160] examples/sec: 2.85519\n",
      "INFO:tensorflow:global_step/sec: 0.0902343\n",
      "I1117 16:01:59.618269 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902343\n",
      "INFO:tensorflow:examples/sec: 2.8875\n",
      "I1117 16:01:59.618546 4595733952 tpu_estimator.py:2160] examples/sec: 2.8875\n",
      "INFO:tensorflow:global_step/sec: 0.0907794\n",
      "I1117 16:02:10.633981 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907794\n",
      "INFO:tensorflow:examples/sec: 2.90494\n",
      "I1117 16:02:10.634203 4595733952 tpu_estimator.py:2160] examples/sec: 2.90494\n",
      "INFO:tensorflow:global_step/sec: 0.0908039\n",
      "I1117 16:02:21.646753 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908039\n",
      "INFO:tensorflow:examples/sec: 2.90572\n",
      "I1117 16:02:21.646976 4595733952 tpu_estimator.py:2160] examples/sec: 2.90572\n",
      "INFO:tensorflow:global_step/sec: 0.090484\n",
      "I1117 16:02:32.698403 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090484\n",
      "INFO:tensorflow:examples/sec: 2.89549\n",
      "I1117 16:02:32.698782 4595733952 tpu_estimator.py:2160] examples/sec: 2.89549\n",
      "INFO:tensorflow:global_step/sec: 0.0904341\n",
      "I1117 16:02:43.756239 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904341\n",
      "INFO:tensorflow:examples/sec: 2.89389\n",
      "I1117 16:02:43.756471 4595733952 tpu_estimator.py:2160] examples/sec: 2.89389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0907891\n",
      "I1117 16:02:54.770723 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907891\n",
      "INFO:tensorflow:examples/sec: 2.90525\n",
      "I1117 16:02:54.770951 4595733952 tpu_estimator.py:2160] examples/sec: 2.90525\n",
      "INFO:tensorflow:global_step/sec: 0.0904354\n",
      "I1117 16:03:05.828344 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904354\n",
      "INFO:tensorflow:examples/sec: 2.89393\n",
      "I1117 16:03:05.828567 4595733952 tpu_estimator.py:2160] examples/sec: 2.89393\n",
      "INFO:tensorflow:global_step/sec: 0.0907356\n",
      "I1117 16:03:16.849399 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907356\n",
      "INFO:tensorflow:examples/sec: 2.90354\n",
      "I1117 16:03:16.849648 4595733952 tpu_estimator.py:2160] examples/sec: 2.90354\n",
      "INFO:tensorflow:global_step/sec: 0.0901685\n",
      "I1117 16:03:27.939723 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901685\n",
      "INFO:tensorflow:examples/sec: 2.88539\n",
      "I1117 16:03:27.939945 4595733952 tpu_estimator.py:2160] examples/sec: 2.88539\n",
      "INFO:tensorflow:global_step/sec: 0.0902365\n",
      "I1117 16:03:39.021718 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902365\n",
      "INFO:tensorflow:examples/sec: 2.88757\n",
      "I1117 16:03:39.021941 4595733952 tpu_estimator.py:2160] examples/sec: 2.88757\n",
      "INFO:tensorflow:global_step/sec: 0.090217\n",
      "I1117 16:03:50.106102 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090217\n",
      "INFO:tensorflow:examples/sec: 2.88694\n",
      "I1117 16:03:50.106331 4595733952 tpu_estimator.py:2160] examples/sec: 2.88694\n",
      "INFO:tensorflow:global_step/sec: 0.090209\n",
      "I1117 16:04:01.191468 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090209\n",
      "INFO:tensorflow:examples/sec: 2.88669\n",
      "I1117 16:04:01.191697 4595733952 tpu_estimator.py:2160] examples/sec: 2.88669\n",
      "INFO:tensorflow:global_step/sec: 0.0900696\n",
      "I1117 16:04:12.293992 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900696\n",
      "INFO:tensorflow:examples/sec: 2.88223\n",
      "I1117 16:04:12.294224 4595733952 tpu_estimator.py:2160] examples/sec: 2.88223\n",
      "INFO:tensorflow:global_step/sec: 0.0905259\n",
      "I1117 16:04:23.340538 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905259\n",
      "INFO:tensorflow:examples/sec: 2.89683\n",
      "I1117 16:04:23.340744 4595733952 tpu_estimator.py:2160] examples/sec: 2.89683\n",
      "INFO:tensorflow:global_step/sec: 0.0900428\n",
      "I1117 16:04:34.446373 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900428\n",
      "INFO:tensorflow:examples/sec: 2.88137\n",
      "I1117 16:04:34.446585 4595733952 tpu_estimator.py:2160] examples/sec: 2.88137\n",
      "INFO:tensorflow:global_step/sec: 0.0905458\n",
      "I1117 16:04:45.490525 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905458\n",
      "INFO:tensorflow:examples/sec: 2.89747\n",
      "I1117 16:04:45.490748 4595733952 tpu_estimator.py:2160] examples/sec: 2.89747\n",
      "INFO:tensorflow:global_step/sec: 0.0904595\n",
      "I1117 16:04:56.545186 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904595\n",
      "INFO:tensorflow:examples/sec: 2.8947\n",
      "I1117 16:04:56.545440 4595733952 tpu_estimator.py:2160] examples/sec: 2.8947\n",
      "INFO:tensorflow:global_step/sec: 0.0907978\n",
      "I1117 16:05:07.558700 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0907978\n",
      "INFO:tensorflow:examples/sec: 2.90553\n",
      "I1117 16:05:07.559121 4595733952 tpu_estimator.py:2160] examples/sec: 2.90553\n",
      "INFO:tensorflow:global_step/sec: 0.0905027\n",
      "I1117 16:05:18.608065 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905027\n",
      "INFO:tensorflow:examples/sec: 2.89609\n",
      "I1117 16:05:18.608292 4595733952 tpu_estimator.py:2160] examples/sec: 2.89609\n",
      "INFO:tensorflow:global_step/sec: 0.090235\n",
      "I1117 16:05:29.690269 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090235\n",
      "INFO:tensorflow:examples/sec: 2.88752\n",
      "I1117 16:05:29.690495 4595733952 tpu_estimator.py:2160] examples/sec: 2.88752\n",
      "INFO:tensorflow:global_step/sec: 0.0897372\n",
      "I1117 16:05:40.833896 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897372\n",
      "INFO:tensorflow:examples/sec: 2.87159\n",
      "I1117 16:05:40.834239 4595733952 tpu_estimator.py:2160] examples/sec: 2.87159\n",
      "INFO:tensorflow:global_step/sec: 0.0909267\n",
      "I1117 16:05:51.831753 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0909267\n",
      "INFO:tensorflow:examples/sec: 2.90966\n",
      "I1117 16:05:51.831983 4595733952 tpu_estimator.py:2160] examples/sec: 2.90966\n",
      "INFO:tensorflow:global_step/sec: 0.0908979\n",
      "I1117 16:06:02.833103 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0908979\n",
      "INFO:tensorflow:examples/sec: 2.90873\n",
      "I1117 16:06:02.833328 4595733952 tpu_estimator.py:2160] examples/sec: 2.90873\n",
      "INFO:tensorflow:global_step/sec: 0.0869017\n",
      "I1117 16:06:14.340331 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869017\n",
      "INFO:tensorflow:examples/sec: 2.78085\n",
      "I1117 16:06:14.340596 4595733952 tpu_estimator.py:2160] examples/sec: 2.78085\n",
      "INFO:tensorflow:global_step/sec: 0.0838802\n",
      "I1117 16:06:26.262076 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0838802\n",
      "INFO:tensorflow:examples/sec: 2.68417\n",
      "I1117 16:06:26.262267 4595733952 tpu_estimator.py:2160] examples/sec: 2.68417\n",
      "INFO:tensorflow:global_step/sec: 0.0856467\n",
      "I1117 16:06:37.937987 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0856467\n",
      "INFO:tensorflow:examples/sec: 2.7407\n",
      "I1117 16:06:37.938202 4595733952 tpu_estimator.py:2160] examples/sec: 2.7407\n",
      "INFO:tensorflow:global_step/sec: 0.0734513\n",
      "I1117 16:06:51.552431 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0734513\n",
      "INFO:tensorflow:examples/sec: 2.35044\n",
      "I1117 16:06:51.552608 4595733952 tpu_estimator.py:2160] examples/sec: 2.35044\n",
      "INFO:tensorflow:global_step/sec: 0.0837118\n",
      "I1117 16:07:03.498202 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837118\n",
      "INFO:tensorflow:examples/sec: 2.67878\n",
      "I1117 16:07:03.498430 4595733952 tpu_estimator.py:2160] examples/sec: 2.67878\n",
      "INFO:tensorflow:global_step/sec: 0.0884498\n",
      "I1117 16:07:14.804047 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884498\n",
      "INFO:tensorflow:examples/sec: 2.8304\n",
      "I1117 16:07:14.804265 4595733952 tpu_estimator.py:2160] examples/sec: 2.8304\n",
      "INFO:tensorflow:global_step/sec: 0.07883\n",
      "I1117 16:07:27.489554 4595733952 tpu_estimator.py:2159] global_step/sec: 0.07883\n",
      "INFO:tensorflow:examples/sec: 2.52256\n",
      "I1117 16:07:27.489929 4595733952 tpu_estimator.py:2160] examples/sec: 2.52256\n",
      "INFO:tensorflow:global_step/sec: 0.0839619\n",
      "I1117 16:07:39.399738 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839619\n",
      "INFO:tensorflow:examples/sec: 2.68678\n",
      "I1117 16:07:39.400009 4595733952 tpu_estimator.py:2160] examples/sec: 2.68678\n",
      "INFO:tensorflow:global_step/sec: 0.0855083\n",
      "I1117 16:07:51.094513 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0855083\n",
      "INFO:tensorflow:examples/sec: 2.73627\n",
      "I1117 16:07:51.094748 4595733952 tpu_estimator.py:2160] examples/sec: 2.73627\n",
      "INFO:tensorflow:global_step/sec: 0.0861649\n",
      "I1117 16:08:02.700155 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861649\n",
      "INFO:tensorflow:examples/sec: 2.75728\n",
      "I1117 16:08:02.700361 4595733952 tpu_estimator.py:2160] examples/sec: 2.75728\n",
      "INFO:tensorflow:global_step/sec: 0.0869857\n",
      "I1117 16:08:14.196300 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869857\n",
      "INFO:tensorflow:examples/sec: 2.78354\n",
      "I1117 16:08:14.196508 4595733952 tpu_estimator.py:2160] examples/sec: 2.78354\n",
      "INFO:tensorflow:global_step/sec: 0.0834516\n",
      "I1117 16:08:26.179282 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0834516\n",
      "INFO:tensorflow:examples/sec: 2.67045\n",
      "I1117 16:08:26.179471 4595733952 tpu_estimator.py:2160] examples/sec: 2.67045\n",
      "INFO:tensorflow:global_step/sec: 0.080683\n",
      "I1117 16:08:38.573485 4595733952 tpu_estimator.py:2159] global_step/sec: 0.080683\n",
      "INFO:tensorflow:examples/sec: 2.58186\n",
      "I1117 16:08:38.573714 4595733952 tpu_estimator.py:2160] examples/sec: 2.58186\n",
      "INFO:tensorflow:global_step/sec: 0.0842481\n",
      "I1117 16:08:50.443166 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842481\n",
      "INFO:tensorflow:examples/sec: 2.69594\n",
      "I1117 16:08:50.443398 4595733952 tpu_estimator.py:2160] examples/sec: 2.69594\n",
      "INFO:tensorflow:global_step/sec: 0.0815319\n",
      "I1117 16:09:02.708310 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0815319\n",
      "INFO:tensorflow:examples/sec: 2.60902\n",
      "I1117 16:09:02.708657 4595733952 tpu_estimator.py:2160] examples/sec: 2.60902\n",
      "INFO:tensorflow:global_step/sec: 0.0820533\n",
      "I1117 16:09:14.895504 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0820533\n",
      "INFO:tensorflow:examples/sec: 2.62571\n",
      "I1117 16:09:14.895721 4595733952 tpu_estimator.py:2160] examples/sec: 2.62571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0806702\n",
      "I1117 16:09:27.291649 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0806702\n",
      "INFO:tensorflow:examples/sec: 2.58145\n",
      "I1117 16:09:27.291861 4595733952 tpu_estimator.py:2160] examples/sec: 2.58145\n",
      "INFO:tensorflow:global_step/sec: 0.0799713\n",
      "I1117 16:09:39.796138 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0799713\n",
      "INFO:tensorflow:examples/sec: 2.55908\n",
      "I1117 16:09:39.796351 4595733952 tpu_estimator.py:2160] examples/sec: 2.55908\n",
      "INFO:tensorflow:global_step/sec: 0.0832316\n",
      "I1117 16:09:51.810796 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832316\n",
      "INFO:tensorflow:examples/sec: 2.66341\n",
      "I1117 16:09:51.810999 4595733952 tpu_estimator.py:2160] examples/sec: 2.66341\n",
      "INFO:tensorflow:global_step/sec: 0.0795024\n",
      "I1117 16:10:04.389045 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0795024\n",
      "INFO:tensorflow:examples/sec: 2.54408\n",
      "I1117 16:10:04.389454 4595733952 tpu_estimator.py:2160] examples/sec: 2.54408\n",
      "INFO:tensorflow:global_step/sec: 0.0805528\n",
      "I1117 16:10:16.803261 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0805528\n",
      "INFO:tensorflow:examples/sec: 2.57769\n",
      "I1117 16:10:16.803475 4595733952 tpu_estimator.py:2160] examples/sec: 2.57769\n",
      "INFO:tensorflow:global_step/sec: 0.0838077\n",
      "I1117 16:10:28.735342 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0838077\n",
      "INFO:tensorflow:examples/sec: 2.68185\n",
      "I1117 16:10:28.735573 4595733952 tpu_estimator.py:2160] examples/sec: 2.68185\n",
      "INFO:tensorflow:global_step/sec: 0.0884158\n",
      "I1117 16:10:40.045539 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884158\n",
      "INFO:tensorflow:examples/sec: 2.8293\n",
      "I1117 16:10:40.045782 4595733952 tpu_estimator.py:2160] examples/sec: 2.8293\n",
      "INFO:tensorflow:global_step/sec: 0.0786086\n",
      "I1117 16:10:52.766775 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0786086\n",
      "INFO:tensorflow:examples/sec: 2.51548\n",
      "I1117 16:10:52.766991 4595733952 tpu_estimator.py:2160] examples/sec: 2.51548\n",
      "INFO:tensorflow:global_step/sec: 0.0813789\n",
      "I1117 16:11:05.055028 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0813789\n",
      "INFO:tensorflow:examples/sec: 2.60412\n",
      "I1117 16:11:05.055274 4595733952 tpu_estimator.py:2160] examples/sec: 2.60412\n",
      "INFO:tensorflow:global_step/sec: 0.082475\n",
      "I1117 16:11:17.179875 4595733952 tpu_estimator.py:2159] global_step/sec: 0.082475\n",
      "INFO:tensorflow:examples/sec: 2.6392\n",
      "I1117 16:11:17.180085 4595733952 tpu_estimator.py:2160] examples/sec: 2.6392\n",
      "INFO:tensorflow:global_step/sec: 0.0865585\n",
      "I1117 16:11:28.732759 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865585\n",
      "INFO:tensorflow:examples/sec: 2.76987\n",
      "I1117 16:11:28.732985 4595733952 tpu_estimator.py:2160] examples/sec: 2.76987\n",
      "INFO:tensorflow:global_step/sec: 0.0820729\n",
      "I1117 16:11:40.917039 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0820729\n",
      "INFO:tensorflow:examples/sec: 2.62633\n",
      "I1117 16:11:40.917251 4595733952 tpu_estimator.py:2160] examples/sec: 2.62633\n",
      "INFO:tensorflow:global_step/sec: 0.0801866\n",
      "I1117 16:11:53.387947 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0801866\n",
      "INFO:tensorflow:examples/sec: 2.56597\n",
      "I1117 16:11:53.388344 4595733952 tpu_estimator.py:2160] examples/sec: 2.56597\n",
      "INFO:tensorflow:global_step/sec: 0.0804154\n",
      "I1117 16:12:05.823364 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0804154\n",
      "INFO:tensorflow:examples/sec: 2.57329\n",
      "I1117 16:12:05.823570 4595733952 tpu_estimator.py:2160] examples/sec: 2.57329\n",
      "INFO:tensorflow:global_step/sec: 0.077983\n",
      "I1117 16:12:18.646671 4595733952 tpu_estimator.py:2159] global_step/sec: 0.077983\n",
      "INFO:tensorflow:examples/sec: 2.49546\n",
      "I1117 16:12:18.646867 4595733952 tpu_estimator.py:2160] examples/sec: 2.49546\n",
      "INFO:tensorflow:global_step/sec: 0.0831994\n",
      "I1117 16:12:30.666042 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0831994\n",
      "INFO:tensorflow:examples/sec: 2.66238\n",
      "I1117 16:12:30.666250 4595733952 tpu_estimator.py:2160] examples/sec: 2.66238\n",
      "INFO:tensorflow:global_step/sec: 0.0843106\n",
      "I1117 16:12:42.526921 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843106\n",
      "INFO:tensorflow:examples/sec: 2.69794\n",
      "I1117 16:12:42.527161 4595733952 tpu_estimator.py:2160] examples/sec: 2.69794\n",
      "INFO:tensorflow:global_step/sec: 0.0844249\n",
      "I1117 16:12:54.371778 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844249\n",
      "INFO:tensorflow:examples/sec: 2.7016\n",
      "I1117 16:12:54.372263 4595733952 tpu_estimator.py:2160] examples/sec: 2.7016\n",
      "INFO:tensorflow:global_step/sec: 0.0844648\n",
      "I1117 16:13:06.210981 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844648\n",
      "INFO:tensorflow:examples/sec: 2.70287\n",
      "I1117 16:13:06.211153 4595733952 tpu_estimator.py:2160] examples/sec: 2.70287\n",
      "INFO:tensorflow:global_step/sec: 0.0849007\n",
      "I1117 16:13:17.989563 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0849007\n",
      "INFO:tensorflow:examples/sec: 2.71682\n",
      "I1117 16:13:17.990071 4595733952 tpu_estimator.py:2160] examples/sec: 2.71682\n",
      "INFO:tensorflow:global_step/sec: 0.0867074\n",
      "I1117 16:13:29.522516 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867074\n",
      "INFO:tensorflow:examples/sec: 2.77464\n",
      "I1117 16:13:29.522737 4595733952 tpu_estimator.py:2160] examples/sec: 2.77464\n",
      "INFO:tensorflow:global_step/sec: 0.0899708\n",
      "I1117 16:13:40.637237 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899708\n",
      "INFO:tensorflow:examples/sec: 2.87907\n",
      "I1117 16:13:40.637459 4595733952 tpu_estimator.py:2160] examples/sec: 2.87907\n",
      "INFO:tensorflow:global_step/sec: 0.0904751\n",
      "I1117 16:13:51.689999 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904751\n",
      "INFO:tensorflow:examples/sec: 2.8952\n",
      "I1117 16:13:51.690223 4595733952 tpu_estimator.py:2160] examples/sec: 2.8952\n",
      "INFO:tensorflow:global_step/sec: 0.0900826\n",
      "I1117 16:14:02.790925 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900826\n",
      "INFO:tensorflow:examples/sec: 2.88264\n",
      "I1117 16:14:02.791152 4595733952 tpu_estimator.py:2160] examples/sec: 2.88264\n",
      "INFO:tensorflow:global_step/sec: 0.0901618\n",
      "I1117 16:14:13.882105 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901618\n",
      "INFO:tensorflow:examples/sec: 2.88518\n",
      "I1117 16:14:13.882327 4595733952 tpu_estimator.py:2160] examples/sec: 2.88518\n",
      "INFO:tensorflow:global_step/sec: 0.0901395\n",
      "I1117 16:14:24.976010 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901395\n",
      "INFO:tensorflow:examples/sec: 2.88446\n",
      "I1117 16:14:24.976325 4595733952 tpu_estimator.py:2160] examples/sec: 2.88446\n",
      "INFO:tensorflow:global_step/sec: 0.0903675\n",
      "I1117 16:14:36.041935 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903675\n",
      "INFO:tensorflow:examples/sec: 2.89176\n",
      "I1117 16:14:36.042160 4595733952 tpu_estimator.py:2160] examples/sec: 2.89176\n",
      "INFO:tensorflow:global_step/sec: 0.0905637\n",
      "I1117 16:14:47.083889 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905637\n",
      "INFO:tensorflow:examples/sec: 2.89804\n",
      "I1117 16:14:47.084115 4595733952 tpu_estimator.py:2160] examples/sec: 2.89804\n",
      "INFO:tensorflow:global_step/sec: 0.0902836\n",
      "I1117 16:14:58.160112 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902836\n",
      "INFO:tensorflow:examples/sec: 2.88907\n",
      "I1117 16:14:58.160338 4595733952 tpu_estimator.py:2160] examples/sec: 2.88907\n",
      "INFO:tensorflow:global_step/sec: 0.0902441\n",
      "I1117 16:15:09.241158 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902441\n",
      "INFO:tensorflow:examples/sec: 2.88781\n",
      "I1117 16:15:09.241381 4595733952 tpu_estimator.py:2160] examples/sec: 2.88781\n",
      "INFO:tensorflow:global_step/sec: 0.0902703\n",
      "I1117 16:15:20.319002 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902703\n",
      "INFO:tensorflow:examples/sec: 2.88865\n",
      "I1117 16:15:20.319229 4595733952 tpu_estimator.py:2160] examples/sec: 2.88865\n",
      "INFO:tensorflow:global_step/sec: 0.0901946\n",
      "I1117 16:15:31.406143 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901946\n",
      "INFO:tensorflow:examples/sec: 2.88623\n",
      "I1117 16:15:31.406370 4595733952 tpu_estimator.py:2160] examples/sec: 2.88623\n",
      "INFO:tensorflow:global_step/sec: 0.0900503\n",
      "I1117 16:15:42.511024 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900503\n",
      "INFO:tensorflow:examples/sec: 2.88161\n",
      "I1117 16:15:42.511351 4595733952 tpu_estimator.py:2160] examples/sec: 2.88161\n",
      "INFO:tensorflow:global_step/sec: 0.0849282\n",
      "I1117 16:15:54.285690 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0849282\n",
      "INFO:tensorflow:examples/sec: 2.7177\n",
      "I1117 16:15:54.285919 4595733952 tpu_estimator.py:2160] examples/sec: 2.7177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0848963\n",
      "I1117 16:16:06.064769 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0848963\n",
      "INFO:tensorflow:examples/sec: 2.71668\n",
      "I1117 16:16:06.064988 4595733952 tpu_estimator.py:2160] examples/sec: 2.71668\n",
      "INFO:tensorflow:global_step/sec: 0.0849872\n",
      "I1117 16:16:17.831238 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0849872\n",
      "INFO:tensorflow:examples/sec: 2.71959\n",
      "I1117 16:16:17.831451 4595733952 tpu_estimator.py:2160] examples/sec: 2.71959\n",
      "INFO:tensorflow:global_step/sec: 0.0862337\n",
      "I1117 16:16:29.427649 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862337\n",
      "INFO:tensorflow:examples/sec: 2.75948\n",
      "I1117 16:16:29.427885 4595733952 tpu_estimator.py:2160] examples/sec: 2.75948\n",
      "INFO:tensorflow:global_step/sec: 0.0862225\n",
      "I1117 16:16:41.025534 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862225\n",
      "INFO:tensorflow:examples/sec: 2.75912\n",
      "I1117 16:16:41.025744 4595733952 tpu_estimator.py:2160] examples/sec: 2.75912\n",
      "INFO:tensorflow:global_step/sec: 0.0848057\n",
      "I1117 16:16:52.817229 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0848057\n",
      "INFO:tensorflow:examples/sec: 2.71378\n",
      "I1117 16:16:52.817468 4595733952 tpu_estimator.py:2160] examples/sec: 2.71378\n",
      "INFO:tensorflow:global_step/sec: 0.0837298\n",
      "I1117 16:17:04.760356 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837298\n",
      "INFO:tensorflow:examples/sec: 2.67935\n",
      "I1117 16:17:04.760551 4595733952 tpu_estimator.py:2160] examples/sec: 2.67935\n",
      "INFO:tensorflow:global_step/sec: 0.0802241\n",
      "I1117 16:17:17.225515 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0802241\n",
      "INFO:tensorflow:examples/sec: 2.56717\n",
      "I1117 16:17:17.225978 4595733952 tpu_estimator.py:2160] examples/sec: 2.56717\n",
      "INFO:tensorflow:global_step/sec: 0.0821457\n",
      "I1117 16:17:29.398983 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0821457\n",
      "INFO:tensorflow:examples/sec: 2.62866\n",
      "I1117 16:17:29.399199 4595733952 tpu_estimator.py:2160] examples/sec: 2.62866\n",
      "INFO:tensorflow:global_step/sec: 0.0863606\n",
      "I1117 16:17:40.978327 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863606\n",
      "INFO:tensorflow:examples/sec: 2.76354\n",
      "I1117 16:17:40.978544 4595733952 tpu_estimator.py:2160] examples/sec: 2.76354\n",
      "INFO:tensorflow:global_step/sec: 0.0866075\n",
      "I1117 16:17:52.524703 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866075\n",
      "INFO:tensorflow:examples/sec: 2.77144\n",
      "I1117 16:17:52.524918 4595733952 tpu_estimator.py:2160] examples/sec: 2.77144\n",
      "INFO:tensorflow:global_step/sec: 0.0861248\n",
      "I1117 16:18:04.135731 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861248\n",
      "INFO:tensorflow:examples/sec: 2.75599\n",
      "I1117 16:18:04.135959 4595733952 tpu_estimator.py:2160] examples/sec: 2.75599\n",
      "INFO:tensorflow:global_step/sec: 0.0861476\n",
      "I1117 16:18:15.743714 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861476\n",
      "INFO:tensorflow:examples/sec: 2.75672\n",
      "I1117 16:18:15.743957 4595733952 tpu_estimator.py:2160] examples/sec: 2.75672\n",
      "INFO:tensorflow:global_step/sec: 0.086281\n",
      "I1117 16:18:27.333750 4595733952 tpu_estimator.py:2159] global_step/sec: 0.086281\n",
      "INFO:tensorflow:examples/sec: 2.76099\n",
      "I1117 16:18:27.333961 4595733952 tpu_estimator.py:2160] examples/sec: 2.76099\n",
      "INFO:tensorflow:global_step/sec: 0.0865396\n",
      "I1117 16:18:38.889166 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865396\n",
      "INFO:tensorflow:examples/sec: 2.76927\n",
      "I1117 16:18:38.889503 4595733952 tpu_estimator.py:2160] examples/sec: 2.76927\n",
      "INFO:tensorflow:global_step/sec: 0.0861199\n",
      "I1117 16:18:50.500783 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861199\n",
      "INFO:tensorflow:examples/sec: 2.75584\n",
      "I1117 16:18:50.500900 4595733952 tpu_estimator.py:2160] examples/sec: 2.75584\n",
      "INFO:tensorflow:global_step/sec: 0.0865687\n",
      "I1117 16:19:02.052392 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865687\n",
      "INFO:tensorflow:examples/sec: 2.7702\n",
      "I1117 16:19:02.052605 4595733952 tpu_estimator.py:2160] examples/sec: 2.7702\n",
      "INFO:tensorflow:global_step/sec: 0.0861573\n",
      "I1117 16:19:13.659067 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861573\n",
      "INFO:tensorflow:examples/sec: 2.75703\n",
      "I1117 16:19:13.659436 4595733952 tpu_estimator.py:2160] examples/sec: 2.75703\n",
      "INFO:tensorflow:global_step/sec: 0.0859965\n",
      "I1117 16:19:25.287445 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859965\n",
      "INFO:tensorflow:examples/sec: 2.75189\n",
      "I1117 16:19:25.287657 4595733952 tpu_estimator.py:2160] examples/sec: 2.75189\n",
      "INFO:tensorflow:global_step/sec: 0.0867591\n",
      "I1117 16:19:36.813614 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867591\n",
      "INFO:tensorflow:examples/sec: 2.77629\n",
      "I1117 16:19:36.813988 4595733952 tpu_estimator.py:2160] examples/sec: 2.77629\n",
      "INFO:tensorflow:global_step/sec: 0.0862351\n",
      "I1117 16:19:48.409823 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862351\n",
      "INFO:tensorflow:examples/sec: 2.75952\n",
      "I1117 16:19:48.410031 4595733952 tpu_estimator.py:2160] examples/sec: 2.75952\n",
      "INFO:tensorflow:global_step/sec: 0.0860438\n",
      "I1117 16:20:00.031847 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0860438\n",
      "INFO:tensorflow:examples/sec: 2.7534\n",
      "I1117 16:20:00.032154 4595733952 tpu_estimator.py:2160] examples/sec: 2.7534\n",
      "INFO:tensorflow:global_step/sec: 0.0857898\n",
      "I1117 16:20:11.688200 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857898\n",
      "INFO:tensorflow:examples/sec: 2.74527\n",
      "I1117 16:20:11.688400 4595733952 tpu_estimator.py:2160] examples/sec: 2.74527\n",
      "INFO:tensorflow:global_step/sec: 0.0853139\n",
      "I1117 16:20:23.409638 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853139\n",
      "INFO:tensorflow:examples/sec: 2.73004\n",
      "I1117 16:20:23.410014 4595733952 tpu_estimator.py:2160] examples/sec: 2.73004\n",
      "INFO:tensorflow:global_step/sec: 0.0865093\n",
      "I1117 16:20:34.969206 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865093\n",
      "INFO:tensorflow:examples/sec: 2.7683\n",
      "I1117 16:20:34.969331 4595733952 tpu_estimator.py:2160] examples/sec: 2.7683\n",
      "INFO:tensorflow:global_step/sec: 0.0780504\n",
      "I1117 16:20:47.781321 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0780504\n",
      "INFO:tensorflow:examples/sec: 2.49761\n",
      "I1117 16:20:47.781526 4595733952 tpu_estimator.py:2160] examples/sec: 2.49761\n",
      "INFO:tensorflow:global_step/sec: 0.0824101\n",
      "I1117 16:20:59.915727 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824101\n",
      "INFO:tensorflow:examples/sec: 2.63712\n",
      "I1117 16:20:59.915921 4595733952 tpu_estimator.py:2160] examples/sec: 2.63712\n",
      "INFO:tensorflow:global_step/sec: 0.0812818\n",
      "I1117 16:21:12.218625 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0812818\n",
      "INFO:tensorflow:examples/sec: 2.60102\n",
      "I1117 16:21:12.218821 4595733952 tpu_estimator.py:2160] examples/sec: 2.60102\n",
      "INFO:tensorflow:global_step/sec: 0.0816369\n",
      "I1117 16:21:24.467972 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0816369\n",
      "INFO:tensorflow:examples/sec: 2.61238\n",
      "I1117 16:21:24.468173 4595733952 tpu_estimator.py:2160] examples/sec: 2.61238\n",
      "INFO:tensorflow:global_step/sec: 0.0817711\n",
      "I1117 16:21:36.697212 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0817711\n",
      "INFO:tensorflow:examples/sec: 2.61668\n",
      "I1117 16:21:36.697411 4595733952 tpu_estimator.py:2160] examples/sec: 2.61668\n",
      "INFO:tensorflow:global_step/sec: 0.0821414\n",
      "I1117 16:21:48.871340 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0821414\n",
      "INFO:tensorflow:examples/sec: 2.62853\n",
      "I1117 16:21:48.871537 4595733952 tpu_estimator.py:2160] examples/sec: 2.62853\n",
      "INFO:tensorflow:global_step/sec: 0.0813875\n",
      "I1117 16:22:01.158252 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0813875\n",
      "INFO:tensorflow:examples/sec: 2.6044\n",
      "I1117 16:22:01.158607 4595733952 tpu_estimator.py:2160] examples/sec: 2.6044\n",
      "INFO:tensorflow:global_step/sec: 0.0836878\n",
      "I1117 16:22:13.107486 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836878\n",
      "INFO:tensorflow:examples/sec: 2.67801\n",
      "I1117 16:22:13.107796 4595733952 tpu_estimator.py:2160] examples/sec: 2.67801\n",
      "INFO:tensorflow:global_step/sec: 0.0812975\n",
      "I1117 16:22:25.407905 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0812975\n",
      "INFO:tensorflow:examples/sec: 2.60152\n",
      "I1117 16:22:25.408218 4595733952 tpu_estimator.py:2160] examples/sec: 2.60152\n",
      "INFO:tensorflow:global_step/sec: 0.0830919\n",
      "I1117 16:22:37.442791 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0830919\n",
      "INFO:tensorflow:examples/sec: 2.65894\n",
      "I1117 16:22:37.442999 4595733952 tpu_estimator.py:2160] examples/sec: 2.65894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0845602\n",
      "I1117 16:22:49.268671 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0845602\n",
      "INFO:tensorflow:examples/sec: 2.70593\n",
      "I1117 16:22:49.268860 4595733952 tpu_estimator.py:2160] examples/sec: 2.70593\n",
      "INFO:tensorflow:global_step/sec: 0.0843485\n",
      "I1117 16:23:01.124282 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843485\n",
      "INFO:tensorflow:examples/sec: 2.69915\n",
      "I1117 16:23:01.124500 4595733952 tpu_estimator.py:2160] examples/sec: 2.69915\n",
      "INFO:tensorflow:global_step/sec: 0.0902674\n",
      "I1117 16:23:12.202475 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902674\n",
      "INFO:tensorflow:examples/sec: 2.88856\n",
      "I1117 16:23:12.202704 4595733952 tpu_estimator.py:2160] examples/sec: 2.88856\n",
      "INFO:tensorflow:global_step/sec: 0.0901113\n",
      "I1117 16:23:23.299868 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901113\n",
      "INFO:tensorflow:examples/sec: 2.88356\n",
      "I1117 16:23:23.300088 4595733952 tpu_estimator.py:2160] examples/sec: 2.88356\n",
      "INFO:tensorflow:global_step/sec: 0.0903237\n",
      "I1117 16:23:34.371163 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903237\n",
      "INFO:tensorflow:examples/sec: 2.89036\n",
      "I1117 16:23:34.371386 4595733952 tpu_estimator.py:2160] examples/sec: 2.89036\n",
      "INFO:tensorflow:global_step/sec: 0.0903354\n",
      "I1117 16:23:45.441024 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903354\n",
      "INFO:tensorflow:examples/sec: 2.89073\n",
      "I1117 16:23:45.441251 4595733952 tpu_estimator.py:2160] examples/sec: 2.89073\n",
      "INFO:tensorflow:global_step/sec: 0.090415\n",
      "I1117 16:23:56.501137 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090415\n",
      "INFO:tensorflow:examples/sec: 2.89328\n",
      "I1117 16:23:56.501363 4595733952 tpu_estimator.py:2160] examples/sec: 2.89328\n",
      "INFO:tensorflow:global_step/sec: 0.0898473\n",
      "I1117 16:24:07.631142 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898473\n",
      "INFO:tensorflow:examples/sec: 2.87511\n",
      "I1117 16:24:07.631370 4595733952 tpu_estimator.py:2160] examples/sec: 2.87511\n",
      "INFO:tensorflow:global_step/sec: 0.09047\n",
      "I1117 16:24:18.684528 4595733952 tpu_estimator.py:2159] global_step/sec: 0.09047\n",
      "INFO:tensorflow:examples/sec: 2.89504\n",
      "I1117 16:24:18.684781 4595733952 tpu_estimator.py:2160] examples/sec: 2.89504\n",
      "INFO:tensorflow:global_step/sec: 0.0901109\n",
      "I1117 16:24:29.781970 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901109\n",
      "INFO:tensorflow:examples/sec: 2.88355\n",
      "I1117 16:24:29.782183 4595733952 tpu_estimator.py:2160] examples/sec: 2.88355\n",
      "INFO:tensorflow:global_step/sec: 0.0898795\n",
      "I1117 16:24:40.907973 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898795\n",
      "INFO:tensorflow:examples/sec: 2.87614\n",
      "I1117 16:24:40.908203 4595733952 tpu_estimator.py:2160] examples/sec: 2.87614\n",
      "INFO:tensorflow:global_step/sec: 0.0900814\n",
      "I1117 16:24:52.009052 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900814\n",
      "INFO:tensorflow:examples/sec: 2.88261\n",
      "I1117 16:24:52.009292 4595733952 tpu_estimator.py:2160] examples/sec: 2.88261\n",
      "INFO:tensorflow:global_step/sec: 0.0896587\n",
      "I1117 16:25:03.162417 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896587\n",
      "INFO:tensorflow:examples/sec: 2.86908\n",
      "I1117 16:25:03.162636 4595733952 tpu_estimator.py:2160] examples/sec: 2.86908\n",
      "INFO:tensorflow:global_step/sec: 0.090335\n",
      "I1117 16:25:14.232352 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090335\n",
      "INFO:tensorflow:examples/sec: 2.89072\n",
      "I1117 16:25:14.232575 4595733952 tpu_estimator.py:2160] examples/sec: 2.89072\n",
      "INFO:tensorflow:global_step/sec: 0.090283\n",
      "I1117 16:25:25.308629 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090283\n",
      "INFO:tensorflow:examples/sec: 2.88905\n",
      "I1117 16:25:25.308847 4595733952 tpu_estimator.py:2160] examples/sec: 2.88905\n",
      "INFO:tensorflow:global_step/sec: 0.0903163\n",
      "I1117 16:25:36.380834 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903163\n",
      "INFO:tensorflow:examples/sec: 2.89012\n",
      "I1117 16:25:36.381063 4595733952 tpu_estimator.py:2160] examples/sec: 2.89012\n",
      "INFO:tensorflow:global_step/sec: 0.0900011\n",
      "I1117 16:25:47.491813 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900011\n",
      "INFO:tensorflow:examples/sec: 2.88003\n",
      "I1117 16:25:47.492037 4595733952 tpu_estimator.py:2160] examples/sec: 2.88003\n",
      "INFO:tensorflow:global_step/sec: 0.0895334\n",
      "I1117 16:25:58.660819 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895334\n",
      "INFO:tensorflow:examples/sec: 2.86507\n",
      "I1117 16:25:58.661036 4595733952 tpu_estimator.py:2160] examples/sec: 2.86507\n",
      "INFO:tensorflow:global_step/sec: 0.090116\n",
      "I1117 16:26:09.757642 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090116\n",
      "INFO:tensorflow:examples/sec: 2.88371\n",
      "I1117 16:26:09.758032 4595733952 tpu_estimator.py:2160] examples/sec: 2.88371\n",
      "INFO:tensorflow:global_step/sec: 0.0898634\n",
      "I1117 16:26:20.885643 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898634\n",
      "INFO:tensorflow:examples/sec: 2.87563\n",
      "I1117 16:26:20.885864 4595733952 tpu_estimator.py:2160] examples/sec: 2.87563\n",
      "INFO:tensorflow:global_step/sec: 0.0903552\n",
      "I1117 16:26:31.953068 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903552\n",
      "INFO:tensorflow:examples/sec: 2.89137\n",
      "I1117 16:26:31.953294 4595733952 tpu_estimator.py:2160] examples/sec: 2.89137\n",
      "INFO:tensorflow:global_step/sec: 0.089797\n",
      "I1117 16:26:43.089292 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089797\n",
      "INFO:tensorflow:examples/sec: 2.8735\n",
      "I1117 16:26:43.089514 4595733952 tpu_estimator.py:2160] examples/sec: 2.8735\n",
      "INFO:tensorflow:global_step/sec: 0.0903965\n",
      "I1117 16:26:54.151674 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903965\n",
      "INFO:tensorflow:examples/sec: 2.89269\n",
      "I1117 16:26:54.151898 4595733952 tpu_estimator.py:2160] examples/sec: 2.89269\n",
      "INFO:tensorflow:global_step/sec: 0.0900981\n",
      "I1117 16:27:05.250686 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900981\n",
      "INFO:tensorflow:examples/sec: 2.88314\n",
      "I1117 16:27:05.250910 4595733952 tpu_estimator.py:2160] examples/sec: 2.88314\n",
      "INFO:tensorflow:global_step/sec: 0.0901898\n",
      "I1117 16:27:16.338421 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901898\n",
      "INFO:tensorflow:examples/sec: 2.88607\n",
      "I1117 16:27:16.338642 4595733952 tpu_estimator.py:2160] examples/sec: 2.88607\n",
      "INFO:tensorflow:global_step/sec: 0.090102\n",
      "I1117 16:27:27.436954 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090102\n",
      "INFO:tensorflow:examples/sec: 2.88326\n",
      "I1117 16:27:27.437180 4595733952 tpu_estimator.py:2160] examples/sec: 2.88326\n",
      "INFO:tensorflow:global_step/sec: 0.0904337\n",
      "I1117 16:27:38.494781 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904337\n",
      "INFO:tensorflow:examples/sec: 2.89388\n",
      "I1117 16:27:38.495170 4595733952 tpu_estimator.py:2160] examples/sec: 2.89388\n",
      "INFO:tensorflow:global_step/sec: 0.0899206\n",
      "I1117 16:27:49.615711 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899206\n",
      "INFO:tensorflow:examples/sec: 2.87746\n",
      "I1117 16:27:49.615936 4595733952 tpu_estimator.py:2160] examples/sec: 2.87746\n",
      "INFO:tensorflow:global_step/sec: 0.0842924\n",
      "I1117 16:28:01.479148 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842924\n",
      "INFO:tensorflow:examples/sec: 2.69736\n",
      "I1117 16:28:01.479348 4595733952 tpu_estimator.py:2160] examples/sec: 2.69736\n",
      "INFO:tensorflow:global_step/sec: 0.0841023\n",
      "I1117 16:28:13.369441 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841023\n",
      "INFO:tensorflow:examples/sec: 2.69127\n",
      "I1117 16:28:13.369657 4595733952 tpu_estimator.py:2160] examples/sec: 2.69127\n",
      "INFO:tensorflow:global_step/sec: 0.082704\n",
      "I1117 16:28:25.460768 4595733952 tpu_estimator.py:2159] global_step/sec: 0.082704\n",
      "INFO:tensorflow:examples/sec: 2.64653\n",
      "I1117 16:28:25.460999 4595733952 tpu_estimator.py:2160] examples/sec: 2.64653\n",
      "INFO:tensorflow:global_step/sec: 0.0871364\n",
      "I1117 16:28:36.937029 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871364\n",
      "INFO:tensorflow:examples/sec: 2.78837\n",
      "I1117 16:28:36.937243 4595733952 tpu_estimator.py:2160] examples/sec: 2.78837\n",
      "INFO:tensorflow:global_step/sec: 0.0872874\n",
      "I1117 16:28:48.393445 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872874\n",
      "INFO:tensorflow:examples/sec: 2.7932\n",
      "I1117 16:28:48.393791 4595733952 tpu_estimator.py:2160] examples/sec: 2.7932\n",
      "INFO:tensorflow:global_step/sec: 0.0866939\n",
      "I1117 16:28:59.928261 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866939\n",
      "INFO:tensorflow:examples/sec: 2.7742\n",
      "I1117 16:28:59.928475 4595733952 tpu_estimator.py:2160] examples/sec: 2.7742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0865872\n",
      "I1117 16:29:11.477309 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865872\n",
      "INFO:tensorflow:examples/sec: 2.77079\n",
      "I1117 16:29:11.477529 4595733952 tpu_estimator.py:2160] examples/sec: 2.77079\n",
      "INFO:tensorflow:global_step/sec: 0.0872169\n",
      "I1117 16:29:22.943001 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872169\n",
      "INFO:tensorflow:examples/sec: 2.79094\n",
      "I1117 16:29:22.943254 4595733952 tpu_estimator.py:2160] examples/sec: 2.79094\n",
      "INFO:tensorflow:global_step/sec: 0.0870297\n",
      "I1117 16:29:34.433300 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870297\n",
      "INFO:tensorflow:examples/sec: 2.78495\n",
      "I1117 16:29:34.434629 4595733952 tpu_estimator.py:2160] examples/sec: 2.78495\n",
      "INFO:tensorflow:global_step/sec: 0.0825217\n",
      "I1117 16:29:46.551317 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0825217\n",
      "INFO:tensorflow:examples/sec: 2.6407\n",
      "I1117 16:29:46.551527 4595733952 tpu_estimator.py:2160] examples/sec: 2.6407\n",
      "INFO:tensorflow:global_step/sec: 0.0785377\n",
      "I1117 16:29:59.284057 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0785377\n",
      "INFO:tensorflow:examples/sec: 2.51321\n",
      "I1117 16:29:59.284258 4595733952 tpu_estimator.py:2160] examples/sec: 2.51321\n",
      "INFO:tensorflow:global_step/sec: 0.0788637\n",
      "I1117 16:30:11.964169 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0788637\n",
      "INFO:tensorflow:examples/sec: 2.52364\n",
      "I1117 16:30:11.964378 4595733952 tpu_estimator.py:2160] examples/sec: 2.52364\n",
      "INFO:tensorflow:global_step/sec: 0.0816959\n",
      "I1117 16:30:24.204690 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0816959\n",
      "INFO:tensorflow:examples/sec: 2.61427\n",
      "I1117 16:30:24.204921 4595733952 tpu_estimator.py:2160] examples/sec: 2.61427\n",
      "INFO:tensorflow:global_step/sec: 0.0861136\n",
      "I1117 16:30:35.817239 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861136\n",
      "INFO:tensorflow:examples/sec: 2.75563\n",
      "I1117 16:30:35.817432 4595733952 tpu_estimator.py:2160] examples/sec: 2.75563\n",
      "INFO:tensorflow:global_step/sec: 0.0813523\n",
      "I1117 16:30:48.109477 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0813523\n",
      "INFO:tensorflow:examples/sec: 2.60327\n",
      "I1117 16:30:48.109691 4595733952 tpu_estimator.py:2160] examples/sec: 2.60327\n",
      "INFO:tensorflow:global_step/sec: 0.0832356\n",
      "I1117 16:31:00.123556 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0832356\n",
      "INFO:tensorflow:examples/sec: 2.66354\n",
      "I1117 16:31:00.123759 4595733952 tpu_estimator.py:2160] examples/sec: 2.66354\n",
      "INFO:tensorflow:global_step/sec: 0.0809877\n",
      "I1117 16:31:12.471096 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0809877\n",
      "INFO:tensorflow:examples/sec: 2.59161\n",
      "I1117 16:31:12.471273 4595733952 tpu_estimator.py:2160] examples/sec: 2.59161\n",
      "INFO:tensorflow:global_step/sec: 0.0796078\n",
      "I1117 16:31:25.032719 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0796078\n",
      "INFO:tensorflow:examples/sec: 2.54745\n",
      "I1117 16:31:25.033124 4595733952 tpu_estimator.py:2160] examples/sec: 2.54745\n",
      "INFO:tensorflow:global_step/sec: 0.0857977\n",
      "I1117 16:31:36.688066 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857977\n",
      "INFO:tensorflow:examples/sec: 2.74553\n",
      "I1117 16:31:36.688286 4595733952 tpu_estimator.py:2160] examples/sec: 2.74553\n",
      "INFO:tensorflow:global_step/sec: 0.0836564\n",
      "I1117 16:31:48.641689 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836564\n",
      "INFO:tensorflow:examples/sec: 2.677\n",
      "I1117 16:31:48.641894 4595733952 tpu_estimator.py:2160] examples/sec: 2.677\n",
      "INFO:tensorflow:global_step/sec: 0.0823316\n",
      "I1117 16:32:00.787698 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0823316\n",
      "INFO:tensorflow:examples/sec: 2.63461\n",
      "I1117 16:32:00.787914 4595733952 tpu_estimator.py:2160] examples/sec: 2.63461\n",
      "INFO:tensorflow:global_step/sec: 0.0812926\n",
      "I1117 16:32:13.088925 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0812926\n",
      "INFO:tensorflow:examples/sec: 2.60136\n",
      "I1117 16:32:13.089132 4595733952 tpu_estimator.py:2160] examples/sec: 2.60136\n",
      "INFO:tensorflow:global_step/sec: 0.0868869\n",
      "I1117 16:32:24.598175 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0868869\n",
      "INFO:tensorflow:examples/sec: 2.78038\n",
      "I1117 16:32:24.598404 4595733952 tpu_estimator.py:2160] examples/sec: 2.78038\n",
      "INFO:tensorflow:global_step/sec: 0.0863906\n",
      "I1117 16:32:36.173498 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863906\n",
      "INFO:tensorflow:examples/sec: 2.7645\n",
      "I1117 16:32:36.173718 4595733952 tpu_estimator.py:2160] examples/sec: 2.7645\n",
      "INFO:tensorflow:global_step/sec: 0.0869692\n",
      "I1117 16:32:47.671814 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869692\n",
      "INFO:tensorflow:examples/sec: 2.78302\n",
      "I1117 16:32:47.672044 4595733952 tpu_estimator.py:2160] examples/sec: 2.78302\n",
      "INFO:tensorflow:global_step/sec: 0.0845498\n",
      "I1117 16:32:59.499153 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0845498\n",
      "INFO:tensorflow:examples/sec: 2.70559\n",
      "I1117 16:32:59.499392 4595733952 tpu_estimator.py:2160] examples/sec: 2.70559\n",
      "INFO:tensorflow:global_step/sec: 0.0810743\n",
      "I1117 16:33:11.833466 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0810743\n",
      "INFO:tensorflow:examples/sec: 2.59438\n",
      "I1117 16:33:11.833778 4595733952 tpu_estimator.py:2160] examples/sec: 2.59438\n",
      "INFO:tensorflow:global_step/sec: 0.0869328\n",
      "I1117 16:33:23.336644 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869328\n",
      "INFO:tensorflow:examples/sec: 2.78185\n",
      "I1117 16:33:23.337082 4595733952 tpu_estimator.py:2160] examples/sec: 2.78185\n",
      "INFO:tensorflow:global_step/sec: 0.0838287\n",
      "I1117 16:33:35.265696 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0838287\n",
      "INFO:tensorflow:examples/sec: 2.68252\n",
      "I1117 16:33:35.265880 4595733952 tpu_estimator.py:2160] examples/sec: 2.68252\n",
      "INFO:tensorflow:global_step/sec: 0.0816567\n",
      "I1117 16:33:47.512261 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0816567\n",
      "INFO:tensorflow:examples/sec: 2.61301\n",
      "I1117 16:33:47.513656 4595733952 tpu_estimator.py:2160] examples/sec: 2.61301\n",
      "INFO:tensorflow:global_step/sec: 0.0836241\n",
      "I1117 16:33:59.470427 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836241\n",
      "INFO:tensorflow:examples/sec: 2.67597\n",
      "I1117 16:33:59.470652 4595733952 tpu_estimator.py:2160] examples/sec: 2.67597\n",
      "INFO:tensorflow:global_step/sec: 0.0894985\n",
      "I1117 16:34:10.643786 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894985\n",
      "INFO:tensorflow:examples/sec: 2.86395\n",
      "I1117 16:34:10.644000 4595733952 tpu_estimator.py:2160] examples/sec: 2.86395\n",
      "INFO:tensorflow:global_step/sec: 0.0891396\n",
      "I1117 16:34:21.862174 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891396\n",
      "INFO:tensorflow:examples/sec: 2.85247\n",
      "I1117 16:34:21.862426 4595733952 tpu_estimator.py:2160] examples/sec: 2.85247\n",
      "INFO:tensorflow:global_step/sec: 0.089677\n",
      "I1117 16:34:33.013279 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089677\n",
      "INFO:tensorflow:examples/sec: 2.86967\n",
      "I1117 16:34:33.013504 4595733952 tpu_estimator.py:2160] examples/sec: 2.86967\n",
      "INFO:tensorflow:global_step/sec: 0.0895562\n",
      "I1117 16:34:44.179497 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895562\n",
      "INFO:tensorflow:examples/sec: 2.8658\n",
      "I1117 16:34:44.179723 4595733952 tpu_estimator.py:2160] examples/sec: 2.8658\n",
      "INFO:tensorflow:global_step/sec: 0.089709\n",
      "I1117 16:34:55.326607 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089709\n",
      "INFO:tensorflow:examples/sec: 2.87069\n",
      "I1117 16:34:55.326821 4595733952 tpu_estimator.py:2160] examples/sec: 2.87069\n",
      "INFO:tensorflow:global_step/sec: 0.0892865\n",
      "I1117 16:35:06.526519 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892865\n",
      "INFO:tensorflow:examples/sec: 2.85717\n",
      "I1117 16:35:06.526735 4595733952 tpu_estimator.py:2160] examples/sec: 2.85717\n",
      "INFO:tensorflow:global_step/sec: 0.0896224\n",
      "I1117 16:35:17.684432 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896224\n",
      "INFO:tensorflow:examples/sec: 2.86792\n",
      "I1117 16:35:17.684645 4595733952 tpu_estimator.py:2160] examples/sec: 2.86792\n",
      "INFO:tensorflow:global_step/sec: 0.0896905\n",
      "I1117 16:35:28.833896 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896905\n",
      "INFO:tensorflow:examples/sec: 2.8701\n",
      "I1117 16:35:28.834118 4595733952 tpu_estimator.py:2160] examples/sec: 2.8701\n",
      "INFO:tensorflow:global_step/sec: 0.089706\n",
      "I1117 16:35:39.981410 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089706\n",
      "INFO:tensorflow:examples/sec: 2.87059\n",
      "I1117 16:35:39.981620 4595733952 tpu_estimator.py:2160] examples/sec: 2.87059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0897112\n",
      "I1117 16:35:51.128309 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897112\n",
      "INFO:tensorflow:examples/sec: 2.87076\n",
      "I1117 16:35:51.128515 4595733952 tpu_estimator.py:2160] examples/sec: 2.87076\n",
      "INFO:tensorflow:global_step/sec: 0.0897769\n",
      "I1117 16:36:02.267026 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897769\n",
      "INFO:tensorflow:examples/sec: 2.87286\n",
      "I1117 16:36:02.267244 4595733952 tpu_estimator.py:2160] examples/sec: 2.87286\n",
      "INFO:tensorflow:global_step/sec: 0.0899449\n",
      "I1117 16:36:13.384954 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899449\n",
      "INFO:tensorflow:examples/sec: 2.87824\n",
      "I1117 16:36:13.385176 4595733952 tpu_estimator.py:2160] examples/sec: 2.87824\n",
      "INFO:tensorflow:global_step/sec: 0.0876261\n",
      "I1117 16:36:24.797178 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876261\n",
      "INFO:tensorflow:examples/sec: 2.80403\n",
      "I1117 16:36:24.797582 4595733952 tpu_estimator.py:2160] examples/sec: 2.80403\n",
      "INFO:tensorflow:global_step/sec: 0.0860607\n",
      "I1117 16:36:36.416769 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0860607\n",
      "INFO:tensorflow:examples/sec: 2.75394\n",
      "I1117 16:36:36.417106 4595733952 tpu_estimator.py:2160] examples/sec: 2.75394\n",
      "INFO:tensorflow:global_step/sec: 0.0807531\n",
      "I1117 16:36:48.800207 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0807531\n",
      "INFO:tensorflow:examples/sec: 2.5841\n",
      "I1117 16:36:48.800613 4595733952 tpu_estimator.py:2160] examples/sec: 2.5841\n",
      "INFO:tensorflow:global_step/sec: 0.0752534\n",
      "I1117 16:37:02.088639 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0752534\n",
      "INFO:tensorflow:examples/sec: 2.40811\n",
      "I1117 16:37:02.088858 4595733952 tpu_estimator.py:2160] examples/sec: 2.40811\n",
      "INFO:tensorflow:global_step/sec: 0.0837956\n",
      "I1117 16:37:14.022437 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837956\n",
      "INFO:tensorflow:examples/sec: 2.68146\n",
      "I1117 16:37:14.022650 4595733952 tpu_estimator.py:2160] examples/sec: 2.68146\n",
      "INFO:tensorflow:global_step/sec: 0.0826044\n",
      "I1117 16:37:26.128365 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0826044\n",
      "INFO:tensorflow:examples/sec: 2.64334\n",
      "I1117 16:37:26.128796 4595733952 tpu_estimator.py:2160] examples/sec: 2.64334\n",
      "INFO:tensorflow:global_step/sec: 0.0839167\n",
      "I1117 16:37:38.044929 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839167\n",
      "INFO:tensorflow:examples/sec: 2.68533\n",
      "I1117 16:37:38.045136 4595733952 tpu_estimator.py:2160] examples/sec: 2.68533\n",
      "INFO:tensorflow:global_step/sec: 0.0839806\n",
      "I1117 16:37:49.952421 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839806\n",
      "INFO:tensorflow:examples/sec: 2.68738\n",
      "I1117 16:37:49.952660 4595733952 tpu_estimator.py:2160] examples/sec: 2.68738\n",
      "INFO:tensorflow:global_step/sec: 0.0842043\n",
      "I1117 16:38:01.828300 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842043\n",
      "INFO:tensorflow:examples/sec: 2.69454\n",
      "I1117 16:38:01.829571 4595733952 tpu_estimator.py:2160] examples/sec: 2.69454\n",
      "INFO:tensorflow:global_step/sec: 0.0846988\n",
      "I1117 16:38:13.634825 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0846988\n",
      "INFO:tensorflow:examples/sec: 2.71036\n",
      "I1117 16:38:13.635030 4595733952 tpu_estimator.py:2160] examples/sec: 2.71036\n",
      "INFO:tensorflow:global_step/sec: 0.0839845\n",
      "I1117 16:38:25.541856 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839845\n",
      "INFO:tensorflow:examples/sec: 2.68751\n",
      "I1117 16:38:25.542021 4595733952 tpu_estimator.py:2160] examples/sec: 2.68751\n",
      "INFO:tensorflow:global_step/sec: 0.0846955\n",
      "I1117 16:38:37.348784 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0846955\n",
      "INFO:tensorflow:examples/sec: 2.71026\n",
      "I1117 16:38:37.348990 4595733952 tpu_estimator.py:2160] examples/sec: 2.71026\n",
      "INFO:tensorflow:global_step/sec: 0.0835797\n",
      "I1117 16:38:49.313414 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0835797\n",
      "INFO:tensorflow:examples/sec: 2.67455\n",
      "I1117 16:38:49.313753 4595733952 tpu_estimator.py:2160] examples/sec: 2.67455\n",
      "INFO:tensorflow:global_step/sec: 0.0843312\n",
      "I1117 16:39:01.171454 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843312\n",
      "INFO:tensorflow:examples/sec: 2.6986\n",
      "I1117 16:39:01.171664 4595733952 tpu_estimator.py:2160] examples/sec: 2.6986\n",
      "INFO:tensorflow:global_step/sec: 0.083451\n",
      "I1117 16:39:13.154522 4595733952 tpu_estimator.py:2159] global_step/sec: 0.083451\n",
      "INFO:tensorflow:examples/sec: 2.67043\n",
      "I1117 16:39:13.154735 4595733952 tpu_estimator.py:2160] examples/sec: 2.67043\n",
      "INFO:tensorflow:global_step/sec: 0.0840653\n",
      "I1117 16:39:25.050029 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840653\n",
      "INFO:tensorflow:examples/sec: 2.69009\n",
      "I1117 16:39:25.050243 4595733952 tpu_estimator.py:2160] examples/sec: 2.69009\n",
      "INFO:tensorflow:global_step/sec: 0.0836517\n",
      "I1117 16:39:37.004384 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836517\n",
      "INFO:tensorflow:examples/sec: 2.67685\n",
      "I1117 16:39:37.004793 4595733952 tpu_estimator.py:2160] examples/sec: 2.67685\n",
      "INFO:tensorflow:global_step/sec: 0.0840079\n",
      "I1117 16:39:48.907984 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840079\n",
      "INFO:tensorflow:examples/sec: 2.68825\n",
      "I1117 16:39:48.908172 4595733952 tpu_estimator.py:2160] examples/sec: 2.68825\n",
      "INFO:tensorflow:global_step/sec: 0.0840943\n",
      "I1117 16:40:00.799427 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840943\n",
      "INFO:tensorflow:examples/sec: 2.69102\n",
      "I1117 16:40:00.799640 4595733952 tpu_estimator.py:2160] examples/sec: 2.69102\n",
      "INFO:tensorflow:global_step/sec: 0.0839145\n",
      "I1117 16:40:12.716327 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839145\n",
      "INFO:tensorflow:examples/sec: 2.68526\n",
      "I1117 16:40:12.716526 4595733952 tpu_estimator.py:2160] examples/sec: 2.68526\n",
      "INFO:tensorflow:global_step/sec: 0.0841088\n",
      "I1117 16:40:24.605726 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841088\n",
      "INFO:tensorflow:examples/sec: 2.69148\n",
      "I1117 16:40:24.606000 4595733952 tpu_estimator.py:2160] examples/sec: 2.69148\n",
      "INFO:tensorflow:global_step/sec: 0.08352\n",
      "I1117 16:40:36.578914 4595733952 tpu_estimator.py:2159] global_step/sec: 0.08352\n",
      "INFO:tensorflow:examples/sec: 2.67264\n",
      "I1117 16:40:36.579121 4595733952 tpu_estimator.py:2160] examples/sec: 2.67264\n",
      "INFO:tensorflow:global_step/sec: 0.0841887\n",
      "I1117 16:40:48.456964 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841887\n",
      "INFO:tensorflow:examples/sec: 2.69404\n",
      "I1117 16:40:48.457187 4595733952 tpu_estimator.py:2160] examples/sec: 2.69404\n",
      "INFO:tensorflow:global_step/sec: 0.0834805\n",
      "I1117 16:41:00.435797 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0834805\n",
      "INFO:tensorflow:examples/sec: 2.67138\n",
      "I1117 16:41:00.436005 4595733952 tpu_estimator.py:2160] examples/sec: 2.67138\n",
      "INFO:tensorflow:global_step/sec: 0.0837075\n",
      "I1117 16:41:12.382171 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837075\n",
      "INFO:tensorflow:examples/sec: 2.67864\n",
      "I1117 16:41:12.382424 4595733952 tpu_estimator.py:2160] examples/sec: 2.67864\n",
      "INFO:tensorflow:global_step/sec: 0.0841281\n",
      "I1117 16:41:24.268779 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841281\n",
      "INFO:tensorflow:examples/sec: 2.6921\n",
      "I1117 16:41:24.269040 4595733952 tpu_estimator.py:2160] examples/sec: 2.6921\n",
      "INFO:tensorflow:global_step/sec: 0.0842717\n",
      "I1117 16:41:36.135162 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842717\n",
      "INFO:tensorflow:examples/sec: 2.69669\n",
      "I1117 16:41:36.135557 4595733952 tpu_estimator.py:2160] examples/sec: 2.69669\n",
      "INFO:tensorflow:global_step/sec: 0.0844169\n",
      "I1117 16:41:47.981150 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844169\n",
      "INFO:tensorflow:examples/sec: 2.70134\n",
      "I1117 16:41:47.981424 4595733952 tpu_estimator.py:2160] examples/sec: 2.70134\n",
      "INFO:tensorflow:global_step/sec: 0.0844001\n",
      "I1117 16:41:59.829466 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844001\n",
      "INFO:tensorflow:examples/sec: 2.7008\n",
      "I1117 16:41:59.829673 4595733952 tpu_estimator.py:2160] examples/sec: 2.7008\n",
      "INFO:tensorflow:global_step/sec: 0.0784168\n",
      "I1117 16:42:12.581818 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0784168\n",
      "INFO:tensorflow:examples/sec: 2.50934\n",
      "I1117 16:42:12.582011 4595733952 tpu_estimator.py:2160] examples/sec: 2.50934\n",
      "INFO:tensorflow:global_step/sec: 0.0770869\n",
      "I1117 16:42:25.554229 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0770869\n",
      "INFO:tensorflow:examples/sec: 2.46678\n",
      "I1117 16:42:25.554440 4595733952 tpu_estimator.py:2160] examples/sec: 2.46678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0778128\n",
      "I1117 16:42:38.405549 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0778128\n",
      "INFO:tensorflow:examples/sec: 2.49001\n",
      "I1117 16:42:38.405894 4595733952 tpu_estimator.py:2160] examples/sec: 2.49001\n",
      "INFO:tensorflow:global_step/sec: 0.0783322\n",
      "I1117 16:42:51.171711 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0783322\n",
      "INFO:tensorflow:examples/sec: 2.50663\n",
      "I1117 16:42:51.172062 4595733952 tpu_estimator.py:2160] examples/sec: 2.50663\n",
      "INFO:tensorflow:global_step/sec: 0.0782219\n",
      "I1117 16:43:03.955899 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0782219\n",
      "INFO:tensorflow:examples/sec: 2.5031\n",
      "I1117 16:43:03.956120 4595733952 tpu_estimator.py:2160] examples/sec: 2.5031\n",
      "INFO:tensorflow:global_step/sec: 0.0760372\n",
      "I1117 16:43:17.107368 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0760372\n",
      "INFO:tensorflow:examples/sec: 2.43319\n",
      "I1117 16:43:17.107580 4595733952 tpu_estimator.py:2160] examples/sec: 2.43319\n",
      "INFO:tensorflow:global_step/sec: 0.0807604\n",
      "I1117 16:43:29.489629 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0807604\n",
      "INFO:tensorflow:examples/sec: 2.58433\n",
      "I1117 16:43:29.489841 4595733952 tpu_estimator.py:2160] examples/sec: 2.58433\n",
      "INFO:tensorflow:global_step/sec: 0.0821243\n",
      "I1117 16:43:41.666298 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0821243\n",
      "INFO:tensorflow:examples/sec: 2.62798\n",
      "I1117 16:43:41.666506 4595733952 tpu_estimator.py:2160] examples/sec: 2.62798\n",
      "INFO:tensorflow:global_step/sec: 0.0831087\n",
      "I1117 16:43:53.698740 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0831087\n",
      "INFO:tensorflow:examples/sec: 2.65948\n",
      "I1117 16:43:53.698951 4595733952 tpu_estimator.py:2160] examples/sec: 2.65948\n",
      "INFO:tensorflow:global_step/sec: 0.0833685\n",
      "I1117 16:44:05.693669 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0833685\n",
      "INFO:tensorflow:examples/sec: 2.66779\n",
      "I1117 16:44:05.693879 4595733952 tpu_estimator.py:2160] examples/sec: 2.66779\n",
      "INFO:tensorflow:global_step/sec: 0.0837897\n",
      "I1117 16:44:17.628340 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837897\n",
      "INFO:tensorflow:examples/sec: 2.68127\n",
      "I1117 16:44:17.628762 4595733952 tpu_estimator.py:2160] examples/sec: 2.68127\n",
      "INFO:tensorflow:global_step/sec: 0.0806677\n",
      "I1117 16:44:30.024850 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0806677\n",
      "INFO:tensorflow:examples/sec: 2.58137\n",
      "I1117 16:44:30.025055 4595733952 tpu_estimator.py:2160] examples/sec: 2.58137\n",
      "INFO:tensorflow:global_step/sec: 0.083207\n",
      "I1117 16:44:42.043065 4595733952 tpu_estimator.py:2159] global_step/sec: 0.083207\n",
      "INFO:tensorflow:examples/sec: 2.66262\n",
      "I1117 16:44:42.043296 4595733952 tpu_estimator.py:2160] examples/sec: 2.66262\n",
      "INFO:tensorflow:global_step/sec: 0.0839688\n",
      "I1117 16:44:53.952250 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839688\n",
      "INFO:tensorflow:examples/sec: 2.687\n",
      "I1117 16:44:53.952571 4595733952 tpu_estimator.py:2160] examples/sec: 2.687\n",
      "INFO:tensorflow:global_step/sec: 0.0839477\n",
      "I1117 16:45:05.864439 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0839477\n",
      "INFO:tensorflow:examples/sec: 2.68633\n",
      "I1117 16:45:05.864799 4595733952 tpu_estimator.py:2160] examples/sec: 2.68633\n",
      "INFO:tensorflow:global_step/sec: 0.0808562\n",
      "I1117 16:45:18.232087 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0808562\n",
      "INFO:tensorflow:examples/sec: 2.5874\n",
      "I1117 16:45:18.232325 4595733952 tpu_estimator.py:2160] examples/sec: 2.5874\n",
      "INFO:tensorflow:global_step/sec: 0.0872029\n",
      "I1117 16:45:29.699586 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872029\n",
      "INFO:tensorflow:examples/sec: 2.79049\n",
      "I1117 16:45:29.699944 4595733952 tpu_estimator.py:2160] examples/sec: 2.79049\n",
      "INFO:tensorflow:global_step/sec: 0.0876376\n",
      "I1117 16:45:41.110224 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876376\n",
      "INFO:tensorflow:examples/sec: 2.8044\n",
      "I1117 16:45:41.110448 4595733952 tpu_estimator.py:2160] examples/sec: 2.8044\n",
      "INFO:tensorflow:global_step/sec: 0.0875808\n",
      "I1117 16:45:52.528268 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875808\n",
      "INFO:tensorflow:examples/sec: 2.80259\n",
      "I1117 16:45:52.528661 4595733952 tpu_estimator.py:2160] examples/sec: 2.80259\n",
      "INFO:tensorflow:global_step/sec: 0.0864428\n",
      "I1117 16:46:04.096572 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0864428\n",
      "INFO:tensorflow:examples/sec: 2.76617\n",
      "I1117 16:46:04.096759 4595733952 tpu_estimator.py:2160] examples/sec: 2.76617\n",
      "INFO:tensorflow:global_step/sec: 0.0866241\n",
      "I1117 16:46:15.640748 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866241\n",
      "INFO:tensorflow:examples/sec: 2.77197\n",
      "I1117 16:46:15.640970 4595733952 tpu_estimator.py:2160] examples/sec: 2.77197\n",
      "INFO:tensorflow:global_step/sec: 0.0889788\n",
      "I1117 16:46:26.879352 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889788\n",
      "INFO:tensorflow:examples/sec: 2.84732\n",
      "I1117 16:46:26.879559 4595733952 tpu_estimator.py:2160] examples/sec: 2.84732\n",
      "INFO:tensorflow:global_step/sec: 0.0898479\n",
      "I1117 16:46:38.009283 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898479\n",
      "INFO:tensorflow:examples/sec: 2.87513\n",
      "I1117 16:46:38.009499 4595733952 tpu_estimator.py:2160] examples/sec: 2.87513\n",
      "INFO:tensorflow:global_step/sec: 0.0903213\n",
      "I1117 16:46:49.080863 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903213\n",
      "INFO:tensorflow:examples/sec: 2.89028\n",
      "I1117 16:46:49.081079 4595733952 tpu_estimator.py:2160] examples/sec: 2.89028\n",
      "INFO:tensorflow:global_step/sec: 0.0887875\n",
      "I1117 16:47:00.343706 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887875\n",
      "INFO:tensorflow:examples/sec: 2.8412\n",
      "I1117 16:47:00.343924 4595733952 tpu_estimator.py:2160] examples/sec: 2.8412\n",
      "INFO:tensorflow:global_step/sec: 0.0825704\n",
      "I1117 16:47:12.454597 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0825704\n",
      "INFO:tensorflow:examples/sec: 2.64225\n",
      "I1117 16:47:12.454853 4595733952 tpu_estimator.py:2160] examples/sec: 2.64225\n",
      "INFO:tensorflow:global_step/sec: 0.0898336\n",
      "I1117 16:47:23.586267 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898336\n",
      "INFO:tensorflow:examples/sec: 2.87467\n",
      "I1117 16:47:23.586472 4595733952 tpu_estimator.py:2160] examples/sec: 2.87467\n",
      "INFO:tensorflow:global_step/sec: 0.0901859\n",
      "I1117 16:47:34.674491 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901859\n",
      "INFO:tensorflow:examples/sec: 2.88595\n",
      "I1117 16:47:34.674700 4595733952 tpu_estimator.py:2160] examples/sec: 2.88595\n",
      "INFO:tensorflow:global_step/sec: 0.089671\n",
      "I1117 16:47:45.826380 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089671\n",
      "INFO:tensorflow:examples/sec: 2.86947\n",
      "I1117 16:47:45.826611 4595733952 tpu_estimator.py:2160] examples/sec: 2.86947\n",
      "INFO:tensorflow:global_step/sec: 0.0903227\n",
      "I1117 16:47:56.897782 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903227\n",
      "INFO:tensorflow:examples/sec: 2.89033\n",
      "I1117 16:47:56.898007 4595733952 tpu_estimator.py:2160] examples/sec: 2.89033\n",
      "INFO:tensorflow:global_step/sec: 0.0898053\n",
      "I1117 16:48:08.033021 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898053\n",
      "INFO:tensorflow:examples/sec: 2.87377\n",
      "I1117 16:48:08.033264 4595733952 tpu_estimator.py:2160] examples/sec: 2.87377\n",
      "INFO:tensorflow:global_step/sec: 0.0893488\n",
      "I1117 16:48:19.225068 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0893488\n",
      "INFO:tensorflow:examples/sec: 2.85916\n",
      "I1117 16:48:19.225276 4595733952 tpu_estimator.py:2160] examples/sec: 2.85916\n",
      "INFO:tensorflow:global_step/sec: 0.0902382\n",
      "I1117 16:48:30.306844 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902382\n",
      "INFO:tensorflow:examples/sec: 2.88762\n",
      "I1117 16:48:30.307082 4595733952 tpu_estimator.py:2160] examples/sec: 2.88762\n",
      "INFO:tensorflow:global_step/sec: 0.0898888\n",
      "I1117 16:48:41.431714 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898888\n",
      "INFO:tensorflow:examples/sec: 2.87644\n",
      "I1117 16:48:41.431942 4595733952 tpu_estimator.py:2160] examples/sec: 2.87644\n",
      "INFO:tensorflow:global_step/sec: 0.0898879\n",
      "I1117 16:48:52.556667 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898879\n",
      "INFO:tensorflow:examples/sec: 2.87641\n",
      "I1117 16:48:52.556894 4595733952 tpu_estimator.py:2160] examples/sec: 2.87641\n",
      "INFO:tensorflow:global_step/sec: 0.0900491\n",
      "I1117 16:49:03.661725 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900491\n",
      "INFO:tensorflow:examples/sec: 2.88157\n",
      "I1117 16:49:03.661948 4595733952 tpu_estimator.py:2160] examples/sec: 2.88157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.089657\n",
      "I1117 16:49:14.815354 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089657\n",
      "INFO:tensorflow:examples/sec: 2.86902\n",
      "I1117 16:49:14.815572 4595733952 tpu_estimator.py:2160] examples/sec: 2.86902\n",
      "INFO:tensorflow:global_step/sec: 0.0901913\n",
      "I1117 16:49:25.902895 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901913\n",
      "INFO:tensorflow:examples/sec: 2.88612\n",
      "I1117 16:49:25.903130 4595733952 tpu_estimator.py:2160] examples/sec: 2.88612\n",
      "INFO:tensorflow:global_step/sec: 0.0898968\n",
      "I1117 16:49:37.026747 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898968\n",
      "INFO:tensorflow:examples/sec: 2.8767\n",
      "I1117 16:49:37.026959 4595733952 tpu_estimator.py:2160] examples/sec: 2.8767\n",
      "INFO:tensorflow:global_step/sec: 0.0900747\n",
      "I1117 16:49:48.128653 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900747\n",
      "INFO:tensorflow:examples/sec: 2.88239\n",
      "I1117 16:49:48.128869 4595733952 tpu_estimator.py:2160] examples/sec: 2.88239\n",
      "INFO:tensorflow:global_step/sec: 0.0899496\n",
      "I1117 16:49:59.245999 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899496\n",
      "INFO:tensorflow:examples/sec: 2.87839\n",
      "I1117 16:49:59.246222 4595733952 tpu_estimator.py:2160] examples/sec: 2.87839\n",
      "INFO:tensorflow:global_step/sec: 0.0900123\n",
      "I1117 16:50:10.355575 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900123\n",
      "INFO:tensorflow:examples/sec: 2.88039\n",
      "I1117 16:50:10.355787 4595733952 tpu_estimator.py:2160] examples/sec: 2.88039\n",
      "INFO:tensorflow:global_step/sec: 0.090461\n",
      "I1117 16:50:21.410071 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090461\n",
      "INFO:tensorflow:examples/sec: 2.89475\n",
      "I1117 16:50:21.410290 4595733952 tpu_estimator.py:2160] examples/sec: 2.89475\n",
      "INFO:tensorflow:global_step/sec: 0.0895687\n",
      "I1117 16:50:32.574697 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895687\n",
      "INFO:tensorflow:examples/sec: 2.8662\n",
      "I1117 16:50:32.574923 4595733952 tpu_estimator.py:2160] examples/sec: 2.8662\n",
      "INFO:tensorflow:global_step/sec: 0.0903622\n",
      "I1117 16:50:43.641274 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903622\n",
      "INFO:tensorflow:examples/sec: 2.89159\n",
      "I1117 16:50:43.641502 4595733952 tpu_estimator.py:2160] examples/sec: 2.89159\n",
      "INFO:tensorflow:global_step/sec: 0.089946\n",
      "I1117 16:50:54.759062 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089946\n",
      "INFO:tensorflow:examples/sec: 2.87827\n",
      "I1117 16:50:54.759281 4595733952 tpu_estimator.py:2160] examples/sec: 2.87827\n",
      "INFO:tensorflow:global_step/sec: 0.0900089\n",
      "I1117 16:51:05.869066 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900089\n",
      "INFO:tensorflow:examples/sec: 2.88029\n",
      "I1117 16:51:05.869289 4595733952 tpu_estimator.py:2160] examples/sec: 2.88029\n",
      "INFO:tensorflow:global_step/sec: 0.0901031\n",
      "I1117 16:51:16.967467 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901031\n",
      "INFO:tensorflow:examples/sec: 2.8833\n",
      "I1117 16:51:16.967685 4595733952 tpu_estimator.py:2160] examples/sec: 2.8833\n",
      "INFO:tensorflow:global_step/sec: 0.0904224\n",
      "I1117 16:51:28.026679 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904224\n",
      "INFO:tensorflow:examples/sec: 2.89352\n",
      "I1117 16:51:28.026903 4595733952 tpu_estimator.py:2160] examples/sec: 2.89352\n",
      "INFO:tensorflow:global_step/sec: 0.089763\n",
      "I1117 16:51:39.167106 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089763\n",
      "INFO:tensorflow:examples/sec: 2.87242\n",
      "I1117 16:51:39.167316 4595733952 tpu_estimator.py:2160] examples/sec: 2.87242\n",
      "INFO:tensorflow:global_step/sec: 0.0899222\n",
      "I1117 16:51:50.287847 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899222\n",
      "INFO:tensorflow:examples/sec: 2.87751\n",
      "I1117 16:51:50.288070 4595733952 tpu_estimator.py:2160] examples/sec: 2.87751\n",
      "INFO:tensorflow:global_step/sec: 0.0901107\n",
      "I1117 16:52:01.385305 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901107\n",
      "INFO:tensorflow:examples/sec: 2.88354\n",
      "I1117 16:52:01.385531 4595733952 tpu_estimator.py:2160] examples/sec: 2.88354\n",
      "INFO:tensorflow:global_step/sec: 0.0894379\n",
      "I1117 16:52:12.566251 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894379\n",
      "INFO:tensorflow:examples/sec: 2.86201\n",
      "I1117 16:52:12.566474 4595733952 tpu_estimator.py:2160] examples/sec: 2.86201\n",
      "INFO:tensorflow:global_step/sec: 0.0900297\n",
      "I1117 16:52:23.673696 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900297\n",
      "INFO:tensorflow:examples/sec: 2.88095\n",
      "I1117 16:52:23.673921 4595733952 tpu_estimator.py:2160] examples/sec: 2.88095\n",
      "INFO:tensorflow:global_step/sec: 0.0899006\n",
      "I1117 16:52:34.797096 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899006\n",
      "INFO:tensorflow:examples/sec: 2.87682\n",
      "I1117 16:52:34.797322 4595733952 tpu_estimator.py:2160] examples/sec: 2.87682\n",
      "INFO:tensorflow:global_step/sec: 0.0900027\n",
      "I1117 16:52:45.907878 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900027\n",
      "INFO:tensorflow:examples/sec: 2.88009\n",
      "I1117 16:52:45.908142 4595733952 tpu_estimator.py:2160] examples/sec: 2.88009\n",
      "INFO:tensorflow:global_step/sec: 0.0899986\n",
      "I1117 16:52:57.019143 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899986\n",
      "INFO:tensorflow:examples/sec: 2.87996\n",
      "I1117 16:52:57.019360 4595733952 tpu_estimator.py:2160] examples/sec: 2.87996\n",
      "INFO:tensorflow:global_step/sec: 0.0901849\n",
      "I1117 16:53:08.107472 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901849\n",
      "INFO:tensorflow:examples/sec: 2.88592\n",
      "I1117 16:53:08.107681 4595733952 tpu_estimator.py:2160] examples/sec: 2.88592\n",
      "INFO:tensorflow:global_step/sec: 0.088168\n",
      "I1117 16:53:19.449499 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088168\n",
      "INFO:tensorflow:examples/sec: 2.82137\n",
      "I1117 16:53:19.449713 4595733952 tpu_estimator.py:2160] examples/sec: 2.82137\n",
      "INFO:tensorflow:global_step/sec: 0.0881062\n",
      "I1117 16:53:30.799407 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0881062\n",
      "INFO:tensorflow:examples/sec: 2.8194\n",
      "I1117 16:53:30.799620 4595733952 tpu_estimator.py:2160] examples/sec: 2.8194\n",
      "INFO:tensorflow:global_step/sec: 0.0900692\n",
      "I1117 16:53:41.901988 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900692\n",
      "INFO:tensorflow:examples/sec: 2.88221\n",
      "I1117 16:53:41.902202 4595733952 tpu_estimator.py:2160] examples/sec: 2.88221\n",
      "INFO:tensorflow:global_step/sec: 0.0900377\n",
      "I1117 16:53:53.008442 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900377\n",
      "INFO:tensorflow:examples/sec: 2.88121\n",
      "I1117 16:53:53.008655 4595733952 tpu_estimator.py:2160] examples/sec: 2.88121\n",
      "INFO:tensorflow:global_step/sec: 0.0899343\n",
      "I1117 16:54:04.127681 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899343\n",
      "INFO:tensorflow:examples/sec: 2.8779\n",
      "I1117 16:54:04.127897 4595733952 tpu_estimator.py:2160] examples/sec: 2.8779\n",
      "INFO:tensorflow:global_step/sec: 0.0899882\n",
      "I1117 16:54:15.240253 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899882\n",
      "INFO:tensorflow:examples/sec: 2.87962\n",
      "I1117 16:54:15.240468 4595733952 tpu_estimator.py:2160] examples/sec: 2.87962\n",
      "INFO:tensorflow:global_step/sec: 0.0904175\n",
      "I1117 16:54:26.300067 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904175\n",
      "INFO:tensorflow:examples/sec: 2.89336\n",
      "I1117 16:54:26.300286 4595733952 tpu_estimator.py:2160] examples/sec: 2.89336\n",
      "INFO:tensorflow:global_step/sec: 0.0902686\n",
      "I1117 16:54:37.378105 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902686\n",
      "INFO:tensorflow:examples/sec: 2.8886\n",
      "I1117 16:54:37.378319 4595733952 tpu_estimator.py:2160] examples/sec: 2.8886\n",
      "INFO:tensorflow:global_step/sec: 0.0902749\n",
      "I1117 16:54:48.455386 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902749\n",
      "INFO:tensorflow:examples/sec: 2.8888\n",
      "I1117 16:54:48.455609 4595733952 tpu_estimator.py:2160] examples/sec: 2.8888\n",
      "INFO:tensorflow:global_step/sec: 0.0900449\n",
      "I1117 16:54:59.560950 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900449\n",
      "INFO:tensorflow:examples/sec: 2.88144\n",
      "I1117 16:54:59.561166 4595733952 tpu_estimator.py:2160] examples/sec: 2.88144\n",
      "INFO:tensorflow:global_step/sec: 0.0903295\n",
      "I1117 16:55:10.631545 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903295\n",
      "INFO:tensorflow:examples/sec: 2.89054\n",
      "I1117 16:55:10.631767 4595733952 tpu_estimator.py:2160] examples/sec: 2.89054\n",
      "INFO:tensorflow:global_step/sec: 0.090206\n",
      "I1117 16:55:21.717291 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090206\n",
      "INFO:tensorflow:examples/sec: 2.88659\n",
      "I1117 16:55:21.717498 4595733952 tpu_estimator.py:2160] examples/sec: 2.88659\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0899841\n",
      "I1117 16:55:32.830341 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899841\n",
      "INFO:tensorflow:examples/sec: 2.87949\n",
      "I1117 16:55:32.830551 4595733952 tpu_estimator.py:2160] examples/sec: 2.87949\n",
      "INFO:tensorflow:global_step/sec: 0.0901577\n",
      "I1117 16:55:43.922027 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901577\n",
      "INFO:tensorflow:examples/sec: 2.88505\n",
      "I1117 16:55:43.922240 4595733952 tpu_estimator.py:2160] examples/sec: 2.88505\n",
      "INFO:tensorflow:global_step/sec: 0.0901362\n",
      "I1117 16:55:55.016340 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901362\n",
      "INFO:tensorflow:examples/sec: 2.88436\n",
      "I1117 16:55:55.016548 4595733952 tpu_estimator.py:2160] examples/sec: 2.88436\n",
      "INFO:tensorflow:global_step/sec: 0.0901769\n",
      "I1117 16:56:06.105664 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901769\n",
      "INFO:tensorflow:examples/sec: 2.88566\n",
      "I1117 16:56:06.105873 4595733952 tpu_estimator.py:2160] examples/sec: 2.88566\n",
      "INFO:tensorflow:global_step/sec: 0.0899227\n",
      "I1117 16:56:17.226366 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0899227\n",
      "INFO:tensorflow:examples/sec: 2.87752\n",
      "I1117 16:56:17.226590 4595733952 tpu_estimator.py:2160] examples/sec: 2.87752\n",
      "INFO:tensorflow:global_step/sec: 0.0902286\n",
      "I1117 16:56:28.309296 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902286\n",
      "INFO:tensorflow:examples/sec: 2.88732\n",
      "I1117 16:56:28.309514 4595733952 tpu_estimator.py:2160] examples/sec: 2.88732\n",
      "INFO:tensorflow:global_step/sec: 0.0891226\n",
      "I1117 16:56:39.529799 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891226\n",
      "INFO:tensorflow:examples/sec: 2.85192\n",
      "I1117 16:56:39.530046 4595733952 tpu_estimator.py:2160] examples/sec: 2.85192\n",
      "INFO:tensorflow:global_step/sec: 0.09022\n",
      "I1117 16:56:50.613797 4595733952 tpu_estimator.py:2159] global_step/sec: 0.09022\n",
      "INFO:tensorflow:examples/sec: 2.88704\n",
      "I1117 16:56:50.614003 4595733952 tpu_estimator.py:2160] examples/sec: 2.88704\n",
      "INFO:tensorflow:global_step/sec: 0.0900812\n",
      "I1117 16:57:01.714894 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900812\n",
      "INFO:tensorflow:examples/sec: 2.8826\n",
      "I1117 16:57:01.715111 4595733952 tpu_estimator.py:2160] examples/sec: 2.8826\n",
      "INFO:tensorflow:global_step/sec: 0.0900902\n",
      "I1117 16:57:12.814885 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900902\n",
      "INFO:tensorflow:examples/sec: 2.88289\n",
      "I1117 16:57:12.815103 4595733952 tpu_estimator.py:2160] examples/sec: 2.88289\n",
      "INFO:tensorflow:global_step/sec: 0.0896861\n",
      "I1117 16:57:23.964900 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896861\n",
      "INFO:tensorflow:examples/sec: 2.86996\n",
      "I1117 16:57:23.965107 4595733952 tpu_estimator.py:2160] examples/sec: 2.86996\n",
      "INFO:tensorflow:global_step/sec: 0.0897879\n",
      "I1117 16:57:35.102241 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897879\n",
      "INFO:tensorflow:examples/sec: 2.87321\n",
      "I1117 16:57:35.102456 4595733952 tpu_estimator.py:2160] examples/sec: 2.87321\n",
      "INFO:tensorflow:global_step/sec: 0.0897338\n",
      "I1117 16:57:46.246335 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897338\n",
      "INFO:tensorflow:examples/sec: 2.87148\n",
      "I1117 16:57:46.246575 4595733952 tpu_estimator.py:2160] examples/sec: 2.87148\n",
      "INFO:tensorflow:global_step/sec: 0.0901294\n",
      "I1117 16:57:57.341501 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901294\n",
      "INFO:tensorflow:examples/sec: 2.88414\n",
      "I1117 16:57:57.341721 4595733952 tpu_estimator.py:2160] examples/sec: 2.88414\n",
      "INFO:tensorflow:global_step/sec: 0.0897162\n",
      "I1117 16:58:08.487740 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897162\n",
      "INFO:tensorflow:examples/sec: 2.87092\n",
      "I1117 16:58:08.487954 4595733952 tpu_estimator.py:2160] examples/sec: 2.87092\n",
      "INFO:tensorflow:global_step/sec: 0.0902436\n",
      "I1117 16:58:19.568886 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902436\n",
      "INFO:tensorflow:examples/sec: 2.88779\n",
      "I1117 16:58:19.569101 4595733952 tpu_estimator.py:2160] examples/sec: 2.88779\n",
      "INFO:tensorflow:global_step/sec: 0.0902111\n",
      "I1117 16:58:30.654042 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902111\n",
      "INFO:tensorflow:examples/sec: 2.88675\n",
      "I1117 16:58:30.654256 4595733952 tpu_estimator.py:2160] examples/sec: 2.88675\n",
      "INFO:tensorflow:global_step/sec: 0.0898935\n",
      "I1117 16:58:41.778264 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898935\n",
      "INFO:tensorflow:examples/sec: 2.87659\n",
      "I1117 16:58:41.778488 4595733952 tpu_estimator.py:2160] examples/sec: 2.87659\n",
      "INFO:tensorflow:global_step/sec: 0.0903153\n",
      "I1117 16:58:52.850605 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903153\n",
      "INFO:tensorflow:examples/sec: 2.89009\n",
      "I1117 16:58:52.851091 4595733952 tpu_estimator.py:2160] examples/sec: 2.89009\n",
      "INFO:tensorflow:global_step/sec: 0.090065\n",
      "I1117 16:59:03.953702 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090065\n",
      "INFO:tensorflow:examples/sec: 2.88208\n",
      "I1117 16:59:03.953919 4595733952 tpu_estimator.py:2160] examples/sec: 2.88208\n",
      "INFO:tensorflow:global_step/sec: 0.0894819\n",
      "I1117 16:59:15.129132 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894819\n",
      "INFO:tensorflow:examples/sec: 2.86342\n",
      "I1117 16:59:15.129348 4595733952 tpu_estimator.py:2160] examples/sec: 2.86342\n",
      "INFO:tensorflow:global_step/sec: 0.0892917\n",
      "I1117 16:59:26.328393 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892917\n",
      "INFO:tensorflow:examples/sec: 2.85733\n",
      "I1117 16:59:26.328619 4595733952 tpu_estimator.py:2160] examples/sec: 2.85733\n",
      "INFO:tensorflow:global_step/sec: 0.0903719\n",
      "I1117 16:59:37.393772 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903719\n",
      "INFO:tensorflow:examples/sec: 2.8919\n",
      "I1117 16:59:37.393990 4595733952 tpu_estimator.py:2160] examples/sec: 2.8919\n",
      "INFO:tensorflow:global_step/sec: 0.0901227\n",
      "I1117 16:59:48.489765 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901227\n",
      "INFO:tensorflow:examples/sec: 2.88393\n",
      "I1117 16:59:48.490161 4595733952 tpu_estimator.py:2160] examples/sec: 2.88393\n",
      "INFO:tensorflow:global_step/sec: 0.0902554\n",
      "I1117 16:59:59.569417 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902554\n",
      "INFO:tensorflow:examples/sec: 2.88817\n",
      "I1117 16:59:59.569630 4595733952 tpu_estimator.py:2160] examples/sec: 2.88817\n",
      "INFO:tensorflow:global_step/sec: 0.0897818\n",
      "I1117 17:00:10.707535 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897818\n",
      "INFO:tensorflow:examples/sec: 2.87302\n",
      "I1117 17:00:10.707739 4595733952 tpu_estimator.py:2160] examples/sec: 2.87302\n",
      "INFO:tensorflow:global_step/sec: 0.0904116\n",
      "I1117 17:00:21.768101 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904116\n",
      "INFO:tensorflow:examples/sec: 2.89317\n",
      "I1117 17:00:21.768310 4595733952 tpu_estimator.py:2160] examples/sec: 2.89317\n",
      "INFO:tensorflow:global_step/sec: 0.090291\n",
      "I1117 17:00:32.843389 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090291\n",
      "INFO:tensorflow:examples/sec: 2.88931\n",
      "I1117 17:00:32.843605 4595733952 tpu_estimator.py:2160] examples/sec: 2.88931\n",
      "INFO:tensorflow:global_step/sec: 0.090499\n",
      "I1117 17:00:43.893228 4595733952 tpu_estimator.py:2159] global_step/sec: 0.090499\n",
      "INFO:tensorflow:examples/sec: 2.89597\n",
      "I1117 17:00:43.893455 4595733952 tpu_estimator.py:2160] examples/sec: 2.89597\n",
      "INFO:tensorflow:global_step/sec: 0.0903216\n",
      "I1117 17:00:54.964760 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903216\n",
      "INFO:tensorflow:examples/sec: 2.89029\n",
      "I1117 17:00:54.964977 4595733952 tpu_estimator.py:2160] examples/sec: 2.89029\n",
      "INFO:tensorflow:global_step/sec: 0.0896414\n",
      "I1117 17:01:06.120316 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896414\n",
      "INFO:tensorflow:examples/sec: 2.86853\n",
      "I1117 17:01:06.120526 4595733952 tpu_estimator.py:2160] examples/sec: 2.86853\n",
      "INFO:tensorflow:global_step/sec: 0.0900578\n",
      "I1117 17:01:17.224305 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900578\n",
      "INFO:tensorflow:examples/sec: 2.88185\n",
      "I1117 17:01:17.224514 4595733952 tpu_estimator.py:2160] examples/sec: 2.88185\n",
      "INFO:tensorflow:global_step/sec: 0.0901505\n",
      "I1117 17:01:28.316858 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901505\n",
      "INFO:tensorflow:examples/sec: 2.88481\n",
      "I1117 17:01:28.317064 4595733952 tpu_estimator.py:2160] examples/sec: 2.88481\n",
      "INFO:tensorflow:global_step/sec: 0.0902264\n",
      "I1117 17:01:39.400104 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902264\n",
      "INFO:tensorflow:examples/sec: 2.88724\n",
      "I1117 17:01:39.400323 4595733952 tpu_estimator.py:2160] examples/sec: 2.88724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0902631\n",
      "I1117 17:01:50.478843 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902631\n",
      "INFO:tensorflow:examples/sec: 2.88842\n",
      "I1117 17:01:50.479216 4595733952 tpu_estimator.py:2160] examples/sec: 2.88842\n",
      "INFO:tensorflow:global_step/sec: 0.0898766\n",
      "I1117 17:02:01.605189 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898766\n",
      "INFO:tensorflow:examples/sec: 2.87605\n",
      "I1117 17:02:01.605397 4595733952 tpu_estimator.py:2160] examples/sec: 2.87605\n",
      "INFO:tensorflow:global_step/sec: 0.089806\n",
      "I1117 17:02:12.740303 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089806\n",
      "INFO:tensorflow:examples/sec: 2.87379\n",
      "I1117 17:02:12.740503 4595733952 tpu_estimator.py:2160] examples/sec: 2.87379\n",
      "INFO:tensorflow:global_step/sec: 0.084371\n",
      "I1117 17:02:24.592698 4595733952 tpu_estimator.py:2159] global_step/sec: 0.084371\n",
      "INFO:tensorflow:examples/sec: 2.69987\n",
      "I1117 17:02:24.592895 4595733952 tpu_estimator.py:2160] examples/sec: 2.69987\n",
      "INFO:tensorflow:global_step/sec: 0.0900782\n",
      "I1117 17:02:35.694186 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900782\n",
      "INFO:tensorflow:examples/sec: 2.8825\n",
      "I1117 17:02:35.694415 4595733952 tpu_estimator.py:2160] examples/sec: 2.8825\n",
      "INFO:tensorflow:global_step/sec: 0.089779\n",
      "I1117 17:02:46.832649 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089779\n",
      "INFO:tensorflow:examples/sec: 2.87293\n",
      "I1117 17:02:46.832890 4595733952 tpu_estimator.py:2160] examples/sec: 2.87293\n",
      "INFO:tensorflow:global_step/sec: 0.0898652\n",
      "I1117 17:02:57.960423 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0898652\n",
      "INFO:tensorflow:examples/sec: 2.87568\n",
      "I1117 17:02:57.960634 4595733952 tpu_estimator.py:2160] examples/sec: 2.87568\n",
      "INFO:tensorflow:global_step/sec: 0.0905231\n",
      "I1117 17:03:09.007336 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905231\n",
      "INFO:tensorflow:examples/sec: 2.89674\n",
      "I1117 17:03:09.007553 4595733952 tpu_estimator.py:2160] examples/sec: 2.89674\n",
      "INFO:tensorflow:global_step/sec: 0.0900156\n",
      "I1117 17:03:20.116509 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900156\n",
      "INFO:tensorflow:examples/sec: 2.8805\n",
      "I1117 17:03:20.116721 4595733952 tpu_estimator.py:2160] examples/sec: 2.8805\n",
      "INFO:tensorflow:global_step/sec: 0.0904901\n",
      "I1117 17:03:31.167444 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0904901\n",
      "INFO:tensorflow:examples/sec: 2.89568\n",
      "I1117 17:03:31.167669 4595733952 tpu_estimator.py:2160] examples/sec: 2.89568\n",
      "INFO:tensorflow:global_step/sec: 0.0905592\n",
      "I1117 17:03:42.209950 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0905592\n",
      "INFO:tensorflow:examples/sec: 2.89789\n",
      "I1117 17:03:42.210166 4595733952 tpu_estimator.py:2160] examples/sec: 2.89789\n",
      "INFO:tensorflow:global_step/sec: 0.0903327\n",
      "I1117 17:03:53.280132 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0903327\n",
      "INFO:tensorflow:examples/sec: 2.89065\n",
      "I1117 17:03:53.280342 4595733952 tpu_estimator.py:2160] examples/sec: 2.89065\n",
      "INFO:tensorflow:global_step/sec: 0.0901113\n",
      "I1117 17:04:04.377518 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0901113\n",
      "INFO:tensorflow:examples/sec: 2.88356\n",
      "I1117 17:04:04.377733 4595733952 tpu_estimator.py:2160] examples/sec: 2.88356\n",
      "INFO:tensorflow:global_step/sec: 0.0897592\n",
      "I1117 17:04:15.518460 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897592\n",
      "INFO:tensorflow:examples/sec: 2.87229\n",
      "I1117 17:04:15.518672 4595733952 tpu_estimator.py:2160] examples/sec: 2.87229\n",
      "INFO:tensorflow:global_step/sec: 0.0902244\n",
      "I1117 17:04:26.601928 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902244\n",
      "INFO:tensorflow:examples/sec: 2.88718\n",
      "I1117 17:04:26.602154 4595733952 tpu_estimator.py:2160] examples/sec: 2.88718\n",
      "INFO:tensorflow:global_step/sec: 0.0897816\n",
      "I1117 17:04:37.740098 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897816\n",
      "INFO:tensorflow:examples/sec: 2.87301\n",
      "I1117 17:04:37.740319 4595733952 tpu_estimator.py:2160] examples/sec: 2.87301\n",
      "INFO:tensorflow:global_step/sec: 0.0878752\n",
      "I1117 17:04:49.119794 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878752\n",
      "INFO:tensorflow:examples/sec: 2.81201\n",
      "I1117 17:04:49.119973 4595733952 tpu_estimator.py:2160] examples/sec: 2.81201\n",
      "INFO:tensorflow:global_step/sec: 0.0787967\n",
      "I1117 17:05:01.810704 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0787967\n",
      "INFO:tensorflow:examples/sec: 2.52149\n",
      "I1117 17:05:01.810917 4595733952 tpu_estimator.py:2160] examples/sec: 2.52149\n",
      "INFO:tensorflow:global_step/sec: 0.0807888\n",
      "I1117 17:05:14.188829 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0807888\n",
      "INFO:tensorflow:examples/sec: 2.58524\n",
      "I1117 17:05:14.189381 4595733952 tpu_estimator.py:2160] examples/sec: 2.58524\n",
      "INFO:tensorflow:global_step/sec: 0.0836206\n",
      "I1117 17:05:26.147431 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0836206\n",
      "INFO:tensorflow:examples/sec: 2.67586\n",
      "I1117 17:05:26.147645 4595733952 tpu_estimator.py:2160] examples/sec: 2.67586\n",
      "INFO:tensorflow:global_step/sec: 0.0877982\n",
      "I1117 17:05:37.537188 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877982\n",
      "INFO:tensorflow:examples/sec: 2.80954\n",
      "I1117 17:05:37.537408 4595733952 tpu_estimator.py:2160] examples/sec: 2.80954\n",
      "INFO:tensorflow:global_step/sec: 0.0851477\n",
      "I1117 17:05:49.281472 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0851477\n",
      "INFO:tensorflow:examples/sec: 2.72473\n",
      "I1117 17:05:49.281666 4595733952 tpu_estimator.py:2160] examples/sec: 2.72473\n",
      "INFO:tensorflow:global_step/sec: 0.0866398\n",
      "I1117 17:06:00.823534 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866398\n",
      "INFO:tensorflow:examples/sec: 2.77247\n",
      "I1117 17:06:00.823738 4595733952 tpu_estimator.py:2160] examples/sec: 2.77247\n",
      "INFO:tensorflow:global_step/sec: 0.0896481\n",
      "I1117 17:06:11.978256 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896481\n",
      "INFO:tensorflow:examples/sec: 2.86874\n",
      "I1117 17:06:11.978469 4595733952 tpu_estimator.py:2160] examples/sec: 2.86874\n",
      "INFO:tensorflow:global_step/sec: 0.0856624\n",
      "I1117 17:06:23.651972 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0856624\n",
      "INFO:tensorflow:examples/sec: 2.7412\n",
      "I1117 17:06:23.652169 4595733952 tpu_estimator.py:2160] examples/sec: 2.7412\n",
      "INFO:tensorflow:global_step/sec: 0.0856608\n",
      "I1117 17:06:35.326014 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0856608\n",
      "INFO:tensorflow:examples/sec: 2.74114\n",
      "I1117 17:06:35.326362 4595733952 tpu_estimator.py:2160] examples/sec: 2.74114\n",
      "INFO:tensorflow:global_step/sec: 0.0868548\n",
      "I1117 17:06:46.839414 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0868548\n",
      "INFO:tensorflow:examples/sec: 2.77935\n",
      "I1117 17:06:46.839629 4595733952 tpu_estimator.py:2160] examples/sec: 2.77935\n",
      "INFO:tensorflow:global_step/sec: 0.0844759\n",
      "I1117 17:06:58.677103 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844759\n",
      "INFO:tensorflow:examples/sec: 2.70323\n",
      "I1117 17:06:58.677318 4595733952 tpu_estimator.py:2160] examples/sec: 2.70323\n",
      "INFO:tensorflow:global_step/sec: 0.0756545\n",
      "I1117 17:07:11.895046 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0756545\n",
      "INFO:tensorflow:examples/sec: 2.42094\n",
      "I1117 17:07:11.895225 4595733952 tpu_estimator.py:2160] examples/sec: 2.42094\n",
      "INFO:tensorflow:global_step/sec: 0.0830757\n",
      "I1117 17:07:23.932282 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0830757\n",
      "INFO:tensorflow:examples/sec: 2.65842\n",
      "I1117 17:07:23.932474 4595733952 tpu_estimator.py:2160] examples/sec: 2.65842\n",
      "INFO:tensorflow:global_step/sec: 0.0841374\n",
      "I1117 17:07:35.817615 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841374\n",
      "INFO:tensorflow:examples/sec: 2.6924\n",
      "I1117 17:07:35.817821 4595733952 tpu_estimator.py:2160] examples/sec: 2.6924\n",
      "INFO:tensorflow:global_step/sec: 0.0838705\n",
      "I1117 17:07:47.740754 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0838705\n",
      "INFO:tensorflow:examples/sec: 2.68386\n",
      "I1117 17:07:47.740983 4595733952 tpu_estimator.py:2160] examples/sec: 2.68386\n",
      "INFO:tensorflow:global_step/sec: 0.082489\n",
      "I1117 17:07:59.863579 4595733952 tpu_estimator.py:2159] global_step/sec: 0.082489\n",
      "INFO:tensorflow:examples/sec: 2.63965\n",
      "I1117 17:07:59.863762 4595733952 tpu_estimator.py:2160] examples/sec: 2.63965\n",
      "INFO:tensorflow:global_step/sec: 0.0860646\n",
      "I1117 17:08:11.482812 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0860646\n",
      "INFO:tensorflow:examples/sec: 2.75407\n",
      "I1117 17:08:11.483183 4595733952 tpu_estimator.py:2160] examples/sec: 2.75407\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0877091\n",
      "I1117 17:08:22.884132 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877091\n",
      "INFO:tensorflow:examples/sec: 2.80669\n",
      "I1117 17:08:22.884421 4595733952 tpu_estimator.py:2160] examples/sec: 2.80669\n",
      "INFO:tensorflow:global_step/sec: 0.0841207\n",
      "I1117 17:08:34.771945 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841207\n",
      "INFO:tensorflow:examples/sec: 2.69186\n",
      "I1117 17:08:34.772209 4595733952 tpu_estimator.py:2160] examples/sec: 2.69186\n",
      "INFO:tensorflow:global_step/sec: 0.0828358\n",
      "I1117 17:08:46.843832 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0828358\n",
      "INFO:tensorflow:examples/sec: 2.65074\n",
      "I1117 17:08:46.844222 4595733952 tpu_estimator.py:2160] examples/sec: 2.65074\n",
      "INFO:tensorflow:global_step/sec: 0.0865064\n",
      "I1117 17:08:58.403684 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865064\n",
      "INFO:tensorflow:examples/sec: 2.76821\n",
      "I1117 17:08:58.403896 4595733952 tpu_estimator.py:2160] examples/sec: 2.76821\n",
      "INFO:tensorflow:global_step/sec: 0.087353\n",
      "I1117 17:09:09.851452 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087353\n",
      "INFO:tensorflow:examples/sec: 2.7953\n",
      "I1117 17:09:09.851632 4595733952 tpu_estimator.py:2160] examples/sec: 2.7953\n",
      "INFO:tensorflow:global_step/sec: 0.0873209\n",
      "I1117 17:09:21.303494 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873209\n",
      "INFO:tensorflow:examples/sec: 2.79427\n",
      "I1117 17:09:21.303709 4595733952 tpu_estimator.py:2160] examples/sec: 2.79427\n",
      "INFO:tensorflow:global_step/sec: 0.0877473\n",
      "I1117 17:09:32.699843 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877473\n",
      "INFO:tensorflow:examples/sec: 2.80791\n",
      "I1117 17:09:32.700046 4595733952 tpu_estimator.py:2160] examples/sec: 2.80791\n",
      "INFO:tensorflow:global_step/sec: 0.0875348\n",
      "I1117 17:09:44.123868 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0875348\n",
      "INFO:tensorflow:examples/sec: 2.80111\n",
      "I1117 17:09:44.124067 4595733952 tpu_estimator.py:2160] examples/sec: 2.80111\n",
      "INFO:tensorflow:global_step/sec: 0.0887308\n",
      "I1117 17:09:55.393928 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887308\n",
      "INFO:tensorflow:examples/sec: 2.83939\n",
      "I1117 17:09:55.394138 4595733952 tpu_estimator.py:2160] examples/sec: 2.83939\n",
      "INFO:tensorflow:global_step/sec: 0.0863997\n",
      "I1117 17:10:06.968050 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863997\n",
      "INFO:tensorflow:examples/sec: 2.76479\n",
      "I1117 17:10:06.968272 4595733952 tpu_estimator.py:2160] examples/sec: 2.76479\n",
      "INFO:tensorflow:global_step/sec: 0.0867575\n",
      "I1117 17:10:18.494441 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867575\n",
      "INFO:tensorflow:examples/sec: 2.77624\n",
      "I1117 17:10:18.494673 4595733952 tpu_estimator.py:2160] examples/sec: 2.77624\n",
      "INFO:tensorflow:global_step/sec: 0.0880178\n",
      "I1117 17:10:29.855769 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880178\n",
      "INFO:tensorflow:examples/sec: 2.81657\n",
      "I1117 17:10:29.855974 4595733952 tpu_estimator.py:2160] examples/sec: 2.81657\n",
      "INFO:tensorflow:global_step/sec: 0.0878996\n",
      "I1117 17:10:41.232352 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878996\n",
      "INFO:tensorflow:examples/sec: 2.81279\n",
      "I1117 17:10:41.232528 4595733952 tpu_estimator.py:2160] examples/sec: 2.81279\n",
      "INFO:tensorflow:global_step/sec: 0.087196\n",
      "I1117 17:10:52.700805 4595733952 tpu_estimator.py:2159] global_step/sec: 0.087196\n",
      "INFO:tensorflow:examples/sec: 2.79027\n",
      "I1117 17:10:52.701020 4595733952 tpu_estimator.py:2160] examples/sec: 2.79027\n",
      "INFO:tensorflow:global_step/sec: 0.0863781\n",
      "I1117 17:11:04.277849 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863781\n",
      "INFO:tensorflow:examples/sec: 2.7641\n",
      "I1117 17:11:04.278064 4595733952 tpu_estimator.py:2160] examples/sec: 2.7641\n",
      "INFO:tensorflow:global_step/sec: 0.0889048\n",
      "I1117 17:11:15.525808 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889048\n",
      "INFO:tensorflow:examples/sec: 2.84495\n",
      "I1117 17:11:15.526021 4595733952 tpu_estimator.py:2160] examples/sec: 2.84495\n",
      "INFO:tensorflow:global_step/sec: 0.0872563\n",
      "I1117 17:11:26.986300 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0872563\n",
      "INFO:tensorflow:examples/sec: 2.7922\n",
      "I1117 17:11:26.986510 4595733952 tpu_estimator.py:2160] examples/sec: 2.7922\n",
      "INFO:tensorflow:global_step/sec: 0.0865543\n",
      "I1117 17:11:38.539731 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0865543\n",
      "INFO:tensorflow:examples/sec: 2.76974\n",
      "I1117 17:11:38.539953 4595733952 tpu_estimator.py:2160] examples/sec: 2.76974\n",
      "INFO:tensorflow:global_step/sec: 0.088599\n",
      "I1117 17:11:49.826536 4595733952 tpu_estimator.py:2159] global_step/sec: 0.088599\n",
      "INFO:tensorflow:examples/sec: 2.83517\n",
      "I1117 17:11:49.826750 4595733952 tpu_estimator.py:2160] examples/sec: 2.83517\n",
      "INFO:tensorflow:global_step/sec: 0.0873801\n",
      "I1117 17:12:01.270774 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0873801\n",
      "INFO:tensorflow:examples/sec: 2.79616\n",
      "I1117 17:12:01.270991 4595733952 tpu_estimator.py:2160] examples/sec: 2.79616\n",
      "INFO:tensorflow:global_step/sec: 0.0880114\n",
      "I1117 17:12:12.632934 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880114\n",
      "INFO:tensorflow:examples/sec: 2.81637\n",
      "I1117 17:12:12.633142 4595733952 tpu_estimator.py:2160] examples/sec: 2.81637\n",
      "INFO:tensorflow:global_step/sec: 0.0880125\n",
      "I1117 17:12:23.994971 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880125\n",
      "INFO:tensorflow:examples/sec: 2.8164\n",
      "I1117 17:12:23.995187 4595733952 tpu_estimator.py:2160] examples/sec: 2.8164\n",
      "INFO:tensorflow:global_step/sec: 0.0861106\n",
      "I1117 17:12:35.607973 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861106\n",
      "INFO:tensorflow:examples/sec: 2.75554\n",
      "I1117 17:12:35.608183 4595733952 tpu_estimator.py:2160] examples/sec: 2.75554\n",
      "INFO:tensorflow:global_step/sec: 0.0833317\n",
      "I1117 17:12:47.608163 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0833317\n",
      "INFO:tensorflow:examples/sec: 2.66662\n",
      "I1117 17:12:47.608371 4595733952 tpu_estimator.py:2160] examples/sec: 2.66662\n",
      "INFO:tensorflow:global_step/sec: 0.0784295\n",
      "I1117 17:13:00.358433 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0784295\n",
      "INFO:tensorflow:examples/sec: 2.50974\n",
      "I1117 17:13:00.358734 4595733952 tpu_estimator.py:2160] examples/sec: 2.50974\n",
      "INFO:tensorflow:global_step/sec: 0.0797961\n",
      "I1117 17:13:12.890383 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0797961\n",
      "INFO:tensorflow:examples/sec: 2.55348\n",
      "I1117 17:13:12.890566 4595733952 tpu_estimator.py:2160] examples/sec: 2.55348\n",
      "INFO:tensorflow:global_step/sec: 0.0867808\n",
      "I1117 17:13:24.413773 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0867808\n",
      "INFO:tensorflow:examples/sec: 2.77699\n",
      "I1117 17:13:24.414077 4595733952 tpu_estimator.py:2160] examples/sec: 2.77699\n",
      "INFO:tensorflow:global_step/sec: 0.0837814\n",
      "I1117 17:13:36.349497 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0837814\n",
      "INFO:tensorflow:examples/sec: 2.681\n",
      "I1117 17:13:36.349690 4595733952 tpu_estimator.py:2160] examples/sec: 2.681\n",
      "INFO:tensorflow:global_step/sec: 0.0878121\n",
      "I1117 17:13:47.737477 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878121\n",
      "INFO:tensorflow:examples/sec: 2.80999\n",
      "I1117 17:13:47.737694 4595733952 tpu_estimator.py:2160] examples/sec: 2.80999\n",
      "INFO:tensorflow:global_step/sec: 0.0877537\n",
      "I1117 17:13:59.132997 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877537\n",
      "INFO:tensorflow:examples/sec: 2.80812\n",
      "I1117 17:13:59.133211 4595733952 tpu_estimator.py:2160] examples/sec: 2.80812\n",
      "INFO:tensorflow:global_step/sec: 0.0874978\n",
      "I1117 17:14:10.561859 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0874978\n",
      "INFO:tensorflow:examples/sec: 2.79993\n",
      "I1117 17:14:10.562074 4595733952 tpu_estimator.py:2160] examples/sec: 2.79993\n",
      "INFO:tensorflow:global_step/sec: 0.0879905\n",
      "I1117 17:14:21.926784 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0879905\n",
      "INFO:tensorflow:examples/sec: 2.81569\n",
      "I1117 17:14:21.927176 4595733952 tpu_estimator.py:2160] examples/sec: 2.81569\n",
      "INFO:tensorflow:global_step/sec: 0.0882839\n",
      "I1117 17:14:33.253838 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882839\n",
      "INFO:tensorflow:examples/sec: 2.82509\n",
      "I1117 17:14:33.254076 4595733952 tpu_estimator.py:2160] examples/sec: 2.82509\n",
      "INFO:tensorflow:global_step/sec: 0.0889568\n",
      "I1117 17:14:44.495237 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889568\n",
      "INFO:tensorflow:examples/sec: 2.84662\n",
      "I1117 17:14:44.495455 4595733952 tpu_estimator.py:2160] examples/sec: 2.84662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0894958\n",
      "I1117 17:14:55.668934 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894958\n",
      "INFO:tensorflow:examples/sec: 2.86387\n",
      "I1117 17:14:55.669148 4595733952 tpu_estimator.py:2160] examples/sec: 2.86387\n",
      "INFO:tensorflow:global_step/sec: 0.0887798\n",
      "I1117 17:15:06.932751 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0887798\n",
      "INFO:tensorflow:examples/sec: 2.84095\n",
      "I1117 17:15:06.932965 4595733952 tpu_estimator.py:2160] examples/sec: 2.84095\n",
      "INFO:tensorflow:global_step/sec: 0.0889067\n",
      "I1117 17:15:18.180521 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0889067\n",
      "INFO:tensorflow:examples/sec: 2.84501\n",
      "I1117 17:15:18.180757 4595733952 tpu_estimator.py:2160] examples/sec: 2.84501\n",
      "INFO:tensorflow:global_step/sec: 0.0891215\n",
      "I1117 17:15:29.401098 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891215\n",
      "INFO:tensorflow:examples/sec: 2.85189\n",
      "I1117 17:15:29.401273 4595733952 tpu_estimator.py:2160] examples/sec: 2.85189\n",
      "INFO:tensorflow:global_step/sec: 0.0884869\n",
      "I1117 17:15:40.702244 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884869\n",
      "INFO:tensorflow:examples/sec: 2.83158\n",
      "I1117 17:15:40.703554 4595733952 tpu_estimator.py:2160] examples/sec: 2.83158\n",
      "INFO:tensorflow:global_step/sec: 0.0870575\n",
      "I1117 17:15:52.188894 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870575\n",
      "INFO:tensorflow:examples/sec: 2.78584\n",
      "I1117 17:15:52.189109 4595733952 tpu_estimator.py:2160] examples/sec: 2.78584\n",
      "INFO:tensorflow:global_step/sec: 0.0840516\n",
      "I1117 17:16:04.086368 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840516\n",
      "INFO:tensorflow:examples/sec: 2.68965\n",
      "I1117 17:16:04.086608 4595733952 tpu_estimator.py:2160] examples/sec: 2.68965\n",
      "INFO:tensorflow:global_step/sec: 0.0866419\n",
      "I1117 17:16:15.628118 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866419\n",
      "INFO:tensorflow:examples/sec: 2.77254\n",
      "I1117 17:16:15.628346 4595733952 tpu_estimator.py:2160] examples/sec: 2.77254\n",
      "INFO:tensorflow:global_step/sec: 0.0861168\n",
      "I1117 17:16:27.240257 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861168\n",
      "INFO:tensorflow:examples/sec: 2.75574\n",
      "I1117 17:16:27.240634 4595733952 tpu_estimator.py:2160] examples/sec: 2.75574\n",
      "INFO:tensorflow:global_step/sec: 0.0840789\n",
      "I1117 17:16:39.133862 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840789\n",
      "INFO:tensorflow:examples/sec: 2.69052\n",
      "I1117 17:16:39.134075 4595733952 tpu_estimator.py:2160] examples/sec: 2.69052\n",
      "INFO:tensorflow:global_step/sec: 0.0795719\n",
      "I1117 17:16:51.701052 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0795719\n",
      "INFO:tensorflow:examples/sec: 2.5463\n",
      "I1117 17:16:51.701350 4595733952 tpu_estimator.py:2160] examples/sec: 2.5463\n",
      "INFO:tensorflow:global_step/sec: 0.0841285\n",
      "I1117 17:17:03.587667 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841285\n",
      "INFO:tensorflow:examples/sec: 2.69211\n",
      "I1117 17:17:03.587885 4595733952 tpu_estimator.py:2160] examples/sec: 2.69211\n",
      "INFO:tensorflow:global_step/sec: 0.0847881\n",
      "I1117 17:17:15.381808 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0847881\n",
      "INFO:tensorflow:examples/sec: 2.71322\n",
      "I1117 17:17:15.382021 4595733952 tpu_estimator.py:2160] examples/sec: 2.71322\n",
      "INFO:tensorflow:global_step/sec: 0.0775051\n",
      "I1117 17:17:28.284196 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0775051\n",
      "INFO:tensorflow:examples/sec: 2.48016\n",
      "I1117 17:17:28.284461 4595733952 tpu_estimator.py:2160] examples/sec: 2.48016\n",
      "INFO:tensorflow:global_step/sec: 0.0858016\n",
      "I1117 17:17:39.938956 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0858016\n",
      "INFO:tensorflow:examples/sec: 2.74565\n",
      "I1117 17:17:39.939154 4595733952 tpu_estimator.py:2160] examples/sec: 2.74565\n",
      "INFO:tensorflow:global_step/sec: 0.0714818\n",
      "I1117 17:17:53.928528 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0714818\n",
      "INFO:tensorflow:examples/sec: 2.28742\n",
      "I1117 17:17:53.928731 4595733952 tpu_estimator.py:2160] examples/sec: 2.28742\n",
      "INFO:tensorflow:global_step/sec: 0.0699827\n",
      "I1117 17:18:08.217772 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0699827\n",
      "INFO:tensorflow:examples/sec: 2.23945\n",
      "I1117 17:18:08.217969 4595733952 tpu_estimator.py:2160] examples/sec: 2.23945\n",
      "INFO:tensorflow:global_step/sec: 0.0775708\n",
      "I1117 17:18:21.109250 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0775708\n",
      "INFO:tensorflow:examples/sec: 2.48227\n",
      "I1117 17:18:21.109534 4595733952 tpu_estimator.py:2160] examples/sec: 2.48227\n",
      "INFO:tensorflow:global_step/sec: 0.0793469\n",
      "I1117 17:18:33.712133 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0793469\n",
      "INFO:tensorflow:examples/sec: 2.5391\n",
      "I1117 17:18:33.712352 4595733952 tpu_estimator.py:2160] examples/sec: 2.5391\n",
      "INFO:tensorflow:global_step/sec: 0.0787829\n",
      "I1117 17:18:46.405232 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0787829\n",
      "INFO:tensorflow:examples/sec: 2.52105\n",
      "I1117 17:18:46.405449 4595733952 tpu_estimator.py:2160] examples/sec: 2.52105\n",
      "INFO:tensorflow:global_step/sec: 0.0841575\n",
      "I1117 17:18:58.287716 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0841575\n",
      "INFO:tensorflow:examples/sec: 2.69304\n",
      "I1117 17:18:58.287925 4595733952 tpu_estimator.py:2160] examples/sec: 2.69304\n",
      "INFO:tensorflow:global_step/sec: 0.0878009\n",
      "I1117 17:19:09.677118 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878009\n",
      "INFO:tensorflow:examples/sec: 2.80963\n",
      "I1117 17:19:09.677328 4595733952 tpu_estimator.py:2160] examples/sec: 2.80963\n",
      "INFO:tensorflow:global_step/sec: 0.0871186\n",
      "I1117 17:19:21.155735 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871186\n",
      "INFO:tensorflow:examples/sec: 2.78779\n",
      "I1117 17:19:21.155952 4595733952 tpu_estimator.py:2160] examples/sec: 2.78779\n",
      "INFO:tensorflow:global_step/sec: 0.0815372\n",
      "I1117 17:19:33.420084 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0815372\n",
      "INFO:tensorflow:examples/sec: 2.60919\n",
      "I1117 17:19:33.420306 4595733952 tpu_estimator.py:2160] examples/sec: 2.60919\n",
      "INFO:tensorflow:global_step/sec: 0.0747999\n",
      "I1117 17:19:46.789067 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0747999\n",
      "INFO:tensorflow:examples/sec: 2.3936\n",
      "I1117 17:19:46.789330 4595733952 tpu_estimator.py:2160] examples/sec: 2.3936\n",
      "INFO:tensorflow:global_step/sec: 0.0762605\n",
      "I1117 17:19:59.902042 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0762605\n",
      "INFO:tensorflow:examples/sec: 2.44033\n",
      "I1117 17:19:59.902260 4595733952 tpu_estimator.py:2160] examples/sec: 2.44033\n",
      "INFO:tensorflow:global_step/sec: 0.080133\n",
      "I1117 17:20:12.381299 4595733952 tpu_estimator.py:2159] global_step/sec: 0.080133\n",
      "INFO:tensorflow:examples/sec: 2.56426\n",
      "I1117 17:20:12.381518 4595733952 tpu_estimator.py:2160] examples/sec: 2.56426\n",
      "INFO:tensorflow:global_step/sec: 0.082169\n",
      "I1117 17:20:24.551408 4595733952 tpu_estimator.py:2159] global_step/sec: 0.082169\n",
      "INFO:tensorflow:examples/sec: 2.62941\n",
      "I1117 17:20:24.551793 4595733952 tpu_estimator.py:2160] examples/sec: 2.62941\n",
      "INFO:tensorflow:global_step/sec: 0.0822591\n",
      "I1117 17:20:36.708060 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0822591\n",
      "INFO:tensorflow:examples/sec: 2.63229\n",
      "I1117 17:20:36.708284 4595733952 tpu_estimator.py:2160] examples/sec: 2.63229\n",
      "INFO:tensorflow:global_step/sec: 0.0857801\n",
      "I1117 17:20:48.365786 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0857801\n",
      "INFO:tensorflow:examples/sec: 2.74496\n",
      "I1117 17:20:48.366005 4595733952 tpu_estimator.py:2160] examples/sec: 2.74496\n",
      "INFO:tensorflow:global_step/sec: 0.0834314\n",
      "I1117 17:21:00.351634 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0834314\n",
      "INFO:tensorflow:examples/sec: 2.66981\n",
      "I1117 17:21:00.351834 4595733952 tpu_estimator.py:2160] examples/sec: 2.66981\n",
      "INFO:tensorflow:global_step/sec: 0.0853201\n",
      "I1117 17:21:12.072222 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0853201\n",
      "INFO:tensorflow:examples/sec: 2.73024\n",
      "I1117 17:21:12.072571 4595733952 tpu_estimator.py:2160] examples/sec: 2.73024\n",
      "INFO:tensorflow:global_step/sec: 0.0877168\n",
      "I1117 17:21:23.472560 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877168\n",
      "INFO:tensorflow:examples/sec: 2.80694\n",
      "I1117 17:21:23.472773 4595733952 tpu_estimator.py:2160] examples/sec: 2.80694\n",
      "INFO:tensorflow:global_step/sec: 0.082028\n",
      "I1117 17:21:35.663577 4595733952 tpu_estimator.py:2159] global_step/sec: 0.082028\n",
      "INFO:tensorflow:examples/sec: 2.6249\n",
      "I1117 17:21:35.663799 4595733952 tpu_estimator.py:2160] examples/sec: 2.6249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0800126\n",
      "I1117 17:21:48.161543 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0800126\n",
      "INFO:tensorflow:examples/sec: 2.5604\n",
      "I1117 17:21:48.161756 4595733952 tpu_estimator.py:2160] examples/sec: 2.5604\n",
      "INFO:tensorflow:global_step/sec: 0.0862958\n",
      "I1117 17:21:59.749592 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862958\n",
      "INFO:tensorflow:examples/sec: 2.76147\n",
      "I1117 17:21:59.749858 4595733952 tpu_estimator.py:2160] examples/sec: 2.76147\n",
      "INFO:tensorflow:global_step/sec: 0.089504\n",
      "I1117 17:22:10.922277 4595733952 tpu_estimator.py:2159] global_step/sec: 0.089504\n",
      "INFO:tensorflow:examples/sec: 2.86413\n",
      "I1117 17:22:10.922511 4595733952 tpu_estimator.py:2160] examples/sec: 2.86413\n",
      "INFO:tensorflow:global_step/sec: 0.0892088\n",
      "I1117 17:22:22.131926 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892088\n",
      "INFO:tensorflow:examples/sec: 2.85468\n",
      "I1117 17:22:22.132138 4595733952 tpu_estimator.py:2160] examples/sec: 2.85468\n",
      "INFO:tensorflow:global_step/sec: 0.0846425\n",
      "I1117 17:22:33.946319 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0846425\n",
      "INFO:tensorflow:examples/sec: 2.70856\n",
      "I1117 17:22:33.946519 4595733952 tpu_estimator.py:2160] examples/sec: 2.70856\n",
      "INFO:tensorflow:global_step/sec: 0.0799455\n",
      "I1117 17:22:46.454846 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0799455\n",
      "INFO:tensorflow:examples/sec: 2.55826\n",
      "I1117 17:22:46.455056 4595733952 tpu_estimator.py:2160] examples/sec: 2.55826\n",
      "INFO:tensorflow:global_step/sec: 0.079224\n",
      "I1117 17:22:59.077243 4595733952 tpu_estimator.py:2159] global_step/sec: 0.079224\n",
      "INFO:tensorflow:examples/sec: 2.53517\n",
      "I1117 17:22:59.077611 4595733952 tpu_estimator.py:2160] examples/sec: 2.53517\n",
      "INFO:tensorflow:global_step/sec: 0.0787289\n",
      "I1117 17:23:11.779098 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0787289\n",
      "INFO:tensorflow:examples/sec: 2.51932\n",
      "I1117 17:23:11.779312 4595733952 tpu_estimator.py:2160] examples/sec: 2.51932\n",
      "INFO:tensorflow:global_step/sec: 0.0824562\n",
      "I1117 17:23:23.906746 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0824562\n",
      "INFO:tensorflow:examples/sec: 2.6386\n",
      "I1117 17:23:23.907145 4595733952 tpu_estimator.py:2160] examples/sec: 2.6386\n",
      "INFO:tensorflow:global_step/sec: 0.0845652\n",
      "I1117 17:23:35.731949 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0845652\n",
      "INFO:tensorflow:examples/sec: 2.70609\n",
      "I1117 17:23:35.732163 4595733952 tpu_estimator.py:2160] examples/sec: 2.70609\n",
      "INFO:tensorflow:global_step/sec: 0.0863538\n",
      "I1117 17:23:47.312198 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0863538\n",
      "INFO:tensorflow:examples/sec: 2.76332\n",
      "I1117 17:23:47.312407 4595733952 tpu_estimator.py:2160] examples/sec: 2.76332\n",
      "INFO:tensorflow:global_step/sec: 0.0861889\n",
      "I1117 17:23:58.914622 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0861889\n",
      "INFO:tensorflow:examples/sec: 2.75804\n",
      "I1117 17:23:58.914828 4595733952 tpu_estimator.py:2160] examples/sec: 2.75804\n",
      "INFO:tensorflow:global_step/sec: 0.0800456\n",
      "I1117 17:24:11.407482 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0800456\n",
      "INFO:tensorflow:examples/sec: 2.56146\n",
      "I1117 17:24:11.407676 4595733952 tpu_estimator.py:2160] examples/sec: 2.56146\n",
      "INFO:tensorflow:global_step/sec: 0.0730067\n",
      "I1117 17:24:25.104858 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0730067\n",
      "INFO:tensorflow:examples/sec: 2.33621\n",
      "I1117 17:24:25.105097 4595733952 tpu_estimator.py:2160] examples/sec: 2.33621\n",
      "INFO:tensorflow:global_step/sec: 0.077942\n",
      "I1117 17:24:37.934939 4595733952 tpu_estimator.py:2159] global_step/sec: 0.077942\n",
      "INFO:tensorflow:examples/sec: 2.49414\n",
      "I1117 17:24:37.935348 4595733952 tpu_estimator.py:2160] examples/sec: 2.49414\n",
      "INFO:tensorflow:global_step/sec: 0.0851909\n",
      "I1117 17:24:49.673286 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0851909\n",
      "INFO:tensorflow:examples/sec: 2.72611\n",
      "I1117 17:24:49.673498 4595733952 tpu_estimator.py:2160] examples/sec: 2.72611\n",
      "INFO:tensorflow:global_step/sec: 0.0866472\n",
      "I1117 17:25:01.214342 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0866472\n",
      "INFO:tensorflow:examples/sec: 2.77271\n",
      "I1117 17:25:01.214717 4595733952 tpu_estimator.py:2160] examples/sec: 2.77271\n",
      "INFO:tensorflow:global_step/sec: 0.0876444\n",
      "I1117 17:25:12.624076 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876444\n",
      "INFO:tensorflow:examples/sec: 2.80462\n",
      "I1117 17:25:12.624325 4595733952 tpu_estimator.py:2160] examples/sec: 2.80462\n",
      "INFO:tensorflow:global_step/sec: 0.0877835\n",
      "I1117 17:25:24.015699 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877835\n",
      "INFO:tensorflow:examples/sec: 2.80907\n",
      "I1117 17:25:24.015895 4595733952 tpu_estimator.py:2160] examples/sec: 2.80907\n",
      "INFO:tensorflow:global_step/sec: 0.0870117\n",
      "I1117 17:25:35.508431 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870117\n",
      "INFO:tensorflow:examples/sec: 2.78438\n",
      "I1117 17:25:35.508645 4595733952 tpu_estimator.py:2160] examples/sec: 2.78438\n",
      "INFO:tensorflow:global_step/sec: 0.0882713\n",
      "I1117 17:25:46.837146 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882713\n",
      "INFO:tensorflow:examples/sec: 2.82468\n",
      "I1117 17:25:46.837363 4595733952 tpu_estimator.py:2160] examples/sec: 2.82468\n",
      "INFO:tensorflow:global_step/sec: 0.0877852\n",
      "I1117 17:25:58.228579 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877852\n",
      "INFO:tensorflow:examples/sec: 2.80913\n",
      "I1117 17:25:58.228796 4595733952 tpu_estimator.py:2160] examples/sec: 2.80913\n",
      "INFO:tensorflow:global_step/sec: 0.0862561\n",
      "I1117 17:26:09.822011 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0862561\n",
      "INFO:tensorflow:examples/sec: 2.76019\n",
      "I1117 17:26:09.822290 4595733952 tpu_estimator.py:2160] examples/sec: 2.76019\n",
      "INFO:tensorflow:global_step/sec: 0.0884088\n",
      "I1117 17:26:21.133062 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0884088\n",
      "INFO:tensorflow:examples/sec: 2.82908\n",
      "I1117 17:26:21.133276 4595733952 tpu_estimator.py:2160] examples/sec: 2.82908\n",
      "INFO:tensorflow:global_step/sec: 0.0895608\n",
      "I1117 17:26:32.298639 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0895608\n",
      "INFO:tensorflow:examples/sec: 2.86595\n",
      "I1117 17:26:32.298844 4595733952 tpu_estimator.py:2160] examples/sec: 2.86595\n",
      "INFO:tensorflow:global_step/sec: 0.0892736\n",
      "I1117 17:26:43.500169 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892736\n",
      "INFO:tensorflow:examples/sec: 2.85676\n",
      "I1117 17:26:43.500378 4595733952 tpu_estimator.py:2160] examples/sec: 2.85676\n",
      "INFO:tensorflow:global_step/sec: 0.0844318\n",
      "I1117 17:26:55.344056 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0844318\n",
      "INFO:tensorflow:examples/sec: 2.70182\n",
      "I1117 17:26:55.344264 4595733952 tpu_estimator.py:2160] examples/sec: 2.70182\n",
      "INFO:tensorflow:global_step/sec: 0.0851144\n",
      "I1117 17:27:07.092919 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0851144\n",
      "INFO:tensorflow:examples/sec: 2.72366\n",
      "I1117 17:27:07.093111 4595733952 tpu_estimator.py:2160] examples/sec: 2.72366\n",
      "INFO:tensorflow:global_step/sec: 0.0869635\n",
      "I1117 17:27:18.592025 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0869635\n",
      "INFO:tensorflow:examples/sec: 2.78283\n",
      "I1117 17:27:18.592364 4595733952 tpu_estimator.py:2160] examples/sec: 2.78283\n",
      "INFO:tensorflow:global_step/sec: 0.0846748\n",
      "I1117 17:27:30.401906 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0846748\n",
      "INFO:tensorflow:examples/sec: 2.70959\n",
      "I1117 17:27:30.402119 4595733952 tpu_estimator.py:2160] examples/sec: 2.70959\n",
      "INFO:tensorflow:global_step/sec: 0.0843942\n",
      "I1117 17:27:42.251070 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843942\n",
      "INFO:tensorflow:examples/sec: 2.70061\n",
      "I1117 17:27:42.251327 4595733952 tpu_estimator.py:2160] examples/sec: 2.70061\n",
      "INFO:tensorflow:global_step/sec: 0.0864611\n",
      "I1117 17:27:53.816962 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0864611\n",
      "INFO:tensorflow:examples/sec: 2.76675\n",
      "I1117 17:27:53.817186 4595733952 tpu_estimator.py:2160] examples/sec: 2.76675\n",
      "INFO:tensorflow:global_step/sec: 0.0877299\n",
      "I1117 17:28:05.215580 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0877299\n",
      "INFO:tensorflow:examples/sec: 2.80736\n",
      "I1117 17:28:05.215795 4595733952 tpu_estimator.py:2160] examples/sec: 2.80736\n",
      "INFO:tensorflow:global_step/sec: 0.0843327\n",
      "I1117 17:28:17.073371 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0843327\n",
      "INFO:tensorflow:examples/sec: 2.69865\n",
      "I1117 17:28:17.073584 4595733952 tpu_estimator.py:2160] examples/sec: 2.69865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0851582\n",
      "I1117 17:28:28.816221 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0851582\n",
      "INFO:tensorflow:examples/sec: 2.72506\n",
      "I1117 17:28:28.816434 4595733952 tpu_estimator.py:2160] examples/sec: 2.72506\n",
      "INFO:tensorflow:global_step/sec: 0.0797202\n",
      "I1117 17:28:41.360084 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0797202\n",
      "INFO:tensorflow:examples/sec: 2.55104\n",
      "I1117 17:28:41.360259 4595733952 tpu_estimator.py:2160] examples/sec: 2.55104\n",
      "INFO:tensorflow:global_step/sec: 0.0840064\n",
      "I1117 17:28:53.263963 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0840064\n",
      "INFO:tensorflow:examples/sec: 2.6882\n",
      "I1117 17:28:53.264176 4595733952 tpu_estimator.py:2160] examples/sec: 2.6882\n",
      "INFO:tensorflow:global_step/sec: 0.0842924\n",
      "I1117 17:29:05.127421 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0842924\n",
      "INFO:tensorflow:examples/sec: 2.69736\n",
      "I1117 17:29:05.127635 4595733952 tpu_estimator.py:2160] examples/sec: 2.69736\n",
      "INFO:tensorflow:global_step/sec: 0.0829806\n",
      "I1117 17:29:17.178437 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0829806\n",
      "INFO:tensorflow:examples/sec: 2.65538\n",
      "I1117 17:29:17.178658 4595733952 tpu_estimator.py:2160] examples/sec: 2.65538\n",
      "INFO:tensorflow:global_step/sec: 0.0858926\n",
      "I1117 17:29:28.820869 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0858926\n",
      "INFO:tensorflow:examples/sec: 2.74856\n",
      "I1117 17:29:28.821078 4595733952 tpu_estimator.py:2160] examples/sec: 2.74856\n",
      "INFO:tensorflow:global_step/sec: 0.0892173\n",
      "I1117 17:29:40.029453 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0892173\n",
      "INFO:tensorflow:examples/sec: 2.85495\n",
      "I1117 17:29:40.029672 4595733952 tpu_estimator.py:2160] examples/sec: 2.85495\n",
      "INFO:tensorflow:global_step/sec: 0.0847817\n",
      "I1117 17:29:51.824450 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0847817\n",
      "INFO:tensorflow:examples/sec: 2.71301\n",
      "I1117 17:29:51.824668 4595733952 tpu_estimator.py:2160] examples/sec: 2.71301\n",
      "INFO:tensorflow:global_step/sec: 0.0859756\n",
      "I1117 17:30:03.455657 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0859756\n",
      "INFO:tensorflow:examples/sec: 2.75122\n",
      "I1117 17:30:03.455873 4595733952 tpu_estimator.py:2160] examples/sec: 2.75122\n",
      "INFO:tensorflow:global_step/sec: 0.0878712\n",
      "I1117 17:30:14.835958 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0878712\n",
      "INFO:tensorflow:examples/sec: 2.81188\n",
      "I1117 17:30:14.836179 4595733952 tpu_estimator.py:2160] examples/sec: 2.81188\n",
      "INFO:tensorflow:global_step/sec: 0.0883784\n",
      "I1117 17:30:26.150926 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883784\n",
      "INFO:tensorflow:examples/sec: 2.82811\n",
      "I1117 17:30:26.151141 4595733952 tpu_estimator.py:2160] examples/sec: 2.82811\n",
      "INFO:tensorflow:global_step/sec: 0.0896086\n",
      "I1117 17:30:37.310579 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896086\n",
      "INFO:tensorflow:examples/sec: 2.86747\n",
      "I1117 17:30:37.310786 4595733952 tpu_estimator.py:2160] examples/sec: 2.86747\n",
      "INFO:tensorflow:global_step/sec: 0.0891603\n",
      "I1117 17:30:48.526318 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0891603\n",
      "INFO:tensorflow:examples/sec: 2.85313\n",
      "I1117 17:30:48.526529 4595733952 tpu_estimator.py:2160] examples/sec: 2.85313\n",
      "INFO:tensorflow:global_step/sec: 0.0885336\n",
      "I1117 17:30:59.821484 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0885336\n",
      "INFO:tensorflow:examples/sec: 2.83308\n",
      "I1117 17:30:59.821872 4595733952 tpu_estimator.py:2160] examples/sec: 2.83308\n",
      "INFO:tensorflow:global_step/sec: 0.0871327\n",
      "I1117 17:31:11.298184 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0871327\n",
      "INFO:tensorflow:examples/sec: 2.78825\n",
      "I1117 17:31:11.298416 4595733952 tpu_estimator.py:2160] examples/sec: 2.78825\n",
      "INFO:tensorflow:global_step/sec: 0.0858409\n",
      "I1117 17:31:22.947674 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0858409\n",
      "INFO:tensorflow:examples/sec: 2.74691\n",
      "I1117 17:31:22.947901 4595733952 tpu_estimator.py:2160] examples/sec: 2.74691\n",
      "INFO:tensorflow:global_step/sec: 0.085828\n",
      "I1117 17:31:34.598867 4595733952 tpu_estimator.py:2159] global_step/sec: 0.085828\n",
      "INFO:tensorflow:examples/sec: 2.7465\n",
      "I1117 17:31:34.599079 4595733952 tpu_estimator.py:2160] examples/sec: 2.7465\n",
      "INFO:tensorflow:global_step/sec: 0.0834327\n",
      "I1117 17:31:46.584589 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0834327\n",
      "INFO:tensorflow:examples/sec: 2.66985\n",
      "I1117 17:31:46.584816 4595733952 tpu_estimator.py:2160] examples/sec: 2.66985\n",
      "INFO:tensorflow:global_step/sec: 0.0776477\n",
      "I1117 17:31:59.463250 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0776477\n",
      "INFO:tensorflow:examples/sec: 2.48473\n",
      "I1117 17:31:59.463429 4595733952 tpu_estimator.py:2160] examples/sec: 2.48473\n",
      "INFO:tensorflow:global_step/sec: 0.0882976\n",
      "I1117 17:32:10.788594 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882976\n",
      "INFO:tensorflow:examples/sec: 2.82552\n",
      "I1117 17:32:10.788807 4595733952 tpu_estimator.py:2160] examples/sec: 2.82552\n",
      "INFO:tensorflow:global_step/sec: 0.0888844\n",
      "I1117 17:32:22.039166 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888844\n",
      "INFO:tensorflow:examples/sec: 2.8443\n",
      "I1117 17:32:22.039381 4595733952 tpu_estimator.py:2160] examples/sec: 2.8443\n",
      "INFO:tensorflow:global_step/sec: 0.0882521\n",
      "I1117 17:32:33.370319 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0882521\n",
      "INFO:tensorflow:examples/sec: 2.82407\n",
      "I1117 17:32:33.370663 4595733952 tpu_estimator.py:2160] examples/sec: 2.82407\n",
      "INFO:tensorflow:global_step/sec: 0.0829424\n",
      "I1117 17:32:45.426941 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0829424\n",
      "INFO:tensorflow:examples/sec: 2.65416\n",
      "I1117 17:32:45.427270 4595733952 tpu_estimator.py:2160] examples/sec: 2.65416\n",
      "INFO:tensorflow:global_step/sec: 0.0795821\n",
      "I1117 17:32:57.992585 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0795821\n",
      "INFO:tensorflow:examples/sec: 2.54663\n",
      "I1117 17:32:57.992990 4595733952 tpu_estimator.py:2160] examples/sec: 2.54663\n",
      "INFO:tensorflow:global_step/sec: 0.0807869\n",
      "I1117 17:33:10.370784 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0807869\n",
      "INFO:tensorflow:examples/sec: 2.58518\n",
      "I1117 17:33:10.371003 4595733952 tpu_estimator.py:2160] examples/sec: 2.58518\n",
      "INFO:tensorflow:global_step/sec: 0.0850264\n",
      "I1117 17:33:22.131837 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0850264\n",
      "INFO:tensorflow:examples/sec: 2.72085\n",
      "I1117 17:33:22.132080 4595733952 tpu_estimator.py:2160] examples/sec: 2.72085\n",
      "INFO:tensorflow:global_step/sec: 0.0888511\n",
      "I1117 17:33:33.386626 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888511\n",
      "INFO:tensorflow:examples/sec: 2.84323\n",
      "I1117 17:33:33.386841 4595733952 tpu_estimator.py:2160] examples/sec: 2.84323\n",
      "INFO:tensorflow:global_step/sec: 0.0888586\n",
      "I1117 17:33:44.640465 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0888586\n",
      "INFO:tensorflow:examples/sec: 2.84348\n",
      "I1117 17:33:44.640676 4595733952 tpu_estimator.py:2160] examples/sec: 2.84348\n",
      "INFO:tensorflow:global_step/sec: 0.0880407\n",
      "I1117 17:33:55.998830 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0880407\n",
      "INFO:tensorflow:examples/sec: 2.8173\n",
      "I1117 17:33:55.999032 4595733952 tpu_estimator.py:2160] examples/sec: 2.8173\n",
      "INFO:tensorflow:global_step/sec: 0.0870649\n",
      "I1117 17:34:07.484543 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0870649\n",
      "INFO:tensorflow:examples/sec: 2.78608\n",
      "I1117 17:34:07.484764 4595733952 tpu_estimator.py:2160] examples/sec: 2.78608\n",
      "INFO:tensorflow:global_step/sec: 0.0876999\n",
      "I1117 17:34:18.887075 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0876999\n",
      "INFO:tensorflow:examples/sec: 2.8064\n",
      "I1117 17:34:18.887306 4595733952 tpu_estimator.py:2160] examples/sec: 2.8064\n",
      "INFO:tensorflow:global_step/sec: 0.0883131\n",
      "I1117 17:34:30.210416 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0883131\n",
      "INFO:tensorflow:examples/sec: 2.82602\n",
      "I1117 17:34:30.210657 4595733952 tpu_estimator.py:2160] examples/sec: 2.82602\n",
      "INFO:tensorflow:global_step/sec: 0.0851565\n",
      "I1117 17:34:41.953514 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0851565\n",
      "INFO:tensorflow:examples/sec: 2.72501\n",
      "I1117 17:34:41.953732 4595733952 tpu_estimator.py:2160] examples/sec: 2.72501\n",
      "INFO:tensorflow:global_step/sec: 0.0900848\n",
      "I1117 17:34:53.054141 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0900848\n",
      "INFO:tensorflow:examples/sec: 2.88271\n",
      "I1117 17:34:53.054362 4595733952 tpu_estimator.py:2160] examples/sec: 2.88271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0897166\n",
      "I1117 17:35:04.200340 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0897166\n",
      "INFO:tensorflow:examples/sec: 2.87093\n",
      "I1117 17:35:04.200558 4595733952 tpu_estimator.py:2160] examples/sec: 2.87093\n",
      "INFO:tensorflow:global_step/sec: 0.0894598\n",
      "I1117 17:35:15.378535 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0894598\n",
      "INFO:tensorflow:examples/sec: 2.86271\n",
      "I1117 17:35:15.378731 4595733952 tpu_estimator.py:2160] examples/sec: 2.86271\n",
      "INFO:tensorflow:global_step/sec: 0.0896464\n",
      "I1117 17:35:26.533473 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0896464\n",
      "INFO:tensorflow:examples/sec: 2.86869\n",
      "I1117 17:35:26.533687 4595733952 tpu_estimator.py:2160] examples/sec: 2.86869\n",
      "INFO:tensorflow:global_step/sec: 0.0902607\n",
      "I1117 17:35:37.612488 4595733952 tpu_estimator.py:2159] global_step/sec: 0.0902607\n",
      "INFO:tensorflow:examples/sec: 2.88834\n",
      "I1117 17:35:37.612705 4595733952 tpu_estimator.py:2160] examples/sec: 2.88834\n",
      "INFO:tensorflow:Saving checkpoints for 1856 into ./bert_output/model.ckpt.\n",
      "I1117 17:35:37.617162 4595733952 basic_session_run_hooks.py:606] Saving checkpoints for 1856 into ./bert_output/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21222198.\n",
      "I1117 17:35:42.339818 4595733952 estimator.py:368] Loss for final step: 0.21222198.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "I1117 17:35:42.352924 4595733952 error_handling.py:96] training_loop marked as finished\n",
      "INFO:tensorflow:Writing example 0 of 200\n",
      "I1117 17:35:42.439298 4595733952 run_classifier.py:487] Writing example 0 of 200\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:35:42.444531 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-0\n",
      "I1117 17:35:42.444620 4595733952 run_classifier.py:462] guid: dev-0\n",
      "INFO:tensorflow:tokens: [CLS] there are a lot of panda ##s [SEP]\n",
      "I1117 17:35:42.444704 4595733952 run_classifier.py:464] tokens: [CLS] there are a lot of panda ##s [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2045 2024 1037 2843 1997 25462 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.444797 4595733952 run_classifier.py:465] input_ids: 101 2045 2024 1037 2843 1997 25462 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.444880 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.445378 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 17:35:42.445510 4595733952 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:35:42.447510 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-1\n",
      "I1117 17:35:42.447582 4595733952 run_classifier.py:462] guid: dev-1\n",
      "INFO:tensorflow:tokens: [CLS] birds fly using their wings . [SEP]\n",
      "I1117 17:35:42.447635 4595733952 run_classifier.py:464] tokens: [CLS] birds fly using their wings . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 5055 4875 2478 2037 4777 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.447707 4595733952 run_classifier.py:465] input_ids: 101 5055 4875 2478 2037 4777 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.447781 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.447920 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 17:35:42.447984 4595733952 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:35:42.449280 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-2\n",
      "I1117 17:35:42.449347 4595733952 run_classifier.py:462] guid: dev-2\n",
      "INFO:tensorflow:tokens: [CLS] cats give birth to kitten ##s . [SEP]\n",
      "I1117 17:35:42.449398 4595733952 run_classifier.py:464] tokens: [CLS] cats give birth to kitten ##s . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 8870 2507 4182 2000 18401 2015 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.449467 4595733952 run_classifier.py:465] input_ids: 101 8870 2507 4182 2000 18401 2015 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.449537 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.449610 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 17:35:42.449764 4595733952 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:35:42.450982 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-3\n",
      "I1117 17:35:42.451067 4595733952 run_classifier.py:462] guid: dev-3\n",
      "INFO:tensorflow:tokens: [CLS] going for a hair ##cut is for eating [SEP]\n",
      "I1117 17:35:42.451123 4595733952 run_classifier.py:464] tokens: [CLS] going for a hair ##cut is for eating [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2183 2005 1037 2606 12690 2003 2005 5983 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.451195 4595733952 run_classifier.py:465] input_ids: 101 2183 2005 1037 2606 12690 2003 2005 5983 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.451279 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.451409 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 17:35:42.451477 4595733952 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:35:42.451975 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-4\n",
      "I1117 17:35:42.452043 4595733952 run_classifier.py:462] guid: dev-4\n",
      "INFO:tensorflow:tokens: [CLS] i saw clouds floating in the sea . [SEP]\n",
      "I1117 17:35:42.452096 4595733952 run_classifier.py:464] tokens: [CLS] i saw clouds floating in the sea . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1045 2387 8044 8274 1999 1996 2712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.452166 4595733952 run_classifier.py:465] input_ids: 101 1045 2387 8044 8274 1999 1996 2712 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.452238 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:35:42.452306 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 17:35:42.452388 4595733952 run_classifier.py:468] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running evaluation *****\n",
      "I1117 17:35:42.530102 4595733952 run_classifier.py:898] ***** Running evaluation *****\n",
      "INFO:tensorflow:  Num examples = 200 (200 actual, 0 padding)\n",
      "I1117 17:35:42.530316 4595733952 run_classifier.py:901]   Num examples = 200 (200 actual, 0 padding)\n",
      "INFO:tensorflow:  Batch size = 8\n",
      "I1117 17:35:42.530480 4595733952 run_classifier.py:902]   Batch size = 8\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I1117 17:35:42.646075 4595733952 estimator.py:1145] Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "I1117 17:35:42.646301 4595733952 tpu_estimator.py:2965] Running eval on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I1117 17:35:42.646595 4595733952 run_classifier.py:627] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
      "I1117 17:35:42.646712 4595733952 run_classifier.py:629]   name = input_ids, shape = (?, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
      "I1117 17:35:42.646785 4595733952 run_classifier.py:629]   name = input_mask, shape = (?, 128)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
      "I1117 17:35:42.646851 4595733952 run_classifier.py:629]   name = is_real_example, shape = (?,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
      "I1117 17:35:42.646913 4595733952 run_classifier.py:629]   name = label_ids, shape = (?,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
      "I1117 17:35:42.646976 4595733952 run_classifier.py:629]   name = segment_ids, shape = (?, 128)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:42.905917 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:42.973578 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffe80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4c1278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4c1278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.037766 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4c1278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4c1278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.120237 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffcf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffcf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.195858 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.264877 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.346143 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.410743 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.475422 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a429b1898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ff7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ff7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.558571 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ff7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ff7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.633814 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4295ac50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42953ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42953ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.703156 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42953ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42953ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.781672 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.846552 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.912621 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a428ffc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42b90128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42b90128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:43.992907 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42b90128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42b90128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:44.068264 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297ff60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297ff60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:44.138185 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297ff60>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297ff60>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:44.217907 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:44.283376 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.080668 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42c719e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42cdab38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42cdab38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.160788 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42cdab38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42cdab38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.237282 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a43389f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a43389f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.306250 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a43389f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a43389f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.384572 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.449182 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.513637 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432ff940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.592767 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb380a2978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb380a2978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.667860 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb380a2978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0xb380a2978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.737359 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.816097 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.880686 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:45.945655 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d578ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a447157b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a447157b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.027312 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a447157b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a447157b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.102125 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fc18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fc18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.171790 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.251997 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.316068 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.380376 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42496160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4fe898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4fe898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.460093 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4fe898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d4fe898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.536447 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.606220 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297fd68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.686019 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.750128 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.814188 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44715eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.892933 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:46.967627 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.037590 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.116677 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.180618 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.245279 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42774860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42774860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.325946 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42774860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42774860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.402338 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4344d358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.550959 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4297f4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42427a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42427a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.630110 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42427a58>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42427a58>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.695552 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.759836 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4348e940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.839071 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd42e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.913846 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44699e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44699e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:47.984215 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44699e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44699e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b8320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b8320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.063759 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b8320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b8320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.128523 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.193862 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a42459eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d5decf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d5decf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.272378 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d5decf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d5decf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.348177 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.419414 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.500941 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.564826 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.629587 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2e0c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.711173 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.786345 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a413983c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.856664 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4273b898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a452d32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a452d32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:35:48.972525 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a452d32b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a452d32b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "I1117 17:35:49.456068 4595733952 run_classifier.py:663] **** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456185 4595733952 run_classifier.py:669]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456265 4595733952 run_classifier.py:669]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456333 4595733952 run_classifier.py:669]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456401 4595733952 run_classifier.py:669]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456462 4595733952 run_classifier.py:669]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456517 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456577 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456632 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456688 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456743 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456803 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456858 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456914 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.456967 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457021 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457076 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457133 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457187 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457243 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457297 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457351 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457406 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457463 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457518 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457576 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457629 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457689 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457743 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457799 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457854 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457909 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.457962 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458020 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458074 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458132 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458188 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458241 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458296 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458352 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458406 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458466 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458521 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458578 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458632 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458691 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458745 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458800 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458853 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458910 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.458963 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459022 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459076 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459130 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459183 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459240 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459295 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459354 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459407 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459465 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459518 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459574 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459629 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459681 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459735 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459791 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459845 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459902 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.459961 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460015 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460067 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460124 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460178 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460237 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460289 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460346 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460400 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460455 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460507 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460560 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460613 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460670 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460725 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460781 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460834 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460888 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460941 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.460999 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461052 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461110 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461163 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461219 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461273 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461328 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461381 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461435 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461488 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461544 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461597 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461655 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461709 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461763 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461816 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461874 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461927 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.461985 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462038 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462094 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462146 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462202 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462256 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462309 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462362 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462419 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462472 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462530 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462584 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462636 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462691 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462746 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462801 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462857 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462909 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.462965 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463018 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463075 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463128 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463181 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463233 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463290 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463343 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463398 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463452 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463505 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463558 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463614 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463667 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463725 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463777 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463834 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463887 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463943 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.463994 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464048 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464101 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464156 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464210 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464264 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464318 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464371 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464423 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464479 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464533 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464590 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464643 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464698 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464752 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464806 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464859 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464911 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.464964 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465023 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465076 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465131 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465184 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465236 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465290 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465347 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465399 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465456 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465509 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465564 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465617 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465673 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465727 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465780 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465833 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465889 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.465944 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466001 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466055 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466107 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466161 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466218 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466273 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466330 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466384 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466440 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466493 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466552 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466604 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466656 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466711 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466767 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466821 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466877 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466932 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.466984 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.467036 4595733952 run_classifier.py:669]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:35:49.467092 4595733952 run_classifier.py:669]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
      "I1117 17:35:49.467158 4595733952 run_classifier.py:669]   name = output_weights:0, shape = (2, 768)\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
      "I1117 17:35:49.467226 4595733952 run_classifier.py:669]   name = output_bias:0, shape = (2,)\n",
      "WARNING:tensorflow:From run_classifier.py:686: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\n",
      "W1117 17:35:49.468293 4595733952 deprecation_wrapper.py:119] From run_classifier.py:686: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From run_classifier.py:688: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
      "\n",
      "W1117 17:35:49.507078 4595733952 deprecation_wrapper.py:119] From run_classifier.py:688: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I1117 17:35:49.543051 4595733952 estimator.py:1147] Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-17T17:35:49Z\n",
      "I1117 17:35:49.555225 4595733952 evaluation.py:255] Starting evaluation at 2019-11-17T17:35:49Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I1117 17:35:49.875779 4595733952 monitored_session.py:240] Graph was finalized.\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W1117 17:35:49.876036 4595733952 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-1856\n",
      "I1117 17:35:49.876905 4595733952 saver.py:1280] Restoring parameters from ./bert_output/model.ckpt-1856\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I1117 17:35:50.393527 4595733952 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I1117 17:35:50.444690 4595733952 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-17-17:36:10\n",
      "I1117 17:36:10.448152 4595733952 evaluation.py:275] Finished evaluation at 2019-11-17-17:36:10\n",
      "INFO:tensorflow:Saving dict for global step 1856: eval_accuracy = 0.715, eval_loss = 0.6112577, global_step = 1856, loss = 0.6112577\n",
      "I1117 17:36:10.448316 4595733952 estimator.py:2039] Saving dict for global step 1856: eval_accuracy = 0.715, eval_loss = 0.6112577, global_step = 1856, loss = 0.6112577\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 1856: ./bert_output/model.ckpt-1856\n",
      "I1117 17:36:10.926172 4595733952 estimator.py:2099] Saving 'checkpoint_path' summary for global step 1856: ./bert_output/model.ckpt-1856\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "I1117 17:36:10.926555 4595733952 error_handling.py:96] evaluation_loop marked as finished\n",
      "INFO:tensorflow:***** Eval results *****\n",
      "I1117 17:36:10.926674 4595733952 run_classifier.py:923] ***** Eval results *****\n",
      "INFO:tensorflow:  eval_accuracy = 0.715\n",
      "I1117 17:36:10.926759 4595733952 run_classifier.py:925]   eval_accuracy = 0.715\n",
      "INFO:tensorflow:  eval_loss = 0.6112577\n",
      "I1117 17:36:10.926962 4595733952 run_classifier.py:925]   eval_loss = 0.6112577\n",
      "INFO:tensorflow:  global_step = 1856\n",
      "I1117 17:36:10.927028 4595733952 run_classifier.py:925]   global_step = 1856\n",
      "INFO:tensorflow:  loss = 0.6112577\n",
      "I1117 17:36:10.927085 4595733952 run_classifier.py:925]   loss = 0.6112577\n",
      "INFO:tensorflow:Writing example 0 of 4042\n",
      "I1117 17:36:10.945204 4595733952 run_classifier.py:487] Writing example 0 of 4042\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:36:10.945492 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-1\n",
      "I1117 17:36:10.945554 4595733952 run_classifier.py:462] guid: test-1\n",
      "INFO:tensorflow:tokens: [CLS] he put an elephant into the fridge [SEP]\n",
      "I1117 17:36:10.945608 4595733952 run_classifier.py:464] tokens: [CLS] he put an elephant into the fridge [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2002 2404 2019 10777 2046 1996 16716 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.945680 4595733952 run_classifier.py:465] input_ids: 101 2002 2404 2019 10777 2046 1996 16716 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.945976 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.946085 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 17:36:10.946144 4595733952 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:36:10.946639 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-2\n",
      "I1117 17:36:10.946708 4595733952 run_classifier.py:462] guid: test-2\n",
      "INFO:tensorflow:tokens: [CLS] my sister eats an apple after breakfast every day [SEP]\n",
      "I1117 17:36:10.946767 4595733952 run_classifier.py:464] tokens: [CLS] my sister eats an apple after breakfast every day [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2026 2905 20323 2019 6207 2044 6350 2296 2154 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.946842 4595733952 run_classifier.py:465] input_ids: 101 2026 2905 20323 2019 6207 2044 6350 2296 2154 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.946917 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.947361 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 17:36:10.947489 4595733952 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:36:10.947939 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-3\n",
      "I1117 17:36:10.948010 4595733952 run_classifier.py:462] guid: test-3\n",
      "INFO:tensorflow:tokens: [CLS] money can be used for buying cars [SEP]\n",
      "I1117 17:36:10.948069 4595733952 run_classifier.py:464] tokens: [CLS] money can be used for buying cars [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2769 2064 2022 2109 2005 9343 3765 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.948145 4595733952 run_classifier.py:465] input_ids: 101 2769 2064 2022 2109 2005 9343 3765 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.948222 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.948346 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 17:36:10.948421 4595733952 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:36:10.948905 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-4\n",
      "I1117 17:36:10.948977 4595733952 run_classifier.py:462] guid: test-4\n",
      "INFO:tensorflow:tokens: [CLS] new york is located in the northeastern part of usa [SEP]\n",
      "I1117 17:36:10.949038 4595733952 run_classifier.py:464] tokens: [CLS] new york is located in the northeastern part of usa [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2047 2259 2003 2284 1999 1996 8763 2112 1997 3915 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.949115 4595733952 run_classifier.py:465] input_ids: 101 2047 2259 2003 2284 1999 1996 8763 2112 1997 3915 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.949194 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.949272 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 17:36:10.949337 4595733952 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 17:36:10.949784 4595733952 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-5\n",
      "I1117 17:36:10.949870 4595733952 run_classifier.py:462] guid: test-5\n",
      "INFO:tensorflow:tokens: [CLS] a man can better see stars and the moon in daytime [SEP]\n",
      "I1117 17:36:10.949954 4595733952 run_classifier.py:464] tokens: [CLS] a man can better see stars and the moon in daytime [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1037 2158 2064 2488 2156 3340 1998 1996 4231 1999 12217 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.950039 4595733952 run_classifier.py:465] input_ids: 101 1037 2158 2064 2488 2156 3340 1998 1996 4231 1999 12217 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.950117 4595733952 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 17:36:10.950253 4595733952 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 17:36:10.950341 4595733952 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running prediction*****\n",
      "I1117 17:36:12.275876 4595733952 run_classifier.py:944] ***** Running prediction*****\n",
      "INFO:tensorflow:  Num examples = 4042 (4042 actual, 0 padding)\n",
      "I1117 17:36:12.275990 4595733952 run_classifier.py:947]   Num examples = 4042 (4042 actual, 0 padding)\n",
      "INFO:tensorflow:  Batch size = 8\n",
      "I1117 17:36:12.276051 4595733952 run_classifier.py:948]   Batch size = 8\n",
      "INFO:tensorflow:***** Predict results *****\n",
      "I1117 17:36:12.276156 4595733952 run_classifier.py:962] ***** Predict results *****\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I1117 17:36:12.299918 4595733952 estimator.py:1145] Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "I1117 17:36:12.300107 4595733952 tpu_estimator.py:2965] Running infer on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I1117 17:36:12.300312 4595733952 run_classifier.py:627] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
      "I1117 17:36:12.300395 4595733952 run_classifier.py:629]   name = input_ids, shape = (?, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
      "I1117 17:36:12.300462 4595733952 run_classifier.py:629]   name = input_mask, shape = (?, 128)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
      "I1117 17:36:12.300519 4595733952 run_classifier.py:629]   name = is_real_example, shape = (?,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
      "I1117 17:36:12.300575 4595733952 run_classifier.py:629]   name = label_ids, shape = (?,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
      "I1117 17:36:12.300631 4595733952 run_classifier.py:629]   name = segment_ids, shape = (?, 128)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.434494 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.502753 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.567247 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf62550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.650544 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.725734 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.796951 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.876590 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf623c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:12.942527 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.009514 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40a0bf98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0f5470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0f5470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.088670 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0f5470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0f5470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0c49b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0c49b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.328000 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0c49b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0c49b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.399200 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.478326 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.543373 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0e5b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ddc3c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ddc3c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.612239 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ddc3c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ddc3c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6fbc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6fbc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.691694 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6fbc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6fbc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.768016 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.838339 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.917068 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:13.982671 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.047868 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d6da4a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0cfb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0cfb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.129657 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0cfb38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0cfb38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ca144a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ca144a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.207034 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ca144a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ca144a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.278059 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.359658 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.425062 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.491279 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a432d7898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.570924 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.648058 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48eb8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.720687 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.800531 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.866503 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:14.932280 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a4225af98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.011811 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ceec898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.088864 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.158613 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.237315 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.302328 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.368037 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a450b84a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de5d518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de5d518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.450346 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de5d518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de5d518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.525815 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.595077 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d2ed390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.756511 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.824872 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.890053 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de3bb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de2ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de2ec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:15.970476 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de2ec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3de2ec50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.047155 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.117152 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.198498 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.263523 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.328229 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40058208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.407354 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf75550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.482377 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48b38>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.552366 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.630186 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.698052 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.762430 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a40053f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3f8c6518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3f8c6518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.845435 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3f8c6518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3f8c6518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.922170 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:16.991855 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.071320 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.136838 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.202941 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a44b21da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d316e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d316e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.283706 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d316e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d316e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.359683 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48c88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cf48c88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.429939 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.508849 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.573999 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.638168 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d313518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3bb630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3bb630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.718586 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3bb630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3bb630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.797409 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3fdfa2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d441128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d441128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.867088 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d441128>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d441128>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c91c320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c91c320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 17:36:17.986876 4595733952 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c91c320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c91c320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "I1117 17:36:18.471258 4595733952 run_classifier.py:663] **** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471376 4595733952 run_classifier.py:669]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471457 4595733952 run_classifier.py:669]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471524 4595733952 run_classifier.py:669]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471585 4595733952 run_classifier.py:669]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471643 4595733952 run_classifier.py:669]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471776 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471849 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471911 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.471971 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.472026 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473081 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473224 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473306 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473371 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473433 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473491 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473556 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473615 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473677 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473734 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473792 4595733952 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473850 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473912 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.473968 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474026 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474083 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474141 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474198 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474262 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474319 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474376 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474432 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474491 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474549 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474608 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474664 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474720 4595733952 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474778 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474837 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474894 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.474953 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475010 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475069 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475125 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475184 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475239 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475296 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475352 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475410 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475467 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475528 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475584 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475639 4595733952 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475694 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475754 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475810 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475869 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475925 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.475986 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476045 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476104 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476161 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476216 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476272 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476331 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476389 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476463 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476517 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476571 4595733952 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476625 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476684 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476737 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476795 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476850 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476907 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.476962 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477018 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477072 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477128 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477181 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477240 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477296 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477355 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477410 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477463 4595733952 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477530 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477586 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477639 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477697 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477751 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477807 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477860 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477916 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.477969 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478024 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478076 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478132 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478187 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478245 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478298 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478351 4595733952 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478403 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478459 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478513 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478569 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478621 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478678 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478732 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478787 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478840 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478937 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.478991 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479048 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479103 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479160 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479212 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479266 4595733952 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479319 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479375 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479429 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479485 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479539 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479596 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479649 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479706 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479759 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479812 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479865 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479920 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.479975 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480031 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480086 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480138 4595733952 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480190 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480247 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480299 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480357 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480410 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480465 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480520 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480576 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480629 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480683 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480736 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480792 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480846 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480901 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.480954 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481007 4595733952 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481061 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481117 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481168 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481225 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481277 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481333 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481385 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481441 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481495 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481549 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481601 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481658 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481712 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481768 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481822 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481874 4595733952 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481929 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.481983 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482037 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482093 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482146 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482201 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482254 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482309 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482363 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482837 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482920 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.482993 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483055 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483116 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483172 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483226 4595733952 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483281 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483340 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483396 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483453 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483507 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483566 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483618 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483675 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483729 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483783 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483835 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483892 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.483947 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.484003 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.484055 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.484110 4595733952 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.484163 4595733952 run_classifier.py:669]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 17:36:18.484220 4595733952 run_classifier.py:669]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
      "I1117 17:36:18.484297 4595733952 run_classifier.py:669]   name = output_weights:0, shape = (2, 768)\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
      "I1117 17:36:18.484367 4595733952 run_classifier.py:669]   name = output_bias:0, shape = (2,)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I1117 17:36:18.484620 4595733952 estimator.py:1147] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "I1117 17:36:18.789113 4595733952 monitored_session.py:240] Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-1856\n",
      "I1117 17:36:18.790276 4595733952 saver.py:1280] Restoring parameters from ./bert_output/model.ckpt-1856\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I1117 17:36:19.300677 4595733952 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I1117 17:36:19.346832 4595733952 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I1117 17:43:13.857414 4595733952 error_handling.py:96] prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I1117 17:43:13.857585 4595733952 error_handling.py:96] prediction_loop marked as finished\n"
     ]
    }
   ],
   "source": [
    "#!python run_classifier.py \\\n",
    "#      --task_name=cola \\\n",
    "#      --do_train=true \\\n",
    "#      --do_eval=true \\\n",
    "#      --do_predict=true \\\n",
    "#      --data_dir=./task1_data/ \\\n",
    "#      --vocab_file=./uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "#      --bert_config_file=./uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "#      --init_checkpoint=./uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#      --max_seq_length=128 /\n",
    "#      --train_batch_size=32 /\n",
    "#      --learning_rate=2e-5 /\n",
    "#      --num_train_epochs=3.0 /\n",
    "#      --output_dir=./bert_output/ /\n",
    "#      --do_lower_case=True /"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT Prediction output\n",
    "\n",
    "Having fine-tuned BERT, the next step was to fetch BERT's output prediction.\n",
    "\n",
    "During the Fine-Tuning the flag for prediction was also passed, so that BERT would be trained in the fine-tuning process with added prediction against the test values.\n",
    "\n",
    "`--do_predict=true`\n",
    "\n",
    "**Prediction file is**\n",
    "![task1_predicted](images/task1_img3.png)\n",
    "\n",
    "\n",
    "### Explaining Bert Output - Task 1\n",
    "\n",
    "For each test sentence, 2 values were predicted, which is the probability of the sentence being part of the target class 0 or 1.\n",
    "\n",
    "Before we pass the values for being evaluated in the Evaluation Tool, a transformation is required to assign each sentence with the highet probable target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        id  text\n",
      "0        1     0\n",
      "1        2     1\n",
      "2        3     1\n",
      "3        4     1\n",
      "4        5     1\n",
      "...    ...   ...\n",
      "2016  2017     1\n",
      "2017  2018     1\n",
      "2018  2019     0\n",
      "2019  2020     0\n",
      "2020  2021     0\n",
      "\n",
      "[2021 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Preparing BERT prediction output for the evaluation tool\n",
    "task1_result_df = pd.read_csv('task1_data/task1_test_results.tsv', sep='\\t', header=None)\n",
    "\n",
    "# BERT outputs the probabilities, so I round it to 1 or 0 according to the prediction\n",
    "def getRound(x):\n",
    "    return(round(x))\n",
    "\n",
    "task1_result_df = task1_result_df.applymap(getRound)\n",
    "\n",
    "task1_result_df.head(10)\n",
    "\n",
    "\n",
    "# creating bert dataframe for the train data\n",
    "parsed_result = pd.DataFrame({\n",
    "    'id':range(len(task1_result_df)),\n",
    "    'text': task1_result_df[1] > 0.5\n",
    "})\n",
    "\n",
    "parsed_result['id'] = parsed_result.id.apply(lambda x: x+1) \n",
    "parsed_result['text'] = parsed_result.text.apply(lambda x: int(x) ) \n",
    "\n",
    "# inverting the target class \n",
    "parsed_result['text'] = parsed_result.text.apply(lambda x: 0 if x == 1 else 1 )\n",
    "\n",
    "parsed_result = parsed_result.head(2021)\n",
    "\n",
    "\n",
    "print(parsed_result)\n",
    "\n",
    "parsed_result.to_csv('evaluation_tools/task1_my_result.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.6141%\r\n"
     ]
    }
   ],
   "source": [
    "! python evaluation_tools/taskA_scorer.py --gold-labels evaluation_tools/taskA_trial_answer.csv --pred-labels evaluation_tools/task1_my_result.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 Accuracy: 79.6141%\n",
    "\n",
    "The final output for Task 1 was taken through the integration of the evaluation tools https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation\n",
    "\n",
    "\n",
    "## Task 2\n",
    "\n",
    "For task 2, BERT pre-trainned **uncased_L-12_H-768_A-12** model was used to tackle the task.\n",
    "\n",
    "Similar to Task 1, Task 2 was approached through a Binary Classification method. \n",
    "\n",
    "The difference is in the fact that each sentence in the Trainning data for Task 2 contains 2 Options to choose from, and each of these options was converted into a individual sentence for trainning, with its respective label (0 or 1).\n",
    "\n",
    "![task2_train](images/task2_img1.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sanity check for train data\n",
      "\n",
      "\n",
      "   0                                                  1  2\n",
      "0  0  100,000 miles is way to long for one person to...  0\n",
      "1  1                        Only humans play volleyball  0\n",
      "2  2                                  Chairs can't walk  0\n",
      "3  3                       An elephant is a wild animal  0\n",
      "4  4  A football is thing to play with not a thing t...  0\n",
      "\n",
      "\n",
      "Sanity check for test data\n",
      "\n",
      "\n",
      "    0                                         1\n",
      "0  id                                       seq\n",
      "1   0  an elephant is much bigger than a fridge\n",
      "2   1          stone is usually in round-shapes\n",
      "3   2    no one can get stars and sell them now\n",
      "4   3        New York is not the capital of USA\n",
      "\n",
      "\n",
      "Sanity check for Dataframe Trainning data\n",
      "\n",
      "\n",
      "   id  label alpha                                               text\n",
      "0   0      0     a  100,000 miles is way to long for one person to...\n",
      "1   1      0     a                        only humans play volleyball\n",
      "2   2      0     a                                  chairs can't walk\n",
      "3   3      0     a                       an elephant is a wild animal\n",
      "4   4      0     a  a football is thing to play with not a thing t...\n",
      "\n",
      "\n",
      "Sanity check for Dataframe Test data\n",
      "\n",
      "\n",
      "   id                                      text\n",
      "0   0                                       seq\n",
      "1   1  an elephant is much bigger than a fridge\n",
      "2   2          stone is usually in round-shapes\n",
      "3   3    no one can get stars and sell them now\n",
      "4   4        new york is not the capital of usa\n"
     ]
    }
   ],
   "source": [
    "# Task 2\n",
    "\n",
    "# sanity check for test data\n",
    "\n",
    "train_df = pd.read_csv('task2_data/task2_train.csv', header=None)\n",
    "print(\"\\n\\nSanity check for train data\\n\\n\")\n",
    "print(train_df.head(5))\n",
    "\n",
    "test_df = pd.read_csv('task2_data/task2_test.csv', header=None)\n",
    "print(\"\\n\\nSanity check for test data\\n\\n\")\n",
    "print(test_df.head(5))\n",
    "\n",
    "# creating bert dataframe for the train data\n",
    "df_bert = pd.DataFrame({\n",
    "    'id':range(len(train_df)),\n",
    "    'label':train_df[2],\n",
    "    'alpha':['a']*train_df.shape[0],\n",
    "    'text': train_df[1].replace(r'\\n', ' ', regex=True).str.lower()\n",
    "})\n",
    "\n",
    "print(\"\\n\\nSanity check for Dataframe Trainning data\\n\\n\")\n",
    "print(df_bert.head(5))\n",
    "\n",
    "\n",
    "# splitting training data file into train and *dev*\n",
    "df_bert_train, df_bert_dev = train_test_split(df_bert, test_size=0.01)\n",
    "\n",
    "# creating test dataframe according to BERT\n",
    "df_bert_test = pd.DataFrame({\n",
    "    'id':range(len(test_df)),\n",
    "    'text': test_df[1].replace(r'\\n', ' ', regex=True).str.lower()\n",
    "})\n",
    "\n",
    "print(\"\\n\\nSanity check for Dataframe Test data\\n\\n\")\n",
    "print(df_bert_test.head(5))\n",
    "\n",
    "# saving dataframes to .tsv format as required by BERT\n",
    "df_bert_train.to_csv('task2_data/train.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_dev.to_csv('task2_data/dev.tsv', sep='\\t', index=False, header=False)\n",
    "df_bert_test.to_csv('task2_data/test.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 Pre-Processing Output\n",
    "\n",
    "The output above shows how data was pre-processed in such a way to allow it to be fed to BERT.\n",
    "\n",
    "The data requires tsv extension for files, so that was performed using pandas. The same process was done for the Trainning and Test data. The Test data being what we want to predict.\n",
    "\n",
    "Data Ready to be fed to BERT\n",
    "\n",
    "![task1_processed](images/task2_img2.png)\n",
    "\n",
    "The final step is then to Fine-Tune BERT, and that is done below, using the pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:981: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:784: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W1117 21:54:56.444966 4435920320 deprecation_wrapper.py:119] From run_classifier.py:784: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:784: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "W1117 21:54:56.445101 4435920320 deprecation_wrapper.py:119] From run_classifier.py:784: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1117 21:54:56.445499 4435920320 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:808: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W1117 21:54:56.445839 4435920320 deprecation_wrapper.py:119] From run_classifier.py:808: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1117 21:54:57.102471 4435920320 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:199: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1117 21:54:57.102915 4435920320 deprecation_wrapper.py:119] From run_classifier.py:199: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x1a33659bf8>) includes params argument, but params are not passed to Estimator.\n",
      "W1117 21:54:57.308814 4435920320 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x1a33659bf8>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a33687518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "I1117 21:54:57.309552 4435920320 estimator.py:209] Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a33687518>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "I1117 21:54:57.310015 4435920320 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "W1117 21:54:57.310163 4435920320 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n",
      "WARNING:tensorflow:From run_classifier.py:483: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W1117 21:54:57.310305 4435920320 deprecation_wrapper.py:119] From run_classifier.py:483: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:487: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W1117 21:54:57.310557 4435920320 deprecation_wrapper.py:119] From run_classifier.py:487: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Writing example 0 of 29700\n",
      "I1117 21:54:57.310643 4435920320 run_classifier.py:487] Writing example 0 of 29700\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 21:54:57.310931 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-0\n",
      "I1117 21:54:57.311011 4435920320 run_classifier.py:462] guid: train-0\n",
      "INFO:tensorflow:tokens: [CLS] movie theatre do not sell fire . [SEP]\n",
      "I1117 21:54:57.311080 4435920320 run_classifier.py:464] tokens: [CLS] movie theatre do not sell fire . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 3185 3004 2079 2025 5271 2543 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.311173 4435920320 run_classifier.py:465] input_ids: 101 3185 3004 2079 2025 5271 2543 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.311267 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.311357 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 21:54:57.311418 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 21:54:57.312011 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-1\n",
      "I1117 21:54:57.312100 4435920320 run_classifier.py:462] guid: train-1\n",
      "INFO:tensorflow:tokens: [CLS] lovers usually give each other gifts on birthday [SEP]\n",
      "I1117 21:54:57.312171 4435920320 run_classifier.py:464] tokens: [CLS] lovers usually give each other gifts on birthday [SEP]\n",
      "INFO:tensorflow:input_ids: 101 10205 2788 2507 2169 2060 9604 2006 5798 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.312278 4435920320 run_classifier.py:465] input_ids: 101 10205 2788 2507 2169 2060 9604 2006 5798 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.312376 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.312546 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 21:54:57.312638 4435920320 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 21:54:57.313191 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-2\n",
      "I1117 21:54:57.313274 4435920320 run_classifier.py:462] guid: train-2\n",
      "INFO:tensorflow:tokens: [CLS] a ball is not a liquid that can be drank . [SEP]\n",
      "I1117 21:54:57.313344 4435920320 run_classifier.py:464] tokens: [CLS] a ball is not a liquid that can be drank . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1037 3608 2003 2025 1037 6381 2008 2064 2022 10749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.313434 4435920320 run_classifier.py:465] input_ids: 101 1037 3608 2003 2025 1037 6381 2008 2064 2022 10749 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.313524 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.313612 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 21:54:57.313764 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 21:54:57.314274 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-3\n",
      "I1117 21:54:57.314360 4435920320 run_classifier.py:462] guid: train-3\n",
      "INFO:tensorflow:tokens: [CLS] notice is no need to answer [SEP]\n",
      "I1117 21:54:57.314429 4435920320 run_classifier.py:464] tokens: [CLS] notice is no need to answer [SEP]\n",
      "INFO:tensorflow:input_ids: 101 5060 2003 2053 2342 2000 3437 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.314522 4435920320 run_classifier.py:465] input_ids: 101 5060 2003 2053 2342 2000 3437 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.314614 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.314761 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1117 21:54:57.314854 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1117 21:54:57.315365 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: train-4\n",
      "I1117 21:54:57.315451 4435920320 run_classifier.py:462] guid: train-4\n",
      "INFO:tensorflow:tokens: [CLS] water can be used to wash dirt ##s [SEP]\n",
      "I1117 21:54:57.315524 4435920320 run_classifier.py:464] tokens: [CLS] water can be used to wash dirt ##s [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2300 2064 2022 2109 2000 9378 6900 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.315617 4435920320 run_classifier.py:465] input_ids: 101 2300 2064 2022 2109 2000 9378 6900 2015 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.315711 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1117 21:54:57.315799 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1117 21:54:57.315861 4435920320 run_classifier.py:468] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 10000 of 29700\n",
      "I1117 21:55:00.597498 4435920320 run_classifier.py:487] Writing example 10000 of 29700\n",
      "INFO:tensorflow:Writing example 20000 of 29700\n",
      "I1117 21:55:03.929162 4435920320 run_classifier.py:487] Writing example 20000 of 29700\n",
      "INFO:tensorflow:***** Running training *****\n",
      "I1117 21:55:07.092422 4435920320 run_classifier.py:871] ***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 29700\n",
      "I1117 21:55:07.092535 4435920320 run_classifier.py:872]   Num examples = 29700\n",
      "INFO:tensorflow:  Batch size = 32\n",
      "I1117 21:55:07.092673 4435920320 run_classifier.py:873]   Batch size = 32\n",
      "INFO:tensorflow:  Num steps = 2784\n",
      "I1117 21:55:07.092724 4435920320 run_classifier.py:874]   Num steps = 2784\n",
      "WARNING:tensorflow:From run_classifier.py:514: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "W1117 21:55:07.092805 4435920320 deprecation_wrapper.py:119] From run_classifier.py:514: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "W1117 21:55:07.096517 4435920320 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From run_classifier.py:550: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "W1117 21:55:07.116258 4435920320 deprecation.py:323] From run_classifier.py:550: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W1117 21:55:07.116385 4435920320 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From run_classifier.py:523: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "W1117 21:55:07.117285 4435920320 deprecation_wrapper.py:119] From run_classifier.py:523: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:530: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1117 21:55:07.119870 4435920320 deprecation.py:323] From run_classifier.py:530: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I1117 21:55:07.132050 4435920320 estimator.py:1145] Calling model_fn.\n",
      "INFO:tensorflow:Running train on CPU\n",
      "I1117 21:55:07.132241 4435920320 tpu_estimator.py:2965] Running train on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I1117 21:55:07.132530 4435920320 run_classifier.py:627] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (32, 128)\n",
      "I1117 21:55:07.132616 4435920320 run_classifier.py:629]   name = input_ids, shape = (32, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (32, 128)\n",
      "I1117 21:55:07.132783 4435920320 run_classifier.py:629]   name = input_mask, shape = (32, 128)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (32,)\n",
      "I1117 21:55:07.132854 4435920320 run_classifier.py:629]   name = is_real_example, shape = (32,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (32,)\n",
      "I1117 21:55:07.132919 4435920320 run_classifier.py:629]   name = label_ids, shape = (32,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (32, 128)\n",
      "I1117 21:55:07.132982 4435920320 run_classifier.py:629]   name = segment_ids, shape = (32, 128)\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1117 21:55:07.133686 4435920320 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1117 21:55:07.134918 4435920320 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W1117 21:55:07.151885 4435920320 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W1117 21:55:07.177361 4435920320 deprecation.py:506] From /Users/kelson/repository/personal/github/bert/modeling.py:358: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1117 21:55:07.189221 4435920320 deprecation.py:323] From /Users/kelson/repository/personal/github/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.269315 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.360679 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba77f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.453989 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338488d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338488d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.554975 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338488d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338488d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.639545 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.714094 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.798219 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.868619 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:07.939203 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.022031 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b828>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3385b828>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.107500 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3397b438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.178102 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba74e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3465e198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3465e198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.263152 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3465e198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3465e198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.328918 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.397398 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346a2be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346b24a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346b24a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.481912 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346b24a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346b24a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.564501 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cd54a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cd54a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.712779 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cd54a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cd54a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.817689 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.888875 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:08.957674 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34839518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33841588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33841588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.047986 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33841588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33841588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.132943 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a347ecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a347ecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.206538 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a347ecb70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a347ecb70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.288918 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.356520 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.425615 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349dbdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349c8208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349c8208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.523031 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349c8208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349c8208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.619044 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.697633 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33cb76a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.784137 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.850039 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:09.917084 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34b61080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.002974 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.085651 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.159116 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454eba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d8d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d8d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.261157 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d8d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d8d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d607f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d607f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.334799 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d607f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d607f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34a82588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34a82588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.407853 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34a82588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34a82588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.504914 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3398b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.610885 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.687878 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33ba7278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f16ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f16ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.772677 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f16ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f16ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.845745 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:10.921206 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f2d588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34e914a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34e914a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.011611 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34e914a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34e914a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.101702 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.175570 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a346717b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.339380 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.414192 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.484399 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350bb518>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3509fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3509fef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.568202 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3509fef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3509fef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3482c7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3482c7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.661274 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3482c7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3482c7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.735003 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a336e8dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b47f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b47f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.817380 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b47f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b47f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352f5fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352f5fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.896474 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352f5fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352f5fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34fb7860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34fb7860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:11.970263 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34fb7860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34fb7860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.057585 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d08a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d08a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.143322 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d08a20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34d08a20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.220112 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a349bd2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545fc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.309389 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545fc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545fc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.381201 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.448107 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3545c2e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.537153 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34cc0d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f0eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f0eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.626784 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f0eda0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34f0eda0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b4898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b4898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.701292 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b4898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a352b4898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.789474 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.866105 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:12.938831 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3568d2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338f5da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338f5da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:13.029804 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338f5da0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a338f5da0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35246048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35246048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:13.126883 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35246048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35246048>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:13.219732 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686c18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35686c18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a339214e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a339214e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1117 21:55:13.351926 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a339214e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a339214e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From run_classifier.py:661: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "W1117 21:55:13.379370 4435920320 deprecation_wrapper.py:119] From run_classifier.py:661: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "I1117 21:55:13.886808 4435920320 run_classifier.py:663] **** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.886926 4435920320 run_classifier.py:669]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887007 4435920320 run_classifier.py:669]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887073 4435920320 run_classifier.py:669]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887135 4435920320 run_classifier.py:669]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887193 4435920320 run_classifier.py:669]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887247 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887305 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887358 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887416 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887469 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887525 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887579 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887635 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887688 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887742 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887794 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887851 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887905 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.887962 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888015 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888067 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888118 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888175 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888228 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888283 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888335 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888392 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888444 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888502 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888558 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888611 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888664 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888719 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1117 21:55:13.888773 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.888828 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.888881 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.888933 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.888986 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.889041 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.889710 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.889791 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.889856 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.889953 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890038 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890110 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890170 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890228 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890284 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890343 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890398 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890457 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890512 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890566 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890623 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890682 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890736 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890794 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890849 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890906 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.890959 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891016 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891069 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891125 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891205 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891432 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891624 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891710 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891808 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891864 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.891919 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892004 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892073 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892138 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892199 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892259 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892313 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892389 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892457 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892510 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892564 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892621 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892675 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892731 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892786 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892839 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892892 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.892950 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893002 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893060 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893113 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893168 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893222 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893280 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893333 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893385 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893439 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893496 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893550 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893606 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893715 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893790 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893852 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893918 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.893975 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894033 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894086 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894144 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894197 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894253 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894307 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894359 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894412 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894468 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894520 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894576 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894630 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894682 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894735 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894791 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894845 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894900 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.894953 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895009 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895061 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895117 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895169 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895220 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895273 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895328 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895380 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895437 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895489 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895541 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895592 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895650 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895702 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895756 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895808 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895865 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895916 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.895973 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896025 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896077 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896129 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896183 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896237 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896293 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896346 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896399 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896450 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896506 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896559 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896615 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896666 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896721 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896773 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896828 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896881 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896933 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.896986 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897042 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897094 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897168 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897226 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897278 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897330 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897387 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897439 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897495 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897547 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897603 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897699 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897786 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897855 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897914 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.897969 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898030 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898084 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898141 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898195 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898247 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898300 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898360 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898414 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898470 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898524 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898581 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898633 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898689 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898744 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898796 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898848 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898905 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.898959 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.899014 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.899067 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.899120 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.899173 4435920320 run_classifier.py:669]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1117 21:55:13.899231 4435920320 run_classifier.py:669]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\r\n",
      "I1117 21:55:13.899307 4435920320 run_classifier.py:669]   name = output_weights:0, shape = (2, 768)\r\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,)\r\n",
      "I1117 21:55:13.899376 4435920320 run_classifier.py:669]   name = output_bias:0, shape = (2,)\r\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n",
      "\r\n",
      "W1117 21:55:13.899518 4435920320 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/optimization.py:27: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\r\n",
      "\r\n",
      "W1117 21:55:13.900153 4435920320 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/optimization.py:32: The name tf.train.polynomial_decay is deprecated. Please use tf.compat.v1.train.polynomial_decay instead.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Deprecated in favor of operator or tf.math.divide.\r\n",
      "W1117 21:55:13.903281 4435920320 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/optimizer_v2/learning_rate_schedule.py:409: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Deprecated in favor of operator or tf.math.divide.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1117 21:55:14.041718 4435920320 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I1117 21:55:19.299884 4435920320 estimator.py:1147] Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "I1117 21:55:19.300780 4435920320 basic_session_run_hooks.py:541] Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I1117 21:55:21.595134 4435920320 monitored_session.py:240] Graph was finalized.\n",
      "2019-11-17 21:55:21.595356: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I1117 21:55:26.210355 4435920320 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I1117 21:55:26.399955 4435920320 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./bert_output/model.ckpt.\n",
      "I1117 21:55:31.888357 4435920320 basic_session_run_hooks.py:606] Saving checkpoints for 0 into ./bert_output/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0611182\n",
      "I1117 21:56:10.937338 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0611182\n",
      "INFO:tensorflow:examples/sec: 1.95578\n",
      "I1117 21:56:10.937786 4435920320 tpu_estimator.py:2160] examples/sec: 1.95578\n",
      "INFO:tensorflow:global_step/sec: 0.0872876\n",
      "I1117 21:56:22.393679 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872876\n",
      "INFO:tensorflow:examples/sec: 2.7932\n",
      "I1117 21:56:22.393891 4435920320 tpu_estimator.py:2160] examples/sec: 2.7932\n",
      "INFO:tensorflow:global_step/sec: 0.0871322\n",
      "I1117 21:56:33.870489 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871322\n",
      "INFO:tensorflow:examples/sec: 2.78823\n",
      "I1117 21:56:33.870934 4435920320 tpu_estimator.py:2160] examples/sec: 2.78823\n",
      "INFO:tensorflow:global_step/sec: 0.0777229\n",
      "I1117 21:56:46.736673 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0777229\n",
      "INFO:tensorflow:examples/sec: 2.48713\n",
      "I1117 21:56:46.736855 4435920320 tpu_estimator.py:2160] examples/sec: 2.48713\n",
      "INFO:tensorflow:global_step/sec: 0.0870213\n",
      "I1117 21:56:58.228112 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870213\n",
      "INFO:tensorflow:examples/sec: 2.78468\n",
      "I1117 21:56:58.228296 4435920320 tpu_estimator.py:2160] examples/sec: 2.78468\n",
      "INFO:tensorflow:global_step/sec: 0.0890052\n",
      "I1117 21:57:09.463438 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890052\n",
      "INFO:tensorflow:examples/sec: 2.84817\n",
      "I1117 21:57:09.463660 4435920320 tpu_estimator.py:2160] examples/sec: 2.84817\n",
      "INFO:tensorflow:global_step/sec: 0.0894219\n",
      "I1117 21:57:20.646389 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894219\n",
      "INFO:tensorflow:examples/sec: 2.8615\n",
      "I1117 21:57:20.646619 4435920320 tpu_estimator.py:2160] examples/sec: 2.8615\n",
      "INFO:tensorflow:global_step/sec: 0.0894218\n",
      "I1117 21:57:31.829339 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894218\n",
      "INFO:tensorflow:examples/sec: 2.8615\n",
      "I1117 21:57:31.829570 4435920320 tpu_estimator.py:2160] examples/sec: 2.8615\n",
      "INFO:tensorflow:global_step/sec: 0.0892913\n",
      "I1117 21:57:43.028640 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892913\n",
      "INFO:tensorflow:examples/sec: 2.85732\n",
      "I1117 21:57:43.028869 4435920320 tpu_estimator.py:2160] examples/sec: 2.85732\n",
      "INFO:tensorflow:global_step/sec: 0.089311\n",
      "I1117 21:57:54.225460 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089311\n",
      "INFO:tensorflow:examples/sec: 2.85795\n",
      "I1117 21:57:54.225682 4435920320 tpu_estimator.py:2160] examples/sec: 2.85795\n",
      "INFO:tensorflow:global_step/sec: 0.0891234\n",
      "I1117 21:58:05.445861 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891234\n",
      "INFO:tensorflow:examples/sec: 2.85195\n",
      "I1117 21:58:05.446080 4435920320 tpu_estimator.py:2160] examples/sec: 2.85195\n",
      "INFO:tensorflow:global_step/sec: 0.0891517\n",
      "I1117 21:58:16.662698 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891517\n",
      "INFO:tensorflow:examples/sec: 2.85286\n",
      "I1117 21:58:16.662930 4435920320 tpu_estimator.py:2160] examples/sec: 2.85286\n",
      "INFO:tensorflow:global_step/sec: 0.089396\n",
      "I1117 21:58:27.848876 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089396\n",
      "INFO:tensorflow:examples/sec: 2.86067\n",
      "I1117 21:58:27.849111 4435920320 tpu_estimator.py:2160] examples/sec: 2.86067\n",
      "INFO:tensorflow:global_step/sec: 0.0892757\n",
      "I1117 21:58:39.050127 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892757\n",
      "INFO:tensorflow:examples/sec: 2.85682\n",
      "I1117 21:58:39.050359 4435920320 tpu_estimator.py:2160] examples/sec: 2.85682\n",
      "INFO:tensorflow:global_step/sec: 0.0887993\n",
      "I1117 21:58:50.311465 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887993\n",
      "INFO:tensorflow:examples/sec: 2.84158\n",
      "I1117 21:58:50.311716 4435920320 tpu_estimator.py:2160] examples/sec: 2.84158\n",
      "INFO:tensorflow:global_step/sec: 0.0890828\n",
      "I1117 21:59:01.536982 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890828\n",
      "INFO:tensorflow:examples/sec: 2.85065\n",
      "I1117 21:59:01.537198 4435920320 tpu_estimator.py:2160] examples/sec: 2.85065\n",
      "INFO:tensorflow:global_step/sec: 0.0888684\n",
      "I1117 21:59:12.789622 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888684\n",
      "INFO:tensorflow:examples/sec: 2.84379\n",
      "I1117 21:59:12.789841 4435920320 tpu_estimator.py:2160] examples/sec: 2.84379\n",
      "INFO:tensorflow:global_step/sec: 0.0895216\n",
      "I1117 21:59:23.960093 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895216\n",
      "INFO:tensorflow:examples/sec: 2.86469\n",
      "I1117 21:59:23.960308 4435920320 tpu_estimator.py:2160] examples/sec: 2.86469\n",
      "INFO:tensorflow:global_step/sec: 0.0891308\n",
      "I1117 21:59:35.179533 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891308\n",
      "INFO:tensorflow:examples/sec: 2.85218\n",
      "I1117 21:59:35.179755 4435920320 tpu_estimator.py:2160] examples/sec: 2.85218\n",
      "INFO:tensorflow:global_step/sec: 0.0892521\n",
      "I1117 21:59:46.383750 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892521\n",
      "INFO:tensorflow:examples/sec: 2.85607\n",
      "I1117 21:59:46.383991 4435920320 tpu_estimator.py:2160] examples/sec: 2.85607\n",
      "INFO:tensorflow:global_step/sec: 0.0894044\n",
      "I1117 21:59:57.568881 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894044\n",
      "INFO:tensorflow:examples/sec: 2.86094\n",
      "I1117 21:59:57.569107 4435920320 tpu_estimator.py:2160] examples/sec: 2.86094\n",
      "INFO:tensorflow:global_step/sec: 0.088816\n",
      "I1117 22:00:08.828135 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088816\n",
      "INFO:tensorflow:examples/sec: 2.84211\n",
      "I1117 22:00:08.828355 4435920320 tpu_estimator.py:2160] examples/sec: 2.84211\n",
      "INFO:tensorflow:global_step/sec: 0.0891699\n",
      "I1117 22:00:20.042648 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891699\n",
      "INFO:tensorflow:examples/sec: 2.85344\n",
      "I1117 22:00:20.042863 4435920320 tpu_estimator.py:2160] examples/sec: 2.85344\n",
      "INFO:tensorflow:global_step/sec: 0.0892463\n",
      "I1117 22:00:31.247612 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892463\n",
      "INFO:tensorflow:examples/sec: 2.85588\n",
      "I1117 22:00:31.247848 4435920320 tpu_estimator.py:2160] examples/sec: 2.85588\n",
      "INFO:tensorflow:global_step/sec: 0.0887881\n",
      "I1117 22:00:42.510384 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887881\n",
      "INFO:tensorflow:examples/sec: 2.84122\n",
      "I1117 22:00:42.510636 4435920320 tpu_estimator.py:2160] examples/sec: 2.84122\n",
      "INFO:tensorflow:global_step/sec: 0.0893711\n",
      "I1117 22:00:53.699677 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893711\n",
      "INFO:tensorflow:examples/sec: 2.85988\n",
      "I1117 22:00:53.699904 4435920320 tpu_estimator.py:2160] examples/sec: 2.85988\n",
      "INFO:tensorflow:global_step/sec: 0.0887704\n",
      "I1117 22:01:04.964714 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887704\n",
      "INFO:tensorflow:examples/sec: 2.84065\n",
      "I1117 22:01:04.964941 4435920320 tpu_estimator.py:2160] examples/sec: 2.84065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0890486\n",
      "I1117 22:01:16.194490 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890486\n",
      "INFO:tensorflow:examples/sec: 2.84956\n",
      "I1117 22:01:16.194705 4435920320 tpu_estimator.py:2160] examples/sec: 2.84956\n",
      "INFO:tensorflow:global_step/sec: 0.0888182\n",
      "I1117 22:01:27.453462 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888182\n",
      "INFO:tensorflow:examples/sec: 2.84218\n",
      "I1117 22:01:27.453680 4435920320 tpu_estimator.py:2160] examples/sec: 2.84218\n",
      "INFO:tensorflow:global_step/sec: 0.0892496\n",
      "I1117 22:01:38.658035 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892496\n",
      "INFO:tensorflow:examples/sec: 2.85599\n",
      "I1117 22:01:38.658257 4435920320 tpu_estimator.py:2160] examples/sec: 2.85599\n",
      "INFO:tensorflow:global_step/sec: 0.0887019\n",
      "I1117 22:01:49.931687 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887019\n",
      "INFO:tensorflow:examples/sec: 2.83846\n",
      "I1117 22:01:49.931905 4435920320 tpu_estimator.py:2160] examples/sec: 2.83846\n",
      "INFO:tensorflow:global_step/sec: 0.0883451\n",
      "I1117 22:02:01.250960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883451\n",
      "INFO:tensorflow:examples/sec: 2.82704\n",
      "I1117 22:02:01.251202 4435920320 tpu_estimator.py:2160] examples/sec: 2.82704\n",
      "INFO:tensorflow:global_step/sec: 0.0884365\n",
      "I1117 22:02:12.558511 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884365\n",
      "INFO:tensorflow:examples/sec: 2.82997\n",
      "I1117 22:02:12.558748 4435920320 tpu_estimator.py:2160] examples/sec: 2.82997\n",
      "INFO:tensorflow:global_step/sec: 0.0893951\n",
      "I1117 22:02:23.744795 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893951\n",
      "INFO:tensorflow:examples/sec: 2.86064\n",
      "I1117 22:02:23.745020 4435920320 tpu_estimator.py:2160] examples/sec: 2.86064\n",
      "INFO:tensorflow:global_step/sec: 0.088761\n",
      "I1117 22:02:35.011042 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088761\n",
      "INFO:tensorflow:examples/sec: 2.84035\n",
      "I1117 22:02:35.011322 4435920320 tpu_estimator.py:2160] examples/sec: 2.84035\n",
      "INFO:tensorflow:global_step/sec: 0.0886095\n",
      "I1117 22:02:46.296494 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886095\n",
      "INFO:tensorflow:examples/sec: 2.8355\n",
      "I1117 22:02:46.296719 4435920320 tpu_estimator.py:2160] examples/sec: 2.8355\n",
      "INFO:tensorflow:global_step/sec: 0.0887826\n",
      "I1117 22:02:57.559957 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887826\n",
      "INFO:tensorflow:examples/sec: 2.84104\n",
      "I1117 22:02:57.560193 4435920320 tpu_estimator.py:2160] examples/sec: 2.84104\n",
      "INFO:tensorflow:global_step/sec: 0.0884564\n",
      "I1117 22:03:08.864969 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884564\n",
      "INFO:tensorflow:examples/sec: 2.8306\n",
      "I1117 22:03:08.865215 4435920320 tpu_estimator.py:2160] examples/sec: 2.8306\n",
      "INFO:tensorflow:global_step/sec: 0.0889371\n",
      "I1117 22:03:20.108892 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889371\n",
      "INFO:tensorflow:examples/sec: 2.84599\n",
      "I1117 22:03:20.109112 4435920320 tpu_estimator.py:2160] examples/sec: 2.84599\n",
      "INFO:tensorflow:global_step/sec: 0.0888313\n",
      "I1117 22:03:31.366149 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888313\n",
      "INFO:tensorflow:examples/sec: 2.8426\n",
      "I1117 22:03:31.366364 4435920320 tpu_estimator.py:2160] examples/sec: 2.8426\n",
      "INFO:tensorflow:global_step/sec: 0.0889964\n",
      "I1117 22:03:42.602556 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889964\n",
      "INFO:tensorflow:examples/sec: 2.84789\n",
      "I1117 22:03:42.602767 4435920320 tpu_estimator.py:2160] examples/sec: 2.84789\n",
      "INFO:tensorflow:global_step/sec: 0.0888571\n",
      "I1117 22:03:53.856585 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888571\n",
      "INFO:tensorflow:examples/sec: 2.84343\n",
      "I1117 22:03:53.856803 4435920320 tpu_estimator.py:2160] examples/sec: 2.84343\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 42 vs previous value: 42. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 22:03:53.857043 4435920320 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 42 vs previous value: 42. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0882844\n",
      "I1117 22:04:05.183631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882844\n",
      "INFO:tensorflow:examples/sec: 2.8251\n",
      "I1117 22:04:05.183846 4435920320 tpu_estimator.py:2160] examples/sec: 2.8251\n",
      "INFO:tensorflow:global_step/sec: 0.0883429\n",
      "I1117 22:04:16.503155 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883429\n",
      "INFO:tensorflow:examples/sec: 2.82697\n",
      "I1117 22:04:16.503384 4435920320 tpu_estimator.py:2160] examples/sec: 2.82697\n",
      "INFO:tensorflow:global_step/sec: 0.0886743\n",
      "I1117 22:04:27.780374 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886743\n",
      "INFO:tensorflow:examples/sec: 2.83758\n",
      "I1117 22:04:27.780592 4435920320 tpu_estimator.py:2160] examples/sec: 2.83758\n",
      "INFO:tensorflow:global_step/sec: 0.0885009\n",
      "I1117 22:04:39.079694 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885009\n",
      "INFO:tensorflow:examples/sec: 2.83203\n",
      "I1117 22:04:39.079919 4435920320 tpu_estimator.py:2160] examples/sec: 2.83203\n",
      "INFO:tensorflow:global_step/sec: 0.088679\n",
      "I1117 22:04:50.356322 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088679\n",
      "INFO:tensorflow:examples/sec: 2.83773\n",
      "I1117 22:04:50.356544 4435920320 tpu_estimator.py:2160] examples/sec: 2.83773\n",
      "INFO:tensorflow:global_step/sec: 0.089076\n",
      "I1117 22:05:01.582697 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089076\n",
      "INFO:tensorflow:examples/sec: 2.85043\n",
      "I1117 22:05:01.582914 4435920320 tpu_estimator.py:2160] examples/sec: 2.85043\n",
      "INFO:tensorflow:global_step/sec: 0.0884594\n",
      "I1117 22:05:12.887313 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884594\n",
      "INFO:tensorflow:examples/sec: 2.8307\n",
      "I1117 22:05:12.887541 4435920320 tpu_estimator.py:2160] examples/sec: 2.8307\n",
      "INFO:tensorflow:global_step/sec: 0.0885116\n",
      "I1117 22:05:24.185292 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885116\n",
      "INFO:tensorflow:examples/sec: 2.83237\n",
      "I1117 22:05:24.185700 4435920320 tpu_estimator.py:2160] examples/sec: 2.83237\n",
      "INFO:tensorflow:global_step/sec: 0.088414\n",
      "I1117 22:05:35.495702 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088414\n",
      "INFO:tensorflow:examples/sec: 2.82925\n",
      "I1117 22:05:35.495934 4435920320 tpu_estimator.py:2160] examples/sec: 2.82925\n",
      "INFO:tensorflow:global_step/sec: 0.0886266\n",
      "I1117 22:05:46.779006 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886266\n",
      "INFO:tensorflow:examples/sec: 2.83605\n",
      "I1117 22:05:46.779236 4435920320 tpu_estimator.py:2160] examples/sec: 2.83605\n",
      "INFO:tensorflow:global_step/sec: 0.0886003\n",
      "I1117 22:05:58.065712 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886003\n",
      "INFO:tensorflow:examples/sec: 2.83521\n",
      "I1117 22:05:58.066032 4435920320 tpu_estimator.py:2160] examples/sec: 2.83521\n",
      "INFO:tensorflow:global_step/sec: 0.0879583\n",
      "I1117 22:06:09.434679 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879583\n",
      "INFO:tensorflow:examples/sec: 2.81467\n",
      "I1117 22:06:09.434896 4435920320 tpu_estimator.py:2160] examples/sec: 2.81467\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 54 vs previous value: 54. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 22:06:09.435036 4435920320 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 54 vs previous value: 54. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0879775\n",
      "I1117 22:06:20.801217 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879775\n",
      "INFO:tensorflow:examples/sec: 2.81528\n",
      "I1117 22:06:20.801456 4435920320 tpu_estimator.py:2160] examples/sec: 2.81528\n",
      "INFO:tensorflow:global_step/sec: 0.0856394\n",
      "I1117 22:06:32.478065 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0856394\n",
      "INFO:tensorflow:examples/sec: 2.74046\n",
      "I1117 22:06:32.478282 4435920320 tpu_estimator.py:2160] examples/sec: 2.74046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0867253\n",
      "I1117 22:06:44.008721 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867253\n",
      "INFO:tensorflow:examples/sec: 2.77521\n",
      "I1117 22:06:44.008939 4435920320 tpu_estimator.py:2160] examples/sec: 2.77521\n",
      "INFO:tensorflow:global_step/sec: 0.086985\n",
      "I1117 22:06:55.504964 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086985\n",
      "INFO:tensorflow:examples/sec: 2.78352\n",
      "I1117 22:06:55.505357 4435920320 tpu_estimator.py:2160] examples/sec: 2.78352\n",
      "INFO:tensorflow:global_step/sec: 0.0878767\n",
      "I1117 22:07:06.884567 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878767\n",
      "INFO:tensorflow:examples/sec: 2.81205\n",
      "I1117 22:07:06.884799 4435920320 tpu_estimator.py:2160] examples/sec: 2.81205\n",
      "INFO:tensorflow:global_step/sec: 0.0880548\n",
      "I1117 22:07:18.241136 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880548\n",
      "INFO:tensorflow:examples/sec: 2.81776\n",
      "I1117 22:07:18.241357 4435920320 tpu_estimator.py:2160] examples/sec: 2.81776\n",
      "INFO:tensorflow:global_step/sec: 0.0883965\n",
      "I1117 22:07:29.553776 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883965\n",
      "INFO:tensorflow:examples/sec: 2.82869\n",
      "I1117 22:07:29.554000 4435920320 tpu_estimator.py:2160] examples/sec: 2.82869\n",
      "INFO:tensorflow:global_step/sec: 0.088021\n",
      "I1117 22:07:40.914699 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088021\n",
      "INFO:tensorflow:examples/sec: 2.81667\n",
      "I1117 22:07:40.914927 4435920320 tpu_estimator.py:2160] examples/sec: 2.81667\n",
      "INFO:tensorflow:global_step/sec: 0.088031\n",
      "I1117 22:07:52.274360 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088031\n",
      "INFO:tensorflow:examples/sec: 2.81699\n",
      "I1117 22:07:52.274583 4435920320 tpu_estimator.py:2160] examples/sec: 2.81699\n",
      "INFO:tensorflow:global_step/sec: 0.0879741\n",
      "I1117 22:08:03.641314 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879741\n",
      "INFO:tensorflow:examples/sec: 2.81517\n",
      "I1117 22:08:03.641533 4435920320 tpu_estimator.py:2160] examples/sec: 2.81517\n",
      "INFO:tensorflow:global_step/sec: 0.0882299\n",
      "I1117 22:08:14.975355 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882299\n",
      "INFO:tensorflow:examples/sec: 2.82336\n",
      "I1117 22:08:14.975584 4435920320 tpu_estimator.py:2160] examples/sec: 2.82336\n",
      "INFO:tensorflow:global_step/sec: 0.0876529\n",
      "I1117 22:08:26.383970 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876529\n",
      "INFO:tensorflow:examples/sec: 2.80489\n",
      "I1117 22:08:26.384192 4435920320 tpu_estimator.py:2160] examples/sec: 2.80489\n",
      "INFO:tensorflow:global_step/sec: 0.0875719\n",
      "I1117 22:08:37.803220 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875719\n",
      "INFO:tensorflow:examples/sec: 2.8023\n",
      "I1117 22:08:37.803457 4435920320 tpu_estimator.py:2160] examples/sec: 2.8023\n",
      "INFO:tensorflow:global_step/sec: 0.0879022\n",
      "I1117 22:08:49.179449 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879022\n",
      "INFO:tensorflow:examples/sec: 2.81287\n",
      "I1117 22:08:49.179674 4435920320 tpu_estimator.py:2160] examples/sec: 2.81287\n",
      "INFO:tensorflow:global_step/sec: 0.0877714\n",
      "I1117 22:09:00.572679 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877714\n",
      "INFO:tensorflow:examples/sec: 2.80868\n",
      "I1117 22:09:00.572898 4435920320 tpu_estimator.py:2160] examples/sec: 2.80868\n",
      "INFO:tensorflow:global_step/sec: 0.0873527\n",
      "I1117 22:09:12.020528 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873527\n",
      "INFO:tensorflow:examples/sec: 2.79529\n",
      "I1117 22:09:12.020748 4435920320 tpu_estimator.py:2160] examples/sec: 2.79529\n",
      "INFO:tensorflow:global_step/sec: 0.0875893\n",
      "I1117 22:09:23.437449 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875893\n",
      "INFO:tensorflow:examples/sec: 2.80286\n",
      "I1117 22:09:23.437680 4435920320 tpu_estimator.py:2160] examples/sec: 2.80286\n",
      "INFO:tensorflow:global_step/sec: 0.0880387\n",
      "I1117 22:09:34.796100 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880387\n",
      "INFO:tensorflow:examples/sec: 2.81724\n",
      "I1117 22:09:34.796343 4435920320 tpu_estimator.py:2160] examples/sec: 2.81724\n",
      "INFO:tensorflow:global_step/sec: 0.08789\n",
      "I1117 22:09:46.173964 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08789\n",
      "INFO:tensorflow:examples/sec: 2.81248\n",
      "I1117 22:09:46.174192 4435920320 tpu_estimator.py:2160] examples/sec: 2.81248\n",
      "INFO:tensorflow:global_step/sec: 0.0879932\n",
      "I1117 22:09:57.538469 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879932\n",
      "INFO:tensorflow:examples/sec: 2.81578\n",
      "I1117 22:09:57.538708 4435920320 tpu_estimator.py:2160] examples/sec: 2.81578\n",
      "INFO:tensorflow:global_step/sec: 0.0868681\n",
      "I1117 22:10:09.050179 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868681\n",
      "INFO:tensorflow:examples/sec: 2.77978\n",
      "I1117 22:10:09.050396 4435920320 tpu_estimator.py:2160] examples/sec: 2.77978\n",
      "INFO:tensorflow:global_step/sec: 0.0866373\n",
      "I1117 22:10:20.592559 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866373\n",
      "INFO:tensorflow:examples/sec: 2.77239\n",
      "I1117 22:10:20.592799 4435920320 tpu_estimator.py:2160] examples/sec: 2.77239\n",
      "INFO:tensorflow:global_step/sec: 0.0878204\n",
      "I1117 22:10:31.979437 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878204\n",
      "INFO:tensorflow:examples/sec: 2.81025\n",
      "I1117 22:10:31.979660 4435920320 tpu_estimator.py:2160] examples/sec: 2.81025\n",
      "INFO:tensorflow:global_step/sec: 0.088293\n",
      "I1117 22:10:43.305349 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088293\n",
      "INFO:tensorflow:examples/sec: 2.82538\n",
      "I1117 22:10:43.305565 4435920320 tpu_estimator.py:2160] examples/sec: 2.82538\n",
      "INFO:tensorflow:global_step/sec: 0.0876652\n",
      "I1117 22:10:54.712396 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876652\n",
      "INFO:tensorflow:examples/sec: 2.80529\n",
      "I1117 22:10:54.712634 4435920320 tpu_estimator.py:2160] examples/sec: 2.80529\n",
      "INFO:tensorflow:global_step/sec: 0.0878921\n",
      "I1117 22:11:06.089965 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878921\n",
      "INFO:tensorflow:examples/sec: 2.81255\n",
      "I1117 22:11:06.090174 4435920320 tpu_estimator.py:2160] examples/sec: 2.81255\n",
      "INFO:tensorflow:global_step/sec: 0.0864778\n",
      "I1117 22:11:17.653623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864778\n",
      "INFO:tensorflow:examples/sec: 2.76729\n",
      "I1117 22:11:17.653851 4435920320 tpu_estimator.py:2160] examples/sec: 2.76729\n",
      "INFO:tensorflow:global_step/sec: 0.0870157\n",
      "I1117 22:11:29.145806 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870157\n",
      "INFO:tensorflow:examples/sec: 2.7845\n",
      "I1117 22:11:29.146024 4435920320 tpu_estimator.py:2160] examples/sec: 2.7845\n",
      "INFO:tensorflow:global_step/sec: 0.0871842\n",
      "I1117 22:11:40.615776 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871842\n",
      "INFO:tensorflow:examples/sec: 2.78989\n",
      "I1117 22:11:40.615998 4435920320 tpu_estimator.py:2160] examples/sec: 2.78989\n",
      "INFO:tensorflow:global_step/sec: 0.0868136\n",
      "I1117 22:11:52.134707 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868136\n",
      "INFO:tensorflow:examples/sec: 2.77804\n",
      "I1117 22:11:52.134927 4435920320 tpu_estimator.py:2160] examples/sec: 2.77804\n",
      "INFO:tensorflow:global_step/sec: 0.08725\n",
      "I1117 22:12:03.596023 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08725\n",
      "INFO:tensorflow:examples/sec: 2.792\n",
      "I1117 22:12:03.596240 4435920320 tpu_estimator.py:2160] examples/sec: 2.792\n",
      "INFO:tensorflow:global_step/sec: 0.0876751\n",
      "I1117 22:12:15.001799 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876751\n",
      "INFO:tensorflow:examples/sec: 2.8056\n",
      "I1117 22:12:15.002012 4435920320 tpu_estimator.py:2160] examples/sec: 2.8056\n",
      "INFO:tensorflow:global_step/sec: 0.0873391\n",
      "I1117 22:12:26.451398 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873391\n",
      "INFO:tensorflow:examples/sec: 2.79485\n",
      "I1117 22:12:26.451623 4435920320 tpu_estimator.py:2160] examples/sec: 2.79485\n",
      "INFO:tensorflow:global_step/sec: 0.0879616\n",
      "I1117 22:12:37.820083 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879616\n",
      "INFO:tensorflow:examples/sec: 2.81477\n",
      "I1117 22:12:37.820572 4435920320 tpu_estimator.py:2160] examples/sec: 2.81477\n",
      "INFO:tensorflow:global_step/sec: 0.0879916\n",
      "I1117 22:12:49.184752 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879916\n",
      "INFO:tensorflow:examples/sec: 2.81573\n",
      "I1117 22:12:49.184983 4435920320 tpu_estimator.py:2160] examples/sec: 2.81573\n",
      "INFO:tensorflow:global_step/sec: 0.0872173\n",
      "I1117 22:13:00.650351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872173\n",
      "INFO:tensorflow:examples/sec: 2.79095\n",
      "I1117 22:13:00.650572 4435920320 tpu_estimator.py:2160] examples/sec: 2.79095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0872661\n",
      "I1117 22:13:12.109545 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872661\n",
      "INFO:tensorflow:examples/sec: 2.79252\n",
      "I1117 22:13:12.109778 4435920320 tpu_estimator.py:2160] examples/sec: 2.79252\n",
      "INFO:tensorflow:global_step/sec: 0.0871993\n",
      "I1117 22:13:23.577530 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871993\n",
      "INFO:tensorflow:examples/sec: 2.79038\n",
      "I1117 22:13:23.577915 4435920320 tpu_estimator.py:2160] examples/sec: 2.79038\n",
      "INFO:tensorflow:global_step/sec: 0.0872438\n",
      "I1117 22:13:35.039644 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872438\n",
      "INFO:tensorflow:examples/sec: 2.7918\n",
      "I1117 22:13:35.039920 4435920320 tpu_estimator.py:2160] examples/sec: 2.7918\n",
      "INFO:tensorflow:global_step/sec: 0.0871144\n",
      "I1117 22:13:46.518823 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871144\n",
      "INFO:tensorflow:examples/sec: 2.78766\n",
      "I1117 22:13:46.519054 4435920320 tpu_estimator.py:2160] examples/sec: 2.78766\n",
      "INFO:tensorflow:global_step/sec: 0.0870904\n",
      "I1117 22:13:58.001125 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870904\n",
      "INFO:tensorflow:examples/sec: 2.78689\n",
      "I1117 22:13:58.001340 4435920320 tpu_estimator.py:2160] examples/sec: 2.78689\n",
      "INFO:tensorflow:global_step/sec: 0.0860016\n",
      "I1117 22:14:09.628847 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860016\n",
      "INFO:tensorflow:examples/sec: 2.75205\n",
      "I1117 22:14:09.629070 4435920320 tpu_estimator.py:2160] examples/sec: 2.75205\n",
      "INFO:tensorflow:global_step/sec: 0.0873738\n",
      "I1117 22:14:21.073917 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873738\n",
      "INFO:tensorflow:examples/sec: 2.79596\n",
      "I1117 22:14:21.074141 4435920320 tpu_estimator.py:2160] examples/sec: 2.79596\n",
      "INFO:tensorflow:global_step/sec: 0.0870358\n",
      "I1117 22:14:32.563412 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870358\n",
      "INFO:tensorflow:examples/sec: 2.78515\n",
      "I1117 22:14:32.563632 4435920320 tpu_estimator.py:2160] examples/sec: 2.78515\n",
      "INFO:tensorflow:global_step/sec: 0.0866647\n",
      "I1117 22:14:44.102112 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866647\n",
      "INFO:tensorflow:examples/sec: 2.77327\n",
      "I1117 22:14:44.102335 4435920320 tpu_estimator.py:2160] examples/sec: 2.77327\n",
      "INFO:tensorflow:global_step/sec: 0.0867173\n",
      "I1117 22:14:55.633850 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867173\n",
      "INFO:tensorflow:examples/sec: 2.77495\n",
      "I1117 22:14:55.634061 4435920320 tpu_estimator.py:2160] examples/sec: 2.77495\n",
      "INFO:tensorflow:global_step/sec: 0.087121\n",
      "I1117 22:15:07.112139 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087121\n",
      "INFO:tensorflow:examples/sec: 2.78787\n",
      "I1117 22:15:07.112365 4435920320 tpu_estimator.py:2160] examples/sec: 2.78787\n",
      "INFO:tensorflow:global_step/sec: 0.0868501\n",
      "I1117 22:15:18.626255 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868501\n",
      "INFO:tensorflow:examples/sec: 2.7792\n",
      "I1117 22:15:18.626497 4435920320 tpu_estimator.py:2160] examples/sec: 2.7792\n",
      "INFO:tensorflow:global_step/sec: 0.0843119\n",
      "I1117 22:15:30.486970 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0843119\n",
      "INFO:tensorflow:examples/sec: 2.69798\n",
      "I1117 22:15:30.487185 4435920320 tpu_estimator.py:2160] examples/sec: 2.69798\n",
      "INFO:tensorflow:global_step/sec: 0.0864974\n",
      "I1117 22:15:42.048002 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864974\n",
      "INFO:tensorflow:examples/sec: 2.76792\n",
      "I1117 22:15:42.048218 4435920320 tpu_estimator.py:2160] examples/sec: 2.76792\n",
      "INFO:tensorflow:global_step/sec: 0.0851153\n",
      "I1117 22:15:53.796795 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0851153\n",
      "INFO:tensorflow:examples/sec: 2.72369\n",
      "I1117 22:15:53.797016 4435920320 tpu_estimator.py:2160] examples/sec: 2.72369\n",
      "INFO:tensorflow:global_step/sec: 0.0833865\n",
      "I1117 22:16:05.789113 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0833865\n",
      "INFO:tensorflow:examples/sec: 2.66837\n",
      "I1117 22:16:05.789330 4435920320 tpu_estimator.py:2160] examples/sec: 2.66837\n",
      "INFO:tensorflow:global_step/sec: 0.0869495\n",
      "I1117 22:16:17.290060 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869495\n",
      "INFO:tensorflow:examples/sec: 2.78238\n",
      "I1117 22:16:17.290279 4435920320 tpu_estimator.py:2160] examples/sec: 2.78238\n",
      "INFO:tensorflow:global_step/sec: 0.0854558\n",
      "I1117 22:16:28.991994 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0854558\n",
      "INFO:tensorflow:examples/sec: 2.73459\n",
      "I1117 22:16:28.992215 4435920320 tpu_estimator.py:2160] examples/sec: 2.73459\n",
      "WARNING:tensorflow:It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 108 vs previous value: 108. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "W1117 22:16:28.992358 4435920320 basic_session_run_hooks.py:724] It seems that global step (tf.train.get_global_step) has not been increased. Current value (could be stable): 108 vs previous value: 108. You could increase the global step by passing tf.train.get_global_step() to Optimizer.apply_gradients or Optimizer.minimize.\n",
      "INFO:tensorflow:global_step/sec: 0.0850885\n",
      "I1117 22:16:40.744456 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0850885\n",
      "INFO:tensorflow:examples/sec: 2.72283\n",
      "I1117 22:16:40.744688 4435920320 tpu_estimator.py:2160] examples/sec: 2.72283\n",
      "INFO:tensorflow:global_step/sec: 0.0855638\n",
      "I1117 22:16:52.431652 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0855638\n",
      "INFO:tensorflow:examples/sec: 2.73804\n",
      "I1117 22:16:52.431888 4435920320 tpu_estimator.py:2160] examples/sec: 2.73804\n",
      "INFO:tensorflow:global_step/sec: 0.0862415\n",
      "I1117 22:17:04.026998 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862415\n",
      "INFO:tensorflow:examples/sec: 2.75973\n",
      "I1117 22:17:04.027230 4435920320 tpu_estimator.py:2160] examples/sec: 2.75973\n",
      "INFO:tensorflow:global_step/sec: 0.0857244\n",
      "I1117 22:17:15.692283 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0857244\n",
      "INFO:tensorflow:examples/sec: 2.74318\n",
      "I1117 22:17:15.692516 4435920320 tpu_estimator.py:2160] examples/sec: 2.74318\n",
      "INFO:tensorflow:global_step/sec: 0.0868726\n",
      "I1117 22:17:27.203402 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868726\n",
      "INFO:tensorflow:examples/sec: 2.77992\n",
      "I1117 22:17:27.203653 4435920320 tpu_estimator.py:2160] examples/sec: 2.77992\n",
      "INFO:tensorflow:global_step/sec: 0.0861164\n",
      "I1117 22:17:38.815603 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861164\n",
      "INFO:tensorflow:examples/sec: 2.75573\n",
      "I1117 22:17:38.815861 4435920320 tpu_estimator.py:2160] examples/sec: 2.75573\n",
      "INFO:tensorflow:global_step/sec: 0.085881\n",
      "I1117 22:17:50.459602 4435920320 tpu_estimator.py:2159] global_step/sec: 0.085881\n",
      "INFO:tensorflow:examples/sec: 2.74819\n",
      "I1117 22:17:50.459857 4435920320 tpu_estimator.py:2160] examples/sec: 2.74819\n",
      "INFO:tensorflow:global_step/sec: 0.0853869\n",
      "I1117 22:18:02.170996 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0853869\n",
      "INFO:tensorflow:examples/sec: 2.73238\n",
      "I1117 22:18:02.171232 4435920320 tpu_estimator.py:2160] examples/sec: 2.73238\n",
      "INFO:tensorflow:global_step/sec: 0.0838807\n",
      "I1117 22:18:14.092681 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0838807\n",
      "INFO:tensorflow:examples/sec: 2.68418\n",
      "I1117 22:18:14.092909 4435920320 tpu_estimator.py:2160] examples/sec: 2.68418\n",
      "INFO:tensorflow:global_step/sec: 0.0864624\n",
      "I1117 22:18:25.658396 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864624\n",
      "INFO:tensorflow:examples/sec: 2.7668\n",
      "I1117 22:18:25.658617 4435920320 tpu_estimator.py:2160] examples/sec: 2.7668\n",
      "INFO:tensorflow:global_step/sec: 0.0865081\n",
      "I1117 22:18:37.218019 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865081\n",
      "INFO:tensorflow:examples/sec: 2.76826\n",
      "I1117 22:18:37.218247 4435920320 tpu_estimator.py:2160] examples/sec: 2.76826\n",
      "INFO:tensorflow:global_step/sec: 0.0865099\n",
      "I1117 22:18:48.777402 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865099\n",
      "INFO:tensorflow:examples/sec: 2.76832\n",
      "I1117 22:18:48.777620 4435920320 tpu_estimator.py:2160] examples/sec: 2.76832\n",
      "INFO:tensorflow:global_step/sec: 0.0866132\n",
      "I1117 22:19:00.322968 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866132\n",
      "INFO:tensorflow:examples/sec: 2.77162\n",
      "I1117 22:19:00.323194 4435920320 tpu_estimator.py:2160] examples/sec: 2.77162\n",
      "INFO:tensorflow:global_step/sec: 0.0846085\n",
      "I1117 22:19:12.142115 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846085\n",
      "INFO:tensorflow:examples/sec: 2.70747\n",
      "I1117 22:19:12.142344 4435920320 tpu_estimator.py:2160] examples/sec: 2.70747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0866376\n",
      "I1117 22:19:23.684448 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866376\n",
      "INFO:tensorflow:examples/sec: 2.7724\n",
      "I1117 22:19:23.684676 4435920320 tpu_estimator.py:2160] examples/sec: 2.7724\n",
      "INFO:tensorflow:global_step/sec: 0.0865048\n",
      "I1117 22:19:35.244512 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865048\n",
      "INFO:tensorflow:examples/sec: 2.76815\n",
      "I1117 22:19:35.244735 4435920320 tpu_estimator.py:2160] examples/sec: 2.76815\n",
      "INFO:tensorflow:global_step/sec: 0.0846226\n",
      "I1117 22:19:47.061671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846226\n",
      "INFO:tensorflow:examples/sec: 2.70792\n",
      "I1117 22:19:47.061895 4435920320 tpu_estimator.py:2160] examples/sec: 2.70792\n",
      "INFO:tensorflow:global_step/sec: 0.0848755\n",
      "I1117 22:19:58.843643 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848755\n",
      "INFO:tensorflow:examples/sec: 2.71602\n",
      "I1117 22:19:58.843868 4435920320 tpu_estimator.py:2160] examples/sec: 2.71602\n",
      "INFO:tensorflow:global_step/sec: 0.0840097\n",
      "I1117 22:20:10.747020 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0840097\n",
      "INFO:tensorflow:examples/sec: 2.68831\n",
      "I1117 22:20:10.747234 4435920320 tpu_estimator.py:2160] examples/sec: 2.68831\n",
      "INFO:tensorflow:global_step/sec: 0.0868015\n",
      "I1117 22:20:22.267570 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868015\n",
      "INFO:tensorflow:examples/sec: 2.77765\n",
      "I1117 22:20:22.267799 4435920320 tpu_estimator.py:2160] examples/sec: 2.77765\n",
      "INFO:tensorflow:global_step/sec: 0.0868422\n",
      "I1117 22:20:33.782701 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868422\n",
      "INFO:tensorflow:examples/sec: 2.77895\n",
      "I1117 22:20:33.782922 4435920320 tpu_estimator.py:2160] examples/sec: 2.77895\n",
      "INFO:tensorflow:global_step/sec: 0.0869401\n",
      "I1117 22:20:45.284888 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869401\n",
      "INFO:tensorflow:examples/sec: 2.78208\n",
      "I1117 22:20:45.285105 4435920320 tpu_estimator.py:2160] examples/sec: 2.78208\n",
      "INFO:tensorflow:global_step/sec: 0.0867423\n",
      "I1117 22:20:56.813317 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867423\n",
      "INFO:tensorflow:examples/sec: 2.77575\n",
      "I1117 22:20:56.813553 4435920320 tpu_estimator.py:2160] examples/sec: 2.77575\n",
      "INFO:tensorflow:global_step/sec: 0.0846941\n",
      "I1117 22:21:08.620494 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846941\n",
      "INFO:tensorflow:examples/sec: 2.71021\n",
      "I1117 22:21:08.620721 4435920320 tpu_estimator.py:2160] examples/sec: 2.71021\n",
      "INFO:tensorflow:global_step/sec: 0.0848543\n",
      "I1117 22:21:20.405402 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848543\n",
      "INFO:tensorflow:examples/sec: 2.71534\n",
      "I1117 22:21:20.405637 4435920320 tpu_estimator.py:2160] examples/sec: 2.71534\n",
      "INFO:tensorflow:global_step/sec: 0.086462\n",
      "I1117 22:21:31.971151 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086462\n",
      "INFO:tensorflow:examples/sec: 2.76678\n",
      "I1117 22:21:31.971373 4435920320 tpu_estimator.py:2160] examples/sec: 2.76678\n",
      "INFO:tensorflow:global_step/sec: 0.0859792\n",
      "I1117 22:21:43.601884 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0859792\n",
      "INFO:tensorflow:examples/sec: 2.75134\n",
      "I1117 22:21:43.602106 4435920320 tpu_estimator.py:2160] examples/sec: 2.75134\n",
      "INFO:tensorflow:global_step/sec: 0.0846615\n",
      "I1117 22:21:55.413631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846615\n",
      "INFO:tensorflow:examples/sec: 2.70917\n",
      "I1117 22:21:55.413846 4435920320 tpu_estimator.py:2160] examples/sec: 2.70917\n",
      "INFO:tensorflow:global_step/sec: 0.0848559\n",
      "I1117 22:22:07.198327 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848559\n",
      "INFO:tensorflow:examples/sec: 2.71539\n",
      "I1117 22:22:07.198559 4435920320 tpu_estimator.py:2160] examples/sec: 2.71539\n",
      "INFO:tensorflow:global_step/sec: 0.0865003\n",
      "I1117 22:22:18.758966 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865003\n",
      "INFO:tensorflow:examples/sec: 2.76801\n",
      "I1117 22:22:18.759183 4435920320 tpu_estimator.py:2160] examples/sec: 2.76801\n",
      "INFO:tensorflow:global_step/sec: 0.0863577\n",
      "I1117 22:22:30.338735 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863577\n",
      "INFO:tensorflow:examples/sec: 2.76345\n",
      "I1117 22:22:30.338970 4435920320 tpu_estimator.py:2160] examples/sec: 2.76345\n",
      "INFO:tensorflow:global_step/sec: 0.0849647\n",
      "I1117 22:22:42.108338 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0849647\n",
      "INFO:tensorflow:examples/sec: 2.71887\n",
      "I1117 22:22:42.108556 4435920320 tpu_estimator.py:2160] examples/sec: 2.71887\n",
      "INFO:tensorflow:global_step/sec: 0.0849096\n",
      "I1117 22:22:53.885548 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0849096\n",
      "INFO:tensorflow:examples/sec: 2.71711\n",
      "I1117 22:22:53.885776 4435920320 tpu_estimator.py:2160] examples/sec: 2.71711\n",
      "INFO:tensorflow:global_step/sec: 0.0863834\n",
      "I1117 22:23:05.461838 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863834\n",
      "INFO:tensorflow:examples/sec: 2.76427\n",
      "I1117 22:23:05.462068 4435920320 tpu_estimator.py:2160] examples/sec: 2.76427\n",
      "INFO:tensorflow:global_step/sec: 0.0848774\n",
      "I1117 22:23:17.243537 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848774\n",
      "INFO:tensorflow:examples/sec: 2.71608\n",
      "I1117 22:23:17.243757 4435920320 tpu_estimator.py:2160] examples/sec: 2.71608\n",
      "INFO:tensorflow:global_step/sec: 0.084943\n",
      "I1117 22:23:29.016131 4435920320 tpu_estimator.py:2159] global_step/sec: 0.084943\n",
      "INFO:tensorflow:examples/sec: 2.71818\n",
      "I1117 22:23:29.016348 4435920320 tpu_estimator.py:2160] examples/sec: 2.71818\n",
      "INFO:tensorflow:global_step/sec: 0.0864522\n",
      "I1117 22:23:40.583221 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864522\n",
      "INFO:tensorflow:examples/sec: 2.76647\n",
      "I1117 22:23:40.583452 4435920320 tpu_estimator.py:2160] examples/sec: 2.76647\n",
      "INFO:tensorflow:global_step/sec: 0.0860952\n",
      "I1117 22:23:52.198286 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860952\n",
      "INFO:tensorflow:examples/sec: 2.75505\n",
      "I1117 22:23:52.198501 4435920320 tpu_estimator.py:2160] examples/sec: 2.75505\n",
      "INFO:tensorflow:global_step/sec: 0.0842971\n",
      "I1117 22:24:04.061082 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0842971\n",
      "INFO:tensorflow:examples/sec: 2.69751\n",
      "I1117 22:24:04.061325 4435920320 tpu_estimator.py:2160] examples/sec: 2.69751\n",
      "INFO:tensorflow:global_step/sec: 0.0846431\n",
      "I1117 22:24:15.875397 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846431\n",
      "INFO:tensorflow:examples/sec: 2.70858\n",
      "I1117 22:24:15.875627 4435920320 tpu_estimator.py:2160] examples/sec: 2.70858\n",
      "INFO:tensorflow:global_step/sec: 0.0864873\n",
      "I1117 22:24:27.437800 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864873\n",
      "INFO:tensorflow:examples/sec: 2.76759\n",
      "I1117 22:24:27.438028 4435920320 tpu_estimator.py:2160] examples/sec: 2.76759\n",
      "INFO:tensorflow:global_step/sec: 0.0862081\n",
      "I1117 22:24:39.037621 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862081\n",
      "INFO:tensorflow:examples/sec: 2.75866\n",
      "I1117 22:24:39.037849 4435920320 tpu_estimator.py:2160] examples/sec: 2.75866\n",
      "INFO:tensorflow:global_step/sec: 0.0848233\n",
      "I1117 22:24:50.826837 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848233\n",
      "INFO:tensorflow:examples/sec: 2.71435\n",
      "I1117 22:24:50.827054 4435920320 tpu_estimator.py:2160] examples/sec: 2.71435\n",
      "INFO:tensorflow:global_step/sec: 0.0846346\n",
      "I1117 22:25:02.642323 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846346\n",
      "INFO:tensorflow:examples/sec: 2.70831\n",
      "I1117 22:25:02.642548 4435920320 tpu_estimator.py:2160] examples/sec: 2.70831\n",
      "INFO:tensorflow:global_step/sec: 0.0863891\n",
      "I1117 22:25:14.217850 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863891\n",
      "INFO:tensorflow:examples/sec: 2.76445\n",
      "I1117 22:25:14.218064 4435920320 tpu_estimator.py:2160] examples/sec: 2.76445\n",
      "INFO:tensorflow:global_step/sec: 0.0849748\n",
      "I1117 22:25:25.986069 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0849748\n",
      "INFO:tensorflow:examples/sec: 2.71919\n",
      "I1117 22:25:25.986319 4435920320 tpu_estimator.py:2160] examples/sec: 2.71919\n",
      "INFO:tensorflow:global_step/sec: 0.0865661\n",
      "I1117 22:25:37.537913 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865661\n",
      "INFO:tensorflow:examples/sec: 2.77012\n",
      "I1117 22:25:37.538137 4435920320 tpu_estimator.py:2160] examples/sec: 2.77012\n",
      "INFO:tensorflow:global_step/sec: 0.0848495\n",
      "I1117 22:25:49.323528 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848495\n",
      "INFO:tensorflow:examples/sec: 2.71518\n",
      "I1117 22:25:49.323762 4435920320 tpu_estimator.py:2160] examples/sec: 2.71518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0846868\n",
      "I1117 22:26:01.131708 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846868\n",
      "INFO:tensorflow:examples/sec: 2.70998\n",
      "I1117 22:26:01.131944 4435920320 tpu_estimator.py:2160] examples/sec: 2.70998\n",
      "INFO:tensorflow:global_step/sec: 0.0865172\n",
      "I1117 22:26:12.690107 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865172\n",
      "INFO:tensorflow:examples/sec: 2.76855\n",
      "I1117 22:26:12.690339 4435920320 tpu_estimator.py:2160] examples/sec: 2.76855\n",
      "INFO:tensorflow:global_step/sec: 0.0851792\n",
      "I1117 22:26:24.430053 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0851792\n",
      "INFO:tensorflow:examples/sec: 2.72574\n",
      "I1117 22:26:24.430287 4435920320 tpu_estimator.py:2160] examples/sec: 2.72574\n",
      "INFO:tensorflow:global_step/sec: 0.0865773\n",
      "I1117 22:26:35.980422 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865773\n",
      "INFO:tensorflow:examples/sec: 2.77047\n",
      "I1117 22:26:35.980653 4435920320 tpu_estimator.py:2160] examples/sec: 2.77047\n",
      "INFO:tensorflow:global_step/sec: 0.0864531\n",
      "I1117 22:26:47.547399 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864531\n",
      "INFO:tensorflow:examples/sec: 2.7665\n",
      "I1117 22:26:47.547616 4435920320 tpu_estimator.py:2160] examples/sec: 2.7665\n",
      "INFO:tensorflow:global_step/sec: 0.083372\n",
      "I1117 22:26:59.541829 4435920320 tpu_estimator.py:2159] global_step/sec: 0.083372\n",
      "INFO:tensorflow:examples/sec: 2.6679\n",
      "I1117 22:26:59.542068 4435920320 tpu_estimator.py:2160] examples/sec: 2.6679\n",
      "INFO:tensorflow:global_step/sec: 0.0778712\n",
      "I1117 22:27:12.383473 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0778712\n",
      "INFO:tensorflow:examples/sec: 2.49188\n",
      "I1117 22:27:12.383628 4435920320 tpu_estimator.py:2160] examples/sec: 2.49188\n",
      "INFO:tensorflow:global_step/sec: 0.082936\n",
      "I1117 22:27:24.440999 4435920320 tpu_estimator.py:2159] global_step/sec: 0.082936\n",
      "INFO:tensorflow:examples/sec: 2.65395\n",
      "I1117 22:27:24.441197 4435920320 tpu_estimator.py:2160] examples/sec: 2.65395\n",
      "INFO:tensorflow:global_step/sec: 0.0847091\n",
      "I1117 22:27:36.246121 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0847091\n",
      "INFO:tensorflow:examples/sec: 2.71069\n",
      "I1117 22:27:36.246332 4435920320 tpu_estimator.py:2160] examples/sec: 2.71069\n",
      "INFO:tensorflow:global_step/sec: 0.0848522\n",
      "I1117 22:27:48.031319 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848522\n",
      "INFO:tensorflow:examples/sec: 2.71527\n",
      "I1117 22:27:48.031536 4435920320 tpu_estimator.py:2160] examples/sec: 2.71527\n",
      "INFO:tensorflow:global_step/sec: 0.0860704\n",
      "I1117 22:27:59.649711 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860704\n",
      "INFO:tensorflow:examples/sec: 2.75425\n",
      "I1117 22:27:59.649913 4435920320 tpu_estimator.py:2160] examples/sec: 2.75425\n",
      "INFO:tensorflow:global_step/sec: 0.085637\n",
      "I1117 22:28:11.326930 4435920320 tpu_estimator.py:2159] global_step/sec: 0.085637\n",
      "INFO:tensorflow:examples/sec: 2.74038\n",
      "I1117 22:28:11.327149 4435920320 tpu_estimator.py:2160] examples/sec: 2.74038\n",
      "INFO:tensorflow:global_step/sec: 0.0850342\n",
      "I1117 22:28:23.086923 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0850342\n",
      "INFO:tensorflow:examples/sec: 2.72109\n",
      "I1117 22:28:23.087142 4435920320 tpu_estimator.py:2160] examples/sec: 2.72109\n",
      "INFO:tensorflow:global_step/sec: 0.0862557\n",
      "I1117 22:28:34.680356 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862557\n",
      "INFO:tensorflow:examples/sec: 2.76018\n",
      "I1117 22:28:34.680594 4435920320 tpu_estimator.py:2160] examples/sec: 2.76018\n",
      "INFO:tensorflow:global_step/sec: 0.0867913\n",
      "I1117 22:28:46.202230 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867913\n",
      "INFO:tensorflow:examples/sec: 2.77732\n",
      "I1117 22:28:46.202455 4435920320 tpu_estimator.py:2160] examples/sec: 2.77732\n",
      "INFO:tensorflow:global_step/sec: 0.0849957\n",
      "I1117 22:28:57.967540 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0849957\n",
      "INFO:tensorflow:examples/sec: 2.71986\n",
      "I1117 22:28:57.967769 4435920320 tpu_estimator.py:2160] examples/sec: 2.71986\n",
      "INFO:tensorflow:global_step/sec: 0.0861098\n",
      "I1117 22:29:09.580627 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861098\n",
      "INFO:tensorflow:examples/sec: 2.75551\n",
      "I1117 22:29:09.580857 4435920320 tpu_estimator.py:2160] examples/sec: 2.75551\n",
      "INFO:tensorflow:global_step/sec: 0.0862223\n",
      "I1117 22:29:21.178560 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862223\n",
      "INFO:tensorflow:examples/sec: 2.75911\n",
      "I1117 22:29:21.178789 4435920320 tpu_estimator.py:2160] examples/sec: 2.75911\n",
      "INFO:tensorflow:global_step/sec: 0.0846593\n",
      "I1117 22:29:32.990648 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0846593\n",
      "INFO:tensorflow:examples/sec: 2.7091\n",
      "I1117 22:29:32.990938 4435920320 tpu_estimator.py:2160] examples/sec: 2.7091\n",
      "INFO:tensorflow:global_step/sec: 0.0866235\n",
      "I1117 22:29:44.534811 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866235\n",
      "INFO:tensorflow:examples/sec: 2.77195\n",
      "I1117 22:29:44.535048 4435920320 tpu_estimator.py:2160] examples/sec: 2.77195\n",
      "INFO:tensorflow:global_step/sec: 0.0865963\n",
      "I1117 22:29:56.082650 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865963\n",
      "INFO:tensorflow:examples/sec: 2.77108\n",
      "I1117 22:29:56.082865 4435920320 tpu_estimator.py:2160] examples/sec: 2.77108\n",
      "INFO:tensorflow:global_step/sec: 0.0848887\n",
      "I1117 22:30:07.862797 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0848887\n",
      "INFO:tensorflow:examples/sec: 2.71644\n",
      "I1117 22:30:07.863011 4435920320 tpu_estimator.py:2160] examples/sec: 2.71644\n",
      "INFO:tensorflow:global_step/sec: 0.0864449\n",
      "I1117 22:30:19.430853 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864449\n",
      "INFO:tensorflow:examples/sec: 2.76624\n",
      "I1117 22:30:19.431061 4435920320 tpu_estimator.py:2160] examples/sec: 2.76624\n",
      "INFO:tensorflow:global_step/sec: 0.0860495\n",
      "I1117 22:30:31.052112 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860495\n",
      "INFO:tensorflow:examples/sec: 2.75358\n",
      "I1117 22:30:31.052375 4435920320 tpu_estimator.py:2160] examples/sec: 2.75358\n",
      "INFO:tensorflow:global_step/sec: 0.0850637\n",
      "I1117 22:30:42.807967 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0850637\n",
      "INFO:tensorflow:examples/sec: 2.72204\n",
      "I1117 22:30:42.808184 4435920320 tpu_estimator.py:2160] examples/sec: 2.72204\n",
      "INFO:tensorflow:global_step/sec: 0.0868839\n",
      "I1117 22:30:54.317584 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868839\n",
      "INFO:tensorflow:examples/sec: 2.78028\n",
      "I1117 22:30:54.317806 4435920320 tpu_estimator.py:2160] examples/sec: 2.78028\n",
      "INFO:tensorflow:global_step/sec: 0.0872605\n",
      "I1117 22:31:05.777537 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872605\n",
      "INFO:tensorflow:examples/sec: 2.79233\n",
      "I1117 22:31:05.777773 4435920320 tpu_estimator.py:2160] examples/sec: 2.79233\n",
      "INFO:tensorflow:global_step/sec: 0.0851249\n",
      "I1117 22:31:17.524965 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0851249\n",
      "INFO:tensorflow:examples/sec: 2.724\n",
      "I1117 22:31:17.525176 4435920320 tpu_estimator.py:2160] examples/sec: 2.724\n",
      "INFO:tensorflow:global_step/sec: 0.0867565\n",
      "I1117 22:31:29.051492 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867565\n",
      "INFO:tensorflow:examples/sec: 2.77621\n",
      "I1117 22:31:29.051713 4435920320 tpu_estimator.py:2160] examples/sec: 2.77621\n",
      "INFO:tensorflow:global_step/sec: 0.0862437\n",
      "I1117 22:31:40.646543 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862437\n",
      "INFO:tensorflow:examples/sec: 2.7598\n",
      "I1117 22:31:40.646764 4435920320 tpu_estimator.py:2160] examples/sec: 2.7598\n",
      "INFO:tensorflow:global_step/sec: 0.0843727\n",
      "I1117 22:31:52.498709 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0843727\n",
      "INFO:tensorflow:examples/sec: 2.69993\n",
      "I1117 22:31:52.498924 4435920320 tpu_estimator.py:2160] examples/sec: 2.69993\n",
      "INFO:tensorflow:global_step/sec: 0.0861205\n",
      "I1117 22:32:04.110343 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861205\n",
      "INFO:tensorflow:examples/sec: 2.75586\n",
      "I1117 22:32:04.110565 4435920320 tpu_estimator.py:2160] examples/sec: 2.75586\n",
      "INFO:tensorflow:global_step/sec: 0.0861995\n",
      "I1117 22:32:15.711352 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861995\n",
      "INFO:tensorflow:examples/sec: 2.75838\n",
      "I1117 22:32:15.711584 4435920320 tpu_estimator.py:2160] examples/sec: 2.75838\n",
      "INFO:tensorflow:global_step/sec: 0.084845\n",
      "I1117 22:32:27.497545 4435920320 tpu_estimator.py:2159] global_step/sec: 0.084845\n",
      "INFO:tensorflow:examples/sec: 2.71504\n",
      "I1117 22:32:27.497774 4435920320 tpu_estimator.py:2160] examples/sec: 2.71504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0866001\n",
      "I1117 22:32:39.044870 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866001\n",
      "INFO:tensorflow:examples/sec: 2.7712\n",
      "I1117 22:32:39.045088 4435920320 tpu_estimator.py:2160] examples/sec: 2.7712\n",
      "INFO:tensorflow:global_step/sec: 0.0863425\n",
      "I1117 22:32:50.626646 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863425\n",
      "INFO:tensorflow:examples/sec: 2.76296\n",
      "I1117 22:32:50.626867 4435920320 tpu_estimator.py:2160] examples/sec: 2.76296\n",
      "INFO:tensorflow:global_step/sec: 0.087089\n",
      "I1117 22:33:02.109160 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087089\n",
      "INFO:tensorflow:examples/sec: 2.78685\n",
      "I1117 22:33:02.109386 4435920320 tpu_estimator.py:2160] examples/sec: 2.78685\n",
      "INFO:tensorflow:global_step/sec: 0.0866794\n",
      "I1117 22:33:13.645923 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866794\n",
      "INFO:tensorflow:examples/sec: 2.77374\n",
      "I1117 22:33:13.646166 4435920320 tpu_estimator.py:2160] examples/sec: 2.77374\n",
      "INFO:tensorflow:global_step/sec: 0.0866925\n",
      "I1117 22:33:25.180965 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866925\n",
      "INFO:tensorflow:examples/sec: 2.77416\n",
      "I1117 22:33:25.181218 4435920320 tpu_estimator.py:2160] examples/sec: 2.77416\n",
      "INFO:tensorflow:global_step/sec: 0.0850065\n",
      "I1117 22:33:36.944788 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0850065\n",
      "INFO:tensorflow:examples/sec: 2.72021\n",
      "I1117 22:33:36.945013 4435920320 tpu_estimator.py:2160] examples/sec: 2.72021\n",
      "INFO:tensorflow:global_step/sec: 0.0862269\n",
      "I1117 22:33:48.542064 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862269\n",
      "INFO:tensorflow:examples/sec: 2.75926\n",
      "I1117 22:33:48.542293 4435920320 tpu_estimator.py:2160] examples/sec: 2.75926\n",
      "INFO:tensorflow:global_step/sec: 0.0871493\n",
      "I1117 22:34:00.016641 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871493\n",
      "INFO:tensorflow:examples/sec: 2.78878\n",
      "I1117 22:34:00.016861 4435920320 tpu_estimator.py:2160] examples/sec: 2.78878\n",
      "INFO:tensorflow:global_step/sec: 0.0860024\n",
      "I1117 22:34:11.644157 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860024\n",
      "INFO:tensorflow:examples/sec: 2.75208\n",
      "I1117 22:34:11.644369 4435920320 tpu_estimator.py:2160] examples/sec: 2.75208\n",
      "INFO:tensorflow:global_step/sec: 0.085392\n",
      "I1117 22:34:23.354915 4435920320 tpu_estimator.py:2159] global_step/sec: 0.085392\n",
      "INFO:tensorflow:examples/sec: 2.73254\n",
      "I1117 22:34:23.355142 4435920320 tpu_estimator.py:2160] examples/sec: 2.73254\n",
      "INFO:tensorflow:global_step/sec: 0.0858876\n",
      "I1117 22:34:34.998035 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0858876\n",
      "INFO:tensorflow:examples/sec: 2.7484\n",
      "I1117 22:34:34.998265 4435920320 tpu_estimator.py:2160] examples/sec: 2.7484\n",
      "INFO:tensorflow:global_step/sec: 0.0867749\n",
      "I1117 22:34:46.522103 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867749\n",
      "INFO:tensorflow:examples/sec: 2.7768\n",
      "I1117 22:34:46.522346 4435920320 tpu_estimator.py:2160] examples/sec: 2.7768\n",
      "INFO:tensorflow:global_step/sec: 0.0850581\n",
      "I1117 22:34:58.278768 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0850581\n",
      "INFO:tensorflow:examples/sec: 2.72186\n",
      "I1117 22:34:58.279184 4435920320 tpu_estimator.py:2160] examples/sec: 2.72186\n",
      "INFO:tensorflow:global_step/sec: 0.0864888\n",
      "I1117 22:35:09.840947 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864888\n",
      "INFO:tensorflow:examples/sec: 2.76764\n",
      "I1117 22:35:09.841164 4435920320 tpu_estimator.py:2160] examples/sec: 2.76764\n",
      "INFO:tensorflow:global_step/sec: 0.0863516\n",
      "I1117 22:35:21.421522 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863516\n",
      "INFO:tensorflow:examples/sec: 2.76325\n",
      "I1117 22:35:21.421765 4435920320 tpu_estimator.py:2160] examples/sec: 2.76325\n",
      "INFO:tensorflow:global_step/sec: 0.0869354\n",
      "I1117 22:35:32.924319 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869354\n",
      "INFO:tensorflow:examples/sec: 2.78193\n",
      "I1117 22:35:32.924556 4435920320 tpu_estimator.py:2160] examples/sec: 2.78193\n",
      "INFO:tensorflow:global_step/sec: 0.0864368\n",
      "I1117 22:35:44.493446 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864368\n",
      "INFO:tensorflow:examples/sec: 2.76598\n",
      "I1117 22:35:44.493664 4435920320 tpu_estimator.py:2160] examples/sec: 2.76598\n",
      "INFO:tensorflow:global_step/sec: 0.0864859\n",
      "I1117 22:35:56.056044 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864859\n",
      "INFO:tensorflow:examples/sec: 2.76755\n",
      "I1117 22:35:56.056261 4435920320 tpu_estimator.py:2160] examples/sec: 2.76755\n",
      "INFO:tensorflow:global_step/sec: 0.0866562\n",
      "I1117 22:36:07.595888 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866562\n",
      "INFO:tensorflow:examples/sec: 2.773\n",
      "I1117 22:36:07.596120 4435920320 tpu_estimator.py:2160] examples/sec: 2.773\n",
      "INFO:tensorflow:global_step/sec: 0.0865931\n",
      "I1117 22:36:19.144138 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865931\n",
      "INFO:tensorflow:examples/sec: 2.77098\n",
      "I1117 22:36:19.144362 4435920320 tpu_estimator.py:2160] examples/sec: 2.77098\n",
      "INFO:tensorflow:global_step/sec: 0.0847916\n",
      "I1117 22:36:30.937744 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0847916\n",
      "INFO:tensorflow:examples/sec: 2.71333\n",
      "I1117 22:36:30.937962 4435920320 tpu_estimator.py:2160] examples/sec: 2.71333\n",
      "INFO:tensorflow:global_step/sec: 0.0867489\n",
      "I1117 22:36:42.465277 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867489\n",
      "INFO:tensorflow:examples/sec: 2.77596\n",
      "I1117 22:36:42.465489 4435920320 tpu_estimator.py:2160] examples/sec: 2.77596\n",
      "INFO:tensorflow:global_step/sec: 0.0857057\n",
      "I1117 22:36:54.133117 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0857057\n",
      "INFO:tensorflow:examples/sec: 2.74258\n",
      "I1117 22:36:54.133336 4435920320 tpu_estimator.py:2160] examples/sec: 2.74258\n",
      "INFO:tensorflow:global_step/sec: 0.0867316\n",
      "I1117 22:37:05.662945 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867316\n",
      "INFO:tensorflow:examples/sec: 2.77541\n",
      "I1117 22:37:05.663166 4435920320 tpu_estimator.py:2160] examples/sec: 2.77541\n",
      "INFO:tensorflow:global_step/sec: 0.0859954\n",
      "I1117 22:37:17.291452 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0859954\n",
      "INFO:tensorflow:examples/sec: 2.75185\n",
      "I1117 22:37:17.291661 4435920320 tpu_estimator.py:2160] examples/sec: 2.75185\n",
      "INFO:tensorflow:global_step/sec: 0.0864772\n",
      "I1117 22:37:28.855221 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864772\n",
      "INFO:tensorflow:examples/sec: 2.76727\n",
      "I1117 22:37:28.855448 4435920320 tpu_estimator.py:2160] examples/sec: 2.76727\n",
      "INFO:tensorflow:global_step/sec: 0.0862704\n",
      "I1117 22:37:40.446671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862704\n",
      "INFO:tensorflow:examples/sec: 2.76065\n",
      "I1117 22:37:40.446884 4435920320 tpu_estimator.py:2160] examples/sec: 2.76065\n",
      "INFO:tensorflow:global_step/sec: 0.0862471\n",
      "I1117 22:37:52.041304 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862471\n",
      "INFO:tensorflow:examples/sec: 2.75991\n",
      "I1117 22:37:52.041538 4435920320 tpu_estimator.py:2160] examples/sec: 2.75991\n",
      "INFO:tensorflow:global_step/sec: 0.0860938\n",
      "I1117 22:38:03.656508 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860938\n",
      "INFO:tensorflow:examples/sec: 2.755\n",
      "I1117 22:38:03.656755 4435920320 tpu_estimator.py:2160] examples/sec: 2.755\n",
      "INFO:tensorflow:global_step/sec: 0.0864151\n",
      "I1117 22:38:15.228569 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864151\n",
      "INFO:tensorflow:examples/sec: 2.76528\n",
      "I1117 22:38:15.228801 4435920320 tpu_estimator.py:2160] examples/sec: 2.76528\n",
      "INFO:tensorflow:global_step/sec: 0.0867815\n",
      "I1117 22:38:26.751752 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867815\n",
      "INFO:tensorflow:examples/sec: 2.77701\n",
      "I1117 22:38:26.751972 4435920320 tpu_estimator.py:2160] examples/sec: 2.77701\n",
      "INFO:tensorflow:global_step/sec: 0.0864581\n",
      "I1117 22:38:38.318062 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864581\n",
      "INFO:tensorflow:examples/sec: 2.76666\n",
      "I1117 22:38:38.318288 4435920320 tpu_estimator.py:2160] examples/sec: 2.76666\n",
      "INFO:tensorflow:global_step/sec: 0.0867554\n",
      "I1117 22:38:49.844702 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867554\n",
      "INFO:tensorflow:examples/sec: 2.77617\n",
      "I1117 22:38:49.844918 4435920320 tpu_estimator.py:2160] examples/sec: 2.77617\n",
      "INFO:tensorflow:global_step/sec: 0.0863723\n",
      "I1117 22:39:01.422479 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863723\n",
      "INFO:tensorflow:examples/sec: 2.76391\n",
      "I1117 22:39:01.422698 4435920320 tpu_estimator.py:2160] examples/sec: 2.76391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0868214\n",
      "I1117 22:39:12.940382 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868214\n",
      "INFO:tensorflow:examples/sec: 2.77829\n",
      "I1117 22:39:12.940601 4435920320 tpu_estimator.py:2160] examples/sec: 2.77829\n",
      "INFO:tensorflow:global_step/sec: 0.0867879\n",
      "I1117 22:39:24.462740 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867879\n",
      "INFO:tensorflow:examples/sec: 2.77721\n",
      "I1117 22:39:24.462961 4435920320 tpu_estimator.py:2160] examples/sec: 2.77721\n",
      "INFO:tensorflow:global_step/sec: 0.0866248\n",
      "I1117 22:39:36.006758 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866248\n",
      "INFO:tensorflow:examples/sec: 2.77199\n",
      "I1117 22:39:36.006977 4435920320 tpu_estimator.py:2160] examples/sec: 2.77199\n",
      "INFO:tensorflow:global_step/sec: 0.0870315\n",
      "I1117 22:39:47.496858 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870315\n",
      "INFO:tensorflow:examples/sec: 2.78501\n",
      "I1117 22:39:47.497093 4435920320 tpu_estimator.py:2160] examples/sec: 2.78501\n",
      "INFO:tensorflow:global_step/sec: 0.0873972\n",
      "I1117 22:39:58.938884 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873972\n",
      "INFO:tensorflow:examples/sec: 2.79671\n",
      "I1117 22:39:58.939116 4435920320 tpu_estimator.py:2160] examples/sec: 2.79671\n",
      "INFO:tensorflow:global_step/sec: 0.0863211\n",
      "I1117 22:40:10.523523 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863211\n",
      "INFO:tensorflow:examples/sec: 2.76228\n",
      "I1117 22:40:10.523741 4435920320 tpu_estimator.py:2160] examples/sec: 2.76228\n",
      "INFO:tensorflow:global_step/sec: 0.0865044\n",
      "I1117 22:40:22.083650 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865044\n",
      "INFO:tensorflow:examples/sec: 2.76814\n",
      "I1117 22:40:22.083886 4435920320 tpu_estimator.py:2160] examples/sec: 2.76814\n",
      "INFO:tensorflow:global_step/sec: 0.0869452\n",
      "I1117 22:40:33.585144 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869452\n",
      "INFO:tensorflow:examples/sec: 2.78225\n",
      "I1117 22:40:33.585376 4435920320 tpu_estimator.py:2160] examples/sec: 2.78225\n",
      "INFO:tensorflow:global_step/sec: 0.0871692\n",
      "I1117 22:40:45.057082 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871692\n",
      "INFO:tensorflow:examples/sec: 2.78941\n",
      "I1117 22:40:45.057312 4435920320 tpu_estimator.py:2160] examples/sec: 2.78941\n",
      "INFO:tensorflow:global_step/sec: 0.0868138\n",
      "I1117 22:40:56.575995 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868138\n",
      "INFO:tensorflow:examples/sec: 2.77804\n",
      "I1117 22:40:56.576228 4435920320 tpu_estimator.py:2160] examples/sec: 2.77804\n",
      "INFO:tensorflow:global_step/sec: 0.0879105\n",
      "I1117 22:41:07.951196 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879105\n",
      "INFO:tensorflow:examples/sec: 2.81314\n",
      "I1117 22:41:07.951421 4435920320 tpu_estimator.py:2160] examples/sec: 2.81314\n",
      "INFO:tensorflow:global_step/sec: 0.0867962\n",
      "I1117 22:41:19.472419 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867962\n",
      "INFO:tensorflow:examples/sec: 2.77748\n",
      "I1117 22:41:19.472644 4435920320 tpu_estimator.py:2160] examples/sec: 2.77748\n",
      "INFO:tensorflow:global_step/sec: 0.086276\n",
      "I1117 22:41:31.063145 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086276\n",
      "INFO:tensorflow:examples/sec: 2.76083\n",
      "I1117 22:41:31.063355 4435920320 tpu_estimator.py:2160] examples/sec: 2.76083\n",
      "INFO:tensorflow:global_step/sec: 0.0866165\n",
      "I1117 22:41:42.608315 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866165\n",
      "INFO:tensorflow:examples/sec: 2.77173\n",
      "I1117 22:41:42.608532 4435920320 tpu_estimator.py:2160] examples/sec: 2.77173\n",
      "INFO:tensorflow:global_step/sec: 0.0867112\n",
      "I1117 22:41:54.140826 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867112\n",
      "INFO:tensorflow:examples/sec: 2.77476\n",
      "I1117 22:41:54.141057 4435920320 tpu_estimator.py:2160] examples/sec: 2.77476\n",
      "INFO:tensorflow:global_step/sec: 0.0862742\n",
      "I1117 22:42:05.731781 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862742\n",
      "INFO:tensorflow:examples/sec: 2.76078\n",
      "I1117 22:42:05.732005 4435920320 tpu_estimator.py:2160] examples/sec: 2.76078\n",
      "INFO:tensorflow:global_step/sec: 0.0878277\n",
      "I1117 22:42:17.117725 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878277\n",
      "INFO:tensorflow:examples/sec: 2.81049\n",
      "I1117 22:42:17.117943 4435920320 tpu_estimator.py:2160] examples/sec: 2.81049\n",
      "INFO:tensorflow:global_step/sec: 0.0867729\n",
      "I1117 22:42:28.642049 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867729\n",
      "INFO:tensorflow:examples/sec: 2.77673\n",
      "I1117 22:42:28.642265 4435920320 tpu_estimator.py:2160] examples/sec: 2.77673\n",
      "INFO:tensorflow:global_step/sec: 0.0870907\n",
      "I1117 22:42:40.124324 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870907\n",
      "INFO:tensorflow:examples/sec: 2.7869\n",
      "I1117 22:42:40.124547 4435920320 tpu_estimator.py:2160] examples/sec: 2.7869\n",
      "INFO:tensorflow:global_step/sec: 0.0866167\n",
      "I1117 22:42:51.669445 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866167\n",
      "INFO:tensorflow:examples/sec: 2.77174\n",
      "I1117 22:42:51.669658 4435920320 tpu_estimator.py:2160] examples/sec: 2.77174\n",
      "INFO:tensorflow:global_step/sec: 0.0866844\n",
      "I1117 22:43:03.205531 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866844\n",
      "INFO:tensorflow:examples/sec: 2.7739\n",
      "I1117 22:43:03.205925 4435920320 tpu_estimator.py:2160] examples/sec: 2.7739\n",
      "INFO:tensorflow:global_step/sec: 0.087695\n",
      "I1117 22:43:14.608695 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087695\n",
      "INFO:tensorflow:examples/sec: 2.80624\n",
      "I1117 22:43:14.608914 4435920320 tpu_estimator.py:2160] examples/sec: 2.80624\n",
      "INFO:tensorflow:global_step/sec: 0.0867925\n",
      "I1117 22:43:26.130426 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867925\n",
      "INFO:tensorflow:examples/sec: 2.77736\n",
      "I1117 22:43:26.130661 4435920320 tpu_estimator.py:2160] examples/sec: 2.77736\n",
      "INFO:tensorflow:global_step/sec: 0.0871182\n",
      "I1117 22:43:37.609091 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871182\n",
      "INFO:tensorflow:examples/sec: 2.78778\n",
      "I1117 22:43:37.609312 4435920320 tpu_estimator.py:2160] examples/sec: 2.78778\n",
      "INFO:tensorflow:global_step/sec: 0.0868618\n",
      "I1117 22:43:49.121628 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868618\n",
      "INFO:tensorflow:examples/sec: 2.77958\n",
      "I1117 22:43:49.121841 4435920320 tpu_estimator.py:2160] examples/sec: 2.77958\n",
      "INFO:tensorflow:global_step/sec: 0.0880152\n",
      "I1117 22:44:00.483307 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880152\n",
      "INFO:tensorflow:examples/sec: 2.81649\n",
      "I1117 22:44:00.483533 4435920320 tpu_estimator.py:2160] examples/sec: 2.81649\n",
      "INFO:tensorflow:global_step/sec: 0.086037\n",
      "I1117 22:44:12.106235 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086037\n",
      "INFO:tensorflow:examples/sec: 2.75318\n",
      "I1117 22:44:12.106455 4435920320 tpu_estimator.py:2160] examples/sec: 2.75318\n",
      "INFO:tensorflow:global_step/sec: 0.0868724\n",
      "I1117 22:44:23.617349 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868724\n",
      "INFO:tensorflow:examples/sec: 2.77992\n",
      "I1117 22:44:23.617568 4435920320 tpu_estimator.py:2160] examples/sec: 2.77992\n",
      "INFO:tensorflow:global_step/sec: 0.0865673\n",
      "I1117 22:44:35.169064 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865673\n",
      "INFO:tensorflow:examples/sec: 2.77015\n",
      "I1117 22:44:35.169295 4435920320 tpu_estimator.py:2160] examples/sec: 2.77015\n",
      "INFO:tensorflow:global_step/sec: 0.0883005\n",
      "I1117 22:44:46.494025 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883005\n",
      "INFO:tensorflow:examples/sec: 2.82562\n",
      "I1117 22:44:46.494257 4435920320 tpu_estimator.py:2160] examples/sec: 2.82562\n",
      "INFO:tensorflow:global_step/sec: 0.086945\n",
      "I1117 22:44:57.995548 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086945\n",
      "INFO:tensorflow:examples/sec: 2.78224\n",
      "I1117 22:44:57.995789 4435920320 tpu_estimator.py:2160] examples/sec: 2.78224\n",
      "INFO:tensorflow:global_step/sec: 0.0860491\n",
      "I1117 22:45:09.616817 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860491\n",
      "INFO:tensorflow:examples/sec: 2.75357\n",
      "I1117 22:45:09.617063 4435920320 tpu_estimator.py:2160] examples/sec: 2.75357\n",
      "INFO:tensorflow:global_step/sec: 0.086637\n",
      "I1117 22:45:21.159229 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086637\n",
      "INFO:tensorflow:examples/sec: 2.77238\n",
      "I1117 22:45:21.159450 4435920320 tpu_estimator.py:2160] examples/sec: 2.77238\n",
      "INFO:tensorflow:global_step/sec: 0.0864126\n",
      "I1117 22:45:32.731605 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864126\n",
      "INFO:tensorflow:examples/sec: 2.7652\n",
      "I1117 22:45:32.731827 4435920320 tpu_estimator.py:2160] examples/sec: 2.7652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0881436\n",
      "I1117 22:45:44.076728 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881436\n",
      "INFO:tensorflow:examples/sec: 2.8206\n",
      "I1117 22:45:44.076946 4435920320 tpu_estimator.py:2160] examples/sec: 2.8206\n",
      "INFO:tensorflow:global_step/sec: 0.0865531\n",
      "I1117 22:45:55.630340 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865531\n",
      "INFO:tensorflow:examples/sec: 2.7697\n",
      "I1117 22:45:55.630576 4435920320 tpu_estimator.py:2160] examples/sec: 2.7697\n",
      "INFO:tensorflow:global_step/sec: 0.0864934\n",
      "I1117 22:46:07.191910 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864934\n",
      "INFO:tensorflow:examples/sec: 2.76779\n",
      "I1117 22:46:07.192133 4435920320 tpu_estimator.py:2160] examples/sec: 2.76779\n",
      "INFO:tensorflow:global_step/sec: 0.0875774\n",
      "I1117 22:46:18.610370 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875774\n",
      "INFO:tensorflow:examples/sec: 2.80248\n",
      "I1117 22:46:18.610594 4435920320 tpu_estimator.py:2160] examples/sec: 2.80248\n",
      "INFO:tensorflow:global_step/sec: 0.0865473\n",
      "I1117 22:46:30.164763 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865473\n",
      "INFO:tensorflow:examples/sec: 2.76951\n",
      "I1117 22:46:30.164994 4435920320 tpu_estimator.py:2160] examples/sec: 2.76951\n",
      "INFO:tensorflow:global_step/sec: 0.0859917\n",
      "I1117 22:46:41.793795 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0859917\n",
      "INFO:tensorflow:examples/sec: 2.75174\n",
      "I1117 22:46:41.794018 4435920320 tpu_estimator.py:2160] examples/sec: 2.75174\n",
      "INFO:tensorflow:global_step/sec: 0.0877108\n",
      "I1117 22:46:53.194896 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877108\n",
      "INFO:tensorflow:examples/sec: 2.80675\n",
      "I1117 22:46:53.195130 4435920320 tpu_estimator.py:2160] examples/sec: 2.80675\n",
      "INFO:tensorflow:global_step/sec: 0.0869398\n",
      "I1117 22:47:04.697095 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869398\n",
      "INFO:tensorflow:examples/sec: 2.78207\n",
      "I1117 22:47:04.697352 4435920320 tpu_estimator.py:2160] examples/sec: 2.78207\n",
      "INFO:tensorflow:global_step/sec: 0.0866402\n",
      "I1117 22:47:16.239077 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866402\n",
      "INFO:tensorflow:examples/sec: 2.77249\n",
      "I1117 22:47:16.239295 4435920320 tpu_estimator.py:2160] examples/sec: 2.77249\n",
      "INFO:tensorflow:global_step/sec: 0.0879647\n",
      "I1117 22:47:27.607275 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879647\n",
      "INFO:tensorflow:examples/sec: 2.81487\n",
      "I1117 22:47:27.607486 4435920320 tpu_estimator.py:2160] examples/sec: 2.81487\n",
      "INFO:tensorflow:global_step/sec: 0.0867552\n",
      "I1117 22:47:39.133960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867552\n",
      "INFO:tensorflow:examples/sec: 2.77617\n",
      "I1117 22:47:39.134179 4435920320 tpu_estimator.py:2160] examples/sec: 2.77617\n",
      "INFO:tensorflow:global_step/sec: 0.0863251\n",
      "I1117 22:47:50.718077 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863251\n",
      "INFO:tensorflow:examples/sec: 2.7624\n",
      "I1117 22:47:50.718298 4435920320 tpu_estimator.py:2160] examples/sec: 2.7624\n",
      "INFO:tensorflow:global_step/sec: 0.0879443\n",
      "I1117 22:48:02.088942 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879443\n",
      "INFO:tensorflow:examples/sec: 2.81422\n",
      "I1117 22:48:02.089171 4435920320 tpu_estimator.py:2160] examples/sec: 2.81422\n",
      "INFO:tensorflow:global_step/sec: 0.0863326\n",
      "I1117 22:48:13.672037 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863326\n",
      "INFO:tensorflow:examples/sec: 2.76264\n",
      "I1117 22:48:13.672273 4435920320 tpu_estimator.py:2160] examples/sec: 2.76264\n",
      "INFO:tensorflow:global_step/sec: 0.0872846\n",
      "I1117 22:48:25.128787 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872846\n",
      "INFO:tensorflow:examples/sec: 2.79311\n",
      "I1117 22:48:25.129008 4435920320 tpu_estimator.py:2160] examples/sec: 2.79311\n",
      "INFO:tensorflow:global_step/sec: 0.0867633\n",
      "I1117 22:48:36.654429 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867633\n",
      "INFO:tensorflow:examples/sec: 2.77643\n",
      "I1117 22:48:36.654647 4435920320 tpu_estimator.py:2160] examples/sec: 2.77643\n",
      "INFO:tensorflow:global_step/sec: 0.0881213\n",
      "I1117 22:48:48.002404 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881213\n",
      "INFO:tensorflow:examples/sec: 2.81988\n",
      "I1117 22:48:48.002627 4435920320 tpu_estimator.py:2160] examples/sec: 2.81988\n",
      "INFO:tensorflow:global_step/sec: 0.0868149\n",
      "I1117 22:48:59.521175 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868149\n",
      "INFO:tensorflow:examples/sec: 2.77808\n",
      "I1117 22:48:59.521403 4435920320 tpu_estimator.py:2160] examples/sec: 2.77808\n",
      "INFO:tensorflow:global_step/sec: 0.0870963\n",
      "I1117 22:49:11.002714 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870963\n",
      "INFO:tensorflow:examples/sec: 2.78708\n",
      "I1117 22:49:11.002938 4435920320 tpu_estimator.py:2160] examples/sec: 2.78708\n",
      "INFO:tensorflow:global_step/sec: 0.0868658\n",
      "I1117 22:49:22.514716 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868658\n",
      "INFO:tensorflow:examples/sec: 2.77971\n",
      "I1117 22:49:22.514930 4435920320 tpu_estimator.py:2160] examples/sec: 2.77971\n",
      "INFO:tensorflow:global_step/sec: 0.0872168\n",
      "I1117 22:49:33.980416 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872168\n",
      "INFO:tensorflow:examples/sec: 2.79094\n",
      "I1117 22:49:33.980643 4435920320 tpu_estimator.py:2160] examples/sec: 2.79094\n",
      "INFO:tensorflow:global_step/sec: 0.0868482\n",
      "I1117 22:49:45.494750 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868482\n",
      "INFO:tensorflow:examples/sec: 2.77914\n",
      "I1117 22:49:45.494970 4435920320 tpu_estimator.py:2160] examples/sec: 2.77914\n",
      "INFO:tensorflow:global_step/sec: 0.0871615\n",
      "I1117 22:49:56.967715 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871615\n",
      "INFO:tensorflow:examples/sec: 2.78917\n",
      "I1117 22:49:56.967941 4435920320 tpu_estimator.py:2160] examples/sec: 2.78917\n",
      "INFO:tensorflow:global_step/sec: 0.0878234\n",
      "I1117 22:50:08.354171 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878234\n",
      "INFO:tensorflow:examples/sec: 2.81035\n",
      "I1117 22:50:08.354387 4435920320 tpu_estimator.py:2160] examples/sec: 2.81035\n",
      "INFO:tensorflow:global_step/sec: 0.0861305\n",
      "I1117 22:50:19.964457 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861305\n",
      "INFO:tensorflow:examples/sec: 2.75618\n",
      "I1117 22:50:19.964670 4435920320 tpu_estimator.py:2160] examples/sec: 2.75618\n",
      "INFO:tensorflow:global_step/sec: 0.0879092\n",
      "I1117 22:50:31.339838 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879092\n",
      "INFO:tensorflow:examples/sec: 2.81309\n",
      "I1117 22:50:31.340054 4435920320 tpu_estimator.py:2160] examples/sec: 2.81309\n",
      "INFO:tensorflow:global_step/sec: 0.0862272\n",
      "I1117 22:50:42.937119 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862272\n",
      "INFO:tensorflow:examples/sec: 2.75927\n",
      "I1117 22:50:42.937335 4435920320 tpu_estimator.py:2160] examples/sec: 2.75927\n",
      "INFO:tensorflow:global_step/sec: 0.0876077\n",
      "I1117 22:50:54.351641 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876077\n",
      "INFO:tensorflow:examples/sec: 2.80345\n",
      "I1117 22:50:54.351866 4435920320 tpu_estimator.py:2160] examples/sec: 2.80345\n",
      "INFO:tensorflow:global_step/sec: 0.0862975\n",
      "I1117 22:51:05.939473 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862975\n",
      "INFO:tensorflow:examples/sec: 2.76152\n",
      "I1117 22:51:05.939702 4435920320 tpu_estimator.py:2160] examples/sec: 2.76152\n",
      "INFO:tensorflow:global_step/sec: 0.0868962\n",
      "I1117 22:51:17.447443 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868962\n",
      "INFO:tensorflow:examples/sec: 2.78068\n",
      "I1117 22:51:17.447664 4435920320 tpu_estimator.py:2160] examples/sec: 2.78068\n",
      "INFO:tensorflow:global_step/sec: 0.0875133\n",
      "I1117 22:51:28.874300 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875133\n",
      "INFO:tensorflow:examples/sec: 2.80043\n",
      "I1117 22:51:28.874537 4435920320 tpu_estimator.py:2160] examples/sec: 2.80043\n",
      "INFO:tensorflow:global_step/sec: 0.0868753\n",
      "I1117 22:51:40.385040 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868753\n",
      "INFO:tensorflow:examples/sec: 2.78001\n",
      "I1117 22:51:40.385262 4435920320 tpu_estimator.py:2160] examples/sec: 2.78001\n",
      "INFO:tensorflow:global_step/sec: 0.0863526\n",
      "I1117 22:51:51.965451 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863526\n",
      "INFO:tensorflow:examples/sec: 2.76328\n",
      "I1117 22:51:51.965670 4435920320 tpu_estimator.py:2160] examples/sec: 2.76328\n",
      "INFO:tensorflow:global_step/sec: 0.0880149\n",
      "I1117 22:52:03.327177 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880149\n",
      "INFO:tensorflow:examples/sec: 2.81648\n",
      "I1117 22:52:03.327404 4435920320 tpu_estimator.py:2160] examples/sec: 2.81648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0863147\n",
      "I1117 22:52:14.912719 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863147\n",
      "INFO:tensorflow:examples/sec: 2.76207\n",
      "I1117 22:52:14.912944 4435920320 tpu_estimator.py:2160] examples/sec: 2.76207\n",
      "INFO:tensorflow:global_step/sec: 0.0869534\n",
      "I1117 22:52:26.413178 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869534\n",
      "INFO:tensorflow:examples/sec: 2.78251\n",
      "I1117 22:52:26.413398 4435920320 tpu_estimator.py:2160] examples/sec: 2.78251\n",
      "INFO:tensorflow:global_step/sec: 0.0881799\n",
      "I1117 22:52:37.753574 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881799\n",
      "INFO:tensorflow:examples/sec: 2.82176\n",
      "I1117 22:52:37.753803 4435920320 tpu_estimator.py:2160] examples/sec: 2.82176\n",
      "INFO:tensorflow:global_step/sec: 0.086349\n",
      "I1117 22:52:49.334490 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086349\n",
      "INFO:tensorflow:examples/sec: 2.76317\n",
      "I1117 22:52:49.334724 4435920320 tpu_estimator.py:2160] examples/sec: 2.76317\n",
      "INFO:tensorflow:global_step/sec: 0.0878764\n",
      "I1117 22:53:00.714095 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878764\n",
      "INFO:tensorflow:examples/sec: 2.81205\n",
      "I1117 22:53:00.714318 4435920320 tpu_estimator.py:2160] examples/sec: 2.81205\n",
      "INFO:tensorflow:global_step/sec: 0.0865409\n",
      "I1117 22:53:12.269330 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865409\n",
      "INFO:tensorflow:examples/sec: 2.76931\n",
      "I1117 22:53:12.269562 4435920320 tpu_estimator.py:2160] examples/sec: 2.76931\n",
      "INFO:tensorflow:global_step/sec: 0.0861432\n",
      "I1117 22:53:23.877898 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861432\n",
      "INFO:tensorflow:examples/sec: 2.75658\n",
      "I1117 22:53:23.878269 4435920320 tpu_estimator.py:2160] examples/sec: 2.75658\n",
      "INFO:tensorflow:global_step/sec: 0.088254\n",
      "I1117 22:53:35.208824 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088254\n",
      "INFO:tensorflow:examples/sec: 2.82413\n",
      "I1117 22:53:35.209040 4435920320 tpu_estimator.py:2160] examples/sec: 2.82413\n",
      "INFO:tensorflow:global_step/sec: 0.0880671\n",
      "I1117 22:53:46.563802 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880671\n",
      "INFO:tensorflow:examples/sec: 2.81815\n",
      "I1117 22:53:46.564029 4435920320 tpu_estimator.py:2160] examples/sec: 2.81815\n",
      "INFO:tensorflow:global_step/sec: 0.0870453\n",
      "I1117 22:53:58.052124 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870453\n",
      "INFO:tensorflow:examples/sec: 2.78545\n",
      "I1117 22:53:58.052552 4435920320 tpu_estimator.py:2160] examples/sec: 2.78545\n",
      "INFO:tensorflow:global_step/sec: 0.0864268\n",
      "I1117 22:54:09.622553 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864268\n",
      "INFO:tensorflow:examples/sec: 2.76566\n",
      "I1117 22:54:09.622767 4435920320 tpu_estimator.py:2160] examples/sec: 2.76566\n",
      "INFO:tensorflow:global_step/sec: 0.0877301\n",
      "I1117 22:54:21.021159 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877301\n",
      "INFO:tensorflow:examples/sec: 2.80736\n",
      "I1117 22:54:21.021396 4435920320 tpu_estimator.py:2160] examples/sec: 2.80736\n",
      "INFO:tensorflow:global_step/sec: 0.0865423\n",
      "I1117 22:54:32.576204 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865423\n",
      "INFO:tensorflow:examples/sec: 2.76935\n",
      "I1117 22:54:32.576430 4435920320 tpu_estimator.py:2160] examples/sec: 2.76935\n",
      "INFO:tensorflow:global_step/sec: 0.0881044\n",
      "I1117 22:54:43.926401 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881044\n",
      "INFO:tensorflow:examples/sec: 2.81934\n",
      "I1117 22:54:43.926621 4435920320 tpu_estimator.py:2160] examples/sec: 2.81934\n",
      "INFO:tensorflow:global_step/sec: 0.0877338\n",
      "I1117 22:54:55.324495 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877338\n",
      "INFO:tensorflow:examples/sec: 2.80748\n",
      "I1117 22:54:55.324726 4435920320 tpu_estimator.py:2160] examples/sec: 2.80748\n",
      "INFO:tensorflow:global_step/sec: 0.0866153\n",
      "I1117 22:55:06.869795 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866153\n",
      "INFO:tensorflow:examples/sec: 2.77169\n",
      "I1117 22:55:06.870025 4435920320 tpu_estimator.py:2160] examples/sec: 2.77169\n",
      "INFO:tensorflow:global_step/sec: 0.0865128\n",
      "I1117 22:55:18.428772 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865128\n",
      "INFO:tensorflow:examples/sec: 2.76841\n",
      "I1117 22:55:18.428992 4435920320 tpu_estimator.py:2160] examples/sec: 2.76841\n",
      "INFO:tensorflow:global_step/sec: 0.088175\n",
      "I1117 22:55:29.769850 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088175\n",
      "INFO:tensorflow:examples/sec: 2.8216\n",
      "I1117 22:55:29.770071 4435920320 tpu_estimator.py:2160] examples/sec: 2.8216\n",
      "INFO:tensorflow:global_step/sec: 0.0880525\n",
      "I1117 22:55:41.126733 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880525\n",
      "INFO:tensorflow:examples/sec: 2.81768\n",
      "I1117 22:55:41.126962 4435920320 tpu_estimator.py:2160] examples/sec: 2.81768\n",
      "INFO:tensorflow:global_step/sec: 0.0866778\n",
      "I1117 22:55:52.663701 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866778\n",
      "INFO:tensorflow:examples/sec: 2.77369\n",
      "I1117 22:55:52.663916 4435920320 tpu_estimator.py:2160] examples/sec: 2.77369\n",
      "INFO:tensorflow:global_step/sec: 0.0866357\n",
      "I1117 22:56:04.206284 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866357\n",
      "INFO:tensorflow:examples/sec: 2.77234\n",
      "I1117 22:56:04.206510 4435920320 tpu_estimator.py:2160] examples/sec: 2.77234\n",
      "INFO:tensorflow:global_step/sec: 0.0878839\n",
      "I1117 22:56:15.584954 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878839\n",
      "INFO:tensorflow:examples/sec: 2.81228\n",
      "I1117 22:56:15.585192 4435920320 tpu_estimator.py:2160] examples/sec: 2.81228\n",
      "INFO:tensorflow:global_step/sec: 0.0877057\n",
      "I1117 22:56:26.986716 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877057\n",
      "INFO:tensorflow:examples/sec: 2.80658\n",
      "I1117 22:56:26.987193 4435920320 tpu_estimator.py:2160] examples/sec: 2.80658\n",
      "INFO:tensorflow:global_step/sec: 0.0869332\n",
      "I1117 22:56:38.489789 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869332\n",
      "INFO:tensorflow:examples/sec: 2.78186\n",
      "I1117 22:56:38.490004 4435920320 tpu_estimator.py:2160] examples/sec: 2.78186\n",
      "INFO:tensorflow:global_step/sec: 0.0865323\n",
      "I1117 22:56:50.046153 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865323\n",
      "INFO:tensorflow:examples/sec: 2.76903\n",
      "I1117 22:56:50.046519 4435920320 tpu_estimator.py:2160] examples/sec: 2.76903\n",
      "INFO:tensorflow:global_step/sec: 0.0879127\n",
      "I1117 22:57:01.421103 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879127\n",
      "INFO:tensorflow:examples/sec: 2.81321\n",
      "I1117 22:57:01.421331 4435920320 tpu_estimator.py:2160] examples/sec: 2.81321\n",
      "INFO:tensorflow:global_step/sec: 0.0871292\n",
      "I1117 22:57:12.898308 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871292\n",
      "INFO:tensorflow:examples/sec: 2.78814\n",
      "I1117 22:57:12.898536 4435920320 tpu_estimator.py:2160] examples/sec: 2.78814\n",
      "INFO:tensorflow:global_step/sec: 0.0871131\n",
      "I1117 22:57:24.377629 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871131\n",
      "INFO:tensorflow:examples/sec: 2.78762\n",
      "I1117 22:57:24.377846 4435920320 tpu_estimator.py:2160] examples/sec: 2.78762\n",
      "INFO:tensorflow:global_step/sec: 0.0869867\n",
      "I1117 22:57:35.873639 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869867\n",
      "INFO:tensorflow:examples/sec: 2.78357\n",
      "I1117 22:57:35.873871 4435920320 tpu_estimator.py:2160] examples/sec: 2.78357\n",
      "INFO:tensorflow:global_step/sec: 0.0881227\n",
      "I1117 22:57:47.221466 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881227\n",
      "INFO:tensorflow:examples/sec: 2.81993\n",
      "I1117 22:57:47.221704 4435920320 tpu_estimator.py:2160] examples/sec: 2.81993\n",
      "INFO:tensorflow:global_step/sec: 0.0881779\n",
      "I1117 22:57:58.562165 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881779\n",
      "INFO:tensorflow:examples/sec: 2.82169\n",
      "I1117 22:57:58.562387 4435920320 tpu_estimator.py:2160] examples/sec: 2.82169\n",
      "INFO:tensorflow:global_step/sec: 0.0859964\n",
      "I1117 22:58:10.190576 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0859964\n",
      "INFO:tensorflow:examples/sec: 2.75189\n",
      "I1117 22:58:10.190805 4435920320 tpu_estimator.py:2160] examples/sec: 2.75189\n",
      "INFO:tensorflow:global_step/sec: 0.0880565\n",
      "I1117 22:58:21.546905 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880565\n",
      "INFO:tensorflow:examples/sec: 2.81781\n",
      "I1117 22:58:21.547130 4435920320 tpu_estimator.py:2160] examples/sec: 2.81781\n",
      "INFO:tensorflow:global_step/sec: 0.0863423\n",
      "I1117 22:58:33.128710 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863423\n",
      "INFO:tensorflow:examples/sec: 2.76295\n",
      "I1117 22:58:33.128927 4435920320 tpu_estimator.py:2160] examples/sec: 2.76295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0879128\n",
      "I1117 22:58:44.503623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879128\n",
      "INFO:tensorflow:examples/sec: 2.81321\n",
      "I1117 22:58:44.503840 4435920320 tpu_estimator.py:2160] examples/sec: 2.81321\n",
      "INFO:tensorflow:global_step/sec: 0.0881401\n",
      "I1117 22:58:55.849188 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881401\n",
      "INFO:tensorflow:examples/sec: 2.82048\n",
      "I1117 22:58:55.849404 4435920320 tpu_estimator.py:2160] examples/sec: 2.82048\n",
      "INFO:tensorflow:global_step/sec: 0.0865341\n",
      "I1117 22:59:07.405328 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865341\n",
      "INFO:tensorflow:examples/sec: 2.76909\n",
      "I1117 22:59:07.405551 4435920320 tpu_estimator.py:2160] examples/sec: 2.76909\n",
      "INFO:tensorflow:global_step/sec: 0.0860105\n",
      "I1117 22:59:19.031844 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860105\n",
      "INFO:tensorflow:examples/sec: 2.75234\n",
      "I1117 22:59:19.032079 4435920320 tpu_estimator.py:2160] examples/sec: 2.75234\n",
      "INFO:tensorflow:global_step/sec: 0.0878231\n",
      "I1117 22:59:30.418375 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878231\n",
      "INFO:tensorflow:examples/sec: 2.81034\n",
      "I1117 22:59:30.418592 4435920320 tpu_estimator.py:2160] examples/sec: 2.81034\n",
      "INFO:tensorflow:global_step/sec: 0.0874112\n",
      "I1117 22:59:41.858544 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874112\n",
      "INFO:tensorflow:examples/sec: 2.79716\n",
      "I1117 22:59:41.858928 4435920320 tpu_estimator.py:2160] examples/sec: 2.79716\n",
      "INFO:tensorflow:global_step/sec: 0.0867342\n",
      "I1117 22:59:53.388021 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867342\n",
      "INFO:tensorflow:examples/sec: 2.77549\n",
      "I1117 22:59:53.388247 4435920320 tpu_estimator.py:2160] examples/sec: 2.77549\n",
      "INFO:tensorflow:global_step/sec: 0.0878779\n",
      "I1117 23:00:04.767446 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878779\n",
      "INFO:tensorflow:examples/sec: 2.81209\n",
      "I1117 23:00:04.767664 4435920320 tpu_estimator.py:2160] examples/sec: 2.81209\n",
      "INFO:tensorflow:global_step/sec: 0.0866945\n",
      "I1117 23:00:16.302217 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866945\n",
      "INFO:tensorflow:examples/sec: 2.77422\n",
      "I1117 23:00:16.302442 4435920320 tpu_estimator.py:2160] examples/sec: 2.77422\n",
      "INFO:tensorflow:global_step/sec: 0.0868425\n",
      "I1117 23:00:27.817294 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868425\n",
      "INFO:tensorflow:examples/sec: 2.77896\n",
      "I1117 23:00:27.817509 4435920320 tpu_estimator.py:2160] examples/sec: 2.77896\n",
      "INFO:tensorflow:global_step/sec: 0.0884173\n",
      "I1117 23:00:39.127309 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884173\n",
      "INFO:tensorflow:examples/sec: 2.82935\n",
      "I1117 23:00:39.127537 4435920320 tpu_estimator.py:2160] examples/sec: 2.82935\n",
      "INFO:tensorflow:global_step/sec: 0.0880134\n",
      "I1117 23:00:50.489214 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880134\n",
      "INFO:tensorflow:examples/sec: 2.81643\n",
      "I1117 23:00:50.489440 4435920320 tpu_estimator.py:2160] examples/sec: 2.81643\n",
      "INFO:tensorflow:global_step/sec: 0.0864092\n",
      "I1117 23:01:02.062048 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864092\n",
      "INFO:tensorflow:examples/sec: 2.7651\n",
      "I1117 23:01:02.062269 4435920320 tpu_estimator.py:2160] examples/sec: 2.7651\n",
      "INFO:tensorflow:global_step/sec: 0.0877964\n",
      "I1117 23:01:13.452042 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877964\n",
      "INFO:tensorflow:examples/sec: 2.80949\n",
      "I1117 23:01:13.452286 4435920320 tpu_estimator.py:2160] examples/sec: 2.80949\n",
      "INFO:tensorflow:global_step/sec: 0.087666\n",
      "I1117 23:01:24.858971 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087666\n",
      "INFO:tensorflow:examples/sec: 2.80531\n",
      "I1117 23:01:24.859205 4435920320 tpu_estimator.py:2160] examples/sec: 2.80531\n",
      "INFO:tensorflow:global_step/sec: 0.0864216\n",
      "I1117 23:01:36.430149 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864216\n",
      "INFO:tensorflow:examples/sec: 2.76549\n",
      "I1117 23:01:36.430383 4435920320 tpu_estimator.py:2160] examples/sec: 2.76549\n",
      "INFO:tensorflow:global_step/sec: 0.0875724\n",
      "I1117 23:01:47.849264 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875724\n",
      "INFO:tensorflow:examples/sec: 2.80232\n",
      "I1117 23:01:47.849669 4435920320 tpu_estimator.py:2160] examples/sec: 2.80232\n",
      "INFO:tensorflow:global_step/sec: 0.086773\n",
      "I1117 23:01:59.373578 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086773\n",
      "INFO:tensorflow:examples/sec: 2.77674\n",
      "I1117 23:01:59.373795 4435920320 tpu_estimator.py:2160] examples/sec: 2.77674\n",
      "INFO:tensorflow:global_step/sec: 0.0864836\n",
      "I1117 23:02:10.936473 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864836\n",
      "INFO:tensorflow:examples/sec: 2.76747\n",
      "I1117 23:02:10.936698 4435920320 tpu_estimator.py:2160] examples/sec: 2.76747\n",
      "INFO:tensorflow:global_step/sec: 0.0877843\n",
      "I1117 23:02:22.328029 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877843\n",
      "INFO:tensorflow:examples/sec: 2.8091\n",
      "I1117 23:02:22.328261 4435920320 tpu_estimator.py:2160] examples/sec: 2.8091\n",
      "INFO:tensorflow:global_step/sec: 0.0877518\n",
      "I1117 23:02:33.723784 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877518\n",
      "INFO:tensorflow:examples/sec: 2.80806\n",
      "I1117 23:02:33.723998 4435920320 tpu_estimator.py:2160] examples/sec: 2.80806\n",
      "INFO:tensorflow:global_step/sec: 0.0863964\n",
      "I1117 23:02:45.298360 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863964\n",
      "INFO:tensorflow:examples/sec: 2.76468\n",
      "I1117 23:02:45.298582 4435920320 tpu_estimator.py:2160] examples/sec: 2.76468\n",
      "INFO:tensorflow:global_step/sec: 0.0881385\n",
      "I1117 23:02:56.644133 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881385\n",
      "INFO:tensorflow:examples/sec: 2.82043\n",
      "I1117 23:02:56.644347 4435920320 tpu_estimator.py:2160] examples/sec: 2.82043\n",
      "INFO:tensorflow:global_step/sec: 0.0877189\n",
      "I1117 23:03:08.044195 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877189\n",
      "INFO:tensorflow:examples/sec: 2.807\n",
      "I1117 23:03:08.044421 4435920320 tpu_estimator.py:2160] examples/sec: 2.807\n",
      "INFO:tensorflow:global_step/sec: 0.0866901\n",
      "I1117 23:03:19.579519 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866901\n",
      "INFO:tensorflow:examples/sec: 2.77408\n",
      "I1117 23:03:19.579732 4435920320 tpu_estimator.py:2160] examples/sec: 2.77408\n",
      "INFO:tensorflow:global_step/sec: 0.0866887\n",
      "I1117 23:03:31.115077 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866887\n",
      "INFO:tensorflow:examples/sec: 2.77404\n",
      "I1117 23:03:31.115307 4435920320 tpu_estimator.py:2160] examples/sec: 2.77404\n",
      "INFO:tensorflow:global_step/sec: 0.0885546\n",
      "I1117 23:03:42.407540 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885546\n",
      "INFO:tensorflow:examples/sec: 2.83375\n",
      "I1117 23:03:42.407774 4435920320 tpu_estimator.py:2160] examples/sec: 2.83375\n",
      "INFO:tensorflow:global_step/sec: 0.0868728\n",
      "I1117 23:03:53.918616 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868728\n",
      "INFO:tensorflow:examples/sec: 2.77993\n",
      "I1117 23:03:53.918832 4435920320 tpu_estimator.py:2160] examples/sec: 2.77993\n",
      "INFO:tensorflow:global_step/sec: 0.0883399\n",
      "I1117 23:04:05.238532 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883399\n",
      "INFO:tensorflow:examples/sec: 2.82688\n",
      "I1117 23:04:05.238764 4435920320 tpu_estimator.py:2160] examples/sec: 2.82688\n",
      "INFO:tensorflow:global_step/sec: 0.0876348\n",
      "I1117 23:04:16.649528 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876348\n",
      "INFO:tensorflow:examples/sec: 2.80432\n",
      "I1117 23:04:16.649759 4435920320 tpu_estimator.py:2160] examples/sec: 2.80432\n",
      "INFO:tensorflow:global_step/sec: 0.087257\n",
      "I1117 23:04:28.109933 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087257\n",
      "INFO:tensorflow:examples/sec: 2.79223\n",
      "I1117 23:04:28.110152 4435920320 tpu_estimator.py:2160] examples/sec: 2.79223\n",
      "INFO:tensorflow:global_step/sec: 0.0878708\n",
      "I1117 23:04:39.490278 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878708\n",
      "INFO:tensorflow:examples/sec: 2.81187\n",
      "I1117 23:04:39.490514 4435920320 tpu_estimator.py:2160] examples/sec: 2.81187\n",
      "INFO:tensorflow:global_step/sec: 0.0881054\n",
      "I1117 23:04:50.840317 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881054\n",
      "INFO:tensorflow:examples/sec: 2.81937\n",
      "I1117 23:04:50.840534 4435920320 tpu_estimator.py:2160] examples/sec: 2.81937\n",
      "INFO:tensorflow:global_step/sec: 0.0867124\n",
      "I1117 23:05:02.372666 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867124\n",
      "INFO:tensorflow:examples/sec: 2.7748\n",
      "I1117 23:05:02.372891 4435920320 tpu_estimator.py:2160] examples/sec: 2.7748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0877856\n",
      "I1117 23:05:13.764065 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877856\n",
      "INFO:tensorflow:examples/sec: 2.80914\n",
      "I1117 23:05:13.764293 4435920320 tpu_estimator.py:2160] examples/sec: 2.80914\n",
      "INFO:tensorflow:global_step/sec: 0.0866207\n",
      "I1117 23:05:25.308645 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866207\n",
      "INFO:tensorflow:examples/sec: 2.77186\n",
      "I1117 23:05:25.308869 4435920320 tpu_estimator.py:2160] examples/sec: 2.77186\n",
      "INFO:tensorflow:global_step/sec: 0.0883225\n",
      "I1117 23:05:36.630764 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883225\n",
      "INFO:tensorflow:examples/sec: 2.82632\n",
      "I1117 23:05:36.630986 4435920320 tpu_estimator.py:2160] examples/sec: 2.82632\n",
      "INFO:tensorflow:global_step/sec: 0.0881129\n",
      "I1117 23:05:47.979840 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881129\n",
      "INFO:tensorflow:examples/sec: 2.81961\n",
      "I1117 23:05:47.980041 4435920320 tpu_estimator.py:2160] examples/sec: 2.81961\n",
      "INFO:tensorflow:global_step/sec: 0.0865782\n",
      "I1117 23:05:59.530127 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865782\n",
      "INFO:tensorflow:examples/sec: 2.7705\n",
      "I1117 23:05:59.530385 4435920320 tpu_estimator.py:2160] examples/sec: 2.7705\n",
      "INFO:tensorflow:global_step/sec: 0.0880761\n",
      "I1117 23:06:10.883919 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880761\n",
      "INFO:tensorflow:examples/sec: 2.81844\n",
      "I1117 23:06:10.884139 4435920320 tpu_estimator.py:2160] examples/sec: 2.81844\n",
      "INFO:tensorflow:global_step/sec: 0.087835\n",
      "I1117 23:06:22.268934 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087835\n",
      "INFO:tensorflow:examples/sec: 2.81072\n",
      "I1117 23:06:22.269164 4435920320 tpu_estimator.py:2160] examples/sec: 2.81072\n",
      "INFO:tensorflow:global_step/sec: 0.0865349\n",
      "I1117 23:06:33.824954 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865349\n",
      "INFO:tensorflow:examples/sec: 2.76912\n",
      "I1117 23:06:33.825186 4435920320 tpu_estimator.py:2160] examples/sec: 2.76912\n",
      "INFO:tensorflow:global_step/sec: 0.0879208\n",
      "I1117 23:06:45.198835 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879208\n",
      "INFO:tensorflow:examples/sec: 2.81347\n",
      "I1117 23:06:45.199064 4435920320 tpu_estimator.py:2160] examples/sec: 2.81347\n",
      "INFO:tensorflow:global_step/sec: 0.086171\n",
      "I1117 23:06:56.803655 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086171\n",
      "INFO:tensorflow:examples/sec: 2.75747\n",
      "I1117 23:06:56.803873 4435920320 tpu_estimator.py:2160] examples/sec: 2.75747\n",
      "INFO:tensorflow:global_step/sec: 0.0879075\n",
      "I1117 23:07:08.179272 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879075\n",
      "INFO:tensorflow:examples/sec: 2.81304\n",
      "I1117 23:07:08.179510 4435920320 tpu_estimator.py:2160] examples/sec: 2.81304\n",
      "INFO:tensorflow:global_step/sec: 0.086431\n",
      "I1117 23:07:19.749176 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086431\n",
      "INFO:tensorflow:examples/sec: 2.76579\n",
      "I1117 23:07:19.749394 4435920320 tpu_estimator.py:2160] examples/sec: 2.76579\n",
      "INFO:tensorflow:global_step/sec: 0.087993\n",
      "I1117 23:07:31.113711 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087993\n",
      "INFO:tensorflow:examples/sec: 2.81578\n",
      "I1117 23:07:31.113948 4435920320 tpu_estimator.py:2160] examples/sec: 2.81578\n",
      "INFO:tensorflow:global_step/sec: 0.0875972\n",
      "I1117 23:07:42.529597 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875972\n",
      "INFO:tensorflow:examples/sec: 2.80311\n",
      "I1117 23:07:42.529810 4435920320 tpu_estimator.py:2160] examples/sec: 2.80311\n",
      "INFO:tensorflow:global_step/sec: 0.0866793\n",
      "I1117 23:07:54.066368 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866793\n",
      "INFO:tensorflow:examples/sec: 2.77374\n",
      "I1117 23:07:54.066599 4435920320 tpu_estimator.py:2160] examples/sec: 2.77374\n",
      "INFO:tensorflow:global_step/sec: 0.0882128\n",
      "I1117 23:08:05.402608 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882128\n",
      "INFO:tensorflow:examples/sec: 2.82281\n",
      "I1117 23:08:05.402840 4435920320 tpu_estimator.py:2160] examples/sec: 2.82281\n",
      "INFO:tensorflow:global_step/sec: 0.0877581\n",
      "I1117 23:08:16.797569 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877581\n",
      "INFO:tensorflow:examples/sec: 2.80826\n",
      "I1117 23:08:16.797797 4435920320 tpu_estimator.py:2160] examples/sec: 2.80826\n",
      "INFO:tensorflow:global_step/sec: 0.0864949\n",
      "I1117 23:08:28.358947 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864949\n",
      "INFO:tensorflow:examples/sec: 2.76784\n",
      "I1117 23:08:28.359183 4435920320 tpu_estimator.py:2160] examples/sec: 2.76784\n",
      "INFO:tensorflow:global_step/sec: 0.0881225\n",
      "I1117 23:08:39.706774 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881225\n",
      "INFO:tensorflow:examples/sec: 2.81992\n",
      "I1117 23:08:39.707005 4435920320 tpu_estimator.py:2160] examples/sec: 2.81992\n",
      "INFO:tensorflow:global_step/sec: 0.0882684\n",
      "I1117 23:08:51.035855 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882684\n",
      "INFO:tensorflow:examples/sec: 2.82459\n",
      "I1117 23:08:51.036087 4435920320 tpu_estimator.py:2160] examples/sec: 2.82459\n",
      "INFO:tensorflow:global_step/sec: 0.086669\n",
      "I1117 23:09:02.574002 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086669\n",
      "INFO:tensorflow:examples/sec: 2.77341\n",
      "I1117 23:09:02.574261 4435920320 tpu_estimator.py:2160] examples/sec: 2.77341\n",
      "INFO:tensorflow:global_step/sec: 0.0875803\n",
      "I1117 23:09:13.992106 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875803\n",
      "INFO:tensorflow:examples/sec: 2.80257\n",
      "I1117 23:09:13.992321 4435920320 tpu_estimator.py:2160] examples/sec: 2.80257\n",
      "INFO:tensorflow:global_step/sec: 0.0879205\n",
      "I1117 23:09:25.366012 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879205\n",
      "INFO:tensorflow:examples/sec: 2.81346\n",
      "I1117 23:09:25.366237 4435920320 tpu_estimator.py:2160] examples/sec: 2.81346\n",
      "INFO:tensorflow:global_step/sec: 0.0868044\n",
      "I1117 23:09:36.886157 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868044\n",
      "INFO:tensorflow:examples/sec: 2.77774\n",
      "I1117 23:09:36.886368 4435920320 tpu_estimator.py:2160] examples/sec: 2.77774\n",
      "INFO:tensorflow:global_step/sec: 0.0882229\n",
      "I1117 23:09:48.221107 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882229\n",
      "INFO:tensorflow:examples/sec: 2.82313\n",
      "I1117 23:09:48.221339 4435920320 tpu_estimator.py:2160] examples/sec: 2.82313\n",
      "INFO:tensorflow:global_step/sec: 0.0878989\n",
      "I1117 23:09:59.597808 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878989\n",
      "INFO:tensorflow:examples/sec: 2.81277\n",
      "I1117 23:09:59.598026 4435920320 tpu_estimator.py:2160] examples/sec: 2.81277\n",
      "INFO:tensorflow:global_step/sec: 0.0861524\n",
      "I1117 23:10:11.205124 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861524\n",
      "INFO:tensorflow:examples/sec: 2.75688\n",
      "I1117 23:10:11.205343 4435920320 tpu_estimator.py:2160] examples/sec: 2.75688\n",
      "INFO:tensorflow:global_step/sec: 0.0880708\n",
      "I1117 23:10:22.559609 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880708\n",
      "INFO:tensorflow:examples/sec: 2.81826\n",
      "I1117 23:10:22.559805 4435920320 tpu_estimator.py:2160] examples/sec: 2.81826\n",
      "INFO:tensorflow:global_step/sec: 0.0866626\n",
      "I1117 23:10:34.098623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866626\n",
      "INFO:tensorflow:examples/sec: 2.7732\n",
      "I1117 23:10:34.098842 4435920320 tpu_estimator.py:2160] examples/sec: 2.7732\n",
      "INFO:tensorflow:global_step/sec: 0.0883313\n",
      "I1117 23:10:45.419650 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883313\n",
      "INFO:tensorflow:examples/sec: 2.8266\n",
      "I1117 23:10:45.419875 4435920320 tpu_estimator.py:2160] examples/sec: 2.8266\n",
      "INFO:tensorflow:global_step/sec: 0.0880087\n",
      "I1117 23:10:56.782158 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880087\n",
      "INFO:tensorflow:examples/sec: 2.81628\n",
      "I1117 23:10:56.782373 4435920320 tpu_estimator.py:2160] examples/sec: 2.81628\n",
      "INFO:tensorflow:global_step/sec: 0.0863336\n",
      "I1117 23:11:08.365157 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863336\n",
      "INFO:tensorflow:examples/sec: 2.76268\n",
      "I1117 23:11:08.365372 4435920320 tpu_estimator.py:2160] examples/sec: 2.76268\n",
      "INFO:tensorflow:global_step/sec: 0.0878765\n",
      "I1117 23:11:19.744745 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878765\n",
      "INFO:tensorflow:examples/sec: 2.81205\n",
      "I1117 23:11:19.744968 4435920320 tpu_estimator.py:2160] examples/sec: 2.81205\n",
      "INFO:tensorflow:global_step/sec: 0.088177\n",
      "I1117 23:11:31.085587 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088177\n",
      "INFO:tensorflow:examples/sec: 2.82166\n",
      "I1117 23:11:31.085814 4435920320 tpu_estimator.py:2160] examples/sec: 2.82166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0873995\n",
      "I1117 23:11:42.527290 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873995\n",
      "INFO:tensorflow:examples/sec: 2.79678\n",
      "I1117 23:11:42.527514 4435920320 tpu_estimator.py:2160] examples/sec: 2.79678\n",
      "INFO:tensorflow:global_step/sec: 0.0866352\n",
      "I1117 23:11:54.069954 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866352\n",
      "INFO:tensorflow:examples/sec: 2.77233\n",
      "I1117 23:11:54.070167 4435920320 tpu_estimator.py:2160] examples/sec: 2.77233\n",
      "INFO:tensorflow:global_step/sec: 0.0885446\n",
      "I1117 23:12:05.363677 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885446\n",
      "INFO:tensorflow:examples/sec: 2.83343\n",
      "I1117 23:12:05.363900 4435920320 tpu_estimator.py:2160] examples/sec: 2.83343\n",
      "INFO:tensorflow:global_step/sec: 0.0874546\n",
      "I1117 23:12:16.798200 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874546\n",
      "INFO:tensorflow:examples/sec: 2.79855\n",
      "I1117 23:12:16.798413 4435920320 tpu_estimator.py:2160] examples/sec: 2.79855\n",
      "INFO:tensorflow:global_step/sec: 0.0866463\n",
      "I1117 23:12:28.339329 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866463\n",
      "INFO:tensorflow:examples/sec: 2.77268\n",
      "I1117 23:12:28.339560 4435920320 tpu_estimator.py:2160] examples/sec: 2.77268\n",
      "INFO:tensorflow:global_step/sec: 0.0881172\n",
      "I1117 23:12:39.687870 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881172\n",
      "INFO:tensorflow:examples/sec: 2.81975\n",
      "I1117 23:12:39.688091 4435920320 tpu_estimator.py:2160] examples/sec: 2.81975\n",
      "INFO:tensorflow:global_step/sec: 0.0876109\n",
      "I1117 23:12:51.101971 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876109\n",
      "INFO:tensorflow:examples/sec: 2.80355\n",
      "I1117 23:12:51.102189 4435920320 tpu_estimator.py:2160] examples/sec: 2.80355\n",
      "INFO:tensorflow:global_step/sec: 0.087844\n",
      "I1117 23:13:02.485784 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087844\n",
      "INFO:tensorflow:examples/sec: 2.81101\n",
      "I1117 23:13:02.486003 4435920320 tpu_estimator.py:2160] examples/sec: 2.81101\n",
      "INFO:tensorflow:global_step/sec: 0.0865631\n",
      "I1117 23:13:14.038071 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865631\n",
      "INFO:tensorflow:examples/sec: 2.77002\n",
      "I1117 23:13:14.038288 4435920320 tpu_estimator.py:2160] examples/sec: 2.77002\n",
      "INFO:tensorflow:global_step/sec: 0.0884104\n",
      "I1117 23:13:25.348962 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884104\n",
      "INFO:tensorflow:examples/sec: 2.82913\n",
      "I1117 23:13:25.349188 4435920320 tpu_estimator.py:2160] examples/sec: 2.82913\n",
      "INFO:tensorflow:global_step/sec: 0.0878364\n",
      "I1117 23:13:36.733776 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878364\n",
      "INFO:tensorflow:examples/sec: 2.81077\n",
      "I1117 23:13:36.734160 4435920320 tpu_estimator.py:2160] examples/sec: 2.81077\n",
      "INFO:tensorflow:global_step/sec: 0.0872585\n",
      "I1117 23:13:48.193963 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872585\n",
      "INFO:tensorflow:examples/sec: 2.79227\n",
      "I1117 23:13:48.194193 4435920320 tpu_estimator.py:2160] examples/sec: 2.79227\n",
      "INFO:tensorflow:global_step/sec: 0.0881033\n",
      "I1117 23:13:59.544249 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881033\n",
      "INFO:tensorflow:examples/sec: 2.81931\n",
      "I1117 23:13:59.544466 4435920320 tpu_estimator.py:2160] examples/sec: 2.81931\n",
      "INFO:tensorflow:global_step/sec: 0.0870708\n",
      "I1117 23:14:11.029165 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870708\n",
      "INFO:tensorflow:examples/sec: 2.78626\n",
      "I1117 23:14:11.029389 4435920320 tpu_estimator.py:2160] examples/sec: 2.78626\n",
      "INFO:tensorflow:global_step/sec: 0.0866222\n",
      "I1117 23:14:22.573544 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866222\n",
      "INFO:tensorflow:examples/sec: 2.77191\n",
      "I1117 23:14:22.573740 4435920320 tpu_estimator.py:2160] examples/sec: 2.77191\n",
      "INFO:tensorflow:global_step/sec: 0.0878058\n",
      "I1117 23:14:33.962306 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878058\n",
      "INFO:tensorflow:examples/sec: 2.80979\n",
      "I1117 23:14:33.962516 4435920320 tpu_estimator.py:2160] examples/sec: 2.80979\n",
      "INFO:tensorflow:global_step/sec: 0.0882658\n",
      "I1117 23:14:45.291737 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882658\n",
      "INFO:tensorflow:examples/sec: 2.8245\n",
      "I1117 23:14:45.291958 4435920320 tpu_estimator.py:2160] examples/sec: 2.8245\n",
      "INFO:tensorflow:global_step/sec: 0.086597\n",
      "I1117 23:14:56.839482 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086597\n",
      "INFO:tensorflow:examples/sec: 2.7711\n",
      "I1117 23:14:56.839695 4435920320 tpu_estimator.py:2160] examples/sec: 2.7711\n",
      "INFO:tensorflow:global_step/sec: 0.0882914\n",
      "I1117 23:15:08.165625 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882914\n",
      "INFO:tensorflow:examples/sec: 2.82532\n",
      "I1117 23:15:08.165853 4435920320 tpu_estimator.py:2160] examples/sec: 2.82532\n",
      "INFO:tensorflow:global_step/sec: 0.0877825\n",
      "I1117 23:15:19.557399 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877825\n",
      "INFO:tensorflow:examples/sec: 2.80904\n",
      "I1117 23:15:19.557610 4435920320 tpu_estimator.py:2160] examples/sec: 2.80904\n",
      "INFO:tensorflow:global_step/sec: 0.0864784\n",
      "I1117 23:15:31.120987 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864784\n",
      "INFO:tensorflow:examples/sec: 2.76731\n",
      "I1117 23:15:31.121205 4435920320 tpu_estimator.py:2160] examples/sec: 2.76731\n",
      "INFO:tensorflow:global_step/sec: 0.0876089\n",
      "I1117 23:15:42.535365 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876089\n",
      "INFO:tensorflow:examples/sec: 2.80349\n",
      "I1117 23:15:42.535589 4435920320 tpu_estimator.py:2160] examples/sec: 2.80349\n",
      "INFO:tensorflow:global_step/sec: 0.0873943\n",
      "I1117 23:15:53.977733 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873943\n",
      "INFO:tensorflow:examples/sec: 2.79662\n",
      "I1117 23:15:53.977962 4435920320 tpu_estimator.py:2160] examples/sec: 2.79662\n",
      "INFO:tensorflow:global_step/sec: 0.0880225\n",
      "I1117 23:16:05.338490 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880225\n",
      "INFO:tensorflow:examples/sec: 2.81672\n",
      "I1117 23:16:05.338707 4435920320 tpu_estimator.py:2160] examples/sec: 2.81672\n",
      "INFO:tensorflow:global_step/sec: 0.0860307\n",
      "I1117 23:16:16.962222 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0860307\n",
      "INFO:tensorflow:examples/sec: 2.75298\n",
      "I1117 23:16:16.962437 4435920320 tpu_estimator.py:2160] examples/sec: 2.75298\n",
      "INFO:tensorflow:global_step/sec: 0.0872842\n",
      "I1117 23:16:28.419147 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872842\n",
      "INFO:tensorflow:examples/sec: 2.79309\n",
      "I1117 23:16:28.419482 4435920320 tpu_estimator.py:2160] examples/sec: 2.79309\n",
      "INFO:tensorflow:global_step/sec: 0.0879221\n",
      "I1117 23:16:39.792758 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879221\n",
      "INFO:tensorflow:examples/sec: 2.81351\n",
      "I1117 23:16:39.792997 4435920320 tpu_estimator.py:2160] examples/sec: 2.81351\n",
      "INFO:tensorflow:global_step/sec: 0.0862994\n",
      "I1117 23:16:51.380348 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862994\n",
      "INFO:tensorflow:examples/sec: 2.76158\n",
      "I1117 23:16:51.380566 4435920320 tpu_estimator.py:2160] examples/sec: 2.76158\n",
      "INFO:tensorflow:global_step/sec: 0.0883369\n",
      "I1117 23:17:02.700631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883369\n",
      "INFO:tensorflow:examples/sec: 2.82678\n",
      "I1117 23:17:02.700853 4435920320 tpu_estimator.py:2160] examples/sec: 2.82678\n",
      "INFO:tensorflow:global_step/sec: 0.0884575\n",
      "I1117 23:17:14.005501 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884575\n",
      "INFO:tensorflow:examples/sec: 2.83064\n",
      "I1117 23:17:14.005727 4435920320 tpu_estimator.py:2160] examples/sec: 2.83064\n",
      "INFO:tensorflow:global_step/sec: 0.0883598\n",
      "I1117 23:17:25.322874 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883598\n",
      "INFO:tensorflow:examples/sec: 2.82751\n",
      "I1117 23:17:25.323101 4435920320 tpu_estimator.py:2160] examples/sec: 2.82751\n",
      "INFO:tensorflow:global_step/sec: 0.0881338\n",
      "I1117 23:17:36.669256 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881338\n",
      "INFO:tensorflow:examples/sec: 2.82028\n",
      "I1117 23:17:36.669488 4435920320 tpu_estimator.py:2160] examples/sec: 2.82028\n",
      "INFO:tensorflow:global_step/sec: 0.0884204\n",
      "I1117 23:17:47.978875 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884204\n",
      "INFO:tensorflow:examples/sec: 2.82945\n",
      "I1117 23:17:47.979118 4435920320 tpu_estimator.py:2160] examples/sec: 2.82945\n",
      "INFO:tensorflow:global_step/sec: 0.0863162\n",
      "I1117 23:17:59.564178 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863162\n",
      "INFO:tensorflow:examples/sec: 2.76212\n",
      "I1117 23:17:59.564425 4435920320 tpu_estimator.py:2160] examples/sec: 2.76212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.088016\n",
      "I1117 23:18:10.925784 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088016\n",
      "INFO:tensorflow:examples/sec: 2.81651\n",
      "I1117 23:18:10.926125 4435920320 tpu_estimator.py:2160] examples/sec: 2.81651\n",
      "INFO:tensorflow:global_step/sec: 0.0877515\n",
      "I1117 23:18:22.321562 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877515\n",
      "INFO:tensorflow:examples/sec: 2.80805\n",
      "I1117 23:18:22.321779 4435920320 tpu_estimator.py:2160] examples/sec: 2.80805\n",
      "INFO:tensorflow:global_step/sec: 0.0879812\n",
      "I1117 23:18:33.687635 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879812\n",
      "INFO:tensorflow:examples/sec: 2.8154\n",
      "I1117 23:18:33.687863 4435920320 tpu_estimator.py:2160] examples/sec: 2.8154\n",
      "INFO:tensorflow:global_step/sec: 0.0879485\n",
      "I1117 23:18:45.057933 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879485\n",
      "INFO:tensorflow:examples/sec: 2.81435\n",
      "I1117 23:18:45.058156 4435920320 tpu_estimator.py:2160] examples/sec: 2.81435\n",
      "INFO:tensorflow:global_step/sec: 0.0878667\n",
      "I1117 23:18:56.438799 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878667\n",
      "INFO:tensorflow:examples/sec: 2.81173\n",
      "I1117 23:18:56.439018 4435920320 tpu_estimator.py:2160] examples/sec: 2.81173\n",
      "INFO:tensorflow:global_step/sec: 0.0868019\n",
      "I1117 23:19:07.959280 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868019\n",
      "INFO:tensorflow:examples/sec: 2.77766\n",
      "I1117 23:19:07.959496 4435920320 tpu_estimator.py:2160] examples/sec: 2.77766\n",
      "INFO:tensorflow:global_step/sec: 0.0879\n",
      "I1117 23:19:19.335873 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879\n",
      "INFO:tensorflow:examples/sec: 2.8128\n",
      "I1117 23:19:19.336096 4435920320 tpu_estimator.py:2160] examples/sec: 2.8128\n",
      "INFO:tensorflow:global_step/sec: 0.0880299\n",
      "I1117 23:19:30.695634 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880299\n",
      "INFO:tensorflow:examples/sec: 2.81696\n",
      "I1117 23:19:30.695863 4435920320 tpu_estimator.py:2160] examples/sec: 2.81696\n",
      "INFO:tensorflow:global_step/sec: 0.0876262\n",
      "I1117 23:19:42.107730 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876262\n",
      "INFO:tensorflow:examples/sec: 2.80404\n",
      "I1117 23:19:42.107949 4435920320 tpu_estimator.py:2160] examples/sec: 2.80404\n",
      "INFO:tensorflow:global_step/sec: 0.0878064\n",
      "I1117 23:19:53.496441 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878064\n",
      "INFO:tensorflow:examples/sec: 2.80981\n",
      "I1117 23:19:53.496652 4435920320 tpu_estimator.py:2160] examples/sec: 2.80981\n",
      "INFO:tensorflow:global_step/sec: 0.0882105\n",
      "I1117 23:20:04.832947 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882105\n",
      "INFO:tensorflow:examples/sec: 2.82274\n",
      "I1117 23:20:04.833177 4435920320 tpu_estimator.py:2160] examples/sec: 2.82274\n",
      "INFO:tensorflow:global_step/sec: 0.087555\n",
      "I1117 23:20:16.254345 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087555\n",
      "INFO:tensorflow:examples/sec: 2.80176\n",
      "I1117 23:20:16.254573 4435920320 tpu_estimator.py:2160] examples/sec: 2.80176\n",
      "INFO:tensorflow:global_step/sec: 0.0867256\n",
      "I1117 23:20:27.784957 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867256\n",
      "INFO:tensorflow:examples/sec: 2.77522\n",
      "I1117 23:20:27.785189 4435920320 tpu_estimator.py:2160] examples/sec: 2.77522\n",
      "INFO:tensorflow:global_step/sec: 0.0880814\n",
      "I1117 23:20:39.138086 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880814\n",
      "INFO:tensorflow:examples/sec: 2.81861\n",
      "I1117 23:20:39.138330 4435920320 tpu_estimator.py:2160] examples/sec: 2.81861\n",
      "INFO:tensorflow:global_step/sec: 0.0878332\n",
      "I1117 23:20:50.523319 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878332\n",
      "INFO:tensorflow:examples/sec: 2.81066\n",
      "I1117 23:20:50.523533 4435920320 tpu_estimator.py:2160] examples/sec: 2.81066\n",
      "INFO:tensorflow:global_step/sec: 0.0876258\n",
      "I1117 23:21:01.935470 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876258\n",
      "INFO:tensorflow:examples/sec: 2.80403\n",
      "I1117 23:21:01.935678 4435920320 tpu_estimator.py:2160] examples/sec: 2.80403\n",
      "INFO:tensorflow:global_step/sec: 0.0880319\n",
      "I1117 23:21:13.294962 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880319\n",
      "INFO:tensorflow:examples/sec: 2.81702\n",
      "I1117 23:21:13.295167 4435920320 tpu_estimator.py:2160] examples/sec: 2.81702\n",
      "INFO:tensorflow:global_step/sec: 0.0878676\n",
      "I1117 23:21:24.675738 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878676\n",
      "INFO:tensorflow:examples/sec: 2.81176\n",
      "I1117 23:21:24.675976 4435920320 tpu_estimator.py:2160] examples/sec: 2.81176\n",
      "INFO:tensorflow:global_step/sec: 0.0881104\n",
      "I1117 23:21:36.025145 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881104\n",
      "INFO:tensorflow:examples/sec: 2.81953\n",
      "I1117 23:21:36.025386 4435920320 tpu_estimator.py:2160] examples/sec: 2.81953\n",
      "INFO:tensorflow:global_step/sec: 0.0882297\n",
      "I1117 23:21:47.359181 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882297\n",
      "INFO:tensorflow:examples/sec: 2.82335\n",
      "I1117 23:21:47.359387 4435920320 tpu_estimator.py:2160] examples/sec: 2.82335\n",
      "INFO:tensorflow:global_step/sec: 0.0877973\n",
      "I1117 23:21:58.749061 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877973\n",
      "INFO:tensorflow:examples/sec: 2.80951\n",
      "I1117 23:21:58.749289 4435920320 tpu_estimator.py:2160] examples/sec: 2.80951\n",
      "INFO:tensorflow:global_step/sec: 0.0879846\n",
      "I1117 23:22:10.114679 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879846\n",
      "INFO:tensorflow:examples/sec: 2.81551\n",
      "I1117 23:22:10.114902 4435920320 tpu_estimator.py:2160] examples/sec: 2.81551\n",
      "INFO:tensorflow:global_step/sec: 0.0861156\n",
      "I1117 23:22:21.726982 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861156\n",
      "INFO:tensorflow:examples/sec: 2.7557\n",
      "I1117 23:22:21.727205 4435920320 tpu_estimator.py:2160] examples/sec: 2.7557\n",
      "INFO:tensorflow:global_step/sec: 0.0880684\n",
      "I1117 23:22:33.081806 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880684\n",
      "INFO:tensorflow:examples/sec: 2.81819\n",
      "I1117 23:22:33.082044 4435920320 tpu_estimator.py:2160] examples/sec: 2.81819\n",
      "INFO:tensorflow:global_step/sec: 0.0879054\n",
      "I1117 23:22:44.457670 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879054\n",
      "INFO:tensorflow:examples/sec: 2.81297\n",
      "I1117 23:22:44.457897 4435920320 tpu_estimator.py:2160] examples/sec: 2.81297\n",
      "INFO:tensorflow:global_step/sec: 0.0895246\n",
      "I1117 23:22:55.627789 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895246\n",
      "INFO:tensorflow:examples/sec: 2.86479\n",
      "I1117 23:22:55.628039 4435920320 tpu_estimator.py:2160] examples/sec: 2.86479\n",
      "INFO:tensorflow:global_step/sec: 0.0880946\n",
      "I1117 23:23:06.979225 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880946\n",
      "INFO:tensorflow:examples/sec: 2.81903\n",
      "I1117 23:23:06.979501 4435920320 tpu_estimator.py:2160] examples/sec: 2.81903\n",
      "INFO:tensorflow:global_step/sec: 0.0880864\n",
      "I1117 23:23:18.331719 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880864\n",
      "INFO:tensorflow:examples/sec: 2.81876\n",
      "I1117 23:23:18.331959 4435920320 tpu_estimator.py:2160] examples/sec: 2.81876\n",
      "INFO:tensorflow:global_step/sec: 0.0871266\n",
      "I1117 23:23:29.809268 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871266\n",
      "INFO:tensorflow:examples/sec: 2.78805\n",
      "I1117 23:23:29.809489 4435920320 tpu_estimator.py:2160] examples/sec: 2.78805\n",
      "INFO:tensorflow:global_step/sec: 0.0877348\n",
      "I1117 23:23:41.207255 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877348\n",
      "INFO:tensorflow:examples/sec: 2.80751\n",
      "I1117 23:23:41.207482 4435920320 tpu_estimator.py:2160] examples/sec: 2.80751\n",
      "INFO:tensorflow:global_step/sec: 0.0882907\n",
      "I1117 23:23:52.533483 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882907\n",
      "INFO:tensorflow:examples/sec: 2.8253\n",
      "I1117 23:23:52.533716 4435920320 tpu_estimator.py:2160] examples/sec: 2.8253\n",
      "INFO:tensorflow:global_step/sec: 0.0884884\n",
      "I1117 23:24:03.834403 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884884\n",
      "INFO:tensorflow:examples/sec: 2.83163\n",
      "I1117 23:24:03.834628 4435920320 tpu_estimator.py:2160] examples/sec: 2.83163\n",
      "INFO:tensorflow:global_step/sec: 0.0866279\n",
      "I1117 23:24:15.378018 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866279\n",
      "INFO:tensorflow:examples/sec: 2.77209\n",
      "I1117 23:24:15.378234 4435920320 tpu_estimator.py:2160] examples/sec: 2.77209\n",
      "INFO:tensorflow:global_step/sec: 0.0875664\n",
      "I1117 23:24:26.797917 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875664\n",
      "INFO:tensorflow:examples/sec: 2.80213\n",
      "I1117 23:24:26.798132 4435920320 tpu_estimator.py:2160] examples/sec: 2.80213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0885293\n",
      "I1117 23:24:38.093621 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885293\n",
      "INFO:tensorflow:examples/sec: 2.83294\n",
      "I1117 23:24:38.093844 4435920320 tpu_estimator.py:2160] examples/sec: 2.83294\n",
      "INFO:tensorflow:global_step/sec: 0.0880095\n",
      "I1117 23:24:49.456048 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880095\n",
      "INFO:tensorflow:examples/sec: 2.8163\n",
      "I1117 23:24:49.456261 4435920320 tpu_estimator.py:2160] examples/sec: 2.8163\n",
      "INFO:tensorflow:global_step/sec: 0.0878567\n",
      "I1117 23:25:00.838230 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878567\n",
      "INFO:tensorflow:examples/sec: 2.81141\n",
      "I1117 23:25:00.838469 4435920320 tpu_estimator.py:2160] examples/sec: 2.81141\n",
      "INFO:tensorflow:global_step/sec: 0.0878407\n",
      "I1117 23:25:12.222440 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878407\n",
      "INFO:tensorflow:examples/sec: 2.8109\n",
      "I1117 23:25:12.222659 4435920320 tpu_estimator.py:2160] examples/sec: 2.8109\n",
      "INFO:tensorflow:global_step/sec: 0.0877603\n",
      "I1117 23:25:23.617114 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877603\n",
      "INFO:tensorflow:examples/sec: 2.80833\n",
      "I1117 23:25:23.617329 4435920320 tpu_estimator.py:2160] examples/sec: 2.80833\n",
      "INFO:tensorflow:global_step/sec: 0.0882507\n",
      "I1117 23:25:34.948475 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882507\n",
      "INFO:tensorflow:examples/sec: 2.82402\n",
      "I1117 23:25:34.948701 4435920320 tpu_estimator.py:2160] examples/sec: 2.82402\n",
      "INFO:tensorflow:global_step/sec: 0.0865017\n",
      "I1117 23:25:46.508935 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865017\n",
      "INFO:tensorflow:examples/sec: 2.76805\n",
      "I1117 23:25:46.509151 4435920320 tpu_estimator.py:2160] examples/sec: 2.76805\n",
      "INFO:tensorflow:global_step/sec: 0.087778\n",
      "I1117 23:25:57.901314 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087778\n",
      "INFO:tensorflow:examples/sec: 2.80889\n",
      "I1117 23:25:57.901540 4435920320 tpu_estimator.py:2160] examples/sec: 2.80889\n",
      "INFO:tensorflow:global_step/sec: 0.088477\n",
      "I1117 23:26:09.203696 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088477\n",
      "INFO:tensorflow:examples/sec: 2.83126\n",
      "I1117 23:26:09.203938 4435920320 tpu_estimator.py:2160] examples/sec: 2.83126\n",
      "INFO:tensorflow:global_step/sec: 0.0879694\n",
      "I1117 23:26:20.571300 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879694\n",
      "INFO:tensorflow:examples/sec: 2.81502\n",
      "I1117 23:26:20.571541 4435920320 tpu_estimator.py:2160] examples/sec: 2.81502\n",
      "INFO:tensorflow:global_step/sec: 0.0879688\n",
      "I1117 23:26:31.938952 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879688\n",
      "INFO:tensorflow:examples/sec: 2.815\n",
      "I1117 23:26:31.939177 4435920320 tpu_estimator.py:2160] examples/sec: 2.815\n",
      "INFO:tensorflow:global_step/sec: 0.0863345\n",
      "I1117 23:26:43.521793 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863345\n",
      "INFO:tensorflow:examples/sec: 2.76271\n",
      "I1117 23:26:43.522010 4435920320 tpu_estimator.py:2160] examples/sec: 2.76271\n",
      "INFO:tensorflow:global_step/sec: 0.0883922\n",
      "I1117 23:26:54.835011 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883922\n",
      "INFO:tensorflow:examples/sec: 2.82855\n",
      "I1117 23:26:54.835225 4435920320 tpu_estimator.py:2160] examples/sec: 2.82855\n",
      "INFO:tensorflow:global_step/sec: 0.0877903\n",
      "I1117 23:27:06.225790 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877903\n",
      "INFO:tensorflow:examples/sec: 2.80929\n",
      "I1117 23:27:06.226019 4435920320 tpu_estimator.py:2160] examples/sec: 2.80929\n",
      "INFO:tensorflow:global_step/sec: 0.0883906\n",
      "I1117 23:27:17.539221 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883906\n",
      "INFO:tensorflow:examples/sec: 2.8285\n",
      "I1117 23:27:17.539450 4435920320 tpu_estimator.py:2160] examples/sec: 2.8285\n",
      "INFO:tensorflow:global_step/sec: 0.0882895\n",
      "I1117 23:27:28.865592 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882895\n",
      "INFO:tensorflow:examples/sec: 2.82526\n",
      "I1117 23:27:28.865840 4435920320 tpu_estimator.py:2160] examples/sec: 2.82526\n",
      "INFO:tensorflow:global_step/sec: 0.0880729\n",
      "I1117 23:27:40.219823 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880729\n",
      "INFO:tensorflow:examples/sec: 2.81833\n",
      "I1117 23:27:40.220069 4435920320 tpu_estimator.py:2160] examples/sec: 2.81833\n",
      "INFO:tensorflow:global_step/sec: 0.0879163\n",
      "I1117 23:27:51.594277 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879163\n",
      "INFO:tensorflow:examples/sec: 2.81332\n",
      "I1117 23:27:51.594496 4435920320 tpu_estimator.py:2160] examples/sec: 2.81332\n",
      "INFO:tensorflow:global_step/sec: 0.0877565\n",
      "I1117 23:28:02.989439 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877565\n",
      "INFO:tensorflow:examples/sec: 2.80821\n",
      "I1117 23:28:02.989671 4435920320 tpu_estimator.py:2160] examples/sec: 2.80821\n",
      "INFO:tensorflow:global_step/sec: 0.0872725\n",
      "I1117 23:28:14.447818 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872725\n",
      "INFO:tensorflow:examples/sec: 2.79272\n",
      "I1117 23:28:14.448031 4435920320 tpu_estimator.py:2160] examples/sec: 2.79272\n",
      "INFO:tensorflow:global_step/sec: 0.0882419\n",
      "I1117 23:28:25.780295 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882419\n",
      "INFO:tensorflow:examples/sec: 2.82374\n",
      "I1117 23:28:25.780520 4435920320 tpu_estimator.py:2160] examples/sec: 2.82374\n",
      "INFO:tensorflow:global_step/sec: 0.0882264\n",
      "I1117 23:28:37.114786 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882264\n",
      "INFO:tensorflow:examples/sec: 2.82325\n",
      "I1117 23:28:37.115012 4435920320 tpu_estimator.py:2160] examples/sec: 2.82325\n",
      "INFO:tensorflow:global_step/sec: 0.0881815\n",
      "I1117 23:28:48.455024 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881815\n",
      "INFO:tensorflow:examples/sec: 2.82181\n",
      "I1117 23:28:48.455236 4435920320 tpu_estimator.py:2160] examples/sec: 2.82181\n",
      "INFO:tensorflow:global_step/sec: 0.0867346\n",
      "I1117 23:28:59.984467 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0867346\n",
      "INFO:tensorflow:examples/sec: 2.77551\n",
      "I1117 23:28:59.984699 4435920320 tpu_estimator.py:2160] examples/sec: 2.77551\n",
      "INFO:tensorflow:global_step/sec: 0.0877754\n",
      "I1117 23:29:11.377192 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877754\n",
      "INFO:tensorflow:examples/sec: 2.80881\n",
      "I1117 23:29:11.377411 4435920320 tpu_estimator.py:2160] examples/sec: 2.80881\n",
      "INFO:tensorflow:global_step/sec: 0.0882964\n",
      "I1117 23:29:22.702676 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882964\n",
      "INFO:tensorflow:examples/sec: 2.82548\n",
      "I1117 23:29:22.702900 4435920320 tpu_estimator.py:2160] examples/sec: 2.82548\n",
      "INFO:tensorflow:global_step/sec: 0.0869454\n",
      "I1117 23:29:34.204148 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869454\n",
      "INFO:tensorflow:examples/sec: 2.78225\n",
      "I1117 23:29:34.204380 4435920320 tpu_estimator.py:2160] examples/sec: 2.78225\n",
      "INFO:tensorflow:global_step/sec: 0.0875952\n",
      "I1117 23:29:45.620285 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875952\n",
      "INFO:tensorflow:examples/sec: 2.80305\n",
      "I1117 23:29:45.620502 4435920320 tpu_estimator.py:2160] examples/sec: 2.80305\n",
      "INFO:tensorflow:global_step/sec: 0.0876254\n",
      "I1117 23:29:57.032513 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876254\n",
      "INFO:tensorflow:examples/sec: 2.80401\n",
      "I1117 23:29:57.032736 4435920320 tpu_estimator.py:2160] examples/sec: 2.80401\n",
      "INFO:tensorflow:global_step/sec: 0.0882266\n",
      "I1117 23:30:08.366966 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882266\n",
      "INFO:tensorflow:examples/sec: 2.82325\n",
      "I1117 23:30:08.367187 4435920320 tpu_estimator.py:2160] examples/sec: 2.82325\n",
      "INFO:tensorflow:global_step/sec: 0.0866363\n",
      "I1117 23:30:19.909475 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866363\n",
      "INFO:tensorflow:examples/sec: 2.77236\n",
      "I1117 23:30:19.909842 4435920320 tpu_estimator.py:2160] examples/sec: 2.77236\n",
      "INFO:tensorflow:global_step/sec: 0.0882296\n",
      "I1117 23:30:31.243540 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882296\n",
      "INFO:tensorflow:examples/sec: 2.82335\n",
      "I1117 23:30:31.243760 4435920320 tpu_estimator.py:2160] examples/sec: 2.82335\n",
      "INFO:tensorflow:global_step/sec: 0.0878499\n",
      "I1117 23:30:42.626587 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878499\n",
      "INFO:tensorflow:examples/sec: 2.8112\n",
      "I1117 23:30:42.626803 4435920320 tpu_estimator.py:2160] examples/sec: 2.8112\n",
      "INFO:tensorflow:global_step/sec: 0.0876814\n",
      "I1117 23:30:54.031545 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876814\n",
      "INFO:tensorflow:examples/sec: 2.8058\n",
      "I1117 23:30:54.031769 4435920320 tpu_estimator.py:2160] examples/sec: 2.8058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0864869\n",
      "I1117 23:31:05.593978 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0864869\n",
      "INFO:tensorflow:examples/sec: 2.76758\n",
      "I1117 23:31:05.594192 4435920320 tpu_estimator.py:2160] examples/sec: 2.76758\n",
      "INFO:tensorflow:global_step/sec: 0.0891854\n",
      "I1117 23:31:16.806573 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891854\n",
      "INFO:tensorflow:examples/sec: 2.85393\n",
      "I1117 23:31:16.806806 4435920320 tpu_estimator.py:2160] examples/sec: 2.85393\n",
      "INFO:tensorflow:global_step/sec: 0.0872421\n",
      "I1117 23:31:28.268922 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872421\n",
      "INFO:tensorflow:examples/sec: 2.79175\n",
      "I1117 23:31:28.269235 4435920320 tpu_estimator.py:2160] examples/sec: 2.79175\n",
      "INFO:tensorflow:global_step/sec: 0.0882358\n",
      "I1117 23:31:39.602186 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882358\n",
      "INFO:tensorflow:examples/sec: 2.82355\n",
      "I1117 23:31:39.602410 4435920320 tpu_estimator.py:2160] examples/sec: 2.82355\n",
      "INFO:tensorflow:global_step/sec: 0.0880034\n",
      "I1117 23:31:50.965403 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880034\n",
      "INFO:tensorflow:examples/sec: 2.81611\n",
      "I1117 23:31:50.965809 4435920320 tpu_estimator.py:2160] examples/sec: 2.81611\n",
      "INFO:tensorflow:global_step/sec: 0.0878692\n",
      "I1117 23:32:02.345920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878692\n",
      "INFO:tensorflow:examples/sec: 2.81181\n",
      "I1117 23:32:02.346139 4435920320 tpu_estimator.py:2160] examples/sec: 2.81181\n",
      "INFO:tensorflow:global_step/sec: 0.0881856\n",
      "I1117 23:32:13.685648 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881856\n",
      "INFO:tensorflow:examples/sec: 2.82194\n",
      "I1117 23:32:13.685891 4435920320 tpu_estimator.py:2160] examples/sec: 2.82194\n",
      "INFO:tensorflow:global_step/sec: 0.087841\n",
      "I1117 23:32:25.069847 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087841\n",
      "INFO:tensorflow:examples/sec: 2.81091\n",
      "I1117 23:32:25.070091 4435920320 tpu_estimator.py:2160] examples/sec: 2.81091\n",
      "INFO:tensorflow:global_step/sec: 0.086819\n",
      "I1117 23:32:36.588069 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086819\n",
      "INFO:tensorflow:examples/sec: 2.77821\n",
      "I1117 23:32:36.588284 4435920320 tpu_estimator.py:2160] examples/sec: 2.77821\n",
      "INFO:tensorflow:global_step/sec: 0.0891265\n",
      "I1117 23:32:47.808109 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891265\n",
      "INFO:tensorflow:examples/sec: 2.85205\n",
      "I1117 23:32:47.808328 4435920320 tpu_estimator.py:2160] examples/sec: 2.85205\n",
      "INFO:tensorflow:global_step/sec: 0.0881541\n",
      "I1117 23:32:59.151863 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881541\n",
      "INFO:tensorflow:examples/sec: 2.82093\n",
      "I1117 23:32:59.152131 4435920320 tpu_estimator.py:2160] examples/sec: 2.82093\n",
      "INFO:tensorflow:global_step/sec: 0.0883112\n",
      "I1117 23:33:10.475445 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883112\n",
      "INFO:tensorflow:examples/sec: 2.82596\n",
      "I1117 23:33:10.475685 4435920320 tpu_estimator.py:2160] examples/sec: 2.82596\n",
      "INFO:tensorflow:global_step/sec: 0.0861713\n",
      "I1117 23:33:22.080219 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861713\n",
      "INFO:tensorflow:examples/sec: 2.75748\n",
      "I1117 23:33:22.080466 4435920320 tpu_estimator.py:2160] examples/sec: 2.75748\n",
      "INFO:tensorflow:global_step/sec: 0.0892254\n",
      "I1117 23:33:33.287801 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892254\n",
      "INFO:tensorflow:examples/sec: 2.85521\n",
      "I1117 23:33:33.288028 4435920320 tpu_estimator.py:2160] examples/sec: 2.85521\n",
      "INFO:tensorflow:global_step/sec: 0.0877011\n",
      "I1117 23:33:44.690165 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877011\n",
      "INFO:tensorflow:examples/sec: 2.80644\n",
      "I1117 23:33:44.690383 4435920320 tpu_estimator.py:2160] examples/sec: 2.80644\n",
      "INFO:tensorflow:global_step/sec: 0.0884473\n",
      "I1117 23:33:55.996362 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884473\n",
      "INFO:tensorflow:examples/sec: 2.83031\n",
      "I1117 23:33:55.996581 4435920320 tpu_estimator.py:2160] examples/sec: 2.83031\n",
      "INFO:tensorflow:global_step/sec: 0.0882808\n",
      "I1117 23:34:07.323827 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882808\n",
      "INFO:tensorflow:examples/sec: 2.82499\n",
      "I1117 23:34:07.324046 4435920320 tpu_estimator.py:2160] examples/sec: 2.82499\n",
      "INFO:tensorflow:global_step/sec: 0.0869216\n",
      "I1117 23:34:18.828448 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869216\n",
      "INFO:tensorflow:examples/sec: 2.78149\n",
      "I1117 23:34:18.828673 4435920320 tpu_estimator.py:2160] examples/sec: 2.78149\n",
      "INFO:tensorflow:global_step/sec: 0.086697\n",
      "I1117 23:34:30.362874 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086697\n",
      "INFO:tensorflow:examples/sec: 2.7743\n",
      "I1117 23:34:30.363241 4435920320 tpu_estimator.py:2160] examples/sec: 2.7743\n",
      "INFO:tensorflow:global_step/sec: 0.0889102\n",
      "I1117 23:34:41.610171 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889102\n",
      "INFO:tensorflow:examples/sec: 2.84513\n",
      "I1117 23:34:41.610392 4435920320 tpu_estimator.py:2160] examples/sec: 2.84513\n",
      "INFO:tensorflow:global_step/sec: 0.0873571\n",
      "I1117 23:34:53.057440 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873571\n",
      "INFO:tensorflow:examples/sec: 2.79543\n",
      "I1117 23:34:53.057658 4435920320 tpu_estimator.py:2160] examples/sec: 2.79543\n",
      "INFO:tensorflow:global_step/sec: 0.0880285\n",
      "I1117 23:35:04.417423 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880285\n",
      "INFO:tensorflow:examples/sec: 2.81691\n",
      "I1117 23:35:04.417646 4435920320 tpu_estimator.py:2160] examples/sec: 2.81691\n",
      "INFO:tensorflow:global_step/sec: 0.0882784\n",
      "I1117 23:35:15.745286 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882784\n",
      "INFO:tensorflow:examples/sec: 2.82491\n",
      "I1117 23:35:15.745564 4435920320 tpu_estimator.py:2160] examples/sec: 2.82491\n",
      "INFO:tensorflow:global_step/sec: 0.087756\n",
      "I1117 23:35:27.140444 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087756\n",
      "INFO:tensorflow:examples/sec: 2.80819\n",
      "I1117 23:35:27.140670 4435920320 tpu_estimator.py:2160] examples/sec: 2.80819\n",
      "INFO:tensorflow:global_step/sec: 0.0880334\n",
      "I1117 23:35:38.499758 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880334\n",
      "INFO:tensorflow:examples/sec: 2.81707\n",
      "I1117 23:35:38.499979 4435920320 tpu_estimator.py:2160] examples/sec: 2.81707\n",
      "INFO:tensorflow:global_step/sec: 0.0882189\n",
      "I1117 23:35:49.835217 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882189\n",
      "INFO:tensorflow:examples/sec: 2.82301\n",
      "I1117 23:35:49.835461 4435920320 tpu_estimator.py:2160] examples/sec: 2.82301\n",
      "INFO:tensorflow:global_step/sec: 0.0880523\n",
      "I1117 23:36:01.192086 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880523\n",
      "INFO:tensorflow:examples/sec: 2.81767\n",
      "I1117 23:36:01.192320 4435920320 tpu_estimator.py:2160] examples/sec: 2.81767\n",
      "INFO:tensorflow:global_step/sec: 0.0880457\n",
      "I1117 23:36:12.549841 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880457\n",
      "INFO:tensorflow:examples/sec: 2.81746\n",
      "I1117 23:36:12.550062 4435920320 tpu_estimator.py:2160] examples/sec: 2.81746\n",
      "INFO:tensorflow:global_step/sec: 0.0881662\n",
      "I1117 23:36:23.892023 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881662\n",
      "INFO:tensorflow:examples/sec: 2.82132\n",
      "I1117 23:36:23.892242 4435920320 tpu_estimator.py:2160] examples/sec: 2.82132\n",
      "INFO:tensorflow:global_step/sec: 0.0878287\n",
      "I1117 23:36:35.277832 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878287\n",
      "INFO:tensorflow:examples/sec: 2.81052\n",
      "I1117 23:36:35.278059 4435920320 tpu_estimator.py:2160] examples/sec: 2.81052\n",
      "INFO:tensorflow:global_step/sec: 0.0880372\n",
      "I1117 23:36:46.636677 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880372\n",
      "INFO:tensorflow:examples/sec: 2.81719\n",
      "I1117 23:36:46.636898 4435920320 tpu_estimator.py:2160] examples/sec: 2.81719\n",
      "INFO:tensorflow:global_step/sec: 0.087855\n",
      "I1117 23:36:58.019056 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087855\n",
      "INFO:tensorflow:examples/sec: 2.81136\n",
      "I1117 23:36:58.019279 4435920320 tpu_estimator.py:2160] examples/sec: 2.81136\n",
      "INFO:tensorflow:global_step/sec: 0.0878536\n",
      "I1117 23:37:09.401642 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878536\n",
      "INFO:tensorflow:examples/sec: 2.81131\n",
      "I1117 23:37:09.401876 4435920320 tpu_estimator.py:2160] examples/sec: 2.81131\n",
      "INFO:tensorflow:global_step/sec: 0.0881996\n",
      "I1117 23:37:20.739570 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881996\n",
      "INFO:tensorflow:examples/sec: 2.82239\n",
      "I1117 23:37:20.739796 4435920320 tpu_estimator.py:2160] examples/sec: 2.82239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0877866\n",
      "I1117 23:37:32.130798 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877866\n",
      "INFO:tensorflow:examples/sec: 2.80917\n",
      "I1117 23:37:32.131014 4435920320 tpu_estimator.py:2160] examples/sec: 2.80917\n",
      "INFO:tensorflow:global_step/sec: 0.0881809\n",
      "I1117 23:37:43.471141 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881809\n",
      "INFO:tensorflow:examples/sec: 2.82179\n",
      "I1117 23:37:43.471366 4435920320 tpu_estimator.py:2160] examples/sec: 2.82179\n",
      "INFO:tensorflow:global_step/sec: 0.0878372\n",
      "I1117 23:37:54.855833 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878372\n",
      "INFO:tensorflow:examples/sec: 2.81079\n",
      "I1117 23:37:54.856060 4435920320 tpu_estimator.py:2160] examples/sec: 2.81079\n",
      "INFO:tensorflow:global_step/sec: 0.0879124\n",
      "I1117 23:38:06.230804 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879124\n",
      "INFO:tensorflow:examples/sec: 2.8132\n",
      "I1117 23:38:06.231034 4435920320 tpu_estimator.py:2160] examples/sec: 2.8132\n",
      "INFO:tensorflow:global_step/sec: 0.0875036\n",
      "I1117 23:38:17.658876 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875036\n",
      "INFO:tensorflow:examples/sec: 2.80012\n",
      "I1117 23:38:17.659077 4435920320 tpu_estimator.py:2160] examples/sec: 2.80012\n",
      "INFO:tensorflow:global_step/sec: 0.0865118\n",
      "I1117 23:38:29.218015 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0865118\n",
      "INFO:tensorflow:examples/sec: 2.76838\n",
      "I1117 23:38:29.218255 4435920320 tpu_estimator.py:2160] examples/sec: 2.76838\n",
      "INFO:tensorflow:global_step/sec: 0.0863994\n",
      "I1117 23:38:40.792154 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0863994\n",
      "INFO:tensorflow:examples/sec: 2.76478\n",
      "I1117 23:38:40.792375 4435920320 tpu_estimator.py:2160] examples/sec: 2.76478\n",
      "INFO:tensorflow:global_step/sec: 0.0890067\n",
      "I1117 23:38:52.027259 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890067\n",
      "INFO:tensorflow:examples/sec: 2.84821\n",
      "I1117 23:38:52.027482 4435920320 tpu_estimator.py:2160] examples/sec: 2.84821\n",
      "INFO:tensorflow:global_step/sec: 0.0878668\n",
      "I1117 23:39:03.408122 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878668\n",
      "INFO:tensorflow:examples/sec: 2.81174\n",
      "I1117 23:39:03.408346 4435920320 tpu_estimator.py:2160] examples/sec: 2.81174\n",
      "INFO:tensorflow:global_step/sec: 0.0883015\n",
      "I1117 23:39:14.732970 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883015\n",
      "INFO:tensorflow:examples/sec: 2.82565\n",
      "I1117 23:39:14.733210 4435920320 tpu_estimator.py:2160] examples/sec: 2.82565\n",
      "INFO:tensorflow:global_step/sec: 0.0878355\n",
      "I1117 23:39:26.117880 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878355\n",
      "INFO:tensorflow:examples/sec: 2.81074\n",
      "I1117 23:39:26.118098 4435920320 tpu_estimator.py:2160] examples/sec: 2.81074\n",
      "INFO:tensorflow:global_step/sec: 0.0883867\n",
      "I1117 23:39:37.431783 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883867\n",
      "INFO:tensorflow:examples/sec: 2.82837\n",
      "I1117 23:39:37.431982 4435920320 tpu_estimator.py:2160] examples/sec: 2.82837\n",
      "INFO:tensorflow:global_step/sec: 0.0875514\n",
      "I1117 23:39:48.853660 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875514\n",
      "INFO:tensorflow:examples/sec: 2.80165\n",
      "I1117 23:39:48.853878 4435920320 tpu_estimator.py:2160] examples/sec: 2.80165\n",
      "INFO:tensorflow:global_step/sec: 0.0874866\n",
      "I1117 23:40:00.283980 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874866\n",
      "INFO:tensorflow:examples/sec: 2.79957\n",
      "I1117 23:40:00.284197 4435920320 tpu_estimator.py:2160] examples/sec: 2.79957\n",
      "INFO:tensorflow:global_step/sec: 0.0877052\n",
      "I1117 23:40:11.685817 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877052\n",
      "INFO:tensorflow:examples/sec: 2.80657\n",
      "I1117 23:40:11.686038 4435920320 tpu_estimator.py:2160] examples/sec: 2.80657\n",
      "INFO:tensorflow:global_step/sec: 0.0879796\n",
      "I1117 23:40:23.052100 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879796\n",
      "INFO:tensorflow:examples/sec: 2.81535\n",
      "I1117 23:40:23.052336 4435920320 tpu_estimator.py:2160] examples/sec: 2.81535\n",
      "INFO:tensorflow:global_step/sec: 0.088048\n",
      "I1117 23:40:34.409540 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088048\n",
      "INFO:tensorflow:examples/sec: 2.81754\n",
      "I1117 23:40:34.409770 4435920320 tpu_estimator.py:2160] examples/sec: 2.81754\n",
      "INFO:tensorflow:global_step/sec: 0.0850223\n",
      "I1117 23:40:46.171109 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0850223\n",
      "INFO:tensorflow:examples/sec: 2.72071\n",
      "I1117 23:40:46.171287 4435920320 tpu_estimator.py:2160] examples/sec: 2.72071\n",
      "INFO:tensorflow:global_step/sec: 0.0880122\n",
      "I1117 23:40:57.533215 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880122\n",
      "INFO:tensorflow:examples/sec: 2.81639\n",
      "I1117 23:40:57.533440 4435920320 tpu_estimator.py:2160] examples/sec: 2.81639\n",
      "INFO:tensorflow:global_step/sec: 0.0882484\n",
      "I1117 23:41:08.864866 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882484\n",
      "INFO:tensorflow:examples/sec: 2.82395\n",
      "I1117 23:41:08.865085 4435920320 tpu_estimator.py:2160] examples/sec: 2.82395\n",
      "INFO:tensorflow:global_step/sec: 0.087856\n",
      "I1117 23:41:20.247141 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087856\n",
      "INFO:tensorflow:examples/sec: 2.81139\n",
      "I1117 23:41:20.247356 4435920320 tpu_estimator.py:2160] examples/sec: 2.81139\n",
      "INFO:tensorflow:global_step/sec: 0.087916\n",
      "I1117 23:41:31.621640 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087916\n",
      "INFO:tensorflow:examples/sec: 2.81331\n",
      "I1117 23:41:31.621875 4435920320 tpu_estimator.py:2160] examples/sec: 2.81331\n",
      "INFO:tensorflow:global_step/sec: 0.0873954\n",
      "I1117 23:41:43.063877 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873954\n",
      "INFO:tensorflow:examples/sec: 2.79665\n",
      "I1117 23:41:43.064094 4435920320 tpu_estimator.py:2160] examples/sec: 2.79665\n",
      "INFO:tensorflow:global_step/sec: 0.0889708\n",
      "I1117 23:41:54.303529 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889708\n",
      "INFO:tensorflow:examples/sec: 2.84707\n",
      "I1117 23:41:54.303756 4435920320 tpu_estimator.py:2160] examples/sec: 2.84707\n",
      "INFO:tensorflow:global_step/sec: 0.087858\n",
      "I1117 23:42:05.685518 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087858\n",
      "INFO:tensorflow:examples/sec: 2.81146\n",
      "I1117 23:42:05.685729 4435920320 tpu_estimator.py:2160] examples/sec: 2.81146\n",
      "INFO:tensorflow:global_step/sec: 0.0882901\n",
      "I1117 23:42:17.011813 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882901\n",
      "INFO:tensorflow:examples/sec: 2.82528\n",
      "I1117 23:42:17.012022 4435920320 tpu_estimator.py:2160] examples/sec: 2.82528\n",
      "INFO:tensorflow:global_step/sec: 0.0884102\n",
      "I1117 23:42:28.322731 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884102\n",
      "INFO:tensorflow:examples/sec: 2.82913\n",
      "I1117 23:42:28.322952 4435920320 tpu_estimator.py:2160] examples/sec: 2.82913\n",
      "INFO:tensorflow:global_step/sec: 0.0882571\n",
      "I1117 23:42:39.653270 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882571\n",
      "INFO:tensorflow:examples/sec: 2.82423\n",
      "I1117 23:42:39.653488 4435920320 tpu_estimator.py:2160] examples/sec: 2.82423\n",
      "INFO:tensorflow:global_step/sec: 0.0878716\n",
      "I1117 23:42:51.033509 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878716\n",
      "INFO:tensorflow:examples/sec: 2.81189\n",
      "I1117 23:42:51.033895 4435920320 tpu_estimator.py:2160] examples/sec: 2.81189\n",
      "INFO:tensorflow:global_step/sec: 0.0893617\n",
      "I1117 23:43:02.223993 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893617\n",
      "INFO:tensorflow:examples/sec: 2.85957\n",
      "I1117 23:43:02.224228 4435920320 tpu_estimator.py:2160] examples/sec: 2.85957\n",
      "INFO:tensorflow:global_step/sec: 0.0882571\n",
      "I1117 23:43:13.554509 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882571\n",
      "INFO:tensorflow:examples/sec: 2.82423\n",
      "I1117 23:43:13.554893 4435920320 tpu_estimator.py:2160] examples/sec: 2.82423\n",
      "INFO:tensorflow:global_step/sec: 0.0879917\n",
      "I1117 23:43:24.919216 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879917\n",
      "INFO:tensorflow:examples/sec: 2.81573\n",
      "I1117 23:43:24.919435 4435920320 tpu_estimator.py:2160] examples/sec: 2.81573\n",
      "INFO:tensorflow:global_step/sec: 0.0879663\n",
      "I1117 23:43:36.287220 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879663\n",
      "INFO:tensorflow:examples/sec: 2.81492\n",
      "I1117 23:43:36.287453 4435920320 tpu_estimator.py:2160] examples/sec: 2.81492\n",
      "INFO:tensorflow:global_step/sec: 0.0884435\n",
      "I1117 23:43:47.593870 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884435\n",
      "INFO:tensorflow:examples/sec: 2.83019\n",
      "I1117 23:43:47.594106 4435920320 tpu_estimator.py:2160] examples/sec: 2.83019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0878575\n",
      "I1117 23:43:58.975931 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878575\n",
      "INFO:tensorflow:examples/sec: 2.81144\n",
      "I1117 23:43:58.976163 4435920320 tpu_estimator.py:2160] examples/sec: 2.81144\n",
      "INFO:tensorflow:global_step/sec: 0.0878274\n",
      "I1117 23:44:10.361914 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878274\n",
      "INFO:tensorflow:examples/sec: 2.81048\n",
      "I1117 23:44:10.362137 4435920320 tpu_estimator.py:2160] examples/sec: 2.81048\n",
      "INFO:tensorflow:global_step/sec: 0.0873617\n",
      "I1117 23:44:21.808555 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873617\n",
      "INFO:tensorflow:examples/sec: 2.79558\n",
      "I1117 23:44:21.808772 4435920320 tpu_estimator.py:2160] examples/sec: 2.79558\n",
      "INFO:tensorflow:global_step/sec: 0.088191\n",
      "I1117 23:44:33.147583 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088191\n",
      "INFO:tensorflow:examples/sec: 2.82211\n",
      "I1117 23:44:33.147813 4435920320 tpu_estimator.py:2160] examples/sec: 2.82211\n",
      "INFO:tensorflow:global_step/sec: 0.0882651\n",
      "I1117 23:44:44.477080 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882651\n",
      "INFO:tensorflow:examples/sec: 2.82448\n",
      "I1117 23:44:44.477447 4435920320 tpu_estimator.py:2160] examples/sec: 2.82448\n",
      "INFO:tensorflow:global_step/sec: 0.0879229\n",
      "I1117 23:44:55.850687 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879229\n",
      "INFO:tensorflow:examples/sec: 2.81353\n",
      "I1117 23:44:55.850945 4435920320 tpu_estimator.py:2160] examples/sec: 2.81353\n",
      "INFO:tensorflow:global_step/sec: 0.0882489\n",
      "I1117 23:45:07.182266 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882489\n",
      "INFO:tensorflow:examples/sec: 2.82397\n",
      "I1117 23:45:07.182487 4435920320 tpu_estimator.py:2160] examples/sec: 2.82397\n",
      "INFO:tensorflow:global_step/sec: 0.0890834\n",
      "I1117 23:45:18.407702 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890834\n",
      "INFO:tensorflow:examples/sec: 2.85067\n",
      "I1117 23:45:18.407922 4435920320 tpu_estimator.py:2160] examples/sec: 2.85067\n",
      "INFO:tensorflow:global_step/sec: 0.0878924\n",
      "I1117 23:45:29.785247 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878924\n",
      "INFO:tensorflow:examples/sec: 2.81256\n",
      "I1117 23:45:29.785629 4435920320 tpu_estimator.py:2160] examples/sec: 2.81256\n",
      "INFO:tensorflow:global_step/sec: 0.0878653\n",
      "I1117 23:45:41.166313 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878653\n",
      "INFO:tensorflow:examples/sec: 2.81169\n",
      "I1117 23:45:41.166547 4435920320 tpu_estimator.py:2160] examples/sec: 2.81169\n",
      "INFO:tensorflow:global_step/sec: 0.0881543\n",
      "I1117 23:45:52.510057 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881543\n",
      "INFO:tensorflow:examples/sec: 2.82094\n",
      "I1117 23:45:52.510277 4435920320 tpu_estimator.py:2160] examples/sec: 2.82094\n",
      "INFO:tensorflow:global_step/sec: 0.0873532\n",
      "I1117 23:46:03.957829 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873532\n",
      "INFO:tensorflow:examples/sec: 2.7953\n",
      "I1117 23:46:03.958045 4435920320 tpu_estimator.py:2160] examples/sec: 2.7953\n",
      "INFO:tensorflow:global_step/sec: 0.0881782\n",
      "I1117 23:46:15.298499 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881782\n",
      "INFO:tensorflow:examples/sec: 2.8217\n",
      "I1117 23:46:15.298757 4435920320 tpu_estimator.py:2160] examples/sec: 2.8217\n",
      "INFO:tensorflow:global_step/sec: 0.0886185\n",
      "I1117 23:46:26.582841 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886185\n",
      "INFO:tensorflow:examples/sec: 2.83579\n",
      "I1117 23:46:26.583078 4435920320 tpu_estimator.py:2160] examples/sec: 2.83579\n",
      "INFO:tensorflow:global_step/sec: 0.0881588\n",
      "I1117 23:46:37.926032 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881588\n",
      "INFO:tensorflow:examples/sec: 2.82108\n",
      "I1117 23:46:37.926278 4435920320 tpu_estimator.py:2160] examples/sec: 2.82108\n",
      "INFO:tensorflow:global_step/sec: 0.087825\n",
      "I1117 23:46:49.312271 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087825\n",
      "INFO:tensorflow:examples/sec: 2.8104\n",
      "I1117 23:46:49.312638 4435920320 tpu_estimator.py:2160] examples/sec: 2.8104\n",
      "INFO:tensorflow:global_step/sec: 0.0881154\n",
      "I1117 23:47:00.661031 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881154\n",
      "INFO:tensorflow:examples/sec: 2.81969\n",
      "I1117 23:47:00.661253 4435920320 tpu_estimator.py:2160] examples/sec: 2.81969\n",
      "INFO:tensorflow:global_step/sec: 0.088238\n",
      "I1117 23:47:11.994055 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088238\n",
      "INFO:tensorflow:examples/sec: 2.82361\n",
      "I1117 23:47:11.994493 4435920320 tpu_estimator.py:2160] examples/sec: 2.82361\n",
      "INFO:tensorflow:global_step/sec: 0.0882694\n",
      "I1117 23:47:23.322968 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882694\n",
      "INFO:tensorflow:examples/sec: 2.82462\n",
      "I1117 23:47:23.323182 4435920320 tpu_estimator.py:2160] examples/sec: 2.82462\n",
      "INFO:tensorflow:global_step/sec: 0.088246\n",
      "I1117 23:47:34.654926 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088246\n",
      "INFO:tensorflow:examples/sec: 2.82387\n",
      "I1117 23:47:34.655144 4435920320 tpu_estimator.py:2160] examples/sec: 2.82387\n",
      "INFO:tensorflow:global_step/sec: 0.087985\n",
      "I1117 23:47:46.020502 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087985\n",
      "INFO:tensorflow:examples/sec: 2.81552\n",
      "I1117 23:47:46.020720 4435920320 tpu_estimator.py:2160] examples/sec: 2.81552\n",
      "INFO:tensorflow:global_step/sec: 0.0890973\n",
      "I1117 23:47:57.244190 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890973\n",
      "INFO:tensorflow:examples/sec: 2.85111\n",
      "I1117 23:47:57.244419 4435920320 tpu_estimator.py:2160] examples/sec: 2.85111\n",
      "INFO:tensorflow:global_step/sec: 0.0884203\n",
      "I1117 23:48:08.553810 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884203\n",
      "INFO:tensorflow:examples/sec: 2.82945\n",
      "I1117 23:48:08.554042 4435920320 tpu_estimator.py:2160] examples/sec: 2.82945\n",
      "INFO:tensorflow:global_step/sec: 0.0876408\n",
      "I1117 23:48:19.964015 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876408\n",
      "INFO:tensorflow:examples/sec: 2.80451\n",
      "I1117 23:48:19.964228 4435920320 tpu_estimator.py:2160] examples/sec: 2.80451\n",
      "INFO:tensorflow:global_step/sec: 0.0881338\n",
      "I1117 23:48:31.310403 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881338\n",
      "INFO:tensorflow:examples/sec: 2.82028\n",
      "I1117 23:48:31.310623 4435920320 tpu_estimator.py:2160] examples/sec: 2.82028\n",
      "INFO:tensorflow:global_step/sec: 0.0878754\n",
      "I1117 23:48:42.690149 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878754\n",
      "INFO:tensorflow:examples/sec: 2.81201\n",
      "I1117 23:48:42.690351 4435920320 tpu_estimator.py:2160] examples/sec: 2.81201\n",
      "INFO:tensorflow:global_step/sec: 0.0888744\n",
      "I1117 23:48:53.942073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888744\n",
      "INFO:tensorflow:examples/sec: 2.84398\n",
      "I1117 23:48:53.942424 4435920320 tpu_estimator.py:2160] examples/sec: 2.84398\n",
      "INFO:tensorflow:global_step/sec: 0.0881997\n",
      "I1117 23:49:05.279886 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881997\n",
      "INFO:tensorflow:examples/sec: 2.82239\n",
      "I1117 23:49:05.280103 4435920320 tpu_estimator.py:2160] examples/sec: 2.82239\n",
      "INFO:tensorflow:global_step/sec: 0.0880355\n",
      "I1117 23:49:16.638963 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880355\n",
      "INFO:tensorflow:examples/sec: 2.81714\n",
      "I1117 23:49:16.639183 4435920320 tpu_estimator.py:2160] examples/sec: 2.81714\n",
      "INFO:tensorflow:global_step/sec: 0.087568\n",
      "I1117 23:49:28.058637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087568\n",
      "INFO:tensorflow:examples/sec: 2.80218\n",
      "I1117 23:49:28.058864 4435920320 tpu_estimator.py:2160] examples/sec: 2.80218\n",
      "INFO:tensorflow:global_step/sec: 0.0890631\n",
      "I1117 23:49:39.286648 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890631\n",
      "INFO:tensorflow:examples/sec: 2.85002\n",
      "I1117 23:49:39.287034 4435920320 tpu_estimator.py:2160] examples/sec: 2.85002\n",
      "INFO:tensorflow:global_step/sec: 0.0879581\n",
      "I1117 23:49:50.655681 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879581\n",
      "INFO:tensorflow:examples/sec: 2.81466\n",
      "I1117 23:49:50.655907 4435920320 tpu_estimator.py:2160] examples/sec: 2.81466\n",
      "INFO:tensorflow:global_step/sec: 0.0881708\n",
      "I1117 23:50:01.997305 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881708\n",
      "INFO:tensorflow:examples/sec: 2.82147\n",
      "I1117 23:50:01.997529 4435920320 tpu_estimator.py:2160] examples/sec: 2.82147\n",
      "INFO:tensorflow:global_step/sec: 0.0888167\n",
      "I1117 23:50:13.256453 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888167\n",
      "INFO:tensorflow:examples/sec: 2.84214\n",
      "I1117 23:50:13.256680 4435920320 tpu_estimator.py:2160] examples/sec: 2.84214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0881954\n",
      "I1117 23:50:24.594871 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881954\n",
      "INFO:tensorflow:examples/sec: 2.82225\n",
      "I1117 23:50:24.595144 4435920320 tpu_estimator.py:2160] examples/sec: 2.82225\n",
      "INFO:tensorflow:global_step/sec: 0.0883372\n",
      "I1117 23:50:35.915157 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883372\n",
      "INFO:tensorflow:examples/sec: 2.82679\n",
      "I1117 23:50:35.915381 4435920320 tpu_estimator.py:2160] examples/sec: 2.82679\n",
      "INFO:tensorflow:global_step/sec: 0.0879095\n",
      "I1117 23:50:47.290505 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879095\n",
      "INFO:tensorflow:examples/sec: 2.8131\n",
      "I1117 23:50:47.290736 4435920320 tpu_estimator.py:2160] examples/sec: 2.8131\n",
      "INFO:tensorflow:global_step/sec: 0.0876357\n",
      "I1117 23:50:58.701383 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876357\n",
      "INFO:tensorflow:examples/sec: 2.80434\n",
      "I1117 23:50:58.701603 4435920320 tpu_estimator.py:2160] examples/sec: 2.80434\n",
      "INFO:tensorflow:global_step/sec: 0.0883306\n",
      "I1117 23:51:10.022469 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883306\n",
      "INFO:tensorflow:examples/sec: 2.82658\n",
      "I1117 23:51:10.022696 4435920320 tpu_estimator.py:2160] examples/sec: 2.82658\n",
      "INFO:tensorflow:global_step/sec: 0.0877322\n",
      "I1117 23:51:21.420821 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877322\n",
      "INFO:tensorflow:examples/sec: 2.80743\n",
      "I1117 23:51:21.421043 4435920320 tpu_estimator.py:2160] examples/sec: 2.80743\n",
      "INFO:tensorflow:global_step/sec: 0.0882781\n",
      "I1117 23:51:32.748645 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882781\n",
      "INFO:tensorflow:examples/sec: 2.8249\n",
      "I1117 23:51:32.748862 4435920320 tpu_estimator.py:2160] examples/sec: 2.8249\n",
      "INFO:tensorflow:global_step/sec: 0.0892644\n",
      "I1117 23:51:43.951318 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892644\n",
      "INFO:tensorflow:examples/sec: 2.85646\n",
      "I1117 23:51:43.951536 4435920320 tpu_estimator.py:2160] examples/sec: 2.85646\n",
      "INFO:tensorflow:global_step/sec: 0.087956\n",
      "I1117 23:51:55.320626 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087956\n",
      "INFO:tensorflow:examples/sec: 2.81459\n",
      "I1117 23:51:55.320857 4435920320 tpu_estimator.py:2160] examples/sec: 2.81459\n",
      "INFO:tensorflow:global_step/sec: 0.0890043\n",
      "I1117 23:52:06.556040 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890043\n",
      "INFO:tensorflow:examples/sec: 2.84814\n",
      "I1117 23:52:06.556259 4435920320 tpu_estimator.py:2160] examples/sec: 2.84814\n",
      "INFO:tensorflow:global_step/sec: 0.0879757\n",
      "I1117 23:52:17.922812 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879757\n",
      "INFO:tensorflow:examples/sec: 2.81522\n",
      "I1117 23:52:17.923034 4435920320 tpu_estimator.py:2160] examples/sec: 2.81522\n",
      "INFO:tensorflow:global_step/sec: 0.0871968\n",
      "I1117 23:52:29.391129 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871968\n",
      "INFO:tensorflow:examples/sec: 2.7903\n",
      "I1117 23:52:29.391343 4435920320 tpu_estimator.py:2160] examples/sec: 2.7903\n",
      "INFO:tensorflow:global_step/sec: 0.0879858\n",
      "I1117 23:52:40.756615 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879858\n",
      "INFO:tensorflow:examples/sec: 2.81554\n",
      "I1117 23:52:40.756851 4435920320 tpu_estimator.py:2160] examples/sec: 2.81554\n",
      "INFO:tensorflow:global_step/sec: 0.088381\n",
      "I1117 23:52:52.071264 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088381\n",
      "INFO:tensorflow:examples/sec: 2.82819\n",
      "I1117 23:52:52.071511 4435920320 tpu_estimator.py:2160] examples/sec: 2.82819\n",
      "INFO:tensorflow:global_step/sec: 0.0882723\n",
      "I1117 23:53:03.399855 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882723\n",
      "INFO:tensorflow:examples/sec: 2.82471\n",
      "I1117 23:53:03.400078 4435920320 tpu_estimator.py:2160] examples/sec: 2.82471\n",
      "INFO:tensorflow:global_step/sec: 0.0888929\n",
      "I1117 23:53:14.649340 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888929\n",
      "INFO:tensorflow:examples/sec: 2.84457\n",
      "I1117 23:53:14.649562 4435920320 tpu_estimator.py:2160] examples/sec: 2.84457\n",
      "INFO:tensorflow:global_step/sec: 0.0881863\n",
      "I1117 23:53:25.988946 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881863\n",
      "INFO:tensorflow:examples/sec: 2.82196\n",
      "I1117 23:53:25.989176 4435920320 tpu_estimator.py:2160] examples/sec: 2.82196\n",
      "INFO:tensorflow:global_step/sec: 0.0879981\n",
      "I1117 23:53:37.352820 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879981\n",
      "INFO:tensorflow:examples/sec: 2.81594\n",
      "I1117 23:53:37.353027 4435920320 tpu_estimator.py:2160] examples/sec: 2.81594\n",
      "INFO:tensorflow:global_step/sec: 0.0879603\n",
      "I1117 23:53:48.721607 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879603\n",
      "INFO:tensorflow:examples/sec: 2.81473\n",
      "I1117 23:53:48.721837 4435920320 tpu_estimator.py:2160] examples/sec: 2.81473\n",
      "INFO:tensorflow:global_step/sec: 0.0895396\n",
      "I1117 23:53:59.889850 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895396\n",
      "INFO:tensorflow:examples/sec: 2.86527\n",
      "I1117 23:53:59.890075 4435920320 tpu_estimator.py:2160] examples/sec: 2.86527\n",
      "INFO:tensorflow:global_step/sec: 0.0882564\n",
      "I1117 23:54:11.220468 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882564\n",
      "INFO:tensorflow:examples/sec: 2.82421\n",
      "I1117 23:54:11.220699 4435920320 tpu_estimator.py:2160] examples/sec: 2.82421\n",
      "INFO:tensorflow:global_step/sec: 0.0879452\n",
      "I1117 23:54:22.591174 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879452\n",
      "INFO:tensorflow:examples/sec: 2.81425\n",
      "I1117 23:54:22.591397 4435920320 tpu_estimator.py:2160] examples/sec: 2.81425\n",
      "INFO:tensorflow:global_step/sec: 0.087718\n",
      "I1117 23:54:33.991334 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087718\n",
      "INFO:tensorflow:examples/sec: 2.80697\n",
      "I1117 23:54:33.991554 4435920320 tpu_estimator.py:2160] examples/sec: 2.80697\n",
      "INFO:tensorflow:global_step/sec: 0.0888359\n",
      "I1117 23:54:45.248053 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888359\n",
      "INFO:tensorflow:examples/sec: 2.84275\n",
      "I1117 23:54:45.248275 4435920320 tpu_estimator.py:2160] examples/sec: 2.84275\n",
      "INFO:tensorflow:global_step/sec: 0.0880465\n",
      "I1117 23:54:56.605695 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880465\n",
      "INFO:tensorflow:examples/sec: 2.81749\n",
      "I1117 23:54:56.605925 4435920320 tpu_estimator.py:2160] examples/sec: 2.81749\n",
      "INFO:tensorflow:global_step/sec: 0.0877896\n",
      "I1117 23:55:07.996563 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877896\n",
      "INFO:tensorflow:examples/sec: 2.80927\n",
      "I1117 23:55:07.996962 4435920320 tpu_estimator.py:2160] examples/sec: 2.80927\n",
      "INFO:tensorflow:global_step/sec: 0.0890846\n",
      "I1117 23:55:19.221854 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890846\n",
      "INFO:tensorflow:examples/sec: 2.85071\n",
      "I1117 23:55:19.222087 4435920320 tpu_estimator.py:2160] examples/sec: 2.85071\n",
      "INFO:tensorflow:global_step/sec: 0.0876716\n",
      "I1117 23:55:30.628048 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876716\n",
      "INFO:tensorflow:examples/sec: 2.80549\n",
      "I1117 23:55:30.628278 4435920320 tpu_estimator.py:2160] examples/sec: 2.80549\n",
      "INFO:tensorflow:global_step/sec: 0.0878901\n",
      "I1117 23:55:42.005908 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878901\n",
      "INFO:tensorflow:examples/sec: 2.81248\n",
      "I1117 23:55:42.006127 4435920320 tpu_estimator.py:2160] examples/sec: 2.81248\n",
      "INFO:tensorflow:global_step/sec: 0.0889898\n",
      "I1117 23:55:53.243128 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889898\n",
      "INFO:tensorflow:examples/sec: 2.84767\n",
      "I1117 23:55:53.243350 4435920320 tpu_estimator.py:2160] examples/sec: 2.84767\n",
      "INFO:tensorflow:global_step/sec: 0.0878258\n",
      "I1117 23:56:04.629315 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878258\n",
      "INFO:tensorflow:examples/sec: 2.81043\n",
      "I1117 23:56:04.629532 4435920320 tpu_estimator.py:2160] examples/sec: 2.81043\n",
      "INFO:tensorflow:global_step/sec: 0.0896596\n",
      "I1117 23:56:15.782597 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896596\n",
      "INFO:tensorflow:examples/sec: 2.86911\n",
      "I1117 23:56:15.782819 4435920320 tpu_estimator.py:2160] examples/sec: 2.86911\n",
      "INFO:tensorflow:global_step/sec: 0.0884022\n",
      "I1117 23:56:27.094532 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884022\n",
      "INFO:tensorflow:examples/sec: 2.82887\n",
      "I1117 23:56:27.094749 4435920320 tpu_estimator.py:2160] examples/sec: 2.82887\n",
      "INFO:tensorflow:global_step/sec: 0.0880685\n",
      "I1117 23:56:38.449317 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880685\n",
      "INFO:tensorflow:examples/sec: 2.81819\n",
      "I1117 23:56:38.449537 4435920320 tpu_estimator.py:2160] examples/sec: 2.81819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0883776\n",
      "I1117 23:56:49.764419 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883776\n",
      "INFO:tensorflow:examples/sec: 2.82808\n",
      "I1117 23:56:49.764654 4435920320 tpu_estimator.py:2160] examples/sec: 2.82808\n",
      "INFO:tensorflow:global_step/sec: 0.0878366\n",
      "I1117 23:57:01.149189 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878366\n",
      "INFO:tensorflow:examples/sec: 2.81077\n",
      "I1117 23:57:01.149410 4435920320 tpu_estimator.py:2160] examples/sec: 2.81077\n",
      "INFO:tensorflow:global_step/sec: 0.0888144\n",
      "I1117 23:57:12.408632 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888144\n",
      "INFO:tensorflow:examples/sec: 2.84206\n",
      "I1117 23:57:12.408850 4435920320 tpu_estimator.py:2160] examples/sec: 2.84206\n",
      "INFO:tensorflow:global_step/sec: 0.0879439\n",
      "I1117 23:57:23.779520 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879439\n",
      "INFO:tensorflow:examples/sec: 2.8142\n",
      "I1117 23:57:23.779742 4435920320 tpu_estimator.py:2160] examples/sec: 2.8142\n",
      "INFO:tensorflow:global_step/sec: 0.0891746\n",
      "I1117 23:57:34.993474 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891746\n",
      "INFO:tensorflow:examples/sec: 2.85359\n",
      "I1117 23:57:34.993695 4435920320 tpu_estimator.py:2160] examples/sec: 2.85359\n",
      "INFO:tensorflow:global_step/sec: 0.0878384\n",
      "I1117 23:57:46.378012 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878384\n",
      "INFO:tensorflow:examples/sec: 2.81083\n",
      "I1117 23:57:46.378232 4435920320 tpu_estimator.py:2160] examples/sec: 2.81083\n",
      "INFO:tensorflow:global_step/sec: 0.0891971\n",
      "I1117 23:57:57.589149 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891971\n",
      "INFO:tensorflow:examples/sec: 2.85431\n",
      "I1117 23:57:57.589549 4435920320 tpu_estimator.py:2160] examples/sec: 2.85431\n",
      "INFO:tensorflow:global_step/sec: 0.0874697\n",
      "I1117 23:58:09.021661 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874697\n",
      "INFO:tensorflow:examples/sec: 2.79903\n",
      "I1117 23:58:09.021877 4435920320 tpu_estimator.py:2160] examples/sec: 2.79903\n",
      "INFO:tensorflow:global_step/sec: 0.0880516\n",
      "I1117 23:58:20.378637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880516\n",
      "INFO:tensorflow:examples/sec: 2.81765\n",
      "I1117 23:58:20.378851 4435920320 tpu_estimator.py:2160] examples/sec: 2.81765\n",
      "INFO:tensorflow:global_step/sec: 0.0892688\n",
      "I1117 23:58:31.580768 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892688\n",
      "INFO:tensorflow:examples/sec: 2.8566\n",
      "I1117 23:58:31.580981 4435920320 tpu_estimator.py:2160] examples/sec: 2.8566\n",
      "INFO:tensorflow:global_step/sec: 0.0882966\n",
      "I1117 23:58:42.906245 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882966\n",
      "INFO:tensorflow:examples/sec: 2.82549\n",
      "I1117 23:58:42.906476 4435920320 tpu_estimator.py:2160] examples/sec: 2.82549\n",
      "INFO:tensorflow:global_step/sec: 0.0892089\n",
      "I1117 23:58:54.115905 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892089\n",
      "INFO:tensorflow:examples/sec: 2.85468\n",
      "I1117 23:58:54.116130 4435920320 tpu_estimator.py:2160] examples/sec: 2.85468\n",
      "INFO:tensorflow:global_step/sec: 0.0880937\n",
      "I1117 23:59:05.467420 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880937\n",
      "INFO:tensorflow:examples/sec: 2.819\n",
      "I1117 23:59:05.467630 4435920320 tpu_estimator.py:2160] examples/sec: 2.819\n",
      "INFO:tensorflow:global_step/sec: 0.0877409\n",
      "I1117 23:59:16.864626 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877409\n",
      "INFO:tensorflow:examples/sec: 2.80771\n",
      "I1117 23:59:16.864848 4435920320 tpu_estimator.py:2160] examples/sec: 2.80771\n",
      "INFO:tensorflow:global_step/sec: 0.089365\n",
      "I1117 23:59:28.054688 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089365\n",
      "INFO:tensorflow:examples/sec: 2.85968\n",
      "I1117 23:59:28.054915 4435920320 tpu_estimator.py:2160] examples/sec: 2.85968\n",
      "INFO:tensorflow:global_step/sec: 0.0885525\n",
      "I1117 23:59:39.347417 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885525\n",
      "INFO:tensorflow:examples/sec: 2.83368\n",
      "I1117 23:59:39.347639 4435920320 tpu_estimator.py:2160] examples/sec: 2.83368\n",
      "INFO:tensorflow:global_step/sec: 0.0878122\n",
      "I1117 23:59:50.735378 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878122\n",
      "INFO:tensorflow:examples/sec: 2.80999\n",
      "I1117 23:59:50.735598 4435920320 tpu_estimator.py:2160] examples/sec: 2.80999\n",
      "INFO:tensorflow:global_step/sec: 0.0879811\n",
      "I1118 00:00:02.101442 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879811\n",
      "INFO:tensorflow:examples/sec: 2.81539\n",
      "I1118 00:00:02.101683 4435920320 tpu_estimator.py:2160] examples/sec: 2.81539\n",
      "INFO:tensorflow:global_step/sec: 0.0803314\n",
      "I1118 00:00:14.549810 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0803314\n",
      "INFO:tensorflow:examples/sec: 2.5706\n",
      "I1118 00:00:14.549972 4435920320 tpu_estimator.py:2160] examples/sec: 2.5706\n",
      "INFO:tensorflow:global_step/sec: 0.0880542\n",
      "I1118 00:00:25.906512 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880542\n",
      "INFO:tensorflow:examples/sec: 2.81773\n",
      "I1118 00:00:25.906732 4435920320 tpu_estimator.py:2160] examples/sec: 2.81773\n",
      "INFO:tensorflow:global_step/sec: 0.087329\n",
      "I1118 00:00:37.357465 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087329\n",
      "INFO:tensorflow:examples/sec: 2.79453\n",
      "I1118 00:00:37.357701 4435920320 tpu_estimator.py:2160] examples/sec: 2.79453\n",
      "INFO:tensorflow:global_step/sec: 0.0881016\n",
      "I1118 00:00:48.707995 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881016\n",
      "INFO:tensorflow:examples/sec: 2.81925\n",
      "I1118 00:00:48.708210 4435920320 tpu_estimator.py:2160] examples/sec: 2.81925\n",
      "INFO:tensorflow:global_step/sec: 0.0887513\n",
      "I1118 00:00:59.975424 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887513\n",
      "INFO:tensorflow:examples/sec: 2.84004\n",
      "I1118 00:00:59.975654 4435920320 tpu_estimator.py:2160] examples/sec: 2.84004\n",
      "INFO:tensorflow:global_step/sec: 0.088721\n",
      "I1118 00:01:11.246727 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088721\n",
      "INFO:tensorflow:examples/sec: 2.83907\n",
      "I1118 00:01:11.246953 4435920320 tpu_estimator.py:2160] examples/sec: 2.83907\n",
      "INFO:tensorflow:global_step/sec: 0.0879851\n",
      "I1118 00:01:22.612273 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879851\n",
      "INFO:tensorflow:examples/sec: 2.81552\n",
      "I1118 00:01:22.612500 4435920320 tpu_estimator.py:2160] examples/sec: 2.81552\n",
      "INFO:tensorflow:global_step/sec: 0.0872817\n",
      "I1118 00:01:34.069488 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872817\n",
      "INFO:tensorflow:examples/sec: 2.79302\n",
      "I1118 00:01:34.069818 4435920320 tpu_estimator.py:2160] examples/sec: 2.79302\n",
      "INFO:tensorflow:global_step/sec: 0.0884217\n",
      "I1118 00:01:45.378864 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884217\n",
      "INFO:tensorflow:examples/sec: 2.8295\n",
      "I1118 00:01:45.379091 4435920320 tpu_estimator.py:2160] examples/sec: 2.8295\n",
      "INFO:tensorflow:global_step/sec: 0.0887952\n",
      "I1118 00:01:56.640745 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887952\n",
      "INFO:tensorflow:examples/sec: 2.84144\n",
      "I1118 00:01:56.640959 4435920320 tpu_estimator.py:2160] examples/sec: 2.84144\n",
      "INFO:tensorflow:global_step/sec: 0.0879437\n",
      "I1118 00:02:08.011632 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879437\n",
      "INFO:tensorflow:examples/sec: 2.8142\n",
      "I1118 00:02:08.011852 4435920320 tpu_estimator.py:2160] examples/sec: 2.8142\n",
      "INFO:tensorflow:global_step/sec: 0.0878145\n",
      "I1118 00:02:19.399309 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878145\n",
      "INFO:tensorflow:examples/sec: 2.81007\n",
      "I1118 00:02:19.399536 4435920320 tpu_estimator.py:2160] examples/sec: 2.81007\n",
      "INFO:tensorflow:global_step/sec: 0.0881236\n",
      "I1118 00:02:30.746984 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881236\n",
      "INFO:tensorflow:examples/sec: 2.81995\n",
      "I1118 00:02:30.747209 4435920320 tpu_estimator.py:2160] examples/sec: 2.81995\n",
      "INFO:tensorflow:global_step/sec: 0.0882892\n",
      "I1118 00:02:42.073391 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882892\n",
      "INFO:tensorflow:examples/sec: 2.82525\n",
      "I1118 00:02:42.073605 4435920320 tpu_estimator.py:2160] examples/sec: 2.82525\n",
      "INFO:tensorflow:global_step/sec: 0.0891953\n",
      "I1118 00:02:53.284754 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891953\n",
      "INFO:tensorflow:examples/sec: 2.85425\n",
      "I1118 00:02:53.284986 4435920320 tpu_estimator.py:2160] examples/sec: 2.85425\n",
      "INFO:tensorflow:global_step/sec: 0.0884203\n",
      "I1118 00:03:04.594362 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884203\n",
      "INFO:tensorflow:examples/sec: 2.82945\n",
      "I1118 00:03:04.594599 4435920320 tpu_estimator.py:2160] examples/sec: 2.82945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0884501\n",
      "I1118 00:03:15.900177 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884501\n",
      "INFO:tensorflow:examples/sec: 2.8304\n",
      "I1118 00:03:15.900410 4435920320 tpu_estimator.py:2160] examples/sec: 2.8304\n",
      "INFO:tensorflow:global_step/sec: 0.0892659\n",
      "I1118 00:03:27.102678 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892659\n",
      "INFO:tensorflow:examples/sec: 2.85651\n",
      "I1118 00:03:27.102905 4435920320 tpu_estimator.py:2160] examples/sec: 2.85651\n",
      "INFO:tensorflow:global_step/sec: 0.0881122\n",
      "I1118 00:03:38.451828 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881122\n",
      "INFO:tensorflow:examples/sec: 2.81959\n",
      "I1118 00:03:38.452051 4435920320 tpu_estimator.py:2160] examples/sec: 2.81959\n",
      "INFO:tensorflow:global_step/sec: 0.0891985\n",
      "I1118 00:03:49.662796 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891985\n",
      "INFO:tensorflow:examples/sec: 2.85435\n",
      "I1118 00:03:49.663013 4435920320 tpu_estimator.py:2160] examples/sec: 2.85435\n",
      "INFO:tensorflow:global_step/sec: 0.0877698\n",
      "I1118 00:04:01.056218 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877698\n",
      "INFO:tensorflow:examples/sec: 2.80863\n",
      "I1118 00:04:01.056436 4435920320 tpu_estimator.py:2160] examples/sec: 2.80863\n",
      "INFO:tensorflow:global_step/sec: 0.0874754\n",
      "I1118 00:04:12.488013 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874754\n",
      "INFO:tensorflow:examples/sec: 2.79921\n",
      "I1118 00:04:12.488241 4435920320 tpu_estimator.py:2160] examples/sec: 2.79921\n",
      "INFO:tensorflow:global_step/sec: 0.0891246\n",
      "I1118 00:04:23.708267 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891246\n",
      "INFO:tensorflow:examples/sec: 2.85199\n",
      "I1118 00:04:23.708499 4435920320 tpu_estimator.py:2160] examples/sec: 2.85199\n",
      "INFO:tensorflow:global_step/sec: 0.0881591\n",
      "I1118 00:04:35.051392 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881591\n",
      "INFO:tensorflow:examples/sec: 2.82109\n",
      "I1118 00:04:35.051610 4435920320 tpu_estimator.py:2160] examples/sec: 2.82109\n",
      "INFO:tensorflow:global_step/sec: 0.0888748\n",
      "I1118 00:04:46.303193 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888748\n",
      "INFO:tensorflow:examples/sec: 2.84399\n",
      "I1118 00:04:46.303421 4435920320 tpu_estimator.py:2160] examples/sec: 2.84399\n",
      "INFO:tensorflow:global_step/sec: 0.0880612\n",
      "I1118 00:04:57.658887 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880612\n",
      "INFO:tensorflow:examples/sec: 2.81796\n",
      "I1118 00:04:57.659082 4435920320 tpu_estimator.py:2160] examples/sec: 2.81796\n",
      "INFO:tensorflow:global_step/sec: 0.0893067\n",
      "I1118 00:05:08.856281 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893067\n",
      "INFO:tensorflow:examples/sec: 2.85782\n",
      "I1118 00:05:08.856509 4435920320 tpu_estimator.py:2160] examples/sec: 2.85782\n",
      "INFO:tensorflow:global_step/sec: 0.0878247\n",
      "I1118 00:05:20.242608 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878247\n",
      "INFO:tensorflow:examples/sec: 2.81039\n",
      "I1118 00:05:20.242835 4435920320 tpu_estimator.py:2160] examples/sec: 2.81039\n",
      "INFO:tensorflow:global_step/sec: 0.0889956\n",
      "I1118 00:05:31.479117 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889956\n",
      "INFO:tensorflow:examples/sec: 2.84786\n",
      "I1118 00:05:31.479514 4435920320 tpu_estimator.py:2160] examples/sec: 2.84786\n",
      "INFO:tensorflow:global_step/sec: 0.0892421\n",
      "I1118 00:05:42.684568 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892421\n",
      "INFO:tensorflow:examples/sec: 2.85575\n",
      "I1118 00:05:42.684784 4435920320 tpu_estimator.py:2160] examples/sec: 2.85575\n",
      "INFO:tensorflow:global_step/sec: 0.0878353\n",
      "I1118 00:05:54.069528 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878353\n",
      "INFO:tensorflow:examples/sec: 2.81073\n",
      "I1118 00:05:54.069758 4435920320 tpu_estimator.py:2160] examples/sec: 2.81073\n",
      "INFO:tensorflow:global_step/sec: 0.0885046\n",
      "I1118 00:06:05.368374 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885046\n",
      "INFO:tensorflow:examples/sec: 2.83215\n",
      "I1118 00:06:05.368591 4435920320 tpu_estimator.py:2160] examples/sec: 2.83215\n",
      "INFO:tensorflow:global_step/sec: 0.0892696\n",
      "I1118 00:06:16.570411 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892696\n",
      "INFO:tensorflow:examples/sec: 2.85663\n",
      "I1118 00:06:16.570652 4435920320 tpu_estimator.py:2160] examples/sec: 2.85663\n",
      "INFO:tensorflow:global_step/sec: 0.0892678\n",
      "I1118 00:06:27.772649 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892678\n",
      "INFO:tensorflow:examples/sec: 2.85657\n",
      "I1118 00:06:27.772868 4435920320 tpu_estimator.py:2160] examples/sec: 2.85657\n",
      "INFO:tensorflow:global_step/sec: 0.0879079\n",
      "I1118 00:06:39.148200 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879079\n",
      "INFO:tensorflow:examples/sec: 2.81305\n",
      "I1118 00:06:39.148416 4435920320 tpu_estimator.py:2160] examples/sec: 2.81305\n",
      "INFO:tensorflow:global_step/sec: 0.0890535\n",
      "I1118 00:06:50.377398 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890535\n",
      "INFO:tensorflow:examples/sec: 2.84971\n",
      "I1118 00:06:50.377620 4435920320 tpu_estimator.py:2160] examples/sec: 2.84971\n",
      "INFO:tensorflow:global_step/sec: 0.0879478\n",
      "I1118 00:07:01.747802 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879478\n",
      "INFO:tensorflow:examples/sec: 2.81433\n",
      "I1118 00:07:01.748032 4435920320 tpu_estimator.py:2160] examples/sec: 2.81433\n",
      "INFO:tensorflow:global_step/sec: 0.0885686\n",
      "I1118 00:07:13.038479 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885686\n",
      "INFO:tensorflow:examples/sec: 2.83419\n",
      "I1118 00:07:13.038697 4435920320 tpu_estimator.py:2160] examples/sec: 2.83419\n",
      "INFO:tensorflow:global_step/sec: 0.0877994\n",
      "I1118 00:07:24.428076 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877994\n",
      "INFO:tensorflow:examples/sec: 2.80958\n",
      "I1118 00:07:24.428323 4435920320 tpu_estimator.py:2160] examples/sec: 2.80958\n",
      "INFO:tensorflow:global_step/sec: 0.0892012\n",
      "I1118 00:07:35.638686 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892012\n",
      "INFO:tensorflow:examples/sec: 2.85444\n",
      "I1118 00:07:35.638900 4435920320 tpu_estimator.py:2160] examples/sec: 2.85444\n",
      "INFO:tensorflow:global_step/sec: 0.0878574\n",
      "I1118 00:07:47.020751 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878574\n",
      "INFO:tensorflow:examples/sec: 2.81144\n",
      "I1118 00:07:47.020986 4435920320 tpu_estimator.py:2160] examples/sec: 2.81144\n",
      "INFO:tensorflow:global_step/sec: 0.089281\n",
      "I1118 00:07:58.221349 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089281\n",
      "INFO:tensorflow:examples/sec: 2.85699\n",
      "I1118 00:07:58.221576 4435920320 tpu_estimator.py:2160] examples/sec: 2.85699\n",
      "INFO:tensorflow:global_step/sec: 0.088011\n",
      "I1118 00:08:09.583587 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088011\n",
      "INFO:tensorflow:examples/sec: 2.81635\n",
      "I1118 00:08:09.583802 4435920320 tpu_estimator.py:2160] examples/sec: 2.81635\n",
      "INFO:tensorflow:global_step/sec: 0.0890874\n",
      "I1118 00:08:20.808498 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890874\n",
      "INFO:tensorflow:examples/sec: 2.8508\n",
      "I1118 00:08:20.808722 4435920320 tpu_estimator.py:2160] examples/sec: 2.8508\n",
      "INFO:tensorflow:global_step/sec: 0.0882533\n",
      "I1118 00:08:32.139507 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882533\n",
      "INFO:tensorflow:examples/sec: 2.8241\n",
      "I1118 00:08:32.139770 4435920320 tpu_estimator.py:2160] examples/sec: 2.8241\n",
      "INFO:tensorflow:global_step/sec: 0.0894877\n",
      "I1118 00:08:43.314236 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894877\n",
      "INFO:tensorflow:examples/sec: 2.86361\n",
      "I1118 00:08:43.314460 4435920320 tpu_estimator.py:2160] examples/sec: 2.86361\n",
      "INFO:tensorflow:global_step/sec: 0.0876357\n",
      "I1118 00:08:54.725116 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876357\n",
      "INFO:tensorflow:examples/sec: 2.80434\n",
      "I1118 00:08:54.725335 4435920320 tpu_estimator.py:2160] examples/sec: 2.80434\n",
      "INFO:tensorflow:global_step/sec: 0.0893426\n",
      "I1118 00:09:05.917984 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893426\n",
      "INFO:tensorflow:examples/sec: 2.85896\n",
      "I1118 00:09:05.918202 4435920320 tpu_estimator.py:2160] examples/sec: 2.85896\n",
      "INFO:tensorflow:global_step/sec: 0.0866868\n",
      "I1118 00:09:17.453727 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866868\n",
      "INFO:tensorflow:examples/sec: 2.77398\n",
      "I1118 00:09:17.453925 4435920320 tpu_estimator.py:2160] examples/sec: 2.77398\n",
      "INFO:tensorflow:global_step/sec: 0.0894502\n",
      "I1118 00:09:28.633193 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894502\n",
      "INFO:tensorflow:examples/sec: 2.8624\n",
      "I1118 00:09:28.633422 4435920320 tpu_estimator.py:2160] examples/sec: 2.8624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893329\n",
      "I1118 00:09:39.827269 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893329\n",
      "INFO:tensorflow:examples/sec: 2.85865\n",
      "I1118 00:09:39.827486 4435920320 tpu_estimator.py:2160] examples/sec: 2.85865\n",
      "INFO:tensorflow:global_step/sec: 0.088178\n",
      "I1118 00:09:51.167968 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088178\n",
      "INFO:tensorflow:examples/sec: 2.8217\n",
      "I1118 00:09:51.168190 4435920320 tpu_estimator.py:2160] examples/sec: 2.8217\n",
      "INFO:tensorflow:global_step/sec: 0.0889851\n",
      "I1118 00:10:02.405875 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889851\n",
      "INFO:tensorflow:examples/sec: 2.84752\n",
      "I1118 00:10:02.406104 4435920320 tpu_estimator.py:2160] examples/sec: 2.84752\n",
      "INFO:tensorflow:global_step/sec: 0.0877965\n",
      "I1118 00:10:13.795768 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877965\n",
      "INFO:tensorflow:examples/sec: 2.80949\n",
      "I1118 00:10:13.795978 4435920320 tpu_estimator.py:2160] examples/sec: 2.80949\n",
      "INFO:tensorflow:global_step/sec: 0.0886556\n",
      "I1118 00:10:25.075392 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886556\n",
      "INFO:tensorflow:examples/sec: 2.83698\n",
      "I1118 00:10:25.075620 4435920320 tpu_estimator.py:2160] examples/sec: 2.83698\n",
      "INFO:tensorflow:global_step/sec: 0.0878917\n",
      "I1118 00:10:36.453092 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878917\n",
      "INFO:tensorflow:examples/sec: 2.81253\n",
      "I1118 00:10:36.453419 4435920320 tpu_estimator.py:2160] examples/sec: 2.81253\n",
      "INFO:tensorflow:global_step/sec: 0.0892269\n",
      "I1118 00:10:47.660408 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892269\n",
      "INFO:tensorflow:examples/sec: 2.85526\n",
      "I1118 00:10:47.660632 4435920320 tpu_estimator.py:2160] examples/sec: 2.85526\n",
      "INFO:tensorflow:global_step/sec: 0.088224\n",
      "I1118 00:10:58.995195 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088224\n",
      "INFO:tensorflow:examples/sec: 2.82317\n",
      "I1118 00:10:58.995425 4435920320 tpu_estimator.py:2160] examples/sec: 2.82317\n",
      "INFO:tensorflow:global_step/sec: 0.0891669\n",
      "I1118 00:11:10.210124 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891669\n",
      "INFO:tensorflow:examples/sec: 2.85334\n",
      "I1118 00:11:10.210361 4435920320 tpu_estimator.py:2160] examples/sec: 2.85334\n",
      "INFO:tensorflow:global_step/sec: 0.0888153\n",
      "I1118 00:11:21.469439 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888153\n",
      "INFO:tensorflow:examples/sec: 2.84209\n",
      "I1118 00:11:21.469659 4435920320 tpu_estimator.py:2160] examples/sec: 2.84209\n",
      "INFO:tensorflow:global_step/sec: 0.0878345\n",
      "I1118 00:11:32.854500 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878345\n",
      "INFO:tensorflow:examples/sec: 2.8107\n",
      "I1118 00:11:32.854727 4435920320 tpu_estimator.py:2160] examples/sec: 2.8107\n",
      "INFO:tensorflow:global_step/sec: 0.0893701\n",
      "I1118 00:11:44.043919 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893701\n",
      "INFO:tensorflow:examples/sec: 2.85984\n",
      "I1118 00:11:44.044136 4435920320 tpu_estimator.py:2160] examples/sec: 2.85984\n",
      "INFO:tensorflow:global_step/sec: 0.0879532\n",
      "I1118 00:11:55.413594 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879532\n",
      "INFO:tensorflow:examples/sec: 2.8145\n",
      "I1118 00:11:55.413846 4435920320 tpu_estimator.py:2160] examples/sec: 2.8145\n",
      "INFO:tensorflow:global_step/sec: 0.0890297\n",
      "I1118 00:12:06.645823 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890297\n",
      "INFO:tensorflow:examples/sec: 2.84895\n",
      "I1118 00:12:06.646041 4435920320 tpu_estimator.py:2160] examples/sec: 2.84895\n",
      "INFO:tensorflow:global_step/sec: 0.0872511\n",
      "I1118 00:12:18.106983 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872511\n",
      "INFO:tensorflow:examples/sec: 2.79203\n",
      "I1118 00:12:18.107198 4435920320 tpu_estimator.py:2160] examples/sec: 2.79203\n",
      "INFO:tensorflow:global_step/sec: 0.0890659\n",
      "I1118 00:12:29.334684 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890659\n",
      "INFO:tensorflow:examples/sec: 2.85011\n",
      "I1118 00:12:29.334908 4435920320 tpu_estimator.py:2160] examples/sec: 2.85011\n",
      "INFO:tensorflow:global_step/sec: 0.0888368\n",
      "I1118 00:12:40.591207 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888368\n",
      "INFO:tensorflow:examples/sec: 2.84278\n",
      "I1118 00:12:40.591418 4435920320 tpu_estimator.py:2160] examples/sec: 2.84278\n",
      "INFO:tensorflow:global_step/sec: 0.0874159\n",
      "I1118 00:12:52.030792 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874159\n",
      "INFO:tensorflow:examples/sec: 2.79731\n",
      "I1118 00:12:52.031074 4435920320 tpu_estimator.py:2160] examples/sec: 2.79731\n",
      "INFO:tensorflow:global_step/sec: 0.0880602\n",
      "I1118 00:13:03.386656 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880602\n",
      "INFO:tensorflow:examples/sec: 2.81792\n",
      "I1118 00:13:03.386904 4435920320 tpu_estimator.py:2160] examples/sec: 2.81792\n",
      "INFO:tensorflow:global_step/sec: 0.0887874\n",
      "I1118 00:13:14.649552 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887874\n",
      "INFO:tensorflow:examples/sec: 2.8412\n",
      "I1118 00:13:14.649770 4435920320 tpu_estimator.py:2160] examples/sec: 2.8412\n",
      "INFO:tensorflow:global_step/sec: 0.0883031\n",
      "I1118 00:13:25.974158 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883031\n",
      "INFO:tensorflow:examples/sec: 2.8257\n",
      "I1118 00:13:25.974397 4435920320 tpu_estimator.py:2160] examples/sec: 2.8257\n",
      "INFO:tensorflow:global_step/sec: 0.0890446\n",
      "I1118 00:13:37.204510 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890446\n",
      "INFO:tensorflow:examples/sec: 2.84943\n",
      "I1118 00:13:37.204745 4435920320 tpu_estimator.py:2160] examples/sec: 2.84943\n",
      "INFO:tensorflow:global_step/sec: 0.0891694\n",
      "I1118 00:13:48.419109 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891694\n",
      "INFO:tensorflow:examples/sec: 2.85342\n",
      "I1118 00:13:48.419340 4435920320 tpu_estimator.py:2160] examples/sec: 2.85342\n",
      "INFO:tensorflow:global_step/sec: 0.0877566\n",
      "I1118 00:13:59.814253 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877566\n",
      "INFO:tensorflow:examples/sec: 2.80821\n",
      "I1118 00:13:59.814468 4435920320 tpu_estimator.py:2160] examples/sec: 2.80821\n",
      "INFO:tensorflow:global_step/sec: 0.0883466\n",
      "I1118 00:14:11.133306 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883466\n",
      "INFO:tensorflow:examples/sec: 2.82709\n",
      "I1118 00:14:11.133523 4435920320 tpu_estimator.py:2160] examples/sec: 2.82709\n",
      "INFO:tensorflow:global_step/sec: 0.0883247\n",
      "I1118 00:14:22.455188 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883247\n",
      "INFO:tensorflow:examples/sec: 2.82639\n",
      "I1118 00:14:22.455428 4435920320 tpu_estimator.py:2160] examples/sec: 2.82639\n",
      "INFO:tensorflow:global_step/sec: 0.0881124\n",
      "I1118 00:14:33.804283 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881124\n",
      "INFO:tensorflow:examples/sec: 2.8196\n",
      "I1118 00:14:33.804477 4435920320 tpu_estimator.py:2160] examples/sec: 2.8196\n",
      "INFO:tensorflow:global_step/sec: 0.0888223\n",
      "I1118 00:14:45.062749 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888223\n",
      "INFO:tensorflow:examples/sec: 2.84231\n",
      "I1118 00:14:45.062981 4435920320 tpu_estimator.py:2160] examples/sec: 2.84231\n",
      "INFO:tensorflow:global_step/sec: 0.088013\n",
      "I1118 00:14:56.424690 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088013\n",
      "INFO:tensorflow:examples/sec: 2.81642\n",
      "I1118 00:14:56.424901 4435920320 tpu_estimator.py:2160] examples/sec: 2.81642\n",
      "INFO:tensorflow:global_step/sec: 0.0888824\n",
      "I1118 00:15:07.675549 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888824\n",
      "INFO:tensorflow:examples/sec: 2.84424\n",
      "I1118 00:15:07.675766 4435920320 tpu_estimator.py:2160] examples/sec: 2.84424\n",
      "INFO:tensorflow:global_step/sec: 0.0880441\n",
      "I1118 00:15:19.033496 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880441\n",
      "INFO:tensorflow:examples/sec: 2.81741\n",
      "I1118 00:15:19.033722 4435920320 tpu_estimator.py:2160] examples/sec: 2.81741\n",
      "INFO:tensorflow:global_step/sec: 0.08838\n",
      "I1118 00:15:30.348251 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08838\n",
      "INFO:tensorflow:examples/sec: 2.82816\n",
      "I1118 00:15:30.348475 4435920320 tpu_estimator.py:2160] examples/sec: 2.82816\n",
      "INFO:tensorflow:global_step/sec: 0.0891788\n",
      "I1118 00:15:41.561687 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891788\n",
      "INFO:tensorflow:examples/sec: 2.85372\n",
      "I1118 00:15:41.564367 4435920320 tpu_estimator.py:2160] examples/sec: 2.85372\n",
      "INFO:tensorflow:global_step/sec: 0.0877278\n",
      "I1118 00:15:52.960570 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877278\n",
      "INFO:tensorflow:examples/sec: 2.80729\n",
      "I1118 00:15:52.960808 4435920320 tpu_estimator.py:2160] examples/sec: 2.80729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893469\n",
      "I1118 00:16:04.152884 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893469\n",
      "INFO:tensorflow:examples/sec: 2.8591\n",
      "I1118 00:16:04.153099 4435920320 tpu_estimator.py:2160] examples/sec: 2.8591\n",
      "INFO:tensorflow:global_step/sec: 0.0885491\n",
      "I1118 00:16:15.446063 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885491\n",
      "INFO:tensorflow:examples/sec: 2.83357\n",
      "I1118 00:16:15.446285 4435920320 tpu_estimator.py:2160] examples/sec: 2.83357\n",
      "INFO:tensorflow:global_step/sec: 0.0885897\n",
      "I1118 00:16:26.734040 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885897\n",
      "INFO:tensorflow:examples/sec: 2.83487\n",
      "I1118 00:16:26.734253 4435920320 tpu_estimator.py:2160] examples/sec: 2.83487\n",
      "INFO:tensorflow:global_step/sec: 0.0880682\n",
      "I1118 00:16:38.088895 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880682\n",
      "INFO:tensorflow:examples/sec: 2.81818\n",
      "I1118 00:16:38.089119 4435920320 tpu_estimator.py:2160] examples/sec: 2.81818\n",
      "INFO:tensorflow:global_step/sec: 0.0892899\n",
      "I1118 00:16:49.288361 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892899\n",
      "INFO:tensorflow:examples/sec: 2.85728\n",
      "I1118 00:16:49.288581 4435920320 tpu_estimator.py:2160] examples/sec: 2.85728\n",
      "INFO:tensorflow:global_step/sec: 0.0891906\n",
      "I1118 00:17:00.500309 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891906\n",
      "INFO:tensorflow:examples/sec: 2.8541\n",
      "I1118 00:17:00.500538 4435920320 tpu_estimator.py:2160] examples/sec: 2.8541\n",
      "INFO:tensorflow:global_step/sec: 0.0876549\n",
      "I1118 00:17:11.908720 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876549\n",
      "INFO:tensorflow:examples/sec: 2.80496\n",
      "I1118 00:17:11.908937 4435920320 tpu_estimator.py:2160] examples/sec: 2.80496\n",
      "INFO:tensorflow:global_step/sec: 0.0883234\n",
      "I1118 00:17:23.230727 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883234\n",
      "INFO:tensorflow:examples/sec: 2.82635\n",
      "I1118 00:17:23.230959 4435920320 tpu_estimator.py:2160] examples/sec: 2.82635\n",
      "INFO:tensorflow:global_step/sec: 0.0894084\n",
      "I1118 00:17:34.415372 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894084\n",
      "INFO:tensorflow:examples/sec: 2.86107\n",
      "I1118 00:17:34.415591 4435920320 tpu_estimator.py:2160] examples/sec: 2.86107\n",
      "INFO:tensorflow:global_step/sec: 0.0879889\n",
      "I1118 00:17:45.780439 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879889\n",
      "INFO:tensorflow:examples/sec: 2.81564\n",
      "I1118 00:17:45.780677 4435920320 tpu_estimator.py:2160] examples/sec: 2.81564\n",
      "INFO:tensorflow:global_step/sec: 0.0892393\n",
      "I1118 00:17:56.986244 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892393\n",
      "INFO:tensorflow:examples/sec: 2.85566\n",
      "I1118 00:17:56.986464 4435920320 tpu_estimator.py:2160] examples/sec: 2.85566\n",
      "INFO:tensorflow:global_step/sec: 0.0879333\n",
      "I1118 00:18:08.358550 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879333\n",
      "INFO:tensorflow:examples/sec: 2.81387\n",
      "I1118 00:18:08.358999 4435920320 tpu_estimator.py:2160] examples/sec: 2.81387\n",
      "INFO:tensorflow:global_step/sec: 0.0892107\n",
      "I1118 00:18:19.567930 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892107\n",
      "INFO:tensorflow:examples/sec: 2.85474\n",
      "I1118 00:18:19.568164 4435920320 tpu_estimator.py:2160] examples/sec: 2.85474\n",
      "INFO:tensorflow:global_step/sec: 0.0889216\n",
      "I1118 00:18:30.813774 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889216\n",
      "INFO:tensorflow:examples/sec: 2.84549\n",
      "I1118 00:18:30.813987 4435920320 tpu_estimator.py:2160] examples/sec: 2.84549\n",
      "INFO:tensorflow:global_step/sec: 0.0880057\n",
      "I1118 00:18:42.176671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880057\n",
      "INFO:tensorflow:examples/sec: 2.81618\n",
      "I1118 00:18:42.176887 4435920320 tpu_estimator.py:2160] examples/sec: 2.81618\n",
      "INFO:tensorflow:global_step/sec: 0.0895861\n",
      "I1118 00:18:53.339119 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895861\n",
      "INFO:tensorflow:examples/sec: 2.86676\n",
      "I1118 00:18:53.339370 4435920320 tpu_estimator.py:2160] examples/sec: 2.86676\n",
      "INFO:tensorflow:global_step/sec: 0.0892579\n",
      "I1118 00:19:04.542610 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892579\n",
      "INFO:tensorflow:examples/sec: 2.85625\n",
      "I1118 00:19:04.542832 4435920320 tpu_estimator.py:2160] examples/sec: 2.85625\n",
      "INFO:tensorflow:global_step/sec: 0.0876169\n",
      "I1118 00:19:15.955981 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876169\n",
      "INFO:tensorflow:examples/sec: 2.80374\n",
      "I1118 00:19:15.956229 4435920320 tpu_estimator.py:2160] examples/sec: 2.80374\n",
      "INFO:tensorflow:global_step/sec: 0.0897759\n",
      "I1118 00:19:27.094789 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897759\n",
      "INFO:tensorflow:examples/sec: 2.87283\n",
      "I1118 00:19:27.095026 4435920320 tpu_estimator.py:2160] examples/sec: 2.87283\n",
      "INFO:tensorflow:global_step/sec: 0.0889233\n",
      "I1118 00:19:38.340440 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889233\n",
      "INFO:tensorflow:examples/sec: 2.84554\n",
      "I1118 00:19:38.340682 4435920320 tpu_estimator.py:2160] examples/sec: 2.84554\n",
      "INFO:tensorflow:global_step/sec: 0.0883957\n",
      "I1118 00:19:49.653187 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883957\n",
      "INFO:tensorflow:examples/sec: 2.82866\n",
      "I1118 00:19:49.653419 4435920320 tpu_estimator.py:2160] examples/sec: 2.82866\n",
      "INFO:tensorflow:global_step/sec: 0.0892718\n",
      "I1118 00:20:00.854925 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892718\n",
      "INFO:tensorflow:examples/sec: 2.8567\n",
      "I1118 00:20:00.855142 4435920320 tpu_estimator.py:2160] examples/sec: 2.8567\n",
      "INFO:tensorflow:global_step/sec: 0.0890727\n",
      "I1118 00:20:12.081722 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890727\n",
      "INFO:tensorflow:examples/sec: 2.85033\n",
      "I1118 00:20:12.081946 4435920320 tpu_estimator.py:2160] examples/sec: 2.85033\n",
      "INFO:tensorflow:global_step/sec: 0.0893532\n",
      "I1118 00:20:23.273264 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893532\n",
      "INFO:tensorflow:examples/sec: 2.8593\n",
      "I1118 00:20:23.273487 4435920320 tpu_estimator.py:2160] examples/sec: 2.8593\n",
      "INFO:tensorflow:global_step/sec: 0.0861783\n",
      "I1118 00:20:34.877109 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861783\n",
      "INFO:tensorflow:examples/sec: 2.7577\n",
      "I1118 00:20:34.877317 4435920320 tpu_estimator.py:2160] examples/sec: 2.7577\n",
      "INFO:tensorflow:global_step/sec: 0.0889808\n",
      "I1118 00:20:46.115509 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889808\n",
      "INFO:tensorflow:examples/sec: 2.84738\n",
      "I1118 00:20:46.115731 4435920320 tpu_estimator.py:2160] examples/sec: 2.84738\n",
      "INFO:tensorflow:global_step/sec: 0.0891037\n",
      "I1118 00:20:57.338366 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891037\n",
      "INFO:tensorflow:examples/sec: 2.85132\n",
      "I1118 00:20:57.338586 4435920320 tpu_estimator.py:2160] examples/sec: 2.85132\n",
      "INFO:tensorflow:global_step/sec: 0.0889573\n",
      "I1118 00:21:08.579713 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889573\n",
      "INFO:tensorflow:examples/sec: 2.84663\n",
      "I1118 00:21:08.579946 4435920320 tpu_estimator.py:2160] examples/sec: 2.84663\n",
      "INFO:tensorflow:global_step/sec: 0.087456\n",
      "I1118 00:21:20.014049 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087456\n",
      "INFO:tensorflow:examples/sec: 2.79859\n",
      "I1118 00:21:20.014289 4435920320 tpu_estimator.py:2160] examples/sec: 2.79859\n",
      "INFO:tensorflow:global_step/sec: 0.0890856\n",
      "I1118 00:21:31.239226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890856\n",
      "INFO:tensorflow:examples/sec: 2.85074\n",
      "I1118 00:21:31.239451 4435920320 tpu_estimator.py:2160] examples/sec: 2.85074\n",
      "INFO:tensorflow:global_step/sec: 0.0887831\n",
      "I1118 00:21:42.502614 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887831\n",
      "INFO:tensorflow:examples/sec: 2.84106\n",
      "I1118 00:21:42.502871 4435920320 tpu_estimator.py:2160] examples/sec: 2.84106\n",
      "INFO:tensorflow:global_step/sec: 0.0889999\n",
      "I1118 00:21:53.738577 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889999\n",
      "INFO:tensorflow:examples/sec: 2.848\n",
      "I1118 00:21:53.738801 4435920320 tpu_estimator.py:2160] examples/sec: 2.848\n",
      "INFO:tensorflow:global_step/sec: 0.0880054\n",
      "I1118 00:22:05.101531 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880054\n",
      "INFO:tensorflow:examples/sec: 2.81617\n",
      "I1118 00:22:05.101763 4435920320 tpu_estimator.py:2160] examples/sec: 2.81617\n",
      "INFO:tensorflow:global_step/sec: 0.0888925\n",
      "I1118 00:22:16.351059 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888925\n",
      "INFO:tensorflow:examples/sec: 2.84456\n",
      "I1118 00:22:16.351292 4435920320 tpu_estimator.py:2160] examples/sec: 2.84456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0890976\n",
      "I1118 00:22:27.574764 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890976\n",
      "INFO:tensorflow:examples/sec: 2.85112\n",
      "I1118 00:22:27.574995 4435920320 tpu_estimator.py:2160] examples/sec: 2.85112\n",
      "INFO:tensorflow:global_step/sec: 0.0876058\n",
      "I1118 00:22:38.989454 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876058\n",
      "INFO:tensorflow:examples/sec: 2.80339\n",
      "I1118 00:22:38.989650 4435920320 tpu_estimator.py:2160] examples/sec: 2.80339\n",
      "INFO:tensorflow:global_step/sec: 0.0876916\n",
      "I1118 00:22:50.393072 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876916\n",
      "INFO:tensorflow:examples/sec: 2.80613\n",
      "I1118 00:22:50.393286 4435920320 tpu_estimator.py:2160] examples/sec: 2.80613\n",
      "INFO:tensorflow:global_step/sec: 0.0878361\n",
      "I1118 00:23:01.777929 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878361\n",
      "INFO:tensorflow:examples/sec: 2.81076\n",
      "I1118 00:23:01.778354 4435920320 tpu_estimator.py:2160] examples/sec: 2.81076\n",
      "INFO:tensorflow:global_step/sec: 0.0896935\n",
      "I1118 00:23:12.926993 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896935\n",
      "INFO:tensorflow:examples/sec: 2.87019\n",
      "I1118 00:23:12.927217 4435920320 tpu_estimator.py:2160] examples/sec: 2.87019\n",
      "INFO:tensorflow:global_step/sec: 0.089205\n",
      "I1118 00:23:24.137147 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089205\n",
      "INFO:tensorflow:examples/sec: 2.85456\n",
      "I1118 00:23:24.137387 4435920320 tpu_estimator.py:2160] examples/sec: 2.85456\n",
      "INFO:tensorflow:global_step/sec: 0.0895509\n",
      "I1118 00:23:35.303971 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895509\n",
      "INFO:tensorflow:examples/sec: 2.86563\n",
      "I1118 00:23:35.304203 4435920320 tpu_estimator.py:2160] examples/sec: 2.86563\n",
      "INFO:tensorflow:global_step/sec: 0.0889788\n",
      "I1118 00:23:46.542613 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889788\n",
      "INFO:tensorflow:examples/sec: 2.84732\n",
      "I1118 00:23:46.542832 4435920320 tpu_estimator.py:2160] examples/sec: 2.84732\n",
      "INFO:tensorflow:global_step/sec: 0.0862922\n",
      "I1118 00:23:58.131120 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0862922\n",
      "INFO:tensorflow:examples/sec: 2.76135\n",
      "I1118 00:23:58.131332 4435920320 tpu_estimator.py:2160] examples/sec: 2.76135\n",
      "INFO:tensorflow:global_step/sec: 0.0892739\n",
      "I1118 00:24:09.332632 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892739\n",
      "INFO:tensorflow:examples/sec: 2.85677\n",
      "I1118 00:24:09.332854 4435920320 tpu_estimator.py:2160] examples/sec: 2.85677\n",
      "INFO:tensorflow:global_step/sec: 0.0890224\n",
      "I1118 00:24:20.565743 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890224\n",
      "INFO:tensorflow:examples/sec: 2.84872\n",
      "I1118 00:24:20.565958 4435920320 tpu_estimator.py:2160] examples/sec: 2.84872\n",
      "INFO:tensorflow:global_step/sec: 0.0890495\n",
      "I1118 00:24:31.795454 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890495\n",
      "INFO:tensorflow:examples/sec: 2.84958\n",
      "I1118 00:24:31.795684 4435920320 tpu_estimator.py:2160] examples/sec: 2.84958\n",
      "INFO:tensorflow:global_step/sec: 0.0868952\n",
      "I1118 00:24:43.303539 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868952\n",
      "INFO:tensorflow:examples/sec: 2.78065\n",
      "I1118 00:24:43.303718 4435920320 tpu_estimator.py:2160] examples/sec: 2.78065\n",
      "INFO:tensorflow:global_step/sec: 0.0885359\n",
      "I1118 00:24:54.598412 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885359\n",
      "INFO:tensorflow:examples/sec: 2.83315\n",
      "I1118 00:24:54.598618 4435920320 tpu_estimator.py:2160] examples/sec: 2.83315\n",
      "INFO:tensorflow:global_step/sec: 0.087788\n",
      "I1118 00:25:05.989512 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087788\n",
      "INFO:tensorflow:examples/sec: 2.80922\n",
      "I1118 00:25:05.989728 4435920320 tpu_estimator.py:2160] examples/sec: 2.80922\n",
      "INFO:tensorflow:global_step/sec: 0.0882084\n",
      "I1118 00:25:17.326278 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882084\n",
      "INFO:tensorflow:examples/sec: 2.82267\n",
      "I1118 00:25:17.326488 4435920320 tpu_estimator.py:2160] examples/sec: 2.82267\n",
      "INFO:tensorflow:global_step/sec: 0.0890029\n",
      "I1118 00:25:28.561879 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890029\n",
      "INFO:tensorflow:examples/sec: 2.84809\n",
      "I1118 00:25:28.562151 4435920320 tpu_estimator.py:2160] examples/sec: 2.84809\n",
      "INFO:tensorflow:global_step/sec: 0.0895658\n",
      "I1118 00:25:39.726863 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895658\n",
      "INFO:tensorflow:examples/sec: 2.8661\n",
      "I1118 00:25:39.727083 4435920320 tpu_estimator.py:2160] examples/sec: 2.8661\n",
      "INFO:tensorflow:global_step/sec: 0.0891793\n",
      "I1118 00:25:50.940237 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891793\n",
      "INFO:tensorflow:examples/sec: 2.85374\n",
      "I1118 00:25:50.940468 4435920320 tpu_estimator.py:2160] examples/sec: 2.85374\n",
      "INFO:tensorflow:global_step/sec: 0.0891667\n",
      "I1118 00:26:02.155171 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891667\n",
      "INFO:tensorflow:examples/sec: 2.85334\n",
      "I1118 00:26:02.155393 4435920320 tpu_estimator.py:2160] examples/sec: 2.85334\n",
      "INFO:tensorflow:global_step/sec: 0.0893469\n",
      "I1118 00:26:13.347507 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893469\n",
      "INFO:tensorflow:examples/sec: 2.8591\n",
      "I1118 00:26:13.347740 4435920320 tpu_estimator.py:2160] examples/sec: 2.8591\n",
      "INFO:tensorflow:global_step/sec: 0.0892673\n",
      "I1118 00:26:24.549808 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892673\n",
      "INFO:tensorflow:examples/sec: 2.85655\n",
      "I1118 00:26:24.550029 4435920320 tpu_estimator.py:2160] examples/sec: 2.85655\n",
      "INFO:tensorflow:global_step/sec: 0.0891454\n",
      "I1118 00:26:35.767442 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891454\n",
      "INFO:tensorflow:examples/sec: 2.85265\n",
      "I1118 00:26:35.767657 4435920320 tpu_estimator.py:2160] examples/sec: 2.85265\n",
      "INFO:tensorflow:global_step/sec: 0.0879954\n",
      "I1118 00:26:47.131683 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879954\n",
      "INFO:tensorflow:examples/sec: 2.81585\n",
      "I1118 00:26:47.131946 4435920320 tpu_estimator.py:2160] examples/sec: 2.81585\n",
      "INFO:tensorflow:global_step/sec: 0.0893014\n",
      "I1118 00:26:58.329761 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893014\n",
      "INFO:tensorflow:examples/sec: 2.85764\n",
      "I1118 00:26:58.330042 4435920320 tpu_estimator.py:2160] examples/sec: 2.85764\n",
      "INFO:tensorflow:global_step/sec: 0.0891855\n",
      "I1118 00:27:09.542296 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891855\n",
      "INFO:tensorflow:examples/sec: 2.85394\n",
      "I1118 00:27:09.542512 4435920320 tpu_estimator.py:2160] examples/sec: 2.85394\n",
      "INFO:tensorflow:global_step/sec: 0.0872699\n",
      "I1118 00:27:21.001048 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872699\n",
      "INFO:tensorflow:examples/sec: 2.79264\n",
      "I1118 00:27:21.001260 4435920320 tpu_estimator.py:2160] examples/sec: 2.79264\n",
      "INFO:tensorflow:global_step/sec: 0.0892243\n",
      "I1118 00:27:32.208724 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892243\n",
      "INFO:tensorflow:examples/sec: 2.85518\n",
      "I1118 00:27:32.209095 4435920320 tpu_estimator.py:2160] examples/sec: 2.85518\n",
      "INFO:tensorflow:global_step/sec: 0.0882966\n",
      "I1118 00:27:43.534181 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882966\n",
      "INFO:tensorflow:examples/sec: 2.82549\n",
      "I1118 00:27:43.534407 4435920320 tpu_estimator.py:2160] examples/sec: 2.82549\n",
      "INFO:tensorflow:global_step/sec: 0.0895329\n",
      "I1118 00:27:54.703253 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895329\n",
      "INFO:tensorflow:examples/sec: 2.86505\n",
      "I1118 00:27:54.703472 4435920320 tpu_estimator.py:2160] examples/sec: 2.86505\n",
      "INFO:tensorflow:global_step/sec: 0.0890002\n",
      "I1118 00:28:05.939166 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890002\n",
      "INFO:tensorflow:examples/sec: 2.848\n",
      "I1118 00:28:05.939407 4435920320 tpu_estimator.py:2160] examples/sec: 2.848\n",
      "INFO:tensorflow:global_step/sec: 0.0889738\n",
      "I1118 00:28:17.178476 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889738\n",
      "INFO:tensorflow:examples/sec: 2.84716\n",
      "I1118 00:28:17.178717 4435920320 tpu_estimator.py:2160] examples/sec: 2.84716\n",
      "INFO:tensorflow:global_step/sec: 0.089065\n",
      "I1118 00:28:28.406239 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089065\n",
      "INFO:tensorflow:examples/sec: 2.85008\n",
      "I1118 00:28:28.406458 4435920320 tpu_estimator.py:2160] examples/sec: 2.85008\n",
      "INFO:tensorflow:global_step/sec: 0.0891748\n",
      "I1118 00:28:39.620162 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891748\n",
      "INFO:tensorflow:examples/sec: 2.85359\n",
      "I1118 00:28:39.620389 4435920320 tpu_estimator.py:2160] examples/sec: 2.85359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0878218\n",
      "I1118 00:28:51.006865 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878218\n",
      "INFO:tensorflow:examples/sec: 2.8103\n",
      "I1118 00:28:51.007078 4435920320 tpu_estimator.py:2160] examples/sec: 2.8103\n",
      "INFO:tensorflow:global_step/sec: 0.0888126\n",
      "I1118 00:29:02.266527 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888126\n",
      "INFO:tensorflow:examples/sec: 2.842\n",
      "I1118 00:29:02.266744 4435920320 tpu_estimator.py:2160] examples/sec: 2.842\n",
      "INFO:tensorflow:global_step/sec: 0.0883557\n",
      "I1118 00:29:13.584396 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883557\n",
      "INFO:tensorflow:examples/sec: 2.82738\n",
      "I1118 00:29:13.584606 4435920320 tpu_estimator.py:2160] examples/sec: 2.82738\n",
      "INFO:tensorflow:global_step/sec: 0.088204\n",
      "I1118 00:29:24.921757 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088204\n",
      "INFO:tensorflow:examples/sec: 2.82253\n",
      "I1118 00:29:24.921976 4435920320 tpu_estimator.py:2160] examples/sec: 2.82253\n",
      "INFO:tensorflow:global_step/sec: 0.0888832\n",
      "I1118 00:29:36.172477 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888832\n",
      "INFO:tensorflow:examples/sec: 2.84426\n",
      "I1118 00:29:36.172697 4435920320 tpu_estimator.py:2160] examples/sec: 2.84426\n",
      "INFO:tensorflow:global_step/sec: 0.0890691\n",
      "I1118 00:29:47.399718 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890691\n",
      "INFO:tensorflow:examples/sec: 2.85021\n",
      "I1118 00:29:47.399950 4435920320 tpu_estimator.py:2160] examples/sec: 2.85021\n",
      "INFO:tensorflow:global_step/sec: 0.086809\n",
      "I1118 00:29:58.919276 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086809\n",
      "INFO:tensorflow:examples/sec: 2.77789\n",
      "I1118 00:29:58.919723 4435920320 tpu_estimator.py:2160] examples/sec: 2.77789\n",
      "INFO:tensorflow:global_step/sec: 0.0877772\n",
      "I1118 00:30:10.311798 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877772\n",
      "INFO:tensorflow:examples/sec: 2.80887\n",
      "I1118 00:30:10.312024 4435920320 tpu_estimator.py:2160] examples/sec: 2.80887\n",
      "INFO:tensorflow:global_step/sec: 0.0874359\n",
      "I1118 00:30:21.748677 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874359\n",
      "INFO:tensorflow:examples/sec: 2.79795\n",
      "I1118 00:30:21.748893 4435920320 tpu_estimator.py:2160] examples/sec: 2.79795\n",
      "INFO:tensorflow:global_step/sec: 0.0889741\n",
      "I1118 00:30:32.987904 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889741\n",
      "INFO:tensorflow:examples/sec: 2.84717\n",
      "I1118 00:30:32.988136 4435920320 tpu_estimator.py:2160] examples/sec: 2.84717\n",
      "INFO:tensorflow:global_step/sec: 0.0882264\n",
      "I1118 00:30:44.322373 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882264\n",
      "INFO:tensorflow:examples/sec: 2.82324\n",
      "I1118 00:30:44.322591 4435920320 tpu_estimator.py:2160] examples/sec: 2.82324\n",
      "INFO:tensorflow:global_step/sec: 0.0887402\n",
      "I1118 00:30:55.591235 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887402\n",
      "INFO:tensorflow:examples/sec: 2.83969\n",
      "I1118 00:30:55.591452 4435920320 tpu_estimator.py:2160] examples/sec: 2.83969\n",
      "INFO:tensorflow:global_step/sec: 0.0893803\n",
      "I1118 00:31:06.779345 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893803\n",
      "INFO:tensorflow:examples/sec: 2.86017\n",
      "I1118 00:31:06.779541 4435920320 tpu_estimator.py:2160] examples/sec: 2.86017\n",
      "INFO:tensorflow:global_step/sec: 0.0869994\n",
      "I1118 00:31:18.273701 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869994\n",
      "INFO:tensorflow:examples/sec: 2.78398\n",
      "I1118 00:31:18.273910 4435920320 tpu_estimator.py:2160] examples/sec: 2.78398\n",
      "INFO:tensorflow:global_step/sec: 0.0879769\n",
      "I1118 00:31:29.640330 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879769\n",
      "INFO:tensorflow:examples/sec: 2.81526\n",
      "I1118 00:31:29.640552 4435920320 tpu_estimator.py:2160] examples/sec: 2.81526\n",
      "INFO:tensorflow:global_step/sec: 0.0884592\n",
      "I1118 00:31:40.944978 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884592\n",
      "INFO:tensorflow:examples/sec: 2.83069\n",
      "I1118 00:31:40.945199 4435920320 tpu_estimator.py:2160] examples/sec: 2.83069\n",
      "INFO:tensorflow:global_step/sec: 0.0871107\n",
      "I1118 00:31:52.424632 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871107\n",
      "INFO:tensorflow:examples/sec: 2.78754\n",
      "I1118 00:31:52.424834 4435920320 tpu_estimator.py:2160] examples/sec: 2.78754\n",
      "INFO:tensorflow:global_step/sec: 0.0884255\n",
      "I1118 00:32:03.733571 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884255\n",
      "INFO:tensorflow:examples/sec: 2.82962\n",
      "I1118 00:32:03.733772 4435920320 tpu_estimator.py:2160] examples/sec: 2.82962\n",
      "INFO:tensorflow:global_step/sec: 0.0868867\n",
      "I1118 00:32:15.242846 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868867\n",
      "INFO:tensorflow:examples/sec: 2.78037\n",
      "I1118 00:32:15.243108 4435920320 tpu_estimator.py:2160] examples/sec: 2.78037\n",
      "INFO:tensorflow:global_step/sec: 0.0880824\n",
      "I1118 00:32:26.595813 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880824\n",
      "INFO:tensorflow:examples/sec: 2.81864\n",
      "I1118 00:32:26.596016 4435920320 tpu_estimator.py:2160] examples/sec: 2.81864\n",
      "INFO:tensorflow:global_step/sec: 0.0877583\n",
      "I1118 00:32:37.990759 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877583\n",
      "INFO:tensorflow:examples/sec: 2.80827\n",
      "I1118 00:32:37.990980 4435920320 tpu_estimator.py:2160] examples/sec: 2.80827\n",
      "INFO:tensorflow:global_step/sec: 0.0882346\n",
      "I1118 00:32:49.324187 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882346\n",
      "INFO:tensorflow:examples/sec: 2.82351\n",
      "I1118 00:32:49.324409 4435920320 tpu_estimator.py:2160] examples/sec: 2.82351\n",
      "INFO:tensorflow:global_step/sec: 0.0890557\n",
      "I1118 00:33:00.553112 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890557\n",
      "INFO:tensorflow:examples/sec: 2.84978\n",
      "I1118 00:33:00.553337 4435920320 tpu_estimator.py:2160] examples/sec: 2.84978\n",
      "INFO:tensorflow:global_step/sec: 0.0876598\n",
      "I1118 00:33:11.960840 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876598\n",
      "INFO:tensorflow:examples/sec: 2.80511\n",
      "I1118 00:33:11.961264 4435920320 tpu_estimator.py:2160] examples/sec: 2.80511\n",
      "INFO:tensorflow:global_step/sec: 0.0886957\n",
      "I1118 00:33:23.235376 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886957\n",
      "INFO:tensorflow:examples/sec: 2.83826\n",
      "I1118 00:33:23.235629 4435920320 tpu_estimator.py:2160] examples/sec: 2.83826\n",
      "INFO:tensorflow:global_step/sec: 0.0889131\n",
      "I1118 00:33:34.482290 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889131\n",
      "INFO:tensorflow:examples/sec: 2.84522\n",
      "I1118 00:33:34.482526 4435920320 tpu_estimator.py:2160] examples/sec: 2.84522\n",
      "INFO:tensorflow:global_step/sec: 0.0887313\n",
      "I1118 00:33:45.752254 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887313\n",
      "INFO:tensorflow:examples/sec: 2.8394\n",
      "I1118 00:33:45.752473 4435920320 tpu_estimator.py:2160] examples/sec: 2.8394\n",
      "INFO:tensorflow:global_step/sec: 0.0890212\n",
      "I1118 00:33:56.985526 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890212\n",
      "INFO:tensorflow:examples/sec: 2.84868\n",
      "I1118 00:33:56.985743 4435920320 tpu_estimator.py:2160] examples/sec: 2.84868\n",
      "INFO:tensorflow:global_step/sec: 0.0888184\n",
      "I1118 00:34:08.244457 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888184\n",
      "INFO:tensorflow:examples/sec: 2.84219\n",
      "I1118 00:34:08.244679 4435920320 tpu_estimator.py:2160] examples/sec: 2.84219\n",
      "INFO:tensorflow:global_step/sec: 0.0879571\n",
      "I1118 00:34:19.613631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879571\n",
      "INFO:tensorflow:examples/sec: 2.81463\n",
      "I1118 00:34:19.613853 4435920320 tpu_estimator.py:2160] examples/sec: 2.81463\n",
      "INFO:tensorflow:global_step/sec: 0.0881921\n",
      "I1118 00:34:30.952532 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881921\n",
      "INFO:tensorflow:examples/sec: 2.82215\n",
      "I1118 00:34:30.952762 4435920320 tpu_estimator.py:2160] examples/sec: 2.82215\n",
      "INFO:tensorflow:global_step/sec: 0.0882478\n",
      "I1118 00:34:42.284270 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882478\n",
      "INFO:tensorflow:examples/sec: 2.82393\n",
      "I1118 00:34:42.284474 4435920320 tpu_estimator.py:2160] examples/sec: 2.82393\n",
      "INFO:tensorflow:global_step/sec: 0.0884925\n",
      "I1118 00:34:53.584724 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884925\n",
      "INFO:tensorflow:examples/sec: 2.83176\n",
      "I1118 00:34:53.584933 4435920320 tpu_estimator.py:2160] examples/sec: 2.83176\n",
      "INFO:tensorflow:global_step/sec: 0.0882841\n",
      "I1118 00:35:04.911714 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882841\n",
      "INFO:tensorflow:examples/sec: 2.82509\n",
      "I1118 00:35:04.911965 4435920320 tpu_estimator.py:2160] examples/sec: 2.82509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891035\n",
      "I1118 00:35:16.134616 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891035\n",
      "INFO:tensorflow:examples/sec: 2.85131\n",
      "I1118 00:35:16.134841 4435920320 tpu_estimator.py:2160] examples/sec: 2.85131\n",
      "INFO:tensorflow:global_step/sec: 0.0890308\n",
      "I1118 00:35:27.366688 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890308\n",
      "INFO:tensorflow:examples/sec: 2.84899\n",
      "I1118 00:35:27.366906 4435920320 tpu_estimator.py:2160] examples/sec: 2.84899\n",
      "INFO:tensorflow:global_step/sec: 0.0881567\n",
      "I1118 00:35:38.710108 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881567\n",
      "INFO:tensorflow:examples/sec: 2.82102\n",
      "I1118 00:35:38.710329 4435920320 tpu_estimator.py:2160] examples/sec: 2.82102\n",
      "INFO:tensorflow:global_step/sec: 0.0885049\n",
      "I1118 00:35:50.008908 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885049\n",
      "INFO:tensorflow:examples/sec: 2.83216\n",
      "I1118 00:35:50.009137 4435920320 tpu_estimator.py:2160] examples/sec: 2.83216\n",
      "INFO:tensorflow:global_step/sec: 0.0887964\n",
      "I1118 00:36:01.270623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887964\n",
      "INFO:tensorflow:examples/sec: 2.84149\n",
      "I1118 00:36:01.270840 4435920320 tpu_estimator.py:2160] examples/sec: 2.84149\n",
      "INFO:tensorflow:global_step/sec: 0.0881502\n",
      "I1118 00:36:12.614905 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881502\n",
      "INFO:tensorflow:examples/sec: 2.8208\n",
      "I1118 00:36:12.615128 4435920320 tpu_estimator.py:2160] examples/sec: 2.8208\n",
      "INFO:tensorflow:global_step/sec: 0.0879072\n",
      "I1118 00:36:23.990545 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879072\n",
      "INFO:tensorflow:examples/sec: 2.81303\n",
      "I1118 00:36:23.990772 4435920320 tpu_estimator.py:2160] examples/sec: 2.81303\n",
      "INFO:tensorflow:global_step/sec: 0.0882706\n",
      "I1118 00:36:35.319333 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882706\n",
      "INFO:tensorflow:examples/sec: 2.82466\n",
      "I1118 00:36:35.319551 4435920320 tpu_estimator.py:2160] examples/sec: 2.82466\n",
      "INFO:tensorflow:global_step/sec: 0.088295\n",
      "I1118 00:36:46.645003 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088295\n",
      "INFO:tensorflow:examples/sec: 2.82544\n",
      "I1118 00:36:46.645228 4435920320 tpu_estimator.py:2160] examples/sec: 2.82544\n",
      "INFO:tensorflow:global_step/sec: 0.0890346\n",
      "I1118 00:36:57.876588 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890346\n",
      "INFO:tensorflow:examples/sec: 2.84911\n",
      "I1118 00:36:57.876806 4435920320 tpu_estimator.py:2160] examples/sec: 2.84911\n",
      "INFO:tensorflow:global_step/sec: 0.0883088\n",
      "I1118 00:37:09.200489 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883088\n",
      "INFO:tensorflow:examples/sec: 2.82588\n",
      "I1118 00:37:09.200855 4435920320 tpu_estimator.py:2160] examples/sec: 2.82588\n",
      "INFO:tensorflow:global_step/sec: 0.0873863\n",
      "I1118 00:37:20.643944 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873863\n",
      "INFO:tensorflow:examples/sec: 2.79636\n",
      "I1118 00:37:20.644174 4435920320 tpu_estimator.py:2160] examples/sec: 2.79636\n",
      "INFO:tensorflow:global_step/sec: 0.0883719\n",
      "I1118 00:37:31.959738 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883719\n",
      "INFO:tensorflow:examples/sec: 2.8279\n",
      "I1118 00:37:31.959954 4435920320 tpu_estimator.py:2160] examples/sec: 2.8279\n",
      "INFO:tensorflow:global_step/sec: 0.0883654\n",
      "I1118 00:37:43.276384 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883654\n",
      "INFO:tensorflow:examples/sec: 2.82769\n",
      "I1118 00:37:43.276605 4435920320 tpu_estimator.py:2160] examples/sec: 2.82769\n",
      "INFO:tensorflow:global_step/sec: 0.0885449\n",
      "I1118 00:37:54.570107 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885449\n",
      "INFO:tensorflow:examples/sec: 2.83344\n",
      "I1118 00:37:54.570338 4435920320 tpu_estimator.py:2160] examples/sec: 2.83344\n",
      "INFO:tensorflow:global_step/sec: 0.0854562\n",
      "I1118 00:38:06.271960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0854562\n",
      "INFO:tensorflow:examples/sec: 2.7346\n",
      "I1118 00:38:06.272142 4435920320 tpu_estimator.py:2160] examples/sec: 2.7346\n",
      "INFO:tensorflow:global_step/sec: 0.0881099\n",
      "I1118 00:38:17.621478 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881099\n",
      "INFO:tensorflow:examples/sec: 2.81952\n",
      "I1118 00:38:17.621737 4435920320 tpu_estimator.py:2160] examples/sec: 2.81952\n",
      "INFO:tensorflow:global_step/sec: 0.0886936\n",
      "I1118 00:38:28.896210 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886936\n",
      "INFO:tensorflow:examples/sec: 2.8382\n",
      "I1118 00:38:28.896409 4435920320 tpu_estimator.py:2160] examples/sec: 2.8382\n",
      "INFO:tensorflow:global_step/sec: 0.08821\n",
      "I1118 00:38:40.232825 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08821\n",
      "INFO:tensorflow:examples/sec: 2.82272\n",
      "I1118 00:38:40.233049 4435920320 tpu_estimator.py:2160] examples/sec: 2.82272\n",
      "INFO:tensorflow:global_step/sec: 0.0878585\n",
      "I1118 00:38:51.614732 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878585\n",
      "INFO:tensorflow:examples/sec: 2.81147\n",
      "I1118 00:38:51.614943 4435920320 tpu_estimator.py:2160] examples/sec: 2.81147\n",
      "INFO:tensorflow:global_step/sec: 0.0880791\n",
      "I1118 00:39:02.968251 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880791\n",
      "INFO:tensorflow:examples/sec: 2.81853\n",
      "I1118 00:39:02.968580 4435920320 tpu_estimator.py:2160] examples/sec: 2.81853\n",
      "INFO:tensorflow:global_step/sec: 0.0886728\n",
      "I1118 00:39:14.245607 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886728\n",
      "INFO:tensorflow:examples/sec: 2.83753\n",
      "I1118 00:39:14.245815 4435920320 tpu_estimator.py:2160] examples/sec: 2.83753\n",
      "INFO:tensorflow:global_step/sec: 0.0889289\n",
      "I1118 00:39:25.490532 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889289\n",
      "INFO:tensorflow:examples/sec: 2.84573\n",
      "I1118 00:39:25.490761 4435920320 tpu_estimator.py:2160] examples/sec: 2.84573\n",
      "INFO:tensorflow:global_step/sec: 0.0889126\n",
      "I1118 00:39:36.737528 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889126\n",
      "INFO:tensorflow:examples/sec: 2.8452\n",
      "I1118 00:39:36.737751 4435920320 tpu_estimator.py:2160] examples/sec: 2.8452\n",
      "INFO:tensorflow:global_step/sec: 0.0891104\n",
      "I1118 00:39:47.959590 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891104\n",
      "INFO:tensorflow:examples/sec: 2.85153\n",
      "I1118 00:39:47.959810 4435920320 tpu_estimator.py:2160] examples/sec: 2.85153\n",
      "INFO:tensorflow:global_step/sec: 0.0888904\n",
      "I1118 00:39:59.209409 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888904\n",
      "INFO:tensorflow:examples/sec: 2.84449\n",
      "I1118 00:39:59.209630 4435920320 tpu_estimator.py:2160] examples/sec: 2.84449\n",
      "INFO:tensorflow:global_step/sec: 0.0890379\n",
      "I1118 00:40:10.440560 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890379\n",
      "INFO:tensorflow:examples/sec: 2.84921\n",
      "I1118 00:40:10.440784 4435920320 tpu_estimator.py:2160] examples/sec: 2.84921\n",
      "INFO:tensorflow:global_step/sec: 0.0885784\n",
      "I1118 00:40:21.729994 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885784\n",
      "INFO:tensorflow:examples/sec: 2.83451\n",
      "I1118 00:40:21.730216 4435920320 tpu_estimator.py:2160] examples/sec: 2.83451\n",
      "INFO:tensorflow:global_step/sec: 0.0883093\n",
      "I1118 00:40:33.053823 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883093\n",
      "INFO:tensorflow:examples/sec: 2.8259\n",
      "I1118 00:40:33.054046 4435920320 tpu_estimator.py:2160] examples/sec: 2.8259\n",
      "INFO:tensorflow:global_step/sec: 0.0887533\n",
      "I1118 00:40:44.321010 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887533\n",
      "INFO:tensorflow:examples/sec: 2.84011\n",
      "I1118 00:40:44.321231 4435920320 tpu_estimator.py:2160] examples/sec: 2.84011\n",
      "INFO:tensorflow:global_step/sec: 0.0887594\n",
      "I1118 00:40:55.587435 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887594\n",
      "INFO:tensorflow:examples/sec: 2.8403\n",
      "I1118 00:40:55.587657 4435920320 tpu_estimator.py:2160] examples/sec: 2.8403\n",
      "INFO:tensorflow:global_step/sec: 0.0886652\n",
      "I1118 00:41:06.865816 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886652\n",
      "INFO:tensorflow:examples/sec: 2.83729\n",
      "I1118 00:41:06.866035 4435920320 tpu_estimator.py:2160] examples/sec: 2.83729\n",
      "INFO:tensorflow:global_step/sec: 0.0887006\n",
      "I1118 00:41:18.139684 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887006\n",
      "INFO:tensorflow:examples/sec: 2.83842\n",
      "I1118 00:41:18.139910 4435920320 tpu_estimator.py:2160] examples/sec: 2.83842\n",
      "INFO:tensorflow:global_step/sec: 0.0883327\n",
      "I1118 00:41:29.460514 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883327\n",
      "INFO:tensorflow:examples/sec: 2.82665\n",
      "I1118 00:41:29.460732 4435920320 tpu_estimator.py:2160] examples/sec: 2.82665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0886406\n",
      "I1118 00:41:40.742031 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886406\n",
      "INFO:tensorflow:examples/sec: 2.8365\n",
      "I1118 00:41:40.742341 4435920320 tpu_estimator.py:2160] examples/sec: 2.8365\n",
      "INFO:tensorflow:global_step/sec: 0.0890666\n",
      "I1118 00:41:51.969588 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890666\n",
      "INFO:tensorflow:examples/sec: 2.85013\n",
      "I1118 00:41:51.969814 4435920320 tpu_estimator.py:2160] examples/sec: 2.85013\n",
      "INFO:tensorflow:global_step/sec: 0.0886341\n",
      "I1118 00:42:03.251899 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886341\n",
      "INFO:tensorflow:examples/sec: 2.83629\n",
      "I1118 00:42:03.252110 4435920320 tpu_estimator.py:2160] examples/sec: 2.83629\n",
      "INFO:tensorflow:global_step/sec: 0.0866761\n",
      "I1118 00:42:14.789118 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866761\n",
      "INFO:tensorflow:examples/sec: 2.77363\n",
      "I1118 00:42:14.789342 4435920320 tpu_estimator.py:2160] examples/sec: 2.77363\n",
      "INFO:tensorflow:global_step/sec: 0.0892883\n",
      "I1118 00:42:25.988821 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892883\n",
      "INFO:tensorflow:examples/sec: 2.85723\n",
      "I1118 00:42:25.989042 4435920320 tpu_estimator.py:2160] examples/sec: 2.85723\n",
      "INFO:tensorflow:global_step/sec: 0.0885612\n",
      "I1118 00:42:37.280430 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885612\n",
      "INFO:tensorflow:examples/sec: 2.83396\n",
      "I1118 00:42:37.280647 4435920320 tpu_estimator.py:2160] examples/sec: 2.83396\n",
      "INFO:tensorflow:global_step/sec: 0.0883158\n",
      "I1118 00:42:48.603446 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883158\n",
      "INFO:tensorflow:examples/sec: 2.82611\n",
      "I1118 00:42:48.603678 4435920320 tpu_estimator.py:2160] examples/sec: 2.82611\n",
      "INFO:tensorflow:global_step/sec: 0.0877296\n",
      "I1118 00:43:00.002096 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877296\n",
      "INFO:tensorflow:examples/sec: 2.80735\n",
      "I1118 00:43:00.002305 4435920320 tpu_estimator.py:2160] examples/sec: 2.80735\n",
      "INFO:tensorflow:global_step/sec: 0.0886779\n",
      "I1118 00:43:11.278885 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886779\n",
      "INFO:tensorflow:examples/sec: 2.83769\n",
      "I1118 00:43:11.279120 4435920320 tpu_estimator.py:2160] examples/sec: 2.83769\n",
      "INFO:tensorflow:global_step/sec: 0.0882289\n",
      "I1118 00:43:22.613031 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882289\n",
      "INFO:tensorflow:examples/sec: 2.82333\n",
      "I1118 00:43:22.613260 4435920320 tpu_estimator.py:2160] examples/sec: 2.82333\n",
      "INFO:tensorflow:global_step/sec: 0.0881776\n",
      "I1118 00:43:33.953768 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881776\n",
      "INFO:tensorflow:examples/sec: 2.82168\n",
      "I1118 00:43:33.953976 4435920320 tpu_estimator.py:2160] examples/sec: 2.82168\n",
      "INFO:tensorflow:global_step/sec: 0.0891716\n",
      "I1118 00:43:45.168123 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891716\n",
      "INFO:tensorflow:examples/sec: 2.85349\n",
      "I1118 00:43:45.168340 4435920320 tpu_estimator.py:2160] examples/sec: 2.85349\n",
      "INFO:tensorflow:global_step/sec: 0.0890288\n",
      "I1118 00:43:56.400411 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890288\n",
      "INFO:tensorflow:examples/sec: 2.84892\n",
      "I1118 00:43:56.400619 4435920320 tpu_estimator.py:2160] examples/sec: 2.84892\n",
      "INFO:tensorflow:global_step/sec: 0.0881924\n",
      "I1118 00:44:07.739256 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881924\n",
      "INFO:tensorflow:examples/sec: 2.82216\n",
      "I1118 00:44:07.739471 4435920320 tpu_estimator.py:2160] examples/sec: 2.82216\n",
      "INFO:tensorflow:global_step/sec: 0.0891131\n",
      "I1118 00:44:18.960973 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891131\n",
      "INFO:tensorflow:examples/sec: 2.85162\n",
      "I1118 00:44:18.961189 4435920320 tpu_estimator.py:2160] examples/sec: 2.85162\n",
      "INFO:tensorflow:global_step/sec: 0.0892677\n",
      "I1118 00:44:30.163215 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892677\n",
      "INFO:tensorflow:examples/sec: 2.85657\n",
      "I1118 00:44:30.163628 4435920320 tpu_estimator.py:2160] examples/sec: 2.85657\n",
      "INFO:tensorflow:global_step/sec: 0.08722\n",
      "I1118 00:44:41.628447 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08722\n",
      "INFO:tensorflow:examples/sec: 2.79104\n",
      "I1118 00:44:41.628638 4435920320 tpu_estimator.py:2160] examples/sec: 2.79104\n",
      "INFO:tensorflow:global_step/sec: 0.087891\n",
      "I1118 00:44:53.006234 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087891\n",
      "INFO:tensorflow:examples/sec: 2.81251\n",
      "I1118 00:44:53.006437 4435920320 tpu_estimator.py:2160] examples/sec: 2.81251\n",
      "INFO:tensorflow:global_step/sec: 0.0891018\n",
      "I1118 00:45:04.229335 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891018\n",
      "INFO:tensorflow:examples/sec: 2.85126\n",
      "I1118 00:45:04.229571 4435920320 tpu_estimator.py:2160] examples/sec: 2.85126\n",
      "INFO:tensorflow:global_step/sec: 0.088209\n",
      "I1118 00:45:15.566042 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088209\n",
      "INFO:tensorflow:examples/sec: 2.82269\n",
      "I1118 00:45:15.566271 4435920320 tpu_estimator.py:2160] examples/sec: 2.82269\n",
      "INFO:tensorflow:global_step/sec: 0.0883421\n",
      "I1118 00:45:26.885694 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883421\n",
      "INFO:tensorflow:examples/sec: 2.82695\n",
      "I1118 00:45:26.885928 4435920320 tpu_estimator.py:2160] examples/sec: 2.82695\n",
      "INFO:tensorflow:global_step/sec: 0.0888627\n",
      "I1118 00:45:38.138978 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888627\n",
      "INFO:tensorflow:examples/sec: 2.84361\n",
      "I1118 00:45:38.139194 4435920320 tpu_estimator.py:2160] examples/sec: 2.84361\n",
      "INFO:tensorflow:global_step/sec: 0.0887473\n",
      "I1118 00:45:49.406931 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887473\n",
      "INFO:tensorflow:examples/sec: 2.83992\n",
      "I1118 00:45:49.407160 4435920320 tpu_estimator.py:2160] examples/sec: 2.83992\n",
      "INFO:tensorflow:global_step/sec: 0.0880797\n",
      "I1118 00:46:00.760285 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880797\n",
      "INFO:tensorflow:examples/sec: 2.81855\n",
      "I1118 00:46:00.760510 4435920320 tpu_estimator.py:2160] examples/sec: 2.81855\n",
      "INFO:tensorflow:global_step/sec: 0.0883868\n",
      "I1118 00:46:12.074183 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883868\n",
      "INFO:tensorflow:examples/sec: 2.82838\n",
      "I1118 00:46:12.074403 4435920320 tpu_estimator.py:2160] examples/sec: 2.82838\n",
      "INFO:tensorflow:global_step/sec: 0.0893574\n",
      "I1118 00:46:23.265228 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893574\n",
      "INFO:tensorflow:examples/sec: 2.85944\n",
      "I1118 00:46:23.265444 4435920320 tpu_estimator.py:2160] examples/sec: 2.85944\n",
      "INFO:tensorflow:global_step/sec: 0.0884646\n",
      "I1118 00:46:34.569173 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884646\n",
      "INFO:tensorflow:examples/sec: 2.83087\n",
      "I1118 00:46:34.569402 4435920320 tpu_estimator.py:2160] examples/sec: 2.83087\n",
      "INFO:tensorflow:global_step/sec: 0.0881625\n",
      "I1118 00:46:45.911875 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881625\n",
      "INFO:tensorflow:examples/sec: 2.8212\n",
      "I1118 00:46:45.912090 4435920320 tpu_estimator.py:2160] examples/sec: 2.8212\n",
      "INFO:tensorflow:global_step/sec: 0.0874634\n",
      "I1118 00:46:57.345202 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874634\n",
      "INFO:tensorflow:examples/sec: 2.79883\n",
      "I1118 00:46:57.345438 4435920320 tpu_estimator.py:2160] examples/sec: 2.79883\n",
      "INFO:tensorflow:global_step/sec: 0.0869875\n",
      "I1118 00:47:08.841216 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0869875\n",
      "INFO:tensorflow:examples/sec: 2.7836\n",
      "I1118 00:47:08.841554 4435920320 tpu_estimator.py:2160] examples/sec: 2.7836\n",
      "INFO:tensorflow:global_step/sec: 0.088477\n",
      "I1118 00:47:20.143504 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088477\n",
      "INFO:tensorflow:examples/sec: 2.83126\n",
      "I1118 00:47:20.143730 4435920320 tpu_estimator.py:2160] examples/sec: 2.83126\n",
      "INFO:tensorflow:global_step/sec: 0.0879708\n",
      "I1118 00:47:31.510920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879708\n",
      "INFO:tensorflow:examples/sec: 2.81507\n",
      "I1118 00:47:31.511162 4435920320 tpu_estimator.py:2160] examples/sec: 2.81507\n",
      "INFO:tensorflow:global_step/sec: 0.0883428\n",
      "I1118 00:47:42.830455 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883428\n",
      "INFO:tensorflow:examples/sec: 2.82697\n",
      "I1118 00:47:42.830682 4435920320 tpu_estimator.py:2160] examples/sec: 2.82697\n",
      "INFO:tensorflow:global_step/sec: 0.0889566\n",
      "I1118 00:47:54.071877 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889566\n",
      "INFO:tensorflow:examples/sec: 2.84661\n",
      "I1118 00:47:54.072093 4435920320 tpu_estimator.py:2160] examples/sec: 2.84661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0887311\n",
      "I1118 00:48:05.341900 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887311\n",
      "INFO:tensorflow:examples/sec: 2.83939\n",
      "I1118 00:48:05.342140 4435920320 tpu_estimator.py:2160] examples/sec: 2.83939\n",
      "INFO:tensorflow:global_step/sec: 0.0885298\n",
      "I1118 00:48:16.637513 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885298\n",
      "INFO:tensorflow:examples/sec: 2.83295\n",
      "I1118 00:48:16.637731 4435920320 tpu_estimator.py:2160] examples/sec: 2.83295\n",
      "INFO:tensorflow:global_step/sec: 0.088535\n",
      "I1118 00:48:27.932492 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088535\n",
      "INFO:tensorflow:examples/sec: 2.83312\n",
      "I1118 00:48:27.932727 4435920320 tpu_estimator.py:2160] examples/sec: 2.83312\n",
      "INFO:tensorflow:global_step/sec: 0.0858999\n",
      "I1118 00:48:39.573915 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0858999\n",
      "INFO:tensorflow:examples/sec: 2.7488\n",
      "I1118 00:48:39.574228 4435920320 tpu_estimator.py:2160] examples/sec: 2.7488\n",
      "INFO:tensorflow:global_step/sec: 0.0876311\n",
      "I1118 00:48:50.985408 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876311\n",
      "INFO:tensorflow:examples/sec: 2.8042\n",
      "I1118 00:48:50.987044 4435920320 tpu_estimator.py:2160] examples/sec: 2.8042\n",
      "INFO:tensorflow:global_step/sec: 0.0881435\n",
      "I1118 00:49:02.330559 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881435\n",
      "INFO:tensorflow:examples/sec: 2.82059\n",
      "I1118 00:49:02.330770 4435920320 tpu_estimator.py:2160] examples/sec: 2.82059\n",
      "INFO:tensorflow:global_step/sec: 0.0881569\n",
      "I1118 00:49:13.673976 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881569\n",
      "INFO:tensorflow:examples/sec: 2.82102\n",
      "I1118 00:49:13.674191 4435920320 tpu_estimator.py:2160] examples/sec: 2.82102\n",
      "INFO:tensorflow:global_step/sec: 0.0885643\n",
      "I1118 00:49:24.965224 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885643\n",
      "INFO:tensorflow:examples/sec: 2.83406\n",
      "I1118 00:49:24.965594 4435920320 tpu_estimator.py:2160] examples/sec: 2.83406\n",
      "INFO:tensorflow:global_step/sec: 0.0889224\n",
      "I1118 00:49:36.210963 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889224\n",
      "INFO:tensorflow:examples/sec: 2.84552\n",
      "I1118 00:49:36.211194 4435920320 tpu_estimator.py:2160] examples/sec: 2.84552\n",
      "INFO:tensorflow:global_step/sec: 0.0880454\n",
      "I1118 00:49:47.568725 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880454\n",
      "INFO:tensorflow:examples/sec: 2.81745\n",
      "I1118 00:49:47.568913 4435920320 tpu_estimator.py:2160] examples/sec: 2.81745\n",
      "INFO:tensorflow:global_step/sec: 0.0890142\n",
      "I1118 00:49:58.802891 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890142\n",
      "INFO:tensorflow:examples/sec: 2.84846\n",
      "I1118 00:49:58.803258 4435920320 tpu_estimator.py:2160] examples/sec: 2.84846\n",
      "INFO:tensorflow:global_step/sec: 0.0893629\n",
      "I1118 00:50:09.993224 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893629\n",
      "INFO:tensorflow:examples/sec: 2.85961\n",
      "I1118 00:50:09.993448 4435920320 tpu_estimator.py:2160] examples/sec: 2.85961\n",
      "INFO:tensorflow:global_step/sec: 0.0888747\n",
      "I1118 00:50:21.245100 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888747\n",
      "INFO:tensorflow:examples/sec: 2.84399\n",
      "I1118 00:50:21.245325 4435920320 tpu_estimator.py:2160] examples/sec: 2.84399\n",
      "INFO:tensorflow:global_step/sec: 0.0879308\n",
      "I1118 00:50:32.617595 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879308\n",
      "INFO:tensorflow:examples/sec: 2.81379\n",
      "I1118 00:50:32.617815 4435920320 tpu_estimator.py:2160] examples/sec: 2.81379\n",
      "INFO:tensorflow:global_step/sec: 0.0871089\n",
      "I1118 00:50:44.097435 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871089\n",
      "INFO:tensorflow:examples/sec: 2.78749\n",
      "I1118 00:50:44.097612 4435920320 tpu_estimator.py:2160] examples/sec: 2.78749\n",
      "INFO:tensorflow:global_step/sec: 0.0877343\n",
      "I1118 00:50:55.495577 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877343\n",
      "INFO:tensorflow:examples/sec: 2.8075\n",
      "I1118 00:50:55.495800 4435920320 tpu_estimator.py:2160] examples/sec: 2.8075\n",
      "INFO:tensorflow:global_step/sec: 0.0890358\n",
      "I1118 00:51:06.726967 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890358\n",
      "INFO:tensorflow:examples/sec: 2.84915\n",
      "I1118 00:51:06.727194 4435920320 tpu_estimator.py:2160] examples/sec: 2.84915\n",
      "INFO:tensorflow:global_step/sec: 0.0890431\n",
      "I1118 00:51:17.957467 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890431\n",
      "INFO:tensorflow:examples/sec: 2.84938\n",
      "I1118 00:51:17.957682 4435920320 tpu_estimator.py:2160] examples/sec: 2.84938\n",
      "INFO:tensorflow:global_step/sec: 0.0889175\n",
      "I1118 00:51:29.203864 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889175\n",
      "INFO:tensorflow:examples/sec: 2.84536\n",
      "I1118 00:51:29.204094 4435920320 tpu_estimator.py:2160] examples/sec: 2.84536\n",
      "INFO:tensorflow:global_step/sec: 0.0888817\n",
      "I1118 00:51:40.454786 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888817\n",
      "INFO:tensorflow:examples/sec: 2.84421\n",
      "I1118 00:51:40.455013 4435920320 tpu_estimator.py:2160] examples/sec: 2.84421\n",
      "INFO:tensorflow:global_step/sec: 0.0877722\n",
      "I1118 00:51:51.847902 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877722\n",
      "INFO:tensorflow:examples/sec: 2.80871\n",
      "I1118 00:51:51.848143 4435920320 tpu_estimator.py:2160] examples/sec: 2.80871\n",
      "INFO:tensorflow:global_step/sec: 0.0878583\n",
      "I1118 00:52:03.229859 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878583\n",
      "INFO:tensorflow:examples/sec: 2.81147\n",
      "I1118 00:52:03.230081 4435920320 tpu_estimator.py:2160] examples/sec: 2.81147\n",
      "INFO:tensorflow:global_step/sec: 0.0885627\n",
      "I1118 00:52:14.521315 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885627\n",
      "INFO:tensorflow:examples/sec: 2.83401\n",
      "I1118 00:52:14.521536 4435920320 tpu_estimator.py:2160] examples/sec: 2.83401\n",
      "INFO:tensorflow:global_step/sec: 0.0894412\n",
      "I1118 00:52:25.701831 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894412\n",
      "INFO:tensorflow:examples/sec: 2.86212\n",
      "I1118 00:52:25.702059 4435920320 tpu_estimator.py:2160] examples/sec: 2.86212\n",
      "INFO:tensorflow:global_step/sec: 0.0896472\n",
      "I1118 00:52:36.856659 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896472\n",
      "INFO:tensorflow:examples/sec: 2.86871\n",
      "I1118 00:52:36.856880 4435920320 tpu_estimator.py:2160] examples/sec: 2.86871\n",
      "INFO:tensorflow:global_step/sec: 0.0890257\n",
      "I1118 00:52:48.089368 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890257\n",
      "INFO:tensorflow:examples/sec: 2.84882\n",
      "I1118 00:52:48.089581 4435920320 tpu_estimator.py:2160] examples/sec: 2.84882\n",
      "INFO:tensorflow:global_step/sec: 0.0886448\n",
      "I1118 00:52:59.370340 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886448\n",
      "INFO:tensorflow:examples/sec: 2.83663\n",
      "I1118 00:52:59.370572 4435920320 tpu_estimator.py:2160] examples/sec: 2.83663\n",
      "INFO:tensorflow:global_step/sec: 0.0847005\n",
      "I1118 00:53:11.176635 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0847005\n",
      "INFO:tensorflow:examples/sec: 2.71042\n",
      "I1118 00:53:11.176797 4435920320 tpu_estimator.py:2160] examples/sec: 2.71042\n",
      "INFO:tensorflow:global_step/sec: 0.0866286\n",
      "I1118 00:53:22.720165 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0866286\n",
      "INFO:tensorflow:examples/sec: 2.77212\n",
      "I1118 00:53:22.720366 4435920320 tpu_estimator.py:2160] examples/sec: 2.77212\n",
      "INFO:tensorflow:global_step/sec: 0.0883501\n",
      "I1118 00:53:34.038779 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883501\n",
      "INFO:tensorflow:examples/sec: 2.8272\n",
      "I1118 00:53:34.039144 4435920320 tpu_estimator.py:2160] examples/sec: 2.8272\n",
      "INFO:tensorflow:global_step/sec: 0.0885079\n",
      "I1118 00:53:45.337245 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885079\n",
      "INFO:tensorflow:examples/sec: 2.83225\n",
      "I1118 00:53:45.337506 4435920320 tpu_estimator.py:2160] examples/sec: 2.83225\n",
      "INFO:tensorflow:global_step/sec: 0.0887855\n",
      "I1118 00:53:56.600322 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887855\n",
      "INFO:tensorflow:examples/sec: 2.84114\n",
      "I1118 00:53:56.600546 4435920320 tpu_estimator.py:2160] examples/sec: 2.84114\n",
      "INFO:tensorflow:global_step/sec: 0.0887485\n",
      "I1118 00:54:07.868135 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887485\n",
      "INFO:tensorflow:examples/sec: 2.83995\n",
      "I1118 00:54:07.868364 4435920320 tpu_estimator.py:2160] examples/sec: 2.83995\n",
      "INFO:tensorflow:global_step/sec: 0.088684\n",
      "I1118 00:54:19.144126 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088684\n",
      "INFO:tensorflow:examples/sec: 2.83789\n",
      "I1118 00:54:19.144362 4435920320 tpu_estimator.py:2160] examples/sec: 2.83789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.088643\n",
      "I1118 00:54:30.425333 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088643\n",
      "INFO:tensorflow:examples/sec: 2.83657\n",
      "I1118 00:54:30.425575 4435920320 tpu_estimator.py:2160] examples/sec: 2.83657\n",
      "INFO:tensorflow:global_step/sec: 0.085679\n",
      "I1118 00:54:42.096810 4435920320 tpu_estimator.py:2159] global_step/sec: 0.085679\n",
      "INFO:tensorflow:examples/sec: 2.74173\n",
      "I1118 00:54:42.097084 4435920320 tpu_estimator.py:2160] examples/sec: 2.74173\n",
      "INFO:tensorflow:global_step/sec: 0.0880844\n",
      "I1118 00:54:53.449543 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880844\n",
      "INFO:tensorflow:examples/sec: 2.8187\n",
      "I1118 00:54:53.449763 4435920320 tpu_estimator.py:2160] examples/sec: 2.8187\n",
      "INFO:tensorflow:global_step/sec: 0.0881211\n",
      "I1118 00:55:04.797585 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881211\n",
      "INFO:tensorflow:examples/sec: 2.81987\n",
      "I1118 00:55:04.797795 4435920320 tpu_estimator.py:2160] examples/sec: 2.81987\n",
      "INFO:tensorflow:global_step/sec: 0.0886575\n",
      "I1118 00:55:16.076932 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886575\n",
      "INFO:tensorflow:examples/sec: 2.83704\n",
      "I1118 00:55:16.077154 4435920320 tpu_estimator.py:2160] examples/sec: 2.83704\n",
      "INFO:tensorflow:global_step/sec: 0.0889088\n",
      "I1118 00:55:27.324418 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889088\n",
      "INFO:tensorflow:examples/sec: 2.84508\n",
      "I1118 00:55:27.324644 4435920320 tpu_estimator.py:2160] examples/sec: 2.84508\n",
      "INFO:tensorflow:global_step/sec: 0.0885053\n",
      "I1118 00:55:38.623173 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885053\n",
      "INFO:tensorflow:examples/sec: 2.83217\n",
      "I1118 00:55:38.623404 4435920320 tpu_estimator.py:2160] examples/sec: 2.83217\n",
      "INFO:tensorflow:global_step/sec: 0.0882216\n",
      "I1118 00:55:49.958274 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882216\n",
      "INFO:tensorflow:examples/sec: 2.82309\n",
      "I1118 00:55:49.958505 4435920320 tpu_estimator.py:2160] examples/sec: 2.82309\n",
      "INFO:tensorflow:global_step/sec: 0.0885215\n",
      "I1118 00:56:01.254956 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885215\n",
      "INFO:tensorflow:examples/sec: 2.83269\n",
      "I1118 00:56:01.255389 4435920320 tpu_estimator.py:2160] examples/sec: 2.83269\n",
      "INFO:tensorflow:global_step/sec: 0.0883513\n",
      "I1118 00:56:12.573400 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883513\n",
      "INFO:tensorflow:examples/sec: 2.82724\n",
      "I1118 00:56:12.573617 4435920320 tpu_estimator.py:2160] examples/sec: 2.82724\n",
      "INFO:tensorflow:global_step/sec: 0.087844\n",
      "I1118 00:56:23.957203 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087844\n",
      "INFO:tensorflow:examples/sec: 2.81101\n",
      "I1118 00:56:23.957412 4435920320 tpu_estimator.py:2160] examples/sec: 2.81101\n",
      "INFO:tensorflow:global_step/sec: 0.0880612\n",
      "I1118 00:56:35.312959 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880612\n",
      "INFO:tensorflow:examples/sec: 2.81796\n",
      "I1118 00:56:35.313178 4435920320 tpu_estimator.py:2160] examples/sec: 2.81796\n",
      "INFO:tensorflow:global_step/sec: 0.0880186\n",
      "I1118 00:56:46.674193 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880186\n",
      "INFO:tensorflow:examples/sec: 2.8166\n",
      "I1118 00:56:46.674421 4435920320 tpu_estimator.py:2160] examples/sec: 2.8166\n",
      "INFO:tensorflow:global_step/sec: 0.0888505\n",
      "I1118 00:56:57.929073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888505\n",
      "INFO:tensorflow:examples/sec: 2.84322\n",
      "I1118 00:56:57.929290 4435920320 tpu_estimator.py:2160] examples/sec: 2.84322\n",
      "INFO:tensorflow:global_step/sec: 0.0877389\n",
      "I1118 00:57:09.326565 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877389\n",
      "INFO:tensorflow:examples/sec: 2.80765\n",
      "I1118 00:57:09.326820 4435920320 tpu_estimator.py:2160] examples/sec: 2.80765\n",
      "INFO:tensorflow:global_step/sec: 0.088659\n",
      "I1118 00:57:20.605668 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088659\n",
      "INFO:tensorflow:examples/sec: 2.83709\n",
      "I1118 00:57:20.605886 4435920320 tpu_estimator.py:2160] examples/sec: 2.83709\n",
      "INFO:tensorflow:global_step/sec: 0.088041\n",
      "I1118 00:57:31.964013 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088041\n",
      "INFO:tensorflow:examples/sec: 2.81731\n",
      "I1118 00:57:31.964229 4435920320 tpu_estimator.py:2160] examples/sec: 2.81731\n",
      "INFO:tensorflow:global_step/sec: 0.0886703\n",
      "I1118 00:57:43.241757 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886703\n",
      "INFO:tensorflow:examples/sec: 2.83745\n",
      "I1118 00:57:43.241989 4435920320 tpu_estimator.py:2160] examples/sec: 2.83745\n",
      "INFO:tensorflow:global_step/sec: 0.0889008\n",
      "I1118 00:57:54.490266 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889008\n",
      "INFO:tensorflow:examples/sec: 2.84482\n",
      "I1118 00:57:54.490483 4435920320 tpu_estimator.py:2160] examples/sec: 2.84482\n",
      "INFO:tensorflow:global_step/sec: 0.0893944\n",
      "I1118 00:58:05.676633 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893944\n",
      "INFO:tensorflow:examples/sec: 2.86062\n",
      "I1118 00:58:05.676867 4435920320 tpu_estimator.py:2160] examples/sec: 2.86062\n",
      "INFO:tensorflow:global_step/sec: 0.089182\n",
      "I1118 00:58:16.889657 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089182\n",
      "INFO:tensorflow:examples/sec: 2.85382\n",
      "I1118 00:58:16.889881 4435920320 tpu_estimator.py:2160] examples/sec: 2.85382\n",
      "INFO:tensorflow:global_step/sec: 0.088727\n",
      "I1118 00:58:28.160189 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088727\n",
      "INFO:tensorflow:examples/sec: 2.83926\n",
      "I1118 00:58:28.160420 4435920320 tpu_estimator.py:2160] examples/sec: 2.83926\n",
      "INFO:tensorflow:global_step/sec: 0.0891452\n",
      "I1118 00:58:39.377832 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891452\n",
      "INFO:tensorflow:examples/sec: 2.85265\n",
      "I1118 00:58:39.378053 4435920320 tpu_estimator.py:2160] examples/sec: 2.85265\n",
      "INFO:tensorflow:global_step/sec: 0.0882684\n",
      "I1118 00:58:50.706930 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882684\n",
      "INFO:tensorflow:examples/sec: 2.82459\n",
      "I1118 00:58:50.707165 4435920320 tpu_estimator.py:2160] examples/sec: 2.82459\n",
      "INFO:tensorflow:global_step/sec: 0.0889905\n",
      "I1118 00:59:01.944065 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889905\n",
      "INFO:tensorflow:examples/sec: 2.8477\n",
      "I1118 00:59:01.944270 4435920320 tpu_estimator.py:2160] examples/sec: 2.8477\n",
      "INFO:tensorflow:global_step/sec: 0.086733\n",
      "I1118 00:59:13.473674 4435920320 tpu_estimator.py:2159] global_step/sec: 0.086733\n",
      "INFO:tensorflow:examples/sec: 2.77546\n",
      "I1118 00:59:13.473856 4435920320 tpu_estimator.py:2160] examples/sec: 2.77546\n",
      "INFO:tensorflow:global_step/sec: 0.0880443\n",
      "I1118 00:59:24.831652 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880443\n",
      "INFO:tensorflow:examples/sec: 2.81742\n",
      "I1118 00:59:24.831869 4435920320 tpu_estimator.py:2160] examples/sec: 2.81742\n",
      "INFO:tensorflow:global_step/sec: 0.0891625\n",
      "I1118 00:59:36.047102 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891625\n",
      "INFO:tensorflow:examples/sec: 2.8532\n",
      "I1118 00:59:36.047325 4435920320 tpu_estimator.py:2160] examples/sec: 2.8532\n",
      "INFO:tensorflow:global_step/sec: 0.0891075\n",
      "I1118 00:59:47.269515 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891075\n",
      "INFO:tensorflow:examples/sec: 2.85144\n",
      "I1118 00:59:47.269744 4435920320 tpu_estimator.py:2160] examples/sec: 2.85144\n",
      "INFO:tensorflow:global_step/sec: 0.0887682\n",
      "I1118 00:59:58.534795 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887682\n",
      "INFO:tensorflow:examples/sec: 2.84058\n",
      "I1118 00:59:58.535013 4435920320 tpu_estimator.py:2160] examples/sec: 2.84058\n",
      "INFO:tensorflow:global_step/sec: 0.0847953\n",
      "I1118 01:00:10.327909 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0847953\n",
      "INFO:tensorflow:examples/sec: 2.71345\n",
      "I1118 01:00:10.328143 4435920320 tpu_estimator.py:2160] examples/sec: 2.71345\n",
      "INFO:tensorflow:global_step/sec: 0.0872642\n",
      "I1118 01:00:21.787366 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872642\n",
      "INFO:tensorflow:examples/sec: 2.79245\n",
      "I1118 01:00:21.787588 4435920320 tpu_estimator.py:2160] examples/sec: 2.79245\n",
      "INFO:tensorflow:global_step/sec: 0.0892397\n",
      "I1118 01:00:32.993146 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892397\n",
      "INFO:tensorflow:examples/sec: 2.85567\n",
      "I1118 01:00:32.993366 4435920320 tpu_estimator.py:2160] examples/sec: 2.85567\n",
      "INFO:tensorflow:global_step/sec: 0.0890362\n",
      "I1118 01:00:44.224540 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890362\n",
      "INFO:tensorflow:examples/sec: 2.84916\n",
      "I1118 01:00:44.224774 4435920320 tpu_estimator.py:2160] examples/sec: 2.84916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891746\n",
      "I1118 01:00:55.438491 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891746\n",
      "INFO:tensorflow:examples/sec: 2.85359\n",
      "I1118 01:00:55.438714 4435920320 tpu_estimator.py:2160] examples/sec: 2.85359\n",
      "INFO:tensorflow:global_step/sec: 0.0854314\n",
      "I1118 01:01:07.143754 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0854314\n",
      "INFO:tensorflow:examples/sec: 2.7338\n",
      "I1118 01:01:07.143930 4435920320 tpu_estimator.py:2160] examples/sec: 2.7338\n",
      "INFO:tensorflow:global_step/sec: 0.0879491\n",
      "I1118 01:01:18.514003 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879491\n",
      "INFO:tensorflow:examples/sec: 2.81437\n",
      "I1118 01:01:18.514220 4435920320 tpu_estimator.py:2160] examples/sec: 2.81437\n",
      "INFO:tensorflow:global_step/sec: 0.0886267\n",
      "I1118 01:01:29.797358 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886267\n",
      "INFO:tensorflow:examples/sec: 2.83605\n",
      "I1118 01:01:29.797594 4435920320 tpu_estimator.py:2160] examples/sec: 2.83605\n",
      "INFO:tensorflow:global_step/sec: 0.0889224\n",
      "I1118 01:01:41.043062 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889224\n",
      "INFO:tensorflow:examples/sec: 2.84552\n",
      "I1118 01:01:41.043290 4435920320 tpu_estimator.py:2160] examples/sec: 2.84552\n",
      "INFO:tensorflow:global_step/sec: 0.0884086\n",
      "I1118 01:01:52.354146 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884086\n",
      "INFO:tensorflow:examples/sec: 2.82908\n",
      "I1118 01:01:52.354341 4435920320 tpu_estimator.py:2160] examples/sec: 2.82908\n",
      "INFO:tensorflow:global_step/sec: 0.0876143\n",
      "I1118 01:02:03.767834 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876143\n",
      "INFO:tensorflow:examples/sec: 2.80366\n",
      "I1118 01:02:03.768049 4435920320 tpu_estimator.py:2160] examples/sec: 2.80366\n",
      "INFO:tensorflow:global_step/sec: 0.0888333\n",
      "I1118 01:02:15.024891 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888333\n",
      "INFO:tensorflow:examples/sec: 2.84266\n",
      "I1118 01:02:15.025123 4435920320 tpu_estimator.py:2160] examples/sec: 2.84266\n",
      "INFO:tensorflow:global_step/sec: 0.0888894\n",
      "I1118 01:02:26.274816 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888894\n",
      "INFO:tensorflow:examples/sec: 2.84446\n",
      "I1118 01:02:26.275055 4435920320 tpu_estimator.py:2160] examples/sec: 2.84446\n",
      "INFO:tensorflow:global_step/sec: 0.0886346\n",
      "I1118 01:02:37.557101 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886346\n",
      "INFO:tensorflow:examples/sec: 2.83631\n",
      "I1118 01:02:37.557325 4435920320 tpu_estimator.py:2160] examples/sec: 2.83631\n",
      "INFO:tensorflow:global_step/sec: 0.0882858\n",
      "I1118 01:02:48.883961 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882858\n",
      "INFO:tensorflow:examples/sec: 2.82514\n",
      "I1118 01:02:48.884191 4435920320 tpu_estimator.py:2160] examples/sec: 2.82514\n",
      "INFO:tensorflow:global_step/sec: 0.088654\n",
      "I1118 01:03:00.163772 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088654\n",
      "INFO:tensorflow:examples/sec: 2.83693\n",
      "I1118 01:03:00.164007 4435920320 tpu_estimator.py:2160] examples/sec: 2.83693\n",
      "INFO:tensorflow:global_step/sec: 0.0884811\n",
      "I1118 01:03:11.465600 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884811\n",
      "INFO:tensorflow:examples/sec: 2.8314\n",
      "I1118 01:03:11.465821 4435920320 tpu_estimator.py:2160] examples/sec: 2.8314\n",
      "INFO:tensorflow:global_step/sec: 0.0885888\n",
      "I1118 01:03:22.753712 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885888\n",
      "INFO:tensorflow:examples/sec: 2.83484\n",
      "I1118 01:03:22.753973 4435920320 tpu_estimator.py:2160] examples/sec: 2.83484\n",
      "INFO:tensorflow:global_step/sec: 0.0873797\n",
      "I1118 01:03:34.198001 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873797\n",
      "INFO:tensorflow:examples/sec: 2.79615\n",
      "I1118 01:03:34.198199 4435920320 tpu_estimator.py:2160] examples/sec: 2.79615\n",
      "INFO:tensorflow:global_step/sec: 0.0884024\n",
      "I1118 01:03:45.509941 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884024\n",
      "INFO:tensorflow:examples/sec: 2.82888\n",
      "I1118 01:03:45.510151 4435920320 tpu_estimator.py:2160] examples/sec: 2.82888\n",
      "INFO:tensorflow:global_step/sec: 0.0877888\n",
      "I1118 01:03:56.901031 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877888\n",
      "INFO:tensorflow:examples/sec: 2.80924\n",
      "I1118 01:03:56.901257 4435920320 tpu_estimator.py:2160] examples/sec: 2.80924\n",
      "INFO:tensorflow:global_step/sec: 0.0886788\n",
      "I1118 01:04:08.177574 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886788\n",
      "INFO:tensorflow:examples/sec: 2.83772\n",
      "I1118 01:04:08.177804 4435920320 tpu_estimator.py:2160] examples/sec: 2.83772\n",
      "INFO:tensorflow:global_step/sec: 0.0883709\n",
      "I1118 01:04:19.493514 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883709\n",
      "INFO:tensorflow:examples/sec: 2.82787\n",
      "I1118 01:04:19.493736 4435920320 tpu_estimator.py:2160] examples/sec: 2.82787\n",
      "INFO:tensorflow:global_step/sec: 0.0882147\n",
      "I1118 01:04:30.829475 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882147\n",
      "INFO:tensorflow:examples/sec: 2.82287\n",
      "I1118 01:04:30.829691 4435920320 tpu_estimator.py:2160] examples/sec: 2.82287\n",
      "INFO:tensorflow:global_step/sec: 0.0883142\n",
      "I1118 01:04:42.152698 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883142\n",
      "INFO:tensorflow:examples/sec: 2.82606\n",
      "I1118 01:04:42.152930 4435920320 tpu_estimator.py:2160] examples/sec: 2.82606\n",
      "INFO:tensorflow:global_step/sec: 0.0885381\n",
      "I1118 01:04:53.447263 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885381\n",
      "INFO:tensorflow:examples/sec: 2.83322\n",
      "I1118 01:04:53.447479 4435920320 tpu_estimator.py:2160] examples/sec: 2.83322\n",
      "INFO:tensorflow:global_step/sec: 0.0873948\n",
      "I1118 01:05:04.889631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873948\n",
      "INFO:tensorflow:examples/sec: 2.79663\n",
      "I1118 01:05:04.889813 4435920320 tpu_estimator.py:2160] examples/sec: 2.79663\n",
      "INFO:tensorflow:global_step/sec: 0.088483\n",
      "I1118 01:05:16.191193 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088483\n",
      "INFO:tensorflow:examples/sec: 2.83146\n",
      "I1118 01:05:16.191409 4435920320 tpu_estimator.py:2160] examples/sec: 2.83146\n",
      "INFO:tensorflow:global_step/sec: 0.0890617\n",
      "I1118 01:05:27.419362 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890617\n",
      "INFO:tensorflow:examples/sec: 2.84997\n",
      "I1118 01:05:27.419581 4435920320 tpu_estimator.py:2160] examples/sec: 2.84997\n",
      "INFO:tensorflow:global_step/sec: 0.0886671\n",
      "I1118 01:05:38.697498 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886671\n",
      "INFO:tensorflow:examples/sec: 2.83735\n",
      "I1118 01:05:38.697719 4435920320 tpu_estimator.py:2160] examples/sec: 2.83735\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into ./bert_output/model.ckpt.\n",
      "I1118 01:05:50.052441 4435920320 basic_session_run_hooks.py:606] Saving checkpoints for 1000 into ./bert_output/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0664264\n",
      "I1118 01:05:53.751664 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0664264\n",
      "INFO:tensorflow:examples/sec: 2.12564\n",
      "I1118 01:05:53.751837 4435920320 tpu_estimator.py:2160] examples/sec: 2.12564\n",
      "INFO:tensorflow:global_step/sec: 0.0889877\n",
      "I1118 01:06:04.989308 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889877\n",
      "INFO:tensorflow:examples/sec: 2.84761\n",
      "I1118 01:06:04.989573 4435920320 tpu_estimator.py:2160] examples/sec: 2.84761\n",
      "INFO:tensorflow:global_step/sec: 0.0894038\n",
      "I1118 01:06:16.174484 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894038\n",
      "INFO:tensorflow:examples/sec: 2.86092\n",
      "I1118 01:06:16.174703 4435920320 tpu_estimator.py:2160] examples/sec: 2.86092\n",
      "INFO:tensorflow:global_step/sec: 0.0900964\n",
      "I1118 01:06:27.273723 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900964\n",
      "INFO:tensorflow:examples/sec: 2.88308\n",
      "I1118 01:06:27.273965 4435920320 tpu_estimator.py:2160] examples/sec: 2.88308\n",
      "INFO:tensorflow:global_step/sec: 0.0861223\n",
      "I1118 01:06:38.885050 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861223\n",
      "INFO:tensorflow:examples/sec: 2.75591\n",
      "I1118 01:06:38.885215 4435920320 tpu_estimator.py:2160] examples/sec: 2.75591\n",
      "INFO:tensorflow:global_step/sec: 0.0874001\n",
      "I1118 01:06:50.326733 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874001\n",
      "INFO:tensorflow:examples/sec: 2.7968\n",
      "I1118 01:06:50.326972 4435920320 tpu_estimator.py:2160] examples/sec: 2.7968\n",
      "INFO:tensorflow:global_step/sec: 0.0861282\n",
      "I1118 01:07:01.937333 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0861282\n",
      "INFO:tensorflow:examples/sec: 2.7561\n",
      "I1118 01:07:01.937562 4435920320 tpu_estimator.py:2160] examples/sec: 2.7561\n",
      "INFO:tensorflow:global_step/sec: 0.0878191\n",
      "I1118 01:07:13.324373 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878191\n",
      "INFO:tensorflow:examples/sec: 2.81021\n",
      "I1118 01:07:13.324592 4435920320 tpu_estimator.py:2160] examples/sec: 2.81021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0883375\n",
      "I1118 01:07:24.644616 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883375\n",
      "INFO:tensorflow:examples/sec: 2.8268\n",
      "I1118 01:07:24.644837 4435920320 tpu_estimator.py:2160] examples/sec: 2.8268\n",
      "INFO:tensorflow:global_step/sec: 0.0888532\n",
      "I1118 01:07:35.899118 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888532\n",
      "INFO:tensorflow:examples/sec: 2.8433\n",
      "I1118 01:07:35.899338 4435920320 tpu_estimator.py:2160] examples/sec: 2.8433\n",
      "INFO:tensorflow:global_step/sec: 0.0887057\n",
      "I1118 01:07:47.172357 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887057\n",
      "INFO:tensorflow:examples/sec: 2.83858\n",
      "I1118 01:07:47.172579 4435920320 tpu_estimator.py:2160] examples/sec: 2.83858\n",
      "INFO:tensorflow:global_step/sec: 0.0882391\n",
      "I1118 01:07:58.505214 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882391\n",
      "INFO:tensorflow:examples/sec: 2.82365\n",
      "I1118 01:07:58.505451 4435920320 tpu_estimator.py:2160] examples/sec: 2.82365\n",
      "INFO:tensorflow:global_step/sec: 0.0883875\n",
      "I1118 01:08:09.819025 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883875\n",
      "INFO:tensorflow:examples/sec: 2.8284\n",
      "I1118 01:08:09.819247 4435920320 tpu_estimator.py:2160] examples/sec: 2.8284\n",
      "INFO:tensorflow:global_step/sec: 0.0888365\n",
      "I1118 01:08:21.075660 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888365\n",
      "INFO:tensorflow:examples/sec: 2.84277\n",
      "I1118 01:08:21.075882 4435920320 tpu_estimator.py:2160] examples/sec: 2.84277\n",
      "INFO:tensorflow:global_step/sec: 0.0904949\n",
      "I1118 01:08:32.125966 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0904949\n",
      "INFO:tensorflow:examples/sec: 2.89584\n",
      "I1118 01:08:32.126115 4435920320 tpu_estimator.py:2160] examples/sec: 2.89584\n",
      "INFO:tensorflow:global_step/sec: 0.0872802\n",
      "I1118 01:08:43.583367 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872802\n",
      "INFO:tensorflow:examples/sec: 2.79296\n",
      "I1118 01:08:43.583602 4435920320 tpu_estimator.py:2160] examples/sec: 2.79296\n",
      "INFO:tensorflow:global_step/sec: 0.0891427\n",
      "I1118 01:08:54.801347 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891427\n",
      "INFO:tensorflow:examples/sec: 2.85257\n",
      "I1118 01:08:54.801572 4435920320 tpu_estimator.py:2160] examples/sec: 2.85257\n",
      "INFO:tensorflow:global_step/sec: 0.0894395\n",
      "I1118 01:09:05.982089 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894395\n",
      "INFO:tensorflow:examples/sec: 2.86206\n",
      "I1118 01:09:05.982309 4435920320 tpu_estimator.py:2160] examples/sec: 2.86206\n",
      "INFO:tensorflow:global_step/sec: 0.0895246\n",
      "I1118 01:09:17.152194 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895246\n",
      "INFO:tensorflow:examples/sec: 2.86479\n",
      "I1118 01:09:17.152415 4435920320 tpu_estimator.py:2160] examples/sec: 2.86479\n",
      "INFO:tensorflow:global_step/sec: 0.0885698\n",
      "I1118 01:09:28.442729 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885698\n",
      "INFO:tensorflow:examples/sec: 2.83423\n",
      "I1118 01:09:28.442954 4435920320 tpu_estimator.py:2160] examples/sec: 2.83423\n",
      "INFO:tensorflow:global_step/sec: 0.0885238\n",
      "I1118 01:09:39.739141 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885238\n",
      "INFO:tensorflow:examples/sec: 2.83276\n",
      "I1118 01:09:39.739360 4435920320 tpu_estimator.py:2160] examples/sec: 2.83276\n",
      "INFO:tensorflow:global_step/sec: 0.0883665\n",
      "I1118 01:09:51.055627 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883665\n",
      "INFO:tensorflow:examples/sec: 2.82773\n",
      "I1118 01:09:51.055843 4435920320 tpu_estimator.py:2160] examples/sec: 2.82773\n",
      "INFO:tensorflow:global_step/sec: 0.0888281\n",
      "I1118 01:10:02.313323 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888281\n",
      "INFO:tensorflow:examples/sec: 2.8425\n",
      "I1118 01:10:02.313538 4435920320 tpu_estimator.py:2160] examples/sec: 2.8425\n",
      "INFO:tensorflow:global_step/sec: 0.0887715\n",
      "I1118 01:10:13.578225 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887715\n",
      "INFO:tensorflow:examples/sec: 2.84069\n",
      "I1118 01:10:13.578443 4435920320 tpu_estimator.py:2160] examples/sec: 2.84069\n",
      "INFO:tensorflow:global_step/sec: 0.0887296\n",
      "I1118 01:10:24.848431 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887296\n",
      "INFO:tensorflow:examples/sec: 2.83935\n",
      "I1118 01:10:24.848649 4435920320 tpu_estimator.py:2160] examples/sec: 2.83935\n",
      "INFO:tensorflow:global_step/sec: 0.0892384\n",
      "I1118 01:10:36.054351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892384\n",
      "INFO:tensorflow:examples/sec: 2.85563\n",
      "I1118 01:10:36.054566 4435920320 tpu_estimator.py:2160] examples/sec: 2.85563\n",
      "INFO:tensorflow:global_step/sec: 0.088806\n",
      "I1118 01:10:47.314860 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088806\n",
      "INFO:tensorflow:examples/sec: 2.84179\n",
      "I1118 01:10:47.315087 4435920320 tpu_estimator.py:2160] examples/sec: 2.84179\n",
      "INFO:tensorflow:global_step/sec: 0.0887839\n",
      "I1118 01:10:58.578175 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887839\n",
      "INFO:tensorflow:examples/sec: 2.84108\n",
      "I1118 01:10:58.578402 4435920320 tpu_estimator.py:2160] examples/sec: 2.84108\n",
      "INFO:tensorflow:global_step/sec: 0.0885172\n",
      "I1118 01:11:09.875407 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885172\n",
      "INFO:tensorflow:examples/sec: 2.83255\n",
      "I1118 01:11:09.875648 4435920320 tpu_estimator.py:2160] examples/sec: 2.83255\n",
      "INFO:tensorflow:global_step/sec: 0.0880247\n",
      "I1118 01:11:21.235841 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880247\n",
      "INFO:tensorflow:examples/sec: 2.81679\n",
      "I1118 01:11:21.236064 4435920320 tpu_estimator.py:2160] examples/sec: 2.81679\n",
      "INFO:tensorflow:global_step/sec: 0.088786\n",
      "I1118 01:11:32.498875 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088786\n",
      "INFO:tensorflow:examples/sec: 2.84115\n",
      "I1118 01:11:32.499111 4435920320 tpu_estimator.py:2160] examples/sec: 2.84115\n",
      "INFO:tensorflow:global_step/sec: 0.0889724\n",
      "I1118 01:11:43.738317 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889724\n",
      "INFO:tensorflow:examples/sec: 2.84712\n",
      "I1118 01:11:43.738553 4435920320 tpu_estimator.py:2160] examples/sec: 2.84712\n",
      "INFO:tensorflow:global_step/sec: 0.0887215\n",
      "I1118 01:11:55.009539 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887215\n",
      "INFO:tensorflow:examples/sec: 2.83909\n",
      "I1118 01:11:55.009778 4435920320 tpu_estimator.py:2160] examples/sec: 2.83909\n",
      "INFO:tensorflow:global_step/sec: 0.0887301\n",
      "I1118 01:12:06.279676 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887301\n",
      "INFO:tensorflow:examples/sec: 2.83936\n",
      "I1118 01:12:06.279898 4435920320 tpu_estimator.py:2160] examples/sec: 2.83936\n",
      "INFO:tensorflow:global_step/sec: 0.0888572\n",
      "I1118 01:12:17.533698 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888572\n",
      "INFO:tensorflow:examples/sec: 2.84343\n",
      "I1118 01:12:17.533927 4435920320 tpu_estimator.py:2160] examples/sec: 2.84343\n",
      "INFO:tensorflow:global_step/sec: 0.0888077\n",
      "I1118 01:12:28.793990 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888077\n",
      "INFO:tensorflow:examples/sec: 2.84185\n",
      "I1118 01:12:28.794229 4435920320 tpu_estimator.py:2160] examples/sec: 2.84185\n",
      "INFO:tensorflow:global_step/sec: 0.0888159\n",
      "I1118 01:12:40.053223 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888159\n",
      "INFO:tensorflow:examples/sec: 2.84211\n",
      "I1118 01:12:40.053451 4435920320 tpu_estimator.py:2160] examples/sec: 2.84211\n",
      "INFO:tensorflow:global_step/sec: 0.0885353\n",
      "I1118 01:12:51.348166 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885353\n",
      "INFO:tensorflow:examples/sec: 2.83313\n",
      "I1118 01:12:51.348410 4435920320 tpu_estimator.py:2160] examples/sec: 2.83313\n",
      "INFO:tensorflow:global_step/sec: 0.0885795\n",
      "I1118 01:13:02.637449 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885795\n",
      "INFO:tensorflow:examples/sec: 2.83454\n",
      "I1118 01:13:02.637672 4435920320 tpu_estimator.py:2160] examples/sec: 2.83454\n",
      "INFO:tensorflow:global_step/sec: 0.0883573\n",
      "I1118 01:13:13.955123 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883573\n",
      "INFO:tensorflow:examples/sec: 2.82743\n",
      "I1118 01:13:13.955343 4435920320 tpu_estimator.py:2160] examples/sec: 2.82743\n",
      "INFO:tensorflow:global_step/sec: 0.0891978\n",
      "I1118 01:13:25.166153 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891978\n",
      "INFO:tensorflow:examples/sec: 2.85433\n",
      "I1118 01:13:25.166364 4435920320 tpu_estimator.py:2160] examples/sec: 2.85433\n",
      "INFO:tensorflow:global_step/sec: 0.0892246\n",
      "I1118 01:13:36.373835 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892246\n",
      "INFO:tensorflow:examples/sec: 2.85519\n",
      "I1118 01:13:36.374089 4435920320 tpu_estimator.py:2160] examples/sec: 2.85519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0880936\n",
      "I1118 01:13:47.725409 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880936\n",
      "INFO:tensorflow:examples/sec: 2.819\n",
      "I1118 01:13:47.725641 4435920320 tpu_estimator.py:2160] examples/sec: 2.819\n",
      "INFO:tensorflow:global_step/sec: 0.0892096\n",
      "I1118 01:13:58.934950 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892096\n",
      "INFO:tensorflow:examples/sec: 2.85471\n",
      "I1118 01:13:58.935167 4435920320 tpu_estimator.py:2160] examples/sec: 2.85471\n",
      "INFO:tensorflow:global_step/sec: 0.0880235\n",
      "I1118 01:14:10.295563 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880235\n",
      "INFO:tensorflow:examples/sec: 2.81675\n",
      "I1118 01:14:10.295789 4435920320 tpu_estimator.py:2160] examples/sec: 2.81675\n",
      "INFO:tensorflow:global_step/sec: 0.0881209\n",
      "I1118 01:14:21.643614 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881209\n",
      "INFO:tensorflow:examples/sec: 2.81987\n",
      "I1118 01:14:21.643913 4435920320 tpu_estimator.py:2160] examples/sec: 2.81987\n",
      "INFO:tensorflow:global_step/sec: 0.0884134\n",
      "I1118 01:14:32.954113 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884134\n",
      "INFO:tensorflow:examples/sec: 2.82923\n",
      "I1118 01:14:32.954321 4435920320 tpu_estimator.py:2160] examples/sec: 2.82923\n",
      "INFO:tensorflow:global_step/sec: 0.0882998\n",
      "I1118 01:14:44.279162 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882998\n",
      "INFO:tensorflow:examples/sec: 2.82559\n",
      "I1118 01:14:44.279398 4435920320 tpu_estimator.py:2160] examples/sec: 2.82559\n",
      "INFO:tensorflow:global_step/sec: 0.0882532\n",
      "I1118 01:14:55.610170 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882532\n",
      "INFO:tensorflow:examples/sec: 2.8241\n",
      "I1118 01:14:55.610383 4435920320 tpu_estimator.py:2160] examples/sec: 2.8241\n",
      "INFO:tensorflow:global_step/sec: 0.0883869\n",
      "I1118 01:15:06.924133 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883869\n",
      "INFO:tensorflow:examples/sec: 2.82838\n",
      "I1118 01:15:06.924517 4435920320 tpu_estimator.py:2160] examples/sec: 2.82838\n",
      "INFO:tensorflow:global_step/sec: 0.0886817\n",
      "I1118 01:15:18.200351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886817\n",
      "INFO:tensorflow:examples/sec: 2.83781\n",
      "I1118 01:15:18.200570 4435920320 tpu_estimator.py:2160] examples/sec: 2.83781\n",
      "INFO:tensorflow:global_step/sec: 0.0888423\n",
      "I1118 01:15:29.456251 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888423\n",
      "INFO:tensorflow:examples/sec: 2.84295\n",
      "I1118 01:15:29.456469 4435920320 tpu_estimator.py:2160] examples/sec: 2.84295\n",
      "INFO:tensorflow:global_step/sec: 0.088608\n",
      "I1118 01:15:40.741950 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088608\n",
      "INFO:tensorflow:examples/sec: 2.83546\n",
      "I1118 01:15:40.742169 4435920320 tpu_estimator.py:2160] examples/sec: 2.83546\n",
      "INFO:tensorflow:global_step/sec: 0.0887525\n",
      "I1118 01:15:52.009198 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887525\n",
      "INFO:tensorflow:examples/sec: 2.84008\n",
      "I1118 01:15:52.009417 4435920320 tpu_estimator.py:2160] examples/sec: 2.84008\n",
      "INFO:tensorflow:global_step/sec: 0.0889594\n",
      "I1118 01:16:03.250288 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889594\n",
      "INFO:tensorflow:examples/sec: 2.8467\n",
      "I1118 01:16:03.250511 4435920320 tpu_estimator.py:2160] examples/sec: 2.8467\n",
      "INFO:tensorflow:global_step/sec: 0.088739\n",
      "I1118 01:16:14.519280 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088739\n",
      "INFO:tensorflow:examples/sec: 2.83965\n",
      "I1118 01:16:14.519493 4435920320 tpu_estimator.py:2160] examples/sec: 2.83965\n",
      "INFO:tensorflow:global_step/sec: 0.0881356\n",
      "I1118 01:16:25.865453 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881356\n",
      "INFO:tensorflow:examples/sec: 2.82034\n",
      "I1118 01:16:25.865673 4435920320 tpu_estimator.py:2160] examples/sec: 2.82034\n",
      "INFO:tensorflow:global_step/sec: 0.0885571\n",
      "I1118 01:16:37.157577 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885571\n",
      "INFO:tensorflow:examples/sec: 2.83383\n",
      "I1118 01:16:37.157762 4435920320 tpu_estimator.py:2160] examples/sec: 2.83383\n",
      "INFO:tensorflow:global_step/sec: 0.0855648\n",
      "I1118 01:16:48.844677 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0855648\n",
      "INFO:tensorflow:examples/sec: 2.73807\n",
      "I1118 01:16:48.844847 4435920320 tpu_estimator.py:2160] examples/sec: 2.73807\n",
      "INFO:tensorflow:global_step/sec: 0.0875913\n",
      "I1118 01:17:00.261284 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875913\n",
      "INFO:tensorflow:examples/sec: 2.80292\n",
      "I1118 01:17:00.261495 4435920320 tpu_estimator.py:2160] examples/sec: 2.80292\n",
      "INFO:tensorflow:global_step/sec: 0.0890209\n",
      "I1118 01:17:11.494629 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890209\n",
      "INFO:tensorflow:examples/sec: 2.84867\n",
      "I1118 01:17:11.494863 4435920320 tpu_estimator.py:2160] examples/sec: 2.84867\n",
      "INFO:tensorflow:global_step/sec: 0.089175\n",
      "I1118 01:17:22.708516 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089175\n",
      "INFO:tensorflow:examples/sec: 2.8536\n",
      "I1118 01:17:22.708729 4435920320 tpu_estimator.py:2160] examples/sec: 2.8536\n",
      "INFO:tensorflow:global_step/sec: 0.0890516\n",
      "I1118 01:17:33.937979 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890516\n",
      "INFO:tensorflow:examples/sec: 2.84965\n",
      "I1118 01:17:33.938195 4435920320 tpu_estimator.py:2160] examples/sec: 2.84965\n",
      "INFO:tensorflow:global_step/sec: 0.0887064\n",
      "I1118 01:17:45.211112 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887064\n",
      "INFO:tensorflow:examples/sec: 2.8386\n",
      "I1118 01:17:45.211346 4435920320 tpu_estimator.py:2160] examples/sec: 2.8386\n",
      "INFO:tensorflow:global_step/sec: 0.0884797\n",
      "I1118 01:17:56.513142 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884797\n",
      "INFO:tensorflow:examples/sec: 2.83135\n",
      "I1118 01:17:56.513371 4435920320 tpu_estimator.py:2160] examples/sec: 2.83135\n",
      "INFO:tensorflow:global_step/sec: 0.0885157\n",
      "I1118 01:18:07.810579 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885157\n",
      "INFO:tensorflow:examples/sec: 2.8325\n",
      "I1118 01:18:07.810810 4435920320 tpu_estimator.py:2160] examples/sec: 2.8325\n",
      "INFO:tensorflow:global_step/sec: 0.0883631\n",
      "I1118 01:18:19.127513 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883631\n",
      "INFO:tensorflow:examples/sec: 2.82762\n",
      "I1118 01:18:19.127730 4435920320 tpu_estimator.py:2160] examples/sec: 2.82762\n",
      "INFO:tensorflow:global_step/sec: 0.0889114\n",
      "I1118 01:18:30.374666 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889114\n",
      "INFO:tensorflow:examples/sec: 2.84516\n",
      "I1118 01:18:30.374885 4435920320 tpu_estimator.py:2160] examples/sec: 2.84516\n",
      "INFO:tensorflow:global_step/sec: 0.0879707\n",
      "I1118 01:18:41.742100 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879707\n",
      "INFO:tensorflow:examples/sec: 2.81506\n",
      "I1118 01:18:41.742330 4435920320 tpu_estimator.py:2160] examples/sec: 2.81506\n",
      "INFO:tensorflow:global_step/sec: 0.0885952\n",
      "I1118 01:18:53.029382 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885952\n",
      "INFO:tensorflow:examples/sec: 2.83505\n",
      "I1118 01:18:53.029604 4435920320 tpu_estimator.py:2160] examples/sec: 2.83505\n",
      "INFO:tensorflow:global_step/sec: 0.0888588\n",
      "I1118 01:19:04.283184 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888588\n",
      "INFO:tensorflow:examples/sec: 2.84348\n",
      "I1118 01:19:04.283402 4435920320 tpu_estimator.py:2160] examples/sec: 2.84348\n",
      "INFO:tensorflow:global_step/sec: 0.0889084\n",
      "I1118 01:19:15.530736 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889084\n",
      "INFO:tensorflow:examples/sec: 2.84507\n",
      "I1118 01:19:15.530961 4435920320 tpu_estimator.py:2160] examples/sec: 2.84507\n",
      "INFO:tensorflow:global_step/sec: 0.089222\n",
      "I1118 01:19:26.738714 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089222\n",
      "INFO:tensorflow:examples/sec: 2.8551\n",
      "I1118 01:19:26.738934 4435920320 tpu_estimator.py:2160] examples/sec: 2.8551\n",
      "INFO:tensorflow:global_step/sec: 0.0891842\n",
      "I1118 01:19:37.951477 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891842\n",
      "INFO:tensorflow:examples/sec: 2.85389\n",
      "I1118 01:19:37.951709 4435920320 tpu_estimator.py:2160] examples/sec: 2.85389\n",
      "INFO:tensorflow:global_step/sec: 0.0891194\n",
      "I1118 01:19:49.172384 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891194\n",
      "INFO:tensorflow:examples/sec: 2.85182\n",
      "I1118 01:19:49.172616 4435920320 tpu_estimator.py:2160] examples/sec: 2.85182\n",
      "INFO:tensorflow:global_step/sec: 0.0886558\n",
      "I1118 01:20:00.451952 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886558\n",
      "INFO:tensorflow:examples/sec: 2.83699\n",
      "I1118 01:20:00.452167 4435920320 tpu_estimator.py:2160] examples/sec: 2.83699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0890317\n",
      "I1118 01:20:11.683930 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890317\n",
      "INFO:tensorflow:examples/sec: 2.84901\n",
      "I1118 01:20:11.684341 4435920320 tpu_estimator.py:2160] examples/sec: 2.84901\n",
      "INFO:tensorflow:global_step/sec: 0.0886906\n",
      "I1118 01:20:22.959073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886906\n",
      "INFO:tensorflow:examples/sec: 2.8381\n",
      "I1118 01:20:22.959302 4435920320 tpu_estimator.py:2160] examples/sec: 2.8381\n",
      "INFO:tensorflow:global_step/sec: 0.0892911\n",
      "I1118 01:20:34.158393 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892911\n",
      "INFO:tensorflow:examples/sec: 2.85731\n",
      "I1118 01:20:34.158608 4435920320 tpu_estimator.py:2160] examples/sec: 2.85731\n",
      "INFO:tensorflow:global_step/sec: 0.088984\n",
      "I1118 01:20:45.396368 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088984\n",
      "INFO:tensorflow:examples/sec: 2.84749\n",
      "I1118 01:20:45.396588 4435920320 tpu_estimator.py:2160] examples/sec: 2.84749\n",
      "INFO:tensorflow:global_step/sec: 0.0877946\n",
      "I1118 01:20:56.786689 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877946\n",
      "INFO:tensorflow:examples/sec: 2.80943\n",
      "I1118 01:20:56.787009 4435920320 tpu_estimator.py:2160] examples/sec: 2.80943\n",
      "INFO:tensorflow:global_step/sec: 0.0886835\n",
      "I1118 01:21:08.062643 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886835\n",
      "INFO:tensorflow:examples/sec: 2.83787\n",
      "I1118 01:21:08.062863 4435920320 tpu_estimator.py:2160] examples/sec: 2.83787\n",
      "INFO:tensorflow:global_step/sec: 0.0883853\n",
      "I1118 01:21:19.376744 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883853\n",
      "INFO:tensorflow:examples/sec: 2.82833\n",
      "I1118 01:21:19.376991 4435920320 tpu_estimator.py:2160] examples/sec: 2.82833\n",
      "INFO:tensorflow:global_step/sec: 0.0885147\n",
      "I1118 01:21:30.674294 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885147\n",
      "INFO:tensorflow:examples/sec: 2.83247\n",
      "I1118 01:21:30.674515 4435920320 tpu_estimator.py:2160] examples/sec: 2.83247\n",
      "INFO:tensorflow:global_step/sec: 0.0888198\n",
      "I1118 01:21:41.933051 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888198\n",
      "INFO:tensorflow:examples/sec: 2.84223\n",
      "I1118 01:21:41.933286 4435920320 tpu_estimator.py:2160] examples/sec: 2.84223\n",
      "INFO:tensorflow:global_step/sec: 0.0882748\n",
      "I1118 01:21:53.261312 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882748\n",
      "INFO:tensorflow:examples/sec: 2.82479\n",
      "I1118 01:21:53.261533 4435920320 tpu_estimator.py:2160] examples/sec: 2.82479\n",
      "INFO:tensorflow:global_step/sec: 0.0883038\n",
      "I1118 01:22:04.585873 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883038\n",
      "INFO:tensorflow:examples/sec: 2.82572\n",
      "I1118 01:22:04.587236 4435920320 tpu_estimator.py:2160] examples/sec: 2.82572\n",
      "INFO:tensorflow:global_step/sec: 0.0868482\n",
      "I1118 01:22:16.100161 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0868482\n",
      "INFO:tensorflow:examples/sec: 2.77914\n",
      "I1118 01:22:16.100366 4435920320 tpu_estimator.py:2160] examples/sec: 2.77914\n",
      "INFO:tensorflow:global_step/sec: 0.0883692\n",
      "I1118 01:22:27.416352 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883692\n",
      "INFO:tensorflow:examples/sec: 2.82781\n",
      "I1118 01:22:27.416567 4435920320 tpu_estimator.py:2160] examples/sec: 2.82781\n",
      "INFO:tensorflow:global_step/sec: 0.0889507\n",
      "I1118 01:22:38.658533 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889507\n",
      "INFO:tensorflow:examples/sec: 2.84642\n",
      "I1118 01:22:38.658737 4435920320 tpu_estimator.py:2160] examples/sec: 2.84642\n",
      "INFO:tensorflow:global_step/sec: 0.088337\n",
      "I1118 01:22:49.978883 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088337\n",
      "INFO:tensorflow:examples/sec: 2.82678\n",
      "I1118 01:22:49.979108 4435920320 tpu_estimator.py:2160] examples/sec: 2.82678\n",
      "INFO:tensorflow:global_step/sec: 0.0888966\n",
      "I1118 01:23:01.227870 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888966\n",
      "INFO:tensorflow:examples/sec: 2.84469\n",
      "I1118 01:23:01.228109 4435920320 tpu_estimator.py:2160] examples/sec: 2.84469\n",
      "INFO:tensorflow:global_step/sec: 0.0883878\n",
      "I1118 01:23:12.541662 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883878\n",
      "INFO:tensorflow:examples/sec: 2.82841\n",
      "I1118 01:23:12.541873 4435920320 tpu_estimator.py:2160] examples/sec: 2.82841\n",
      "INFO:tensorflow:global_step/sec: 0.0882209\n",
      "I1118 01:23:23.876815 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882209\n",
      "INFO:tensorflow:examples/sec: 2.82307\n",
      "I1118 01:23:23.877037 4435920320 tpu_estimator.py:2160] examples/sec: 2.82307\n",
      "INFO:tensorflow:global_step/sec: 0.0887756\n",
      "I1118 01:23:35.141236 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887756\n",
      "INFO:tensorflow:examples/sec: 2.84082\n",
      "I1118 01:23:35.141462 4435920320 tpu_estimator.py:2160] examples/sec: 2.84082\n",
      "INFO:tensorflow:global_step/sec: 0.0886159\n",
      "I1118 01:23:46.425822 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886159\n",
      "INFO:tensorflow:examples/sec: 2.83571\n",
      "I1118 01:23:46.426045 4435920320 tpu_estimator.py:2160] examples/sec: 2.83571\n",
      "INFO:tensorflow:global_step/sec: 0.0883195\n",
      "I1118 01:23:57.748356 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883195\n",
      "INFO:tensorflow:examples/sec: 2.82623\n",
      "I1118 01:23:57.748585 4435920320 tpu_estimator.py:2160] examples/sec: 2.82623\n",
      "INFO:tensorflow:global_step/sec: 0.0884343\n",
      "I1118 01:24:09.056179 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884343\n",
      "INFO:tensorflow:examples/sec: 2.8299\n",
      "I1118 01:24:09.056401 4435920320 tpu_estimator.py:2160] examples/sec: 2.8299\n",
      "INFO:tensorflow:global_step/sec: 0.0884377\n",
      "I1118 01:24:20.363594 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884377\n",
      "INFO:tensorflow:examples/sec: 2.83001\n",
      "I1118 01:24:20.363807 4435920320 tpu_estimator.py:2160] examples/sec: 2.83001\n",
      "INFO:tensorflow:global_step/sec: 0.0884738\n",
      "I1118 01:24:31.666362 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884738\n",
      "INFO:tensorflow:examples/sec: 2.83116\n",
      "I1118 01:24:31.666597 4435920320 tpu_estimator.py:2160] examples/sec: 2.83116\n",
      "INFO:tensorflow:global_step/sec: 0.0871668\n",
      "I1118 01:24:43.138589 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0871668\n",
      "INFO:tensorflow:examples/sec: 2.78934\n",
      "I1118 01:24:43.138822 4435920320 tpu_estimator.py:2160] examples/sec: 2.78934\n",
      "INFO:tensorflow:global_step/sec: 0.0887665\n",
      "I1118 01:24:54.404121 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887665\n",
      "INFO:tensorflow:examples/sec: 2.84053\n",
      "I1118 01:24:54.404338 4435920320 tpu_estimator.py:2160] examples/sec: 2.84053\n",
      "INFO:tensorflow:global_step/sec: 0.0888802\n",
      "I1118 01:25:05.655282 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888802\n",
      "INFO:tensorflow:examples/sec: 2.84416\n",
      "I1118 01:25:05.655518 4435920320 tpu_estimator.py:2160] examples/sec: 2.84416\n",
      "INFO:tensorflow:global_step/sec: 0.0890123\n",
      "I1118 01:25:16.889631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890123\n",
      "INFO:tensorflow:examples/sec: 2.84839\n",
      "I1118 01:25:16.889851 4435920320 tpu_estimator.py:2160] examples/sec: 2.84839\n",
      "INFO:tensorflow:global_step/sec: 0.0891605\n",
      "I1118 01:25:28.105366 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891605\n",
      "INFO:tensorflow:examples/sec: 2.85313\n",
      "I1118 01:25:28.105588 4435920320 tpu_estimator.py:2160] examples/sec: 2.85313\n",
      "INFO:tensorflow:global_step/sec: 0.0890928\n",
      "I1118 01:25:39.329646 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890928\n",
      "INFO:tensorflow:examples/sec: 2.85097\n",
      "I1118 01:25:39.329867 4435920320 tpu_estimator.py:2160] examples/sec: 2.85097\n",
      "INFO:tensorflow:global_step/sec: 0.0893148\n",
      "I1118 01:25:50.525974 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893148\n",
      "INFO:tensorflow:examples/sec: 2.85807\n",
      "I1118 01:25:50.526196 4435920320 tpu_estimator.py:2160] examples/sec: 2.85807\n",
      "INFO:tensorflow:global_step/sec: 0.0889853\n",
      "I1118 01:26:01.763787 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889853\n",
      "INFO:tensorflow:examples/sec: 2.84753\n",
      "I1118 01:26:01.764003 4435920320 tpu_estimator.py:2160] examples/sec: 2.84753\n",
      "INFO:tensorflow:global_step/sec: 0.0884352\n",
      "I1118 01:26:13.071511 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884352\n",
      "INFO:tensorflow:examples/sec: 2.82993\n",
      "I1118 01:26:13.071738 4435920320 tpu_estimator.py:2160] examples/sec: 2.82993\n",
      "INFO:tensorflow:global_step/sec: 0.0889704\n",
      "I1118 01:26:24.311197 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889704\n",
      "INFO:tensorflow:examples/sec: 2.84705\n",
      "I1118 01:26:24.311441 4435920320 tpu_estimator.py:2160] examples/sec: 2.84705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888663\n",
      "I1118 01:26:35.564054 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888663\n",
      "INFO:tensorflow:examples/sec: 2.84372\n",
      "I1118 01:26:35.564275 4435920320 tpu_estimator.py:2160] examples/sec: 2.84372\n",
      "INFO:tensorflow:global_step/sec: 0.0885208\n",
      "I1118 01:26:46.860867 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885208\n",
      "INFO:tensorflow:examples/sec: 2.83267\n",
      "I1118 01:26:46.861102 4435920320 tpu_estimator.py:2160] examples/sec: 2.83267\n",
      "INFO:tensorflow:global_step/sec: 0.0890246\n",
      "I1118 01:26:58.093693 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890246\n",
      "INFO:tensorflow:examples/sec: 2.84879\n",
      "I1118 01:26:58.093930 4435920320 tpu_estimator.py:2160] examples/sec: 2.84879\n",
      "INFO:tensorflow:global_step/sec: 0.0892148\n",
      "I1118 01:27:09.302588 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892148\n",
      "INFO:tensorflow:examples/sec: 2.85487\n",
      "I1118 01:27:09.302809 4435920320 tpu_estimator.py:2160] examples/sec: 2.85487\n",
      "INFO:tensorflow:global_step/sec: 0.0884352\n",
      "I1118 01:27:20.610323 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884352\n",
      "INFO:tensorflow:examples/sec: 2.82993\n",
      "I1118 01:27:20.610541 4435920320 tpu_estimator.py:2160] examples/sec: 2.82993\n",
      "INFO:tensorflow:global_step/sec: 0.0893955\n",
      "I1118 01:27:31.796559 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893955\n",
      "INFO:tensorflow:examples/sec: 2.86066\n",
      "I1118 01:27:31.796798 4435920320 tpu_estimator.py:2160] examples/sec: 2.86066\n",
      "INFO:tensorflow:global_step/sec: 0.0892547\n",
      "I1118 01:27:43.000444 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892547\n",
      "INFO:tensorflow:examples/sec: 2.85615\n",
      "I1118 01:27:43.000669 4435920320 tpu_estimator.py:2160] examples/sec: 2.85615\n",
      "INFO:tensorflow:global_step/sec: 0.0892731\n",
      "I1118 01:27:54.202027 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892731\n",
      "INFO:tensorflow:examples/sec: 2.85674\n",
      "I1118 01:27:54.202240 4435920320 tpu_estimator.py:2160] examples/sec: 2.85674\n",
      "INFO:tensorflow:global_step/sec: 0.0887847\n",
      "I1118 01:28:05.465250 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887847\n",
      "INFO:tensorflow:examples/sec: 2.84111\n",
      "I1118 01:28:05.465481 4435920320 tpu_estimator.py:2160] examples/sec: 2.84111\n",
      "INFO:tensorflow:global_step/sec: 0.0887319\n",
      "I1118 01:28:16.735140 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887319\n",
      "INFO:tensorflow:examples/sec: 2.83942\n",
      "I1118 01:28:16.735358 4435920320 tpu_estimator.py:2160] examples/sec: 2.83942\n",
      "INFO:tensorflow:global_step/sec: 0.0886205\n",
      "I1118 01:28:28.019239 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886205\n",
      "INFO:tensorflow:examples/sec: 2.83586\n",
      "I1118 01:28:28.019482 4435920320 tpu_estimator.py:2160] examples/sec: 2.83586\n",
      "INFO:tensorflow:global_step/sec: 0.0887889\n",
      "I1118 01:28:39.281891 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887889\n",
      "INFO:tensorflow:examples/sec: 2.84124\n",
      "I1118 01:28:39.282124 4435920320 tpu_estimator.py:2160] examples/sec: 2.84124\n",
      "INFO:tensorflow:global_step/sec: 0.0889982\n",
      "I1118 01:28:50.518074 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889982\n",
      "INFO:tensorflow:examples/sec: 2.84794\n",
      "I1118 01:28:50.518306 4435920320 tpu_estimator.py:2160] examples/sec: 2.84794\n",
      "INFO:tensorflow:global_step/sec: 0.0891314\n",
      "I1118 01:29:01.737468 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891314\n",
      "INFO:tensorflow:examples/sec: 2.8522\n",
      "I1118 01:29:01.737722 4435920320 tpu_estimator.py:2160] examples/sec: 2.8522\n",
      "INFO:tensorflow:global_step/sec: 0.0882905\n",
      "I1118 01:29:13.063706 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882905\n",
      "INFO:tensorflow:examples/sec: 2.82529\n",
      "I1118 01:29:13.063929 4435920320 tpu_estimator.py:2160] examples/sec: 2.82529\n",
      "INFO:tensorflow:global_step/sec: 0.0889735\n",
      "I1118 01:29:24.303009 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889735\n",
      "INFO:tensorflow:examples/sec: 2.84715\n",
      "I1118 01:29:24.303262 4435920320 tpu_estimator.py:2160] examples/sec: 2.84715\n",
      "INFO:tensorflow:global_step/sec: 0.0887823\n",
      "I1118 01:29:35.566522 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887823\n",
      "INFO:tensorflow:examples/sec: 2.84103\n",
      "I1118 01:29:35.566741 4435920320 tpu_estimator.py:2160] examples/sec: 2.84103\n",
      "INFO:tensorflow:global_step/sec: 0.0876204\n",
      "I1118 01:29:46.979385 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876204\n",
      "INFO:tensorflow:examples/sec: 2.80385\n",
      "I1118 01:29:46.979604 4435920320 tpu_estimator.py:2160] examples/sec: 2.80385\n",
      "INFO:tensorflow:global_step/sec: 0.0885052\n",
      "I1118 01:29:58.278172 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885052\n",
      "INFO:tensorflow:examples/sec: 2.83217\n",
      "I1118 01:29:58.278575 4435920320 tpu_estimator.py:2160] examples/sec: 2.83217\n",
      "INFO:tensorflow:global_step/sec: 0.0889937\n",
      "I1118 01:30:09.514914 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889937\n",
      "INFO:tensorflow:examples/sec: 2.8478\n",
      "I1118 01:30:09.515147 4435920320 tpu_estimator.py:2160] examples/sec: 2.8478\n",
      "INFO:tensorflow:global_step/sec: 0.0892629\n",
      "I1118 01:30:20.717770 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892629\n",
      "INFO:tensorflow:examples/sec: 2.85641\n",
      "I1118 01:30:20.717989 4435920320 tpu_estimator.py:2160] examples/sec: 2.85641\n",
      "INFO:tensorflow:global_step/sec: 0.0887203\n",
      "I1118 01:30:31.989136 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887203\n",
      "INFO:tensorflow:examples/sec: 2.83905\n",
      "I1118 01:30:31.989345 4435920320 tpu_estimator.py:2160] examples/sec: 2.83905\n",
      "INFO:tensorflow:global_step/sec: 0.0890004\n",
      "I1118 01:30:43.225052 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890004\n",
      "INFO:tensorflow:examples/sec: 2.84801\n",
      "I1118 01:30:43.225274 4435920320 tpu_estimator.py:2160] examples/sec: 2.84801\n",
      "INFO:tensorflow:global_step/sec: 0.0889045\n",
      "I1118 01:30:54.473075 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889045\n",
      "INFO:tensorflow:examples/sec: 2.84495\n",
      "I1118 01:30:54.473295 4435920320 tpu_estimator.py:2160] examples/sec: 2.84495\n",
      "INFO:tensorflow:global_step/sec: 0.088054\n",
      "I1118 01:31:05.829744 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088054\n",
      "INFO:tensorflow:examples/sec: 2.81773\n",
      "I1118 01:31:05.829963 4435920320 tpu_estimator.py:2160] examples/sec: 2.81773\n",
      "INFO:tensorflow:global_step/sec: 0.088683\n",
      "I1118 01:31:17.105869 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088683\n",
      "INFO:tensorflow:examples/sec: 2.83786\n",
      "I1118 01:31:17.106095 4435920320 tpu_estimator.py:2160] examples/sec: 2.83786\n",
      "INFO:tensorflow:global_step/sec: 0.0891677\n",
      "I1118 01:31:28.320682 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891677\n",
      "INFO:tensorflow:examples/sec: 2.85337\n",
      "I1118 01:31:28.320901 4435920320 tpu_estimator.py:2160] examples/sec: 2.85337\n",
      "INFO:tensorflow:global_step/sec: 0.0829492\n",
      "I1118 01:31:40.376197 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0829492\n",
      "INFO:tensorflow:examples/sec: 2.65438\n",
      "I1118 01:31:40.376357 4435920320 tpu_estimator.py:2160] examples/sec: 2.65438\n",
      "INFO:tensorflow:global_step/sec: 0.0858885\n",
      "I1118 01:31:52.019294 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0858885\n",
      "INFO:tensorflow:examples/sec: 2.74843\n",
      "I1118 01:31:52.019516 4435920320 tpu_estimator.py:2160] examples/sec: 2.74843\n",
      "INFO:tensorflow:global_step/sec: 0.0893482\n",
      "I1118 01:32:03.211429 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893482\n",
      "INFO:tensorflow:examples/sec: 2.85914\n",
      "I1118 01:32:03.211658 4435920320 tpu_estimator.py:2160] examples/sec: 2.85914\n",
      "INFO:tensorflow:global_step/sec: 0.0895516\n",
      "I1118 01:32:14.378172 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895516\n",
      "INFO:tensorflow:examples/sec: 2.86565\n",
      "I1118 01:32:14.378395 4435920320 tpu_estimator.py:2160] examples/sec: 2.86565\n",
      "INFO:tensorflow:global_step/sec: 0.0890551\n",
      "I1118 01:32:25.607184 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890551\n",
      "INFO:tensorflow:examples/sec: 2.84976\n",
      "I1118 01:32:25.607403 4435920320 tpu_estimator.py:2160] examples/sec: 2.84976\n",
      "INFO:tensorflow:global_step/sec: 0.0885238\n",
      "I1118 01:32:36.903584 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885238\n",
      "INFO:tensorflow:examples/sec: 2.83276\n",
      "I1118 01:32:36.903820 4435920320 tpu_estimator.py:2160] examples/sec: 2.83276\n",
      "INFO:tensorflow:global_step/sec: 0.0882557\n",
      "I1118 01:32:48.234302 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882557\n",
      "INFO:tensorflow:examples/sec: 2.82418\n",
      "I1118 01:32:48.234533 4435920320 tpu_estimator.py:2160] examples/sec: 2.82418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0881659\n",
      "I1118 01:32:59.576549 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881659\n",
      "INFO:tensorflow:examples/sec: 2.82131\n",
      "I1118 01:32:59.576772 4435920320 tpu_estimator.py:2160] examples/sec: 2.82131\n",
      "INFO:tensorflow:global_step/sec: 0.0879508\n",
      "I1118 01:33:10.946559 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879508\n",
      "INFO:tensorflow:examples/sec: 2.81443\n",
      "I1118 01:33:10.946779 4435920320 tpu_estimator.py:2160] examples/sec: 2.81443\n",
      "INFO:tensorflow:global_step/sec: 0.0889451\n",
      "I1118 01:33:22.189440 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889451\n",
      "INFO:tensorflow:examples/sec: 2.84624\n",
      "I1118 01:33:22.189666 4435920320 tpu_estimator.py:2160] examples/sec: 2.84624\n",
      "INFO:tensorflow:global_step/sec: 0.0893438\n",
      "I1118 01:33:33.382169 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893438\n",
      "INFO:tensorflow:examples/sec: 2.859\n",
      "I1118 01:33:33.382401 4435920320 tpu_estimator.py:2160] examples/sec: 2.859\n",
      "INFO:tensorflow:global_step/sec: 0.0886621\n",
      "I1118 01:33:44.660930 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886621\n",
      "INFO:tensorflow:examples/sec: 2.83719\n",
      "I1118 01:33:44.661149 4435920320 tpu_estimator.py:2160] examples/sec: 2.83719\n",
      "INFO:tensorflow:global_step/sec: 0.0886834\n",
      "I1118 01:33:55.937004 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886834\n",
      "INFO:tensorflow:examples/sec: 2.83787\n",
      "I1118 01:33:55.937227 4435920320 tpu_estimator.py:2160] examples/sec: 2.83787\n",
      "INFO:tensorflow:global_step/sec: 0.088863\n",
      "I1118 01:34:07.190287 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088863\n",
      "INFO:tensorflow:examples/sec: 2.84361\n",
      "I1118 01:34:07.190509 4435920320 tpu_estimator.py:2160] examples/sec: 2.84361\n",
      "INFO:tensorflow:global_step/sec: 0.088552\n",
      "I1118 01:34:18.483092 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088552\n",
      "INFO:tensorflow:examples/sec: 2.83366\n",
      "I1118 01:34:18.483336 4435920320 tpu_estimator.py:2160] examples/sec: 2.83366\n",
      "INFO:tensorflow:global_step/sec: 0.0888988\n",
      "I1118 01:34:29.731830 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888988\n",
      "INFO:tensorflow:examples/sec: 2.84476\n",
      "I1118 01:34:29.732043 4435920320 tpu_estimator.py:2160] examples/sec: 2.84476\n",
      "INFO:tensorflow:global_step/sec: 0.0889236\n",
      "I1118 01:34:40.977430 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889236\n",
      "INFO:tensorflow:examples/sec: 2.84555\n",
      "I1118 01:34:40.977632 4435920320 tpu_estimator.py:2160] examples/sec: 2.84555\n",
      "INFO:tensorflow:global_step/sec: 0.088434\n",
      "I1118 01:34:52.285331 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088434\n",
      "INFO:tensorflow:examples/sec: 2.82989\n",
      "I1118 01:34:52.285551 4435920320 tpu_estimator.py:2160] examples/sec: 2.82989\n",
      "INFO:tensorflow:global_step/sec: 0.0894187\n",
      "I1118 01:35:03.468666 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894187\n",
      "INFO:tensorflow:examples/sec: 2.8614\n",
      "I1118 01:35:03.468901 4435920320 tpu_estimator.py:2160] examples/sec: 2.8614\n",
      "INFO:tensorflow:global_step/sec: 0.0881794\n",
      "I1118 01:35:14.809185 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881794\n",
      "INFO:tensorflow:examples/sec: 2.82174\n",
      "I1118 01:35:14.809407 4435920320 tpu_estimator.py:2160] examples/sec: 2.82174\n",
      "INFO:tensorflow:global_step/sec: 0.0895178\n",
      "I1118 01:35:25.980163 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895178\n",
      "INFO:tensorflow:examples/sec: 2.86457\n",
      "I1118 01:35:25.980571 4435920320 tpu_estimator.py:2160] examples/sec: 2.86457\n",
      "INFO:tensorflow:global_step/sec: 0.0891731\n",
      "I1118 01:35:37.194286 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891731\n",
      "INFO:tensorflow:examples/sec: 2.85354\n",
      "I1118 01:35:37.194502 4435920320 tpu_estimator.py:2160] examples/sec: 2.85354\n",
      "INFO:tensorflow:global_step/sec: 0.0890384\n",
      "I1118 01:35:48.425398 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890384\n",
      "INFO:tensorflow:examples/sec: 2.84923\n",
      "I1118 01:35:48.425615 4435920320 tpu_estimator.py:2160] examples/sec: 2.84923\n",
      "INFO:tensorflow:global_step/sec: 0.088577\n",
      "I1118 01:35:59.715054 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088577\n",
      "INFO:tensorflow:examples/sec: 2.83446\n",
      "I1118 01:35:59.715270 4435920320 tpu_estimator.py:2160] examples/sec: 2.83446\n",
      "INFO:tensorflow:global_step/sec: 0.0886527\n",
      "I1118 01:36:10.995002 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886527\n",
      "INFO:tensorflow:examples/sec: 2.83689\n",
      "I1118 01:36:10.995255 4435920320 tpu_estimator.py:2160] examples/sec: 2.83689\n",
      "INFO:tensorflow:global_step/sec: 0.0891474\n",
      "I1118 01:36:22.212372 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891474\n",
      "INFO:tensorflow:examples/sec: 2.85272\n",
      "I1118 01:36:22.212610 4435920320 tpu_estimator.py:2160] examples/sec: 2.85272\n",
      "INFO:tensorflow:global_step/sec: 0.089486\n",
      "I1118 01:36:33.387295 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089486\n",
      "INFO:tensorflow:examples/sec: 2.86355\n",
      "I1118 01:36:33.387791 4435920320 tpu_estimator.py:2160] examples/sec: 2.86355\n",
      "INFO:tensorflow:global_step/sec: 0.0883131\n",
      "I1118 01:36:44.710658 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883131\n",
      "INFO:tensorflow:examples/sec: 2.82602\n",
      "I1118 01:36:44.710877 4435920320 tpu_estimator.py:2160] examples/sec: 2.82602\n",
      "INFO:tensorflow:global_step/sec: 0.0884729\n",
      "I1118 01:36:56.013542 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884729\n",
      "INFO:tensorflow:examples/sec: 2.83113\n",
      "I1118 01:36:56.013764 4435920320 tpu_estimator.py:2160] examples/sec: 2.83113\n",
      "INFO:tensorflow:global_step/sec: 0.0892896\n",
      "I1118 01:37:07.213052 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892896\n",
      "INFO:tensorflow:examples/sec: 2.85727\n",
      "I1118 01:37:07.213274 4435920320 tpu_estimator.py:2160] examples/sec: 2.85727\n",
      "INFO:tensorflow:global_step/sec: 0.0889967\n",
      "I1118 01:37:18.449429 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889967\n",
      "INFO:tensorflow:examples/sec: 2.84789\n",
      "I1118 01:37:18.449649 4435920320 tpu_estimator.py:2160] examples/sec: 2.84789\n",
      "INFO:tensorflow:global_step/sec: 0.0882246\n",
      "I1118 01:37:29.784151 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882246\n",
      "INFO:tensorflow:examples/sec: 2.82319\n",
      "I1118 01:37:29.784383 4435920320 tpu_estimator.py:2160] examples/sec: 2.82319\n",
      "INFO:tensorflow:global_step/sec: 0.0884605\n",
      "I1118 01:37:41.088647 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884605\n",
      "INFO:tensorflow:examples/sec: 2.83074\n",
      "I1118 01:37:41.088869 4435920320 tpu_estimator.py:2160] examples/sec: 2.83074\n",
      "INFO:tensorflow:global_step/sec: 0.088587\n",
      "I1118 01:37:52.376984 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088587\n",
      "INFO:tensorflow:examples/sec: 2.83478\n",
      "I1118 01:37:52.377204 4435920320 tpu_estimator.py:2160] examples/sec: 2.83478\n",
      "INFO:tensorflow:global_step/sec: 0.08839\n",
      "I1118 01:38:03.690458 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08839\n",
      "INFO:tensorflow:examples/sec: 2.82848\n",
      "I1118 01:38:03.690714 4435920320 tpu_estimator.py:2160] examples/sec: 2.82848\n",
      "INFO:tensorflow:global_step/sec: 0.0890951\n",
      "I1118 01:38:14.914427 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890951\n",
      "INFO:tensorflow:examples/sec: 2.85104\n",
      "I1118 01:38:14.914661 4435920320 tpu_estimator.py:2160] examples/sec: 2.85104\n",
      "INFO:tensorflow:global_step/sec: 0.0891334\n",
      "I1118 01:38:26.133575 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891334\n",
      "INFO:tensorflow:examples/sec: 2.85227\n",
      "I1118 01:38:26.133829 4435920320 tpu_estimator.py:2160] examples/sec: 2.85227\n",
      "INFO:tensorflow:global_step/sec: 0.0895223\n",
      "I1118 01:38:37.303981 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895223\n",
      "INFO:tensorflow:examples/sec: 2.86471\n",
      "I1118 01:38:37.304213 4435920320 tpu_estimator.py:2160] examples/sec: 2.86471\n",
      "INFO:tensorflow:global_step/sec: 0.088868\n",
      "I1118 01:38:48.556613 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088868\n",
      "INFO:tensorflow:examples/sec: 2.84378\n",
      "I1118 01:38:48.556832 4435920320 tpu_estimator.py:2160] examples/sec: 2.84378\n",
      "INFO:tensorflow:global_step/sec: 0.088675\n",
      "I1118 01:38:59.833742 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088675\n",
      "INFO:tensorflow:examples/sec: 2.8376\n",
      "I1118 01:38:59.833962 4435920320 tpu_estimator.py:2160] examples/sec: 2.8376\n",
      "INFO:tensorflow:global_step/sec: 0.0891736\n",
      "I1118 01:39:11.047833 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891736\n",
      "INFO:tensorflow:examples/sec: 2.85356\n",
      "I1118 01:39:11.048234 4435920320 tpu_estimator.py:2160] examples/sec: 2.85356\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891255\n",
      "I1118 01:39:22.267971 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891255\n",
      "INFO:tensorflow:examples/sec: 2.85202\n",
      "I1118 01:39:22.268208 4435920320 tpu_estimator.py:2160] examples/sec: 2.85202\n",
      "INFO:tensorflow:global_step/sec: 0.0888714\n",
      "I1118 01:39:33.520186 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888714\n",
      "INFO:tensorflow:examples/sec: 2.84388\n",
      "I1118 01:39:33.520437 4435920320 tpu_estimator.py:2160] examples/sec: 2.84388\n",
      "INFO:tensorflow:global_step/sec: 0.0892625\n",
      "I1118 01:39:44.723073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892625\n",
      "INFO:tensorflow:examples/sec: 2.8564\n",
      "I1118 01:39:44.723277 4435920320 tpu_estimator.py:2160] examples/sec: 2.8564\n",
      "INFO:tensorflow:global_step/sec: 0.0890875\n",
      "I1118 01:39:55.948019 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890875\n",
      "INFO:tensorflow:examples/sec: 2.8508\n",
      "I1118 01:39:55.948239 4435920320 tpu_estimator.py:2160] examples/sec: 2.8508\n",
      "INFO:tensorflow:global_step/sec: 0.0886768\n",
      "I1118 01:40:07.224915 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886768\n",
      "INFO:tensorflow:examples/sec: 2.83766\n",
      "I1118 01:40:07.225131 4435920320 tpu_estimator.py:2160] examples/sec: 2.83766\n",
      "INFO:tensorflow:global_step/sec: 0.0888246\n",
      "I1118 01:40:18.483061 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888246\n",
      "INFO:tensorflow:examples/sec: 2.84239\n",
      "I1118 01:40:18.483278 4435920320 tpu_estimator.py:2160] examples/sec: 2.84239\n",
      "INFO:tensorflow:global_step/sec: 0.0887735\n",
      "I1118 01:40:29.747689 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887735\n",
      "INFO:tensorflow:examples/sec: 2.84075\n",
      "I1118 01:40:29.747923 4435920320 tpu_estimator.py:2160] examples/sec: 2.84075\n",
      "INFO:tensorflow:global_step/sec: 0.0887838\n",
      "I1118 01:40:41.011005 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887838\n",
      "INFO:tensorflow:examples/sec: 2.84108\n",
      "I1118 01:40:41.011237 4435920320 tpu_estimator.py:2160] examples/sec: 2.84108\n",
      "INFO:tensorflow:global_step/sec: 0.0882702\n",
      "I1118 01:40:52.339838 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882702\n",
      "INFO:tensorflow:examples/sec: 2.82465\n",
      "I1118 01:40:52.340057 4435920320 tpu_estimator.py:2160] examples/sec: 2.82465\n",
      "INFO:tensorflow:global_step/sec: 0.0894835\n",
      "I1118 01:41:03.515110 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894835\n",
      "INFO:tensorflow:examples/sec: 2.86347\n",
      "I1118 01:41:03.515470 4435920320 tpu_estimator.py:2160] examples/sec: 2.86347\n",
      "INFO:tensorflow:global_step/sec: 0.0891779\n",
      "I1118 01:41:14.728645 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891779\n",
      "INFO:tensorflow:examples/sec: 2.85369\n",
      "I1118 01:41:14.728880 4435920320 tpu_estimator.py:2160] examples/sec: 2.85369\n",
      "INFO:tensorflow:global_step/sec: 0.0894281\n",
      "I1118 01:41:25.910787 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894281\n",
      "INFO:tensorflow:examples/sec: 2.8617\n",
      "I1118 01:41:25.911012 4435920320 tpu_estimator.py:2160] examples/sec: 2.8617\n",
      "INFO:tensorflow:global_step/sec: 0.0895427\n",
      "I1118 01:41:37.078651 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895427\n",
      "INFO:tensorflow:examples/sec: 2.86537\n",
      "I1118 01:41:37.078897 4435920320 tpu_estimator.py:2160] examples/sec: 2.86537\n",
      "INFO:tensorflow:global_step/sec: 0.0885049\n",
      "I1118 01:41:48.377408 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885049\n",
      "INFO:tensorflow:examples/sec: 2.83216\n",
      "I1118 01:41:48.377588 4435920320 tpu_estimator.py:2160] examples/sec: 2.83216\n",
      "INFO:tensorflow:global_step/sec: 0.0882068\n",
      "I1118 01:41:59.714462 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882068\n",
      "INFO:tensorflow:examples/sec: 2.82262\n",
      "I1118 01:41:59.714677 4435920320 tpu_estimator.py:2160] examples/sec: 2.82262\n",
      "INFO:tensorflow:global_step/sec: 0.088746\n",
      "I1118 01:42:10.982571 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088746\n",
      "INFO:tensorflow:examples/sec: 2.83987\n",
      "I1118 01:42:10.982788 4435920320 tpu_estimator.py:2160] examples/sec: 2.83987\n",
      "INFO:tensorflow:global_step/sec: 0.088677\n",
      "I1118 01:42:22.259447 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088677\n",
      "INFO:tensorflow:examples/sec: 2.83766\n",
      "I1118 01:42:22.259670 4435920320 tpu_estimator.py:2160] examples/sec: 2.83766\n",
      "INFO:tensorflow:global_step/sec: 0.0882893\n",
      "I1118 01:42:33.585855 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882893\n",
      "INFO:tensorflow:examples/sec: 2.82526\n",
      "I1118 01:42:33.586085 4435920320 tpu_estimator.py:2160] examples/sec: 2.82526\n",
      "INFO:tensorflow:global_step/sec: 0.0883854\n",
      "I1118 01:42:44.899929 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883854\n",
      "INFO:tensorflow:examples/sec: 2.82833\n",
      "I1118 01:42:44.900156 4435920320 tpu_estimator.py:2160] examples/sec: 2.82833\n",
      "INFO:tensorflow:global_step/sec: 0.0891837\n",
      "I1118 01:42:56.112735 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891837\n",
      "INFO:tensorflow:examples/sec: 2.85388\n",
      "I1118 01:42:56.112982 4435920320 tpu_estimator.py:2160] examples/sec: 2.85388\n",
      "INFO:tensorflow:global_step/sec: 0.0895776\n",
      "I1118 01:43:07.276277 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895776\n",
      "INFO:tensorflow:examples/sec: 2.86648\n",
      "I1118 01:43:07.276511 4435920320 tpu_estimator.py:2160] examples/sec: 2.86648\n",
      "INFO:tensorflow:global_step/sec: 0.0892582\n",
      "I1118 01:43:18.479708 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892582\n",
      "INFO:tensorflow:examples/sec: 2.85626\n",
      "I1118 01:43:18.479928 4435920320 tpu_estimator.py:2160] examples/sec: 2.85626\n",
      "INFO:tensorflow:global_step/sec: 0.0870504\n",
      "I1118 01:43:29.967274 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870504\n",
      "INFO:tensorflow:examples/sec: 2.78561\n",
      "I1118 01:43:29.967491 4435920320 tpu_estimator.py:2160] examples/sec: 2.78561\n",
      "INFO:tensorflow:global_step/sec: 0.0888868\n",
      "I1118 01:43:41.217584 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888868\n",
      "INFO:tensorflow:examples/sec: 2.84438\n",
      "I1118 01:43:41.217816 4435920320 tpu_estimator.py:2160] examples/sec: 2.84438\n",
      "INFO:tensorflow:global_step/sec: 0.0895108\n",
      "I1118 01:43:52.389422 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895108\n",
      "INFO:tensorflow:examples/sec: 2.86434\n",
      "I1118 01:43:52.389648 4435920320 tpu_estimator.py:2160] examples/sec: 2.86434\n",
      "INFO:tensorflow:global_step/sec: 0.0886139\n",
      "I1118 01:44:03.674309 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886139\n",
      "INFO:tensorflow:examples/sec: 2.83565\n",
      "I1118 01:44:03.674518 4435920320 tpu_estimator.py:2160] examples/sec: 2.83565\n",
      "INFO:tensorflow:global_step/sec: 0.0883294\n",
      "I1118 01:44:14.995576 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883294\n",
      "INFO:tensorflow:examples/sec: 2.82654\n",
      "I1118 01:44:14.995799 4435920320 tpu_estimator.py:2160] examples/sec: 2.82654\n",
      "INFO:tensorflow:global_step/sec: 0.0895265\n",
      "I1118 01:44:26.165462 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895265\n",
      "INFO:tensorflow:examples/sec: 2.86485\n",
      "I1118 01:44:26.165688 4435920320 tpu_estimator.py:2160] examples/sec: 2.86485\n",
      "INFO:tensorflow:global_step/sec: 0.0898442\n",
      "I1118 01:44:37.295841 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898442\n",
      "INFO:tensorflow:examples/sec: 2.87501\n",
      "I1118 01:44:37.296066 4435920320 tpu_estimator.py:2160] examples/sec: 2.87501\n",
      "INFO:tensorflow:global_step/sec: 0.0889589\n",
      "I1118 01:44:48.536986 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889589\n",
      "INFO:tensorflow:examples/sec: 2.84668\n",
      "I1118 01:44:48.537210 4435920320 tpu_estimator.py:2160] examples/sec: 2.84668\n",
      "INFO:tensorflow:global_step/sec: 0.0888129\n",
      "I1118 01:44:59.796610 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888129\n",
      "INFO:tensorflow:examples/sec: 2.84201\n",
      "I1118 01:44:59.796828 4435920320 tpu_estimator.py:2160] examples/sec: 2.84201\n",
      "INFO:tensorflow:global_step/sec: 0.0888483\n",
      "I1118 01:45:11.051758 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888483\n",
      "INFO:tensorflow:examples/sec: 2.84315\n",
      "I1118 01:45:11.051994 4435920320 tpu_estimator.py:2160] examples/sec: 2.84315\n",
      "INFO:tensorflow:global_step/sec: 0.0892939\n",
      "I1118 01:45:22.250721 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892939\n",
      "INFO:tensorflow:examples/sec: 2.85741\n",
      "I1118 01:45:22.250952 4435920320 tpu_estimator.py:2160] examples/sec: 2.85741\n",
      "INFO:tensorflow:global_step/sec: 0.0885187\n",
      "I1118 01:45:33.547772 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885187\n",
      "INFO:tensorflow:examples/sec: 2.8326\n",
      "I1118 01:45:33.548017 4435920320 tpu_estimator.py:2160] examples/sec: 2.8326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0885271\n",
      "I1118 01:45:44.843734 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885271\n",
      "INFO:tensorflow:examples/sec: 2.83287\n",
      "I1118 01:45:44.843952 4435920320 tpu_estimator.py:2160] examples/sec: 2.83287\n",
      "INFO:tensorflow:global_step/sec: 0.0890837\n",
      "I1118 01:45:56.069133 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890837\n",
      "INFO:tensorflow:examples/sec: 2.85068\n",
      "I1118 01:45:56.069357 4435920320 tpu_estimator.py:2160] examples/sec: 2.85068\n",
      "INFO:tensorflow:global_step/sec: 0.0895566\n",
      "I1118 01:46:07.235267 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895566\n",
      "INFO:tensorflow:examples/sec: 2.86581\n",
      "I1118 01:46:07.235489 4435920320 tpu_estimator.py:2160] examples/sec: 2.86581\n",
      "INFO:tensorflow:global_step/sec: 0.0895736\n",
      "I1118 01:46:18.399268 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895736\n",
      "INFO:tensorflow:examples/sec: 2.86636\n",
      "I1118 01:46:18.399488 4435920320 tpu_estimator.py:2160] examples/sec: 2.86636\n",
      "INFO:tensorflow:global_step/sec: 0.0886094\n",
      "I1118 01:46:29.684720 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886094\n",
      "INFO:tensorflow:examples/sec: 2.8355\n",
      "I1118 01:46:29.684913 4435920320 tpu_estimator.py:2160] examples/sec: 2.8355\n",
      "INFO:tensorflow:global_step/sec: 0.0885604\n",
      "I1118 01:46:40.976490 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885604\n",
      "INFO:tensorflow:examples/sec: 2.83393\n",
      "I1118 01:46:40.976719 4435920320 tpu_estimator.py:2160] examples/sec: 2.83393\n",
      "INFO:tensorflow:global_step/sec: 0.0884719\n",
      "I1118 01:46:52.279512 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884719\n",
      "INFO:tensorflow:examples/sec: 2.8311\n",
      "I1118 01:46:52.279739 4435920320 tpu_estimator.py:2160] examples/sec: 2.8311\n",
      "INFO:tensorflow:global_step/sec: 0.0890345\n",
      "I1118 01:47:03.511115 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890345\n",
      "INFO:tensorflow:examples/sec: 2.8491\n",
      "I1118 01:47:03.511344 4435920320 tpu_estimator.py:2160] examples/sec: 2.8491\n",
      "INFO:tensorflow:global_step/sec: 0.0881702\n",
      "I1118 01:47:14.852837 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881702\n",
      "INFO:tensorflow:examples/sec: 2.82145\n",
      "I1118 01:47:14.853047 4435920320 tpu_estimator.py:2160] examples/sec: 2.82145\n",
      "INFO:tensorflow:global_step/sec: 0.0894113\n",
      "I1118 01:47:26.037089 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894113\n",
      "INFO:tensorflow:examples/sec: 2.86116\n",
      "I1118 01:47:26.037297 4435920320 tpu_estimator.py:2160] examples/sec: 2.86116\n",
      "INFO:tensorflow:global_step/sec: 0.0893508\n",
      "I1118 01:47:37.228918 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893508\n",
      "INFO:tensorflow:examples/sec: 2.85923\n",
      "I1118 01:47:37.229134 4435920320 tpu_estimator.py:2160] examples/sec: 2.85923\n",
      "INFO:tensorflow:global_step/sec: 0.0887286\n",
      "I1118 01:47:48.499247 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887286\n",
      "INFO:tensorflow:examples/sec: 2.83931\n",
      "I1118 01:47:48.499471 4435920320 tpu_estimator.py:2160] examples/sec: 2.83931\n",
      "INFO:tensorflow:global_step/sec: 0.0883382\n",
      "I1118 01:47:59.819380 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883382\n",
      "INFO:tensorflow:examples/sec: 2.82682\n",
      "I1118 01:47:59.819613 4435920320 tpu_estimator.py:2160] examples/sec: 2.82682\n",
      "INFO:tensorflow:global_step/sec: 0.088938\n",
      "I1118 01:48:11.063160 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088938\n",
      "INFO:tensorflow:examples/sec: 2.84602\n",
      "I1118 01:48:11.063383 4435920320 tpu_estimator.py:2160] examples/sec: 2.84602\n",
      "INFO:tensorflow:global_step/sec: 0.0888419\n",
      "I1118 01:48:22.319117 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888419\n",
      "INFO:tensorflow:examples/sec: 2.84294\n",
      "I1118 01:48:22.319346 4435920320 tpu_estimator.py:2160] examples/sec: 2.84294\n",
      "INFO:tensorflow:global_step/sec: 0.0890021\n",
      "I1118 01:48:33.554800 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890021\n",
      "INFO:tensorflow:examples/sec: 2.84807\n",
      "I1118 01:48:33.555011 4435920320 tpu_estimator.py:2160] examples/sec: 2.84807\n",
      "INFO:tensorflow:global_step/sec: 0.088773\n",
      "I1118 01:48:44.819545 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088773\n",
      "INFO:tensorflow:examples/sec: 2.84074\n",
      "I1118 01:48:44.820096 4435920320 tpu_estimator.py:2160] examples/sec: 2.84074\n",
      "INFO:tensorflow:global_step/sec: 0.0894385\n",
      "I1118 01:48:56.000351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894385\n",
      "INFO:tensorflow:examples/sec: 2.86203\n",
      "I1118 01:48:56.000589 4435920320 tpu_estimator.py:2160] examples/sec: 2.86203\n",
      "INFO:tensorflow:global_step/sec: 0.0892271\n",
      "I1118 01:49:07.207728 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892271\n",
      "INFO:tensorflow:examples/sec: 2.85527\n",
      "I1118 01:49:07.208130 4435920320 tpu_estimator.py:2160] examples/sec: 2.85527\n",
      "INFO:tensorflow:global_step/sec: 0.0895786\n",
      "I1118 01:49:18.371087 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895786\n",
      "INFO:tensorflow:examples/sec: 2.86652\n",
      "I1118 01:49:18.371304 4435920320 tpu_estimator.py:2160] examples/sec: 2.86652\n",
      "INFO:tensorflow:global_step/sec: 0.0889459\n",
      "I1118 01:49:29.613862 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889459\n",
      "INFO:tensorflow:examples/sec: 2.84627\n",
      "I1118 01:49:29.614099 4435920320 tpu_estimator.py:2160] examples/sec: 2.84627\n",
      "INFO:tensorflow:global_step/sec: 0.0888675\n",
      "I1118 01:49:40.866579 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888675\n",
      "INFO:tensorflow:examples/sec: 2.84376\n",
      "I1118 01:49:40.866803 4435920320 tpu_estimator.py:2160] examples/sec: 2.84376\n",
      "INFO:tensorflow:global_step/sec: 0.0887742\n",
      "I1118 01:49:52.131111 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887742\n",
      "INFO:tensorflow:examples/sec: 2.84077\n",
      "I1118 01:49:52.131336 4435920320 tpu_estimator.py:2160] examples/sec: 2.84077\n",
      "INFO:tensorflow:global_step/sec: 0.0891506\n",
      "I1118 01:50:03.348090 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891506\n",
      "INFO:tensorflow:examples/sec: 2.85282\n",
      "I1118 01:50:03.348319 4435920320 tpu_estimator.py:2160] examples/sec: 2.85282\n",
      "INFO:tensorflow:global_step/sec: 0.0888285\n",
      "I1118 01:50:14.605726 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888285\n",
      "INFO:tensorflow:examples/sec: 2.84251\n",
      "I1118 01:50:14.605944 4435920320 tpu_estimator.py:2160] examples/sec: 2.84251\n",
      "INFO:tensorflow:global_step/sec: 0.0894065\n",
      "I1118 01:50:25.790606 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894065\n",
      "INFO:tensorflow:examples/sec: 2.86101\n",
      "I1118 01:50:25.790842 4435920320 tpu_estimator.py:2160] examples/sec: 2.86101\n",
      "INFO:tensorflow:global_step/sec: 0.0885576\n",
      "I1118 01:50:37.082678 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885576\n",
      "INFO:tensorflow:examples/sec: 2.83384\n",
      "I1118 01:50:37.082901 4435920320 tpu_estimator.py:2160] examples/sec: 2.83384\n",
      "INFO:tensorflow:global_step/sec: 0.0885555\n",
      "I1118 01:50:48.375034 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885555\n",
      "INFO:tensorflow:examples/sec: 2.83378\n",
      "I1118 01:50:48.375460 4435920320 tpu_estimator.py:2160] examples/sec: 2.83378\n",
      "INFO:tensorflow:global_step/sec: 0.0883336\n",
      "I1118 01:50:59.695772 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883336\n",
      "INFO:tensorflow:examples/sec: 2.82668\n",
      "I1118 01:50:59.695996 4435920320 tpu_estimator.py:2160] examples/sec: 2.82668\n",
      "INFO:tensorflow:global_step/sec: 0.0886643\n",
      "I1118 01:51:10.974259 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886643\n",
      "INFO:tensorflow:examples/sec: 2.83726\n",
      "I1118 01:51:10.974488 4435920320 tpu_estimator.py:2160] examples/sec: 2.83726\n",
      "INFO:tensorflow:global_step/sec: 0.0889426\n",
      "I1118 01:51:22.217452 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889426\n",
      "INFO:tensorflow:examples/sec: 2.84616\n",
      "I1118 01:51:22.217653 4435920320 tpu_estimator.py:2160] examples/sec: 2.84616\n",
      "INFO:tensorflow:global_step/sec: 0.0885288\n",
      "I1118 01:51:33.513234 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885288\n",
      "INFO:tensorflow:examples/sec: 2.83292\n",
      "I1118 01:51:33.513472 4435920320 tpu_estimator.py:2160] examples/sec: 2.83292\n",
      "INFO:tensorflow:global_step/sec: 0.0891089\n",
      "I1118 01:51:44.735442 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891089\n",
      "INFO:tensorflow:examples/sec: 2.85149\n",
      "I1118 01:51:44.735664 4435920320 tpu_estimator.py:2160] examples/sec: 2.85149\n",
      "INFO:tensorflow:global_step/sec: 0.0889153\n",
      "I1118 01:51:55.982103 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889153\n",
      "INFO:tensorflow:examples/sec: 2.84529\n",
      "I1118 01:51:55.982326 4435920320 tpu_estimator.py:2160] examples/sec: 2.84529\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0887191\n",
      "I1118 01:52:07.253630 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887191\n",
      "INFO:tensorflow:examples/sec: 2.83901\n",
      "I1118 01:52:07.253851 4435920320 tpu_estimator.py:2160] examples/sec: 2.83901\n",
      "INFO:tensorflow:global_step/sec: 0.0885985\n",
      "I1118 01:52:18.540517 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885985\n",
      "INFO:tensorflow:examples/sec: 2.83515\n",
      "I1118 01:52:18.540784 4435920320 tpu_estimator.py:2160] examples/sec: 2.83515\n",
      "INFO:tensorflow:global_step/sec: 0.088687\n",
      "I1118 01:52:29.816118 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088687\n",
      "INFO:tensorflow:examples/sec: 2.83799\n",
      "I1118 01:52:29.816334 4435920320 tpu_estimator.py:2160] examples/sec: 2.83799\n",
      "INFO:tensorflow:global_step/sec: 0.0893779\n",
      "I1118 01:52:41.004570 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893779\n",
      "INFO:tensorflow:examples/sec: 2.86009\n",
      "I1118 01:52:41.004827 4435920320 tpu_estimator.py:2160] examples/sec: 2.86009\n",
      "INFO:tensorflow:global_step/sec: 0.0888073\n",
      "I1118 01:52:52.264945 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888073\n",
      "INFO:tensorflow:examples/sec: 2.84184\n",
      "I1118 01:52:52.265180 4435920320 tpu_estimator.py:2160] examples/sec: 2.84184\n",
      "INFO:tensorflow:global_step/sec: 0.0894702\n",
      "I1118 01:53:03.441798 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894702\n",
      "INFO:tensorflow:examples/sec: 2.86305\n",
      "I1118 01:53:03.442035 4435920320 tpu_estimator.py:2160] examples/sec: 2.86305\n",
      "INFO:tensorflow:global_step/sec: 0.0885026\n",
      "I1118 01:53:14.740885 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885026\n",
      "INFO:tensorflow:examples/sec: 2.83208\n",
      "I1118 01:53:14.741111 4435920320 tpu_estimator.py:2160] examples/sec: 2.83208\n",
      "INFO:tensorflow:global_step/sec: 0.0887606\n",
      "I1118 01:53:26.007164 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887606\n",
      "INFO:tensorflow:examples/sec: 2.84034\n",
      "I1118 01:53:26.007387 4435920320 tpu_estimator.py:2160] examples/sec: 2.84034\n",
      "INFO:tensorflow:global_step/sec: 0.0893612\n",
      "I1118 01:53:37.197685 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893612\n",
      "INFO:tensorflow:examples/sec: 2.85956\n",
      "I1118 01:53:37.197910 4435920320 tpu_estimator.py:2160] examples/sec: 2.85956\n",
      "INFO:tensorflow:global_step/sec: 0.08932\n",
      "I1118 01:53:48.393354 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08932\n",
      "INFO:tensorflow:examples/sec: 2.85824\n",
      "I1118 01:53:48.393551 4435920320 tpu_estimator.py:2160] examples/sec: 2.85824\n",
      "INFO:tensorflow:global_step/sec: 0.0893049\n",
      "I1118 01:53:59.590976 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893049\n",
      "INFO:tensorflow:examples/sec: 2.85776\n",
      "I1118 01:53:59.591211 4435920320 tpu_estimator.py:2160] examples/sec: 2.85776\n",
      "INFO:tensorflow:global_step/sec: 0.0890109\n",
      "I1118 01:54:10.825568 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890109\n",
      "INFO:tensorflow:examples/sec: 2.84835\n",
      "I1118 01:54:10.825783 4435920320 tpu_estimator.py:2160] examples/sec: 2.84835\n",
      "INFO:tensorflow:global_step/sec: 0.0891268\n",
      "I1118 01:54:22.045522 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891268\n",
      "INFO:tensorflow:examples/sec: 2.85206\n",
      "I1118 01:54:22.045751 4435920320 tpu_estimator.py:2160] examples/sec: 2.85206\n",
      "INFO:tensorflow:global_step/sec: 0.0890049\n",
      "I1118 01:54:33.280870 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890049\n",
      "INFO:tensorflow:examples/sec: 2.84816\n",
      "I1118 01:54:33.281086 4435920320 tpu_estimator.py:2160] examples/sec: 2.84816\n",
      "INFO:tensorflow:global_step/sec: 0.0885408\n",
      "I1118 01:54:44.575085 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885408\n",
      "INFO:tensorflow:examples/sec: 2.83331\n",
      "I1118 01:54:44.575297 4435920320 tpu_estimator.py:2160] examples/sec: 2.83331\n",
      "INFO:tensorflow:global_step/sec: 0.0888231\n",
      "I1118 01:54:55.833468 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888231\n",
      "INFO:tensorflow:examples/sec: 2.84234\n",
      "I1118 01:54:55.833679 4435920320 tpu_estimator.py:2160] examples/sec: 2.84234\n",
      "INFO:tensorflow:global_step/sec: 0.0888554\n",
      "I1118 01:55:07.087663 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888554\n",
      "INFO:tensorflow:examples/sec: 2.84337\n",
      "I1118 01:55:07.087868 4435920320 tpu_estimator.py:2160] examples/sec: 2.84337\n",
      "INFO:tensorflow:global_step/sec: 0.0893706\n",
      "I1118 01:55:18.277009 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893706\n",
      "INFO:tensorflow:examples/sec: 2.85986\n",
      "I1118 01:55:18.277233 4435920320 tpu_estimator.py:2160] examples/sec: 2.85986\n",
      "INFO:tensorflow:global_step/sec: 0.0891282\n",
      "I1118 01:55:29.496814 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891282\n",
      "INFO:tensorflow:examples/sec: 2.8521\n",
      "I1118 01:55:29.497215 4435920320 tpu_estimator.py:2160] examples/sec: 2.8521\n",
      "INFO:tensorflow:global_step/sec: 0.088697\n",
      "I1118 01:55:40.771151 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088697\n",
      "INFO:tensorflow:examples/sec: 2.83831\n",
      "I1118 01:55:40.771385 4435920320 tpu_estimator.py:2160] examples/sec: 2.83831\n",
      "INFO:tensorflow:global_step/sec: 0.0889728\n",
      "I1118 01:55:52.010533 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889728\n",
      "INFO:tensorflow:examples/sec: 2.84713\n",
      "I1118 01:55:52.010779 4435920320 tpu_estimator.py:2160] examples/sec: 2.84713\n",
      "INFO:tensorflow:global_step/sec: 0.0887396\n",
      "I1118 01:56:03.279469 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887396\n",
      "INFO:tensorflow:examples/sec: 2.83967\n",
      "I1118 01:56:03.279706 4435920320 tpu_estimator.py:2160] examples/sec: 2.83967\n",
      "INFO:tensorflow:global_step/sec: 0.0888519\n",
      "I1118 01:56:14.534128 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888519\n",
      "INFO:tensorflow:examples/sec: 2.84326\n",
      "I1118 01:56:14.534347 4435920320 tpu_estimator.py:2160] examples/sec: 2.84326\n",
      "INFO:tensorflow:global_step/sec: 0.0889692\n",
      "I1118 01:56:25.773983 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889692\n",
      "INFO:tensorflow:examples/sec: 2.84701\n",
      "I1118 01:56:25.774202 4435920320 tpu_estimator.py:2160] examples/sec: 2.84701\n",
      "INFO:tensorflow:global_step/sec: 0.0888177\n",
      "I1118 01:56:37.033020 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888177\n",
      "INFO:tensorflow:examples/sec: 2.84217\n",
      "I1118 01:56:37.033236 4435920320 tpu_estimator.py:2160] examples/sec: 2.84217\n",
      "INFO:tensorflow:global_step/sec: 0.0884543\n",
      "I1118 01:56:48.338276 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884543\n",
      "INFO:tensorflow:examples/sec: 2.83054\n",
      "I1118 01:56:48.338495 4435920320 tpu_estimator.py:2160] examples/sec: 2.83054\n",
      "INFO:tensorflow:global_step/sec: 0.08893\n",
      "I1118 01:56:59.583082 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08893\n",
      "INFO:tensorflow:examples/sec: 2.84576\n",
      "I1118 01:56:59.583315 4435920320 tpu_estimator.py:2160] examples/sec: 2.84576\n",
      "INFO:tensorflow:global_step/sec: 0.0888858\n",
      "I1118 01:57:10.833464 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888858\n",
      "INFO:tensorflow:examples/sec: 2.84434\n",
      "I1118 01:57:10.833683 4435920320 tpu_estimator.py:2160] examples/sec: 2.84434\n",
      "INFO:tensorflow:global_step/sec: 0.0891591\n",
      "I1118 01:57:22.049381 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891591\n",
      "INFO:tensorflow:examples/sec: 2.85309\n",
      "I1118 01:57:22.049614 4435920320 tpu_estimator.py:2160] examples/sec: 2.85309\n",
      "INFO:tensorflow:global_step/sec: 0.0891141\n",
      "I1118 01:57:33.270941 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891141\n",
      "INFO:tensorflow:examples/sec: 2.85165\n",
      "I1118 01:57:33.271158 4435920320 tpu_estimator.py:2160] examples/sec: 2.85165\n",
      "INFO:tensorflow:global_step/sec: 0.088828\n",
      "I1118 01:57:44.528651 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088828\n",
      "INFO:tensorflow:examples/sec: 2.8425\n",
      "I1118 01:57:44.528872 4435920320 tpu_estimator.py:2160] examples/sec: 2.8425\n",
      "INFO:tensorflow:global_step/sec: 0.0886799\n",
      "I1118 01:57:55.805182 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886799\n",
      "INFO:tensorflow:examples/sec: 2.83776\n",
      "I1118 01:57:55.805413 4435920320 tpu_estimator.py:2160] examples/sec: 2.83776\n",
      "INFO:tensorflow:global_step/sec: 0.0889733\n",
      "I1118 01:58:07.044504 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889733\n",
      "INFO:tensorflow:examples/sec: 2.84714\n",
      "I1118 01:58:07.044724 4435920320 tpu_estimator.py:2160] examples/sec: 2.84714\n",
      "INFO:tensorflow:global_step/sec: 0.089122\n",
      "I1118 01:58:18.265082 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089122\n",
      "INFO:tensorflow:examples/sec: 2.8519\n",
      "I1118 01:58:18.265312 4435920320 tpu_estimator.py:2160] examples/sec: 2.8519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888405\n",
      "I1118 01:58:29.521214 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888405\n",
      "INFO:tensorflow:examples/sec: 2.8429\n",
      "I1118 01:58:29.521441 4435920320 tpu_estimator.py:2160] examples/sec: 2.8429\n",
      "INFO:tensorflow:global_step/sec: 0.0887115\n",
      "I1118 01:58:40.793718 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887115\n",
      "INFO:tensorflow:examples/sec: 2.83877\n",
      "I1118 01:58:40.793971 4435920320 tpu_estimator.py:2160] examples/sec: 2.83877\n",
      "INFO:tensorflow:global_step/sec: 0.0888785\n",
      "I1118 01:58:52.045029 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888785\n",
      "INFO:tensorflow:examples/sec: 2.84411\n",
      "I1118 01:58:52.045268 4435920320 tpu_estimator.py:2160] examples/sec: 2.84411\n",
      "INFO:tensorflow:global_step/sec: 0.089354\n",
      "I1118 01:59:03.236490 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089354\n",
      "INFO:tensorflow:examples/sec: 2.85933\n",
      "I1118 01:59:03.236732 4435920320 tpu_estimator.py:2160] examples/sec: 2.85933\n",
      "INFO:tensorflow:global_step/sec: 0.0888564\n",
      "I1118 01:59:14.490570 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888564\n",
      "INFO:tensorflow:examples/sec: 2.84341\n",
      "I1118 01:59:14.490785 4435920320 tpu_estimator.py:2160] examples/sec: 2.84341\n",
      "INFO:tensorflow:global_step/sec: 0.0884431\n",
      "I1118 01:59:25.797297 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884431\n",
      "INFO:tensorflow:examples/sec: 2.83018\n",
      "I1118 01:59:25.797604 4435920320 tpu_estimator.py:2160] examples/sec: 2.83018\n",
      "INFO:tensorflow:global_step/sec: 0.0888683\n",
      "I1118 01:59:37.049896 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888683\n",
      "INFO:tensorflow:examples/sec: 2.84378\n",
      "I1118 01:59:37.050129 4435920320 tpu_estimator.py:2160] examples/sec: 2.84378\n",
      "INFO:tensorflow:global_step/sec: 0.08944\n",
      "I1118 01:59:48.230565 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08944\n",
      "INFO:tensorflow:examples/sec: 2.86208\n",
      "I1118 01:59:48.230780 4435920320 tpu_estimator.py:2160] examples/sec: 2.86208\n",
      "INFO:tensorflow:global_step/sec: 0.0893847\n",
      "I1118 01:59:59.418174 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893847\n",
      "INFO:tensorflow:examples/sec: 2.86031\n",
      "I1118 01:59:59.418401 4435920320 tpu_estimator.py:2160] examples/sec: 2.86031\n",
      "INFO:tensorflow:global_step/sec: 0.0890405\n",
      "I1118 02:00:10.649011 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890405\n",
      "INFO:tensorflow:examples/sec: 2.8493\n",
      "I1118 02:00:10.649231 4435920320 tpu_estimator.py:2160] examples/sec: 2.8493\n",
      "INFO:tensorflow:global_step/sec: 0.0886087\n",
      "I1118 02:00:21.934609 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886087\n",
      "INFO:tensorflow:examples/sec: 2.83548\n",
      "I1118 02:00:21.934827 4435920320 tpu_estimator.py:2160] examples/sec: 2.83548\n",
      "INFO:tensorflow:global_step/sec: 0.0894763\n",
      "I1118 02:00:33.110738 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894763\n",
      "INFO:tensorflow:examples/sec: 2.86324\n",
      "I1118 02:00:33.110961 4435920320 tpu_estimator.py:2160] examples/sec: 2.86324\n",
      "INFO:tensorflow:global_step/sec: 0.0890129\n",
      "I1118 02:00:44.345071 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890129\n",
      "INFO:tensorflow:examples/sec: 2.84841\n",
      "I1118 02:00:44.345307 4435920320 tpu_estimator.py:2160] examples/sec: 2.84841\n",
      "INFO:tensorflow:global_step/sec: 0.0886323\n",
      "I1118 02:00:55.627645 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886323\n",
      "INFO:tensorflow:examples/sec: 2.83623\n",
      "I1118 02:00:55.627882 4435920320 tpu_estimator.py:2160] examples/sec: 2.83623\n",
      "INFO:tensorflow:global_step/sec: 0.088347\n",
      "I1118 02:01:06.946638 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088347\n",
      "INFO:tensorflow:examples/sec: 2.8271\n",
      "I1118 02:01:06.946875 4435920320 tpu_estimator.py:2160] examples/sec: 2.8271\n",
      "INFO:tensorflow:global_step/sec: 0.0888588\n",
      "I1118 02:01:18.200448 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888588\n",
      "INFO:tensorflow:examples/sec: 2.84348\n",
      "I1118 02:01:18.200677 4435920320 tpu_estimator.py:2160] examples/sec: 2.84348\n",
      "INFO:tensorflow:global_step/sec: 0.0889928\n",
      "I1118 02:01:29.437397 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889928\n",
      "INFO:tensorflow:examples/sec: 2.84777\n",
      "I1118 02:01:29.437771 4435920320 tpu_estimator.py:2160] examples/sec: 2.84777\n",
      "INFO:tensorflow:global_step/sec: 0.0890133\n",
      "I1118 02:01:40.671599 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890133\n",
      "INFO:tensorflow:examples/sec: 2.84842\n",
      "I1118 02:01:40.671811 4435920320 tpu_estimator.py:2160] examples/sec: 2.84842\n",
      "INFO:tensorflow:global_step/sec: 0.0892626\n",
      "I1118 02:01:51.874498 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892626\n",
      "INFO:tensorflow:examples/sec: 2.8564\n",
      "I1118 02:01:51.874724 4435920320 tpu_estimator.py:2160] examples/sec: 2.8564\n",
      "INFO:tensorflow:global_step/sec: 0.0894803\n",
      "I1118 02:02:03.050140 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894803\n",
      "INFO:tensorflow:examples/sec: 2.86337\n",
      "I1118 02:02:03.050373 4435920320 tpu_estimator.py:2160] examples/sec: 2.86337\n",
      "INFO:tensorflow:global_step/sec: 0.0894142\n",
      "I1118 02:02:14.233995 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894142\n",
      "INFO:tensorflow:examples/sec: 2.86126\n",
      "I1118 02:02:14.234224 4435920320 tpu_estimator.py:2160] examples/sec: 2.86126\n",
      "INFO:tensorflow:global_step/sec: 0.0887408\n",
      "I1118 02:02:25.502830 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887408\n",
      "INFO:tensorflow:examples/sec: 2.83971\n",
      "I1118 02:02:25.503066 4435920320 tpu_estimator.py:2160] examples/sec: 2.83971\n",
      "INFO:tensorflow:global_step/sec: 0.0884101\n",
      "I1118 02:02:36.813749 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884101\n",
      "INFO:tensorflow:examples/sec: 2.82912\n",
      "I1118 02:02:36.813977 4435920320 tpu_estimator.py:2160] examples/sec: 2.82912\n",
      "INFO:tensorflow:global_step/sec: 0.0884631\n",
      "I1118 02:02:48.117897 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884631\n",
      "INFO:tensorflow:examples/sec: 2.83082\n",
      "I1118 02:02:48.118123 4435920320 tpu_estimator.py:2160] examples/sec: 2.83082\n",
      "INFO:tensorflow:global_step/sec: 0.0891591\n",
      "I1118 02:02:59.333797 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891591\n",
      "INFO:tensorflow:examples/sec: 2.85309\n",
      "I1118 02:02:59.334020 4435920320 tpu_estimator.py:2160] examples/sec: 2.85309\n",
      "INFO:tensorflow:global_step/sec: 0.0890022\n",
      "I1118 02:03:10.569475 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890022\n",
      "INFO:tensorflow:examples/sec: 2.84807\n",
      "I1118 02:03:10.569702 4435920320 tpu_estimator.py:2160] examples/sec: 2.84807\n",
      "INFO:tensorflow:global_step/sec: 0.0889225\n",
      "I1118 02:03:21.815226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889225\n",
      "INFO:tensorflow:examples/sec: 2.84552\n",
      "I1118 02:03:21.815434 4435920320 tpu_estimator.py:2160] examples/sec: 2.84552\n",
      "INFO:tensorflow:global_step/sec: 0.0888934\n",
      "I1118 02:03:33.064655 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888934\n",
      "INFO:tensorflow:examples/sec: 2.84459\n",
      "I1118 02:03:33.064886 4435920320 tpu_estimator.py:2160] examples/sec: 2.84459\n",
      "INFO:tensorflow:global_step/sec: 0.0892815\n",
      "I1118 02:03:44.265176 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892815\n",
      "INFO:tensorflow:examples/sec: 2.85701\n",
      "I1118 02:03:44.265393 4435920320 tpu_estimator.py:2160] examples/sec: 2.85701\n",
      "INFO:tensorflow:global_step/sec: 0.0896297\n",
      "I1118 02:03:55.422202 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896297\n",
      "INFO:tensorflow:examples/sec: 2.86815\n",
      "I1118 02:03:55.422420 4435920320 tpu_estimator.py:2160] examples/sec: 2.86815\n",
      "INFO:tensorflow:global_step/sec: 0.0892385\n",
      "I1118 02:04:06.628146 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892385\n",
      "INFO:tensorflow:examples/sec: 2.85563\n",
      "I1118 02:04:06.628388 4435920320 tpu_estimator.py:2160] examples/sec: 2.85563\n",
      "INFO:tensorflow:global_step/sec: 0.088728\n",
      "I1118 02:04:17.898539 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088728\n",
      "INFO:tensorflow:examples/sec: 2.8393\n",
      "I1118 02:04:17.898939 4435920320 tpu_estimator.py:2160] examples/sec: 2.8393\n",
      "INFO:tensorflow:global_step/sec: 0.0890417\n",
      "I1118 02:04:29.129225 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890417\n",
      "INFO:tensorflow:examples/sec: 2.84933\n",
      "I1118 02:04:29.129437 4435920320 tpu_estimator.py:2160] examples/sec: 2.84933\n",
      "INFO:tensorflow:global_step/sec: 0.0892657\n",
      "I1118 02:04:40.331738 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892657\n",
      "INFO:tensorflow:examples/sec: 2.8565\n",
      "I1118 02:04:40.331965 4435920320 tpu_estimator.py:2160] examples/sec: 2.8565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888964\n",
      "I1118 02:04:51.580780 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888964\n",
      "INFO:tensorflow:examples/sec: 2.84469\n",
      "I1118 02:04:51.580997 4435920320 tpu_estimator.py:2160] examples/sec: 2.84469\n",
      "INFO:tensorflow:global_step/sec: 0.0892722\n",
      "I1118 02:05:02.782482 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892722\n",
      "INFO:tensorflow:examples/sec: 2.85671\n",
      "I1118 02:05:02.782704 4435920320 tpu_estimator.py:2160] examples/sec: 2.85671\n",
      "INFO:tensorflow:global_step/sec: 0.0888881\n",
      "I1118 02:05:14.032586 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888881\n",
      "INFO:tensorflow:examples/sec: 2.84442\n",
      "I1118 02:05:14.032809 4435920320 tpu_estimator.py:2160] examples/sec: 2.84442\n",
      "INFO:tensorflow:global_step/sec: 0.0893619\n",
      "I1118 02:05:25.223044 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893619\n",
      "INFO:tensorflow:examples/sec: 2.85958\n",
      "I1118 02:05:25.223273 4435920320 tpu_estimator.py:2160] examples/sec: 2.85958\n",
      "INFO:tensorflow:global_step/sec: 0.0892833\n",
      "I1118 02:05:36.423340 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892833\n",
      "INFO:tensorflow:examples/sec: 2.85706\n",
      "I1118 02:05:36.423558 4435920320 tpu_estimator.py:2160] examples/sec: 2.85706\n",
      "INFO:tensorflow:global_step/sec: 0.088817\n",
      "I1118 02:05:47.682449 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088817\n",
      "INFO:tensorflow:examples/sec: 2.84214\n",
      "I1118 02:05:47.682667 4435920320 tpu_estimator.py:2160] examples/sec: 2.84214\n",
      "INFO:tensorflow:global_step/sec: 0.0888524\n",
      "I1118 02:05:58.937064 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888524\n",
      "INFO:tensorflow:examples/sec: 2.84328\n",
      "I1118 02:05:58.937285 4435920320 tpu_estimator.py:2160] examples/sec: 2.84328\n",
      "INFO:tensorflow:global_step/sec: 0.0891459\n",
      "I1118 02:06:10.154629 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891459\n",
      "INFO:tensorflow:examples/sec: 2.85267\n",
      "I1118 02:06:10.154844 4435920320 tpu_estimator.py:2160] examples/sec: 2.85267\n",
      "INFO:tensorflow:global_step/sec: 0.0890604\n",
      "I1118 02:06:21.382980 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890604\n",
      "INFO:tensorflow:examples/sec: 2.84993\n",
      "I1118 02:06:21.383202 4435920320 tpu_estimator.py:2160] examples/sec: 2.84993\n",
      "INFO:tensorflow:global_step/sec: 0.089099\n",
      "I1118 02:06:32.606458 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089099\n",
      "INFO:tensorflow:examples/sec: 2.85117\n",
      "I1118 02:06:32.606687 4435920320 tpu_estimator.py:2160] examples/sec: 2.85117\n",
      "INFO:tensorflow:global_step/sec: 0.0888365\n",
      "I1118 02:06:43.863102 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888365\n",
      "INFO:tensorflow:examples/sec: 2.84277\n",
      "I1118 02:06:43.863355 4435920320 tpu_estimator.py:2160] examples/sec: 2.84277\n",
      "INFO:tensorflow:global_step/sec: 0.0890302\n",
      "I1118 02:06:55.095232 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890302\n",
      "INFO:tensorflow:examples/sec: 2.84897\n",
      "I1118 02:06:55.095454 4435920320 tpu_estimator.py:2160] examples/sec: 2.84897\n",
      "INFO:tensorflow:global_step/sec: 0.0895595\n",
      "I1118 02:07:06.260996 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895595\n",
      "INFO:tensorflow:examples/sec: 2.8659\n",
      "I1118 02:07:06.261215 4435920320 tpu_estimator.py:2160] examples/sec: 2.8659\n",
      "INFO:tensorflow:global_step/sec: 0.0889158\n",
      "I1118 02:07:17.507582 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889158\n",
      "INFO:tensorflow:examples/sec: 2.84531\n",
      "I1118 02:07:17.507802 4435920320 tpu_estimator.py:2160] examples/sec: 2.84531\n",
      "INFO:tensorflow:global_step/sec: 0.0882617\n",
      "I1118 02:07:28.837551 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882617\n",
      "INFO:tensorflow:examples/sec: 2.82438\n",
      "I1118 02:07:28.837770 4435920320 tpu_estimator.py:2160] examples/sec: 2.82438\n",
      "INFO:tensorflow:global_step/sec: 0.0888091\n",
      "I1118 02:07:40.097708 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888091\n",
      "INFO:tensorflow:examples/sec: 2.84189\n",
      "I1118 02:07:40.097944 4435920320 tpu_estimator.py:2160] examples/sec: 2.84189\n",
      "INFO:tensorflow:global_step/sec: 0.0892542\n",
      "I1118 02:07:51.301583 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892542\n",
      "INFO:tensorflow:examples/sec: 2.85613\n",
      "I1118 02:07:51.301805 4435920320 tpu_estimator.py:2160] examples/sec: 2.85613\n",
      "INFO:tensorflow:global_step/sec: 0.0890508\n",
      "I1118 02:08:02.531141 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890508\n",
      "INFO:tensorflow:examples/sec: 2.84963\n",
      "I1118 02:08:02.531386 4435920320 tpu_estimator.py:2160] examples/sec: 2.84963\n",
      "INFO:tensorflow:global_step/sec: 0.0884918\n",
      "I1118 02:08:13.831622 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884918\n",
      "INFO:tensorflow:examples/sec: 2.83174\n",
      "I1118 02:08:13.831850 4435920320 tpu_estimator.py:2160] examples/sec: 2.83174\n",
      "INFO:tensorflow:global_step/sec: 0.0891898\n",
      "I1118 02:08:25.043665 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891898\n",
      "INFO:tensorflow:examples/sec: 2.85407\n",
      "I1118 02:08:25.043900 4435920320 tpu_estimator.py:2160] examples/sec: 2.85407\n",
      "INFO:tensorflow:global_step/sec: 0.0893431\n",
      "I1118 02:08:36.236473 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893431\n",
      "INFO:tensorflow:examples/sec: 2.85898\n",
      "I1118 02:08:36.236698 4435920320 tpu_estimator.py:2160] examples/sec: 2.85898\n",
      "INFO:tensorflow:global_step/sec: 0.0894022\n",
      "I1118 02:08:47.421865 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894022\n",
      "INFO:tensorflow:examples/sec: 2.86087\n",
      "I1118 02:08:47.422087 4435920320 tpu_estimator.py:2160] examples/sec: 2.86087\n",
      "INFO:tensorflow:global_step/sec: 0.0888989\n",
      "I1118 02:08:58.670597 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888989\n",
      "INFO:tensorflow:examples/sec: 2.84476\n",
      "I1118 02:08:58.670832 4435920320 tpu_estimator.py:2160] examples/sec: 2.84476\n",
      "INFO:tensorflow:global_step/sec: 0.0889095\n",
      "I1118 02:09:09.917993 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889095\n",
      "INFO:tensorflow:examples/sec: 2.8451\n",
      "I1118 02:09:09.918210 4435920320 tpu_estimator.py:2160] examples/sec: 2.8451\n",
      "INFO:tensorflow:global_step/sec: 0.0891554\n",
      "I1118 02:09:21.134380 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891554\n",
      "INFO:tensorflow:examples/sec: 2.85297\n",
      "I1118 02:09:21.134603 4435920320 tpu_estimator.py:2160] examples/sec: 2.85297\n",
      "INFO:tensorflow:global_step/sec: 0.0897534\n",
      "I1118 02:09:32.275995 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897534\n",
      "INFO:tensorflow:examples/sec: 2.87211\n",
      "I1118 02:09:32.276226 4435920320 tpu_estimator.py:2160] examples/sec: 2.87211\n",
      "INFO:tensorflow:global_step/sec: 0.0894826\n",
      "I1118 02:09:43.451356 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894826\n",
      "INFO:tensorflow:examples/sec: 2.86344\n",
      "I1118 02:09:43.451575 4435920320 tpu_estimator.py:2160] examples/sec: 2.86344\n",
      "INFO:tensorflow:global_step/sec: 0.0891233\n",
      "I1118 02:09:54.671767 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891233\n",
      "INFO:tensorflow:examples/sec: 2.85195\n",
      "I1118 02:09:54.671994 4435920320 tpu_estimator.py:2160] examples/sec: 2.85195\n",
      "INFO:tensorflow:global_step/sec: 0.0887194\n",
      "I1118 02:10:05.943269 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887194\n",
      "INFO:tensorflow:examples/sec: 2.83902\n",
      "I1118 02:10:05.943496 4435920320 tpu_estimator.py:2160] examples/sec: 2.83902\n",
      "INFO:tensorflow:global_step/sec: 0.0886741\n",
      "I1118 02:10:17.220504 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886741\n",
      "INFO:tensorflow:examples/sec: 2.83757\n",
      "I1118 02:10:17.220726 4435920320 tpu_estimator.py:2160] examples/sec: 2.83757\n",
      "INFO:tensorflow:global_step/sec: 0.0891552\n",
      "I1118 02:10:28.436904 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891552\n",
      "INFO:tensorflow:examples/sec: 2.85297\n",
      "I1118 02:10:28.437161 4435920320 tpu_estimator.py:2160] examples/sec: 2.85297\n",
      "INFO:tensorflow:global_step/sec: 0.0893069\n",
      "I1118 02:10:39.634249 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893069\n",
      "INFO:tensorflow:examples/sec: 2.85782\n",
      "I1118 02:10:39.634480 4435920320 tpu_estimator.py:2160] examples/sec: 2.85782\n",
      "INFO:tensorflow:global_step/sec: 0.0888434\n",
      "I1118 02:10:50.890010 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888434\n",
      "INFO:tensorflow:examples/sec: 2.84299\n",
      "I1118 02:10:50.890234 4435920320 tpu_estimator.py:2160] examples/sec: 2.84299\n",
      "INFO:tensorflow:global_step/sec: 0.0886881\n",
      "I1118 02:11:02.165497 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886881\n",
      "INFO:tensorflow:examples/sec: 2.83802\n",
      "I1118 02:11:02.165733 4435920320 tpu_estimator.py:2160] examples/sec: 2.83802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0886637\n",
      "I1118 02:11:13.444067 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886637\n",
      "INFO:tensorflow:examples/sec: 2.83724\n",
      "I1118 02:11:13.444308 4435920320 tpu_estimator.py:2160] examples/sec: 2.83724\n",
      "INFO:tensorflow:global_step/sec: 0.0887955\n",
      "I1118 02:11:24.705894 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887955\n",
      "INFO:tensorflow:examples/sec: 2.84146\n",
      "I1118 02:11:24.706131 4435920320 tpu_estimator.py:2160] examples/sec: 2.84146\n",
      "INFO:tensorflow:global_step/sec: 0.0890771\n",
      "I1118 02:11:35.932121 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890771\n",
      "INFO:tensorflow:examples/sec: 2.85047\n",
      "I1118 02:11:35.932346 4435920320 tpu_estimator.py:2160] examples/sec: 2.85047\n",
      "INFO:tensorflow:global_step/sec: 0.0893365\n",
      "I1118 02:11:47.125758 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893365\n",
      "INFO:tensorflow:examples/sec: 2.85877\n",
      "I1118 02:11:47.125989 4435920320 tpu_estimator.py:2160] examples/sec: 2.85877\n",
      "INFO:tensorflow:global_step/sec: 0.089214\n",
      "I1118 02:11:58.334760 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089214\n",
      "INFO:tensorflow:examples/sec: 2.85485\n",
      "I1118 02:11:58.334991 4435920320 tpu_estimator.py:2160] examples/sec: 2.85485\n",
      "INFO:tensorflow:global_step/sec: 0.0891612\n",
      "I1118 02:12:09.550392 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891612\n",
      "INFO:tensorflow:examples/sec: 2.85316\n",
      "I1118 02:12:09.550611 4435920320 tpu_estimator.py:2160] examples/sec: 2.85316\n",
      "INFO:tensorflow:global_step/sec: 0.0881283\n",
      "I1118 02:12:20.897498 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881283\n",
      "INFO:tensorflow:examples/sec: 2.8201\n",
      "I1118 02:12:20.897730 4435920320 tpu_estimator.py:2160] examples/sec: 2.8201\n",
      "INFO:tensorflow:global_step/sec: 0.089247\n",
      "I1118 02:12:32.102339 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089247\n",
      "INFO:tensorflow:examples/sec: 2.8559\n",
      "I1118 02:12:32.102557 4435920320 tpu_estimator.py:2160] examples/sec: 2.8559\n",
      "INFO:tensorflow:global_step/sec: 0.0893576\n",
      "I1118 02:12:43.293329 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893576\n",
      "INFO:tensorflow:examples/sec: 2.85944\n",
      "I1118 02:12:43.293570 4435920320 tpu_estimator.py:2160] examples/sec: 2.85944\n",
      "INFO:tensorflow:global_step/sec: 0.0887576\n",
      "I1118 02:12:54.559991 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887576\n",
      "INFO:tensorflow:examples/sec: 2.84024\n",
      "I1118 02:12:54.560221 4435920320 tpu_estimator.py:2160] examples/sec: 2.84024\n",
      "INFO:tensorflow:global_step/sec: 0.0888018\n",
      "I1118 02:13:05.821015 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888018\n",
      "INFO:tensorflow:examples/sec: 2.84166\n",
      "I1118 02:13:05.821244 4435920320 tpu_estimator.py:2160] examples/sec: 2.84166\n",
      "INFO:tensorflow:global_step/sec: 0.0888497\n",
      "I1118 02:13:17.075976 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888497\n",
      "INFO:tensorflow:examples/sec: 2.84319\n",
      "I1118 02:13:17.076203 4435920320 tpu_estimator.py:2160] examples/sec: 2.84319\n",
      "INFO:tensorflow:global_step/sec: 0.0887247\n",
      "I1118 02:13:28.346817 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887247\n",
      "INFO:tensorflow:examples/sec: 2.83919\n",
      "I1118 02:13:28.347043 4435920320 tpu_estimator.py:2160] examples/sec: 2.83919\n",
      "INFO:tensorflow:global_step/sec: 0.0892644\n",
      "I1118 02:13:39.549473 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892644\n",
      "INFO:tensorflow:examples/sec: 2.85646\n",
      "I1118 02:13:39.549701 4435920320 tpu_estimator.py:2160] examples/sec: 2.85646\n",
      "INFO:tensorflow:global_step/sec: 0.088475\n",
      "I1118 02:13:50.852108 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088475\n",
      "INFO:tensorflow:examples/sec: 2.8312\n",
      "I1118 02:13:50.852343 4435920320 tpu_estimator.py:2160] examples/sec: 2.8312\n",
      "INFO:tensorflow:global_step/sec: 0.0895617\n",
      "I1118 02:14:02.017517 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895617\n",
      "INFO:tensorflow:examples/sec: 2.86597\n",
      "I1118 02:14:02.017659 4435920320 tpu_estimator.py:2160] examples/sec: 2.86597\n",
      "INFO:tensorflow:global_step/sec: 0.0881066\n",
      "I1118 02:14:13.367482 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881066\n",
      "INFO:tensorflow:examples/sec: 2.81941\n",
      "I1118 02:14:13.367717 4435920320 tpu_estimator.py:2160] examples/sec: 2.81941\n",
      "INFO:tensorflow:global_step/sec: 0.0887568\n",
      "I1118 02:14:24.634229 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887568\n",
      "INFO:tensorflow:examples/sec: 2.84022\n",
      "I1118 02:14:24.634461 4435920320 tpu_estimator.py:2160] examples/sec: 2.84022\n",
      "INFO:tensorflow:global_step/sec: 0.0884481\n",
      "I1118 02:14:35.940299 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884481\n",
      "INFO:tensorflow:examples/sec: 2.83034\n",
      "I1118 02:14:35.940619 4435920320 tpu_estimator.py:2160] examples/sec: 2.83034\n",
      "INFO:tensorflow:global_step/sec: 0.0889994\n",
      "I1118 02:14:47.176306 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889994\n",
      "INFO:tensorflow:examples/sec: 2.84798\n",
      "I1118 02:14:47.176523 4435920320 tpu_estimator.py:2160] examples/sec: 2.84798\n",
      "INFO:tensorflow:global_step/sec: 0.0890256\n",
      "I1118 02:14:58.409035 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890256\n",
      "INFO:tensorflow:examples/sec: 2.84882\n",
      "I1118 02:14:58.409252 4435920320 tpu_estimator.py:2160] examples/sec: 2.84882\n",
      "INFO:tensorflow:global_step/sec: 0.0891031\n",
      "I1118 02:15:09.631999 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891031\n",
      "INFO:tensorflow:examples/sec: 2.8513\n",
      "I1118 02:15:09.632229 4435920320 tpu_estimator.py:2160] examples/sec: 2.8513\n",
      "INFO:tensorflow:global_step/sec: 0.0892442\n",
      "I1118 02:15:20.837210 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892442\n",
      "INFO:tensorflow:examples/sec: 2.85582\n",
      "I1118 02:15:20.837442 4435920320 tpu_estimator.py:2160] examples/sec: 2.85582\n",
      "INFO:tensorflow:global_step/sec: 0.0891269\n",
      "I1118 02:15:32.057152 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891269\n",
      "INFO:tensorflow:examples/sec: 2.85206\n",
      "I1118 02:15:32.057371 4435920320 tpu_estimator.py:2160] examples/sec: 2.85206\n",
      "INFO:tensorflow:global_step/sec: 0.0895814\n",
      "I1118 02:15:43.220218 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895814\n",
      "INFO:tensorflow:examples/sec: 2.86661\n",
      "I1118 02:15:43.220455 4435920320 tpu_estimator.py:2160] examples/sec: 2.86661\n",
      "INFO:tensorflow:global_step/sec: 0.0893453\n",
      "I1118 02:15:54.412708 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893453\n",
      "INFO:tensorflow:examples/sec: 2.85905\n",
      "I1118 02:15:54.412930 4435920320 tpu_estimator.py:2160] examples/sec: 2.85905\n",
      "INFO:tensorflow:global_step/sec: 0.0897125\n",
      "I1118 02:16:05.559436 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897125\n",
      "INFO:tensorflow:examples/sec: 2.8708\n",
      "I1118 02:16:05.559926 4435920320 tpu_estimator.py:2160] examples/sec: 2.8708\n",
      "INFO:tensorflow:global_step/sec: 0.0890409\n",
      "I1118 02:16:16.790226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890409\n",
      "INFO:tensorflow:examples/sec: 2.84931\n",
      "I1118 02:16:16.790448 4435920320 tpu_estimator.py:2160] examples/sec: 2.84931\n",
      "INFO:tensorflow:global_step/sec: 0.0895437\n",
      "I1118 02:16:27.957957 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895437\n",
      "INFO:tensorflow:examples/sec: 2.8654\n",
      "I1118 02:16:27.958173 4435920320 tpu_estimator.py:2160] examples/sec: 2.8654\n",
      "INFO:tensorflow:global_step/sec: 0.0888337\n",
      "I1118 02:16:39.214951 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888337\n",
      "INFO:tensorflow:examples/sec: 2.84268\n",
      "I1118 02:16:39.215168 4435920320 tpu_estimator.py:2160] examples/sec: 2.84268\n",
      "INFO:tensorflow:global_step/sec: 0.0888453\n",
      "I1118 02:16:50.470469 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888453\n",
      "INFO:tensorflow:examples/sec: 2.84305\n",
      "I1118 02:16:50.470690 4435920320 tpu_estimator.py:2160] examples/sec: 2.84305\n",
      "INFO:tensorflow:global_step/sec: 0.0885589\n",
      "I1118 02:17:01.762401 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885589\n",
      "INFO:tensorflow:examples/sec: 2.83388\n",
      "I1118 02:17:01.762624 4435920320 tpu_estimator.py:2160] examples/sec: 2.83388\n",
      "INFO:tensorflow:global_step/sec: 0.0887503\n",
      "I1118 02:17:13.029978 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887503\n",
      "INFO:tensorflow:examples/sec: 2.84001\n",
      "I1118 02:17:13.030200 4435920320 tpu_estimator.py:2160] examples/sec: 2.84001\n",
      "INFO:tensorflow:global_step/sec: 0.0892691\n",
      "I1118 02:17:24.232059 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892691\n",
      "INFO:tensorflow:examples/sec: 2.85661\n",
      "I1118 02:17:24.232293 4435920320 tpu_estimator.py:2160] examples/sec: 2.85661\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891844\n",
      "I1118 02:17:35.444790 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891844\n",
      "INFO:tensorflow:examples/sec: 2.8539\n",
      "I1118 02:17:35.445042 4435920320 tpu_estimator.py:2160] examples/sec: 2.8539\n",
      "INFO:tensorflow:global_step/sec: 0.0893829\n",
      "I1118 02:17:46.632606 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893829\n",
      "INFO:tensorflow:examples/sec: 2.86025\n",
      "I1118 02:17:46.632826 4435920320 tpu_estimator.py:2160] examples/sec: 2.86025\n",
      "INFO:tensorflow:global_step/sec: 0.0892281\n",
      "I1118 02:17:57.839828 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892281\n",
      "INFO:tensorflow:examples/sec: 2.8553\n",
      "I1118 02:17:57.840043 4435920320 tpu_estimator.py:2160] examples/sec: 2.8553\n",
      "INFO:tensorflow:global_step/sec: 0.0889142\n",
      "I1118 02:18:09.086637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889142\n",
      "INFO:tensorflow:examples/sec: 2.84526\n",
      "I1118 02:18:09.086873 4435920320 tpu_estimator.py:2160] examples/sec: 2.84526\n",
      "INFO:tensorflow:global_step/sec: 0.0895115\n",
      "I1118 02:18:20.258383 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895115\n",
      "INFO:tensorflow:examples/sec: 2.86437\n",
      "I1118 02:18:20.258613 4435920320 tpu_estimator.py:2160] examples/sec: 2.86437\n",
      "INFO:tensorflow:global_step/sec: 0.0889064\n",
      "I1118 02:18:31.506159 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889064\n",
      "INFO:tensorflow:examples/sec: 2.84501\n",
      "I1118 02:18:31.506376 4435920320 tpu_estimator.py:2160] examples/sec: 2.84501\n",
      "INFO:tensorflow:global_step/sec: 0.0890937\n",
      "I1118 02:18:42.730293 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890937\n",
      "INFO:tensorflow:examples/sec: 2.851\n",
      "I1118 02:18:42.730522 4435920320 tpu_estimator.py:2160] examples/sec: 2.851\n",
      "INFO:tensorflow:global_step/sec: 0.0881991\n",
      "I1118 02:18:54.068259 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881991\n",
      "INFO:tensorflow:examples/sec: 2.82237\n",
      "I1118 02:18:54.068474 4435920320 tpu_estimator.py:2160] examples/sec: 2.82237\n",
      "INFO:tensorflow:global_step/sec: 0.0888482\n",
      "I1118 02:19:05.323436 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888482\n",
      "INFO:tensorflow:examples/sec: 2.84314\n",
      "I1118 02:19:05.323655 4435920320 tpu_estimator.py:2160] examples/sec: 2.84314\n",
      "INFO:tensorflow:global_step/sec: 0.0886285\n",
      "I1118 02:19:16.606498 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886285\n",
      "INFO:tensorflow:examples/sec: 2.83611\n",
      "I1118 02:19:16.606715 4435920320 tpu_estimator.py:2160] examples/sec: 2.83611\n",
      "INFO:tensorflow:global_step/sec: 0.0887463\n",
      "I1118 02:19:27.874526 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887463\n",
      "INFO:tensorflow:examples/sec: 2.83988\n",
      "I1118 02:19:27.874722 4435920320 tpu_estimator.py:2160] examples/sec: 2.83988\n",
      "INFO:tensorflow:global_step/sec: 0.0887941\n",
      "I1118 02:19:39.136568 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887941\n",
      "INFO:tensorflow:examples/sec: 2.84141\n",
      "I1118 02:19:39.136803 4435920320 tpu_estimator.py:2160] examples/sec: 2.84141\n",
      "INFO:tensorflow:global_step/sec: 0.0896753\n",
      "I1118 02:19:50.287914 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896753\n",
      "INFO:tensorflow:examples/sec: 2.86961\n",
      "I1118 02:19:50.288148 4435920320 tpu_estimator.py:2160] examples/sec: 2.86961\n",
      "INFO:tensorflow:global_step/sec: 0.0895758\n",
      "I1118 02:20:01.451650 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895758\n",
      "INFO:tensorflow:examples/sec: 2.86643\n",
      "I1118 02:20:01.451883 4435920320 tpu_estimator.py:2160] examples/sec: 2.86643\n",
      "INFO:tensorflow:global_step/sec: 0.0885975\n",
      "I1118 02:20:12.738651 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885975\n",
      "INFO:tensorflow:examples/sec: 2.83512\n",
      "I1118 02:20:12.738883 4435920320 tpu_estimator.py:2160] examples/sec: 2.83512\n",
      "INFO:tensorflow:global_step/sec: 0.0891233\n",
      "I1118 02:20:23.959066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891233\n",
      "INFO:tensorflow:examples/sec: 2.85194\n",
      "I1118 02:20:23.959292 4435920320 tpu_estimator.py:2160] examples/sec: 2.85194\n",
      "INFO:tensorflow:global_step/sec: 0.0893114\n",
      "I1118 02:20:35.155842 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893114\n",
      "INFO:tensorflow:examples/sec: 2.85797\n",
      "I1118 02:20:35.156069 4435920320 tpu_estimator.py:2160] examples/sec: 2.85797\n",
      "INFO:tensorflow:global_step/sec: 0.0891451\n",
      "I1118 02:20:46.373509 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891451\n",
      "INFO:tensorflow:examples/sec: 2.85264\n",
      "I1118 02:20:46.373739 4435920320 tpu_estimator.py:2160] examples/sec: 2.85264\n",
      "INFO:tensorflow:global_step/sec: 0.0877675\n",
      "I1118 02:20:57.767231 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877675\n",
      "INFO:tensorflow:examples/sec: 2.80856\n",
      "I1118 02:20:57.767502 4435920320 tpu_estimator.py:2160] examples/sec: 2.80856\n",
      "INFO:tensorflow:global_step/sec: 0.0882775\n",
      "I1118 02:21:09.095176 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882775\n",
      "INFO:tensorflow:examples/sec: 2.82488\n",
      "I1118 02:21:09.095432 4435920320 tpu_estimator.py:2160] examples/sec: 2.82488\n",
      "INFO:tensorflow:global_step/sec: 0.0891983\n",
      "I1118 02:21:20.306149 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891983\n",
      "INFO:tensorflow:examples/sec: 2.85434\n",
      "I1118 02:21:20.306379 4435920320 tpu_estimator.py:2160] examples/sec: 2.85434\n",
      "INFO:tensorflow:global_step/sec: 0.0892236\n",
      "I1118 02:21:31.514022 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892236\n",
      "INFO:tensorflow:examples/sec: 2.85516\n",
      "I1118 02:21:31.514337 4435920320 tpu_estimator.py:2160] examples/sec: 2.85516\n",
      "INFO:tensorflow:global_step/sec: 0.0890624\n",
      "I1118 02:21:42.742023 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890624\n",
      "INFO:tensorflow:examples/sec: 2.85\n",
      "I1118 02:21:42.742254 4435920320 tpu_estimator.py:2160] examples/sec: 2.85\n",
      "INFO:tensorflow:global_step/sec: 0.0892934\n",
      "I1118 02:21:53.941073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892934\n",
      "INFO:tensorflow:examples/sec: 2.85739\n",
      "I1118 02:21:53.942342 4435920320 tpu_estimator.py:2160] examples/sec: 2.85739\n",
      "INFO:tensorflow:global_step/sec: 0.0892296\n",
      "I1118 02:22:05.148101 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892296\n",
      "INFO:tensorflow:examples/sec: 2.85535\n",
      "I1118 02:22:05.148332 4435920320 tpu_estimator.py:2160] examples/sec: 2.85535\n",
      "INFO:tensorflow:global_step/sec: 0.0893901\n",
      "I1118 02:22:16.335002 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893901\n",
      "INFO:tensorflow:examples/sec: 2.86048\n",
      "I1118 02:22:16.335215 4435920320 tpu_estimator.py:2160] examples/sec: 2.86048\n",
      "INFO:tensorflow:global_step/sec: 0.0889839\n",
      "I1118 02:22:27.572993 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889839\n",
      "INFO:tensorflow:examples/sec: 2.84749\n",
      "I1118 02:22:27.573211 4435920320 tpu_estimator.py:2160] examples/sec: 2.84749\n",
      "INFO:tensorflow:global_step/sec: 0.0888985\n",
      "I1118 02:22:38.821798 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888985\n",
      "INFO:tensorflow:examples/sec: 2.84475\n",
      "I1118 02:22:38.822018 4435920320 tpu_estimator.py:2160] examples/sec: 2.84475\n",
      "INFO:tensorflow:global_step/sec: 0.0892645\n",
      "I1118 02:22:50.024452 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892645\n",
      "INFO:tensorflow:examples/sec: 2.85646\n",
      "I1118 02:22:50.024678 4435920320 tpu_estimator.py:2160] examples/sec: 2.85646\n",
      "INFO:tensorflow:global_step/sec: 0.0891091\n",
      "I1118 02:23:01.246641 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891091\n",
      "INFO:tensorflow:examples/sec: 2.85149\n",
      "I1118 02:23:01.247014 4435920320 tpu_estimator.py:2160] examples/sec: 2.85149\n",
      "INFO:tensorflow:global_step/sec: 0.0894152\n",
      "I1118 02:23:12.430406 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894152\n",
      "INFO:tensorflow:examples/sec: 2.86129\n",
      "I1118 02:23:12.430626 4435920320 tpu_estimator.py:2160] examples/sec: 2.86129\n",
      "INFO:tensorflow:global_step/sec: 0.0894838\n",
      "I1118 02:23:23.605637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894838\n",
      "INFO:tensorflow:examples/sec: 2.86348\n",
      "I1118 02:23:23.605889 4435920320 tpu_estimator.py:2160] examples/sec: 2.86348\n",
      "INFO:tensorflow:global_step/sec: 0.0889384\n",
      "I1118 02:23:34.849368 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889384\n",
      "INFO:tensorflow:examples/sec: 2.84603\n",
      "I1118 02:23:34.849589 4435920320 tpu_estimator.py:2160] examples/sec: 2.84603\n",
      "INFO:tensorflow:global_step/sec: 0.089005\n",
      "I1118 02:23:46.084686 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089005\n",
      "INFO:tensorflow:examples/sec: 2.84816\n",
      "I1118 02:23:46.084901 4435920320 tpu_estimator.py:2160] examples/sec: 2.84816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.089759\n",
      "I1118 02:23:57.225630 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089759\n",
      "INFO:tensorflow:examples/sec: 2.87229\n",
      "I1118 02:23:57.225842 4435920320 tpu_estimator.py:2160] examples/sec: 2.87229\n",
      "INFO:tensorflow:global_step/sec: 0.0894547\n",
      "I1118 02:24:08.404495 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894547\n",
      "INFO:tensorflow:examples/sec: 2.86255\n",
      "I1118 02:24:08.404728 4435920320 tpu_estimator.py:2160] examples/sec: 2.86255\n",
      "INFO:tensorflow:global_step/sec: 0.0891294\n",
      "I1118 02:24:19.624128 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891294\n",
      "INFO:tensorflow:examples/sec: 2.85214\n",
      "I1118 02:24:19.624346 4435920320 tpu_estimator.py:2160] examples/sec: 2.85214\n",
      "INFO:tensorflow:global_step/sec: 0.0895847\n",
      "I1118 02:24:30.786757 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895847\n",
      "INFO:tensorflow:examples/sec: 2.86671\n",
      "I1118 02:24:30.786988 4435920320 tpu_estimator.py:2160] examples/sec: 2.86671\n",
      "INFO:tensorflow:global_step/sec: 0.0890337\n",
      "I1118 02:24:42.018450 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890337\n",
      "INFO:tensorflow:examples/sec: 2.84908\n",
      "I1118 02:24:42.018831 4435920320 tpu_estimator.py:2160] examples/sec: 2.84908\n",
      "INFO:tensorflow:global_step/sec: 0.0889561\n",
      "I1118 02:24:53.259954 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889561\n",
      "INFO:tensorflow:examples/sec: 2.8466\n",
      "I1118 02:24:53.260183 4435920320 tpu_estimator.py:2160] examples/sec: 2.8466\n",
      "INFO:tensorflow:global_step/sec: 0.0882249\n",
      "I1118 02:25:04.594631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882249\n",
      "INFO:tensorflow:examples/sec: 2.8232\n",
      "I1118 02:25:04.594853 4435920320 tpu_estimator.py:2160] examples/sec: 2.8232\n",
      "INFO:tensorflow:global_step/sec: 0.0889276\n",
      "I1118 02:25:15.839717 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889276\n",
      "INFO:tensorflow:examples/sec: 2.84568\n",
      "I1118 02:25:15.839942 4435920320 tpu_estimator.py:2160] examples/sec: 2.84568\n",
      "INFO:tensorflow:global_step/sec: 0.0894341\n",
      "I1118 02:25:27.021142 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894341\n",
      "INFO:tensorflow:examples/sec: 2.86189\n",
      "I1118 02:25:27.021385 4435920320 tpu_estimator.py:2160] examples/sec: 2.86189\n",
      "INFO:tensorflow:global_step/sec: 0.088891\n",
      "I1118 02:25:38.270865 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088891\n",
      "INFO:tensorflow:examples/sec: 2.84451\n",
      "I1118 02:25:38.271092 4435920320 tpu_estimator.py:2160] examples/sec: 2.84451\n",
      "INFO:tensorflow:global_step/sec: 0.0891386\n",
      "I1118 02:25:49.489355 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891386\n",
      "INFO:tensorflow:examples/sec: 2.85244\n",
      "I1118 02:25:49.489601 4435920320 tpu_estimator.py:2160] examples/sec: 2.85244\n",
      "INFO:tensorflow:global_step/sec: 0.0880885\n",
      "I1118 02:26:00.841579 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880885\n",
      "INFO:tensorflow:examples/sec: 2.81883\n",
      "I1118 02:26:00.841818 4435920320 tpu_estimator.py:2160] examples/sec: 2.81883\n",
      "INFO:tensorflow:global_step/sec: 0.0888999\n",
      "I1118 02:26:12.090191 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888999\n",
      "INFO:tensorflow:examples/sec: 2.8448\n",
      "I1118 02:26:12.090430 4435920320 tpu_estimator.py:2160] examples/sec: 2.8448\n",
      "INFO:tensorflow:global_step/sec: 0.0892483\n",
      "I1118 02:26:23.294888 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892483\n",
      "INFO:tensorflow:examples/sec: 2.85595\n",
      "I1118 02:26:23.295148 4435920320 tpu_estimator.py:2160] examples/sec: 2.85595\n",
      "INFO:tensorflow:global_step/sec: 0.089285\n",
      "I1118 02:26:34.494966 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089285\n",
      "INFO:tensorflow:examples/sec: 2.85712\n",
      "I1118 02:26:34.495198 4435920320 tpu_estimator.py:2160] examples/sec: 2.85712\n",
      "INFO:tensorflow:global_step/sec: 0.0882752\n",
      "I1118 02:26:45.823161 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882752\n",
      "INFO:tensorflow:examples/sec: 2.82481\n",
      "I1118 02:26:45.823377 4435920320 tpu_estimator.py:2160] examples/sec: 2.82481\n",
      "INFO:tensorflow:global_step/sec: 0.0884515\n",
      "I1118 02:26:57.128812 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884515\n",
      "INFO:tensorflow:examples/sec: 2.83045\n",
      "I1118 02:26:57.129049 4435920320 tpu_estimator.py:2160] examples/sec: 2.83045\n",
      "INFO:tensorflow:global_step/sec: 0.0891338\n",
      "I1118 02:27:08.347882 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891338\n",
      "INFO:tensorflow:examples/sec: 2.85228\n",
      "I1118 02:27:08.348101 4435920320 tpu_estimator.py:2160] examples/sec: 2.85228\n",
      "INFO:tensorflow:global_step/sec: 0.0890262\n",
      "I1118 02:27:19.580561 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890262\n",
      "INFO:tensorflow:examples/sec: 2.84884\n",
      "I1118 02:27:19.580780 4435920320 tpu_estimator.py:2160] examples/sec: 2.84884\n",
      "INFO:tensorflow:global_step/sec: 0.0893485\n",
      "I1118 02:27:30.772670 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893485\n",
      "INFO:tensorflow:examples/sec: 2.85915\n",
      "I1118 02:27:30.772890 4435920320 tpu_estimator.py:2160] examples/sec: 2.85915\n",
      "INFO:tensorflow:global_step/sec: 0.0887137\n",
      "I1118 02:27:42.044880 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887137\n",
      "INFO:tensorflow:examples/sec: 2.83884\n",
      "I1118 02:27:42.045089 4435920320 tpu_estimator.py:2160] examples/sec: 2.83884\n",
      "INFO:tensorflow:global_step/sec: 0.0890809\n",
      "I1118 02:27:53.270637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890809\n",
      "INFO:tensorflow:examples/sec: 2.85059\n",
      "I1118 02:27:53.270853 4435920320 tpu_estimator.py:2160] examples/sec: 2.85059\n",
      "INFO:tensorflow:global_step/sec: 0.089735\n",
      "I1118 02:28:04.414588 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089735\n",
      "INFO:tensorflow:examples/sec: 2.87152\n",
      "I1118 02:28:04.414810 4435920320 tpu_estimator.py:2160] examples/sec: 2.87152\n",
      "INFO:tensorflow:global_step/sec: 0.0890552\n",
      "I1118 02:28:15.643568 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890552\n",
      "INFO:tensorflow:examples/sec: 2.84977\n",
      "I1118 02:28:15.643800 4435920320 tpu_estimator.py:2160] examples/sec: 2.84977\n",
      "INFO:tensorflow:global_step/sec: 0.089044\n",
      "I1118 02:28:26.873970 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089044\n",
      "INFO:tensorflow:examples/sec: 2.84941\n",
      "I1118 02:28:26.874213 4435920320 tpu_estimator.py:2160] examples/sec: 2.84941\n",
      "INFO:tensorflow:global_step/sec: 0.0893085\n",
      "I1118 02:28:38.071100 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893085\n",
      "INFO:tensorflow:examples/sec: 2.85787\n",
      "I1118 02:28:38.071321 4435920320 tpu_estimator.py:2160] examples/sec: 2.85787\n",
      "INFO:tensorflow:global_step/sec: 0.089244\n",
      "I1118 02:28:49.276340 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089244\n",
      "INFO:tensorflow:examples/sec: 2.85581\n",
      "I1118 02:28:49.276572 4435920320 tpu_estimator.py:2160] examples/sec: 2.85581\n",
      "INFO:tensorflow:global_step/sec: 0.0890166\n",
      "I1118 02:29:00.510189 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890166\n",
      "INFO:tensorflow:examples/sec: 2.84853\n",
      "I1118 02:29:00.510407 4435920320 tpu_estimator.py:2160] examples/sec: 2.84853\n",
      "INFO:tensorflow:global_step/sec: 0.088373\n",
      "I1118 02:29:11.825870 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088373\n",
      "INFO:tensorflow:examples/sec: 2.82794\n",
      "I1118 02:29:11.826097 4435920320 tpu_estimator.py:2160] examples/sec: 2.82794\n",
      "INFO:tensorflow:global_step/sec: 0.0888262\n",
      "I1118 02:29:23.083794 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888262\n",
      "INFO:tensorflow:examples/sec: 2.84244\n",
      "I1118 02:29:23.084002 4435920320 tpu_estimator.py:2160] examples/sec: 2.84244\n",
      "INFO:tensorflow:global_step/sec: 0.0894763\n",
      "I1118 02:29:34.259948 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894763\n",
      "INFO:tensorflow:examples/sec: 2.86324\n",
      "I1118 02:29:34.260170 4435920320 tpu_estimator.py:2160] examples/sec: 2.86324\n",
      "INFO:tensorflow:global_step/sec: 0.0899106\n",
      "I1118 02:29:45.382143 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899106\n",
      "INFO:tensorflow:examples/sec: 2.87714\n",
      "I1118 02:29:45.382371 4435920320 tpu_estimator.py:2160] examples/sec: 2.87714\n",
      "INFO:tensorflow:global_step/sec: 0.0889719\n",
      "I1118 02:29:56.621618 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889719\n",
      "INFO:tensorflow:examples/sec: 2.8471\n",
      "I1118 02:29:56.621831 4435920320 tpu_estimator.py:2160] examples/sec: 2.8471\n",
      "INFO:tensorflow:global_step/sec: 0.0893337\n",
      "I1118 02:30:07.815613 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893337\n",
      "INFO:tensorflow:examples/sec: 2.85868\n",
      "I1118 02:30:07.815858 4435920320 tpu_estimator.py:2160] examples/sec: 2.85868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0889282\n",
      "I1118 02:30:19.060623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889282\n",
      "INFO:tensorflow:examples/sec: 2.8457\n",
      "I1118 02:30:19.060845 4435920320 tpu_estimator.py:2160] examples/sec: 2.8457\n",
      "INFO:tensorflow:global_step/sec: 0.0892688\n",
      "I1118 02:30:30.262757 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892688\n",
      "INFO:tensorflow:examples/sec: 2.8566\n",
      "I1118 02:30:30.262984 4435920320 tpu_estimator.py:2160] examples/sec: 2.8566\n",
      "INFO:tensorflow:global_step/sec: 0.0899093\n",
      "I1118 02:30:41.385076 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899093\n",
      "INFO:tensorflow:examples/sec: 2.8771\n",
      "I1118 02:30:41.385308 4435920320 tpu_estimator.py:2160] examples/sec: 2.8771\n",
      "INFO:tensorflow:global_step/sec: 0.0890826\n",
      "I1118 02:30:52.610599 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890826\n",
      "INFO:tensorflow:examples/sec: 2.85064\n",
      "I1118 02:30:52.610812 4435920320 tpu_estimator.py:2160] examples/sec: 2.85064\n",
      "INFO:tensorflow:global_step/sec: 0.0880869\n",
      "I1118 02:31:03.963047 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880869\n",
      "INFO:tensorflow:examples/sec: 2.81878\n",
      "I1118 02:31:03.963273 4435920320 tpu_estimator.py:2160] examples/sec: 2.81878\n",
      "INFO:tensorflow:global_step/sec: 0.0889543\n",
      "I1118 02:31:15.204746 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889543\n",
      "INFO:tensorflow:examples/sec: 2.84654\n",
      "I1118 02:31:15.204965 4435920320 tpu_estimator.py:2160] examples/sec: 2.84654\n",
      "INFO:tensorflow:global_step/sec: 0.0898427\n",
      "I1118 02:31:26.335312 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898427\n",
      "INFO:tensorflow:examples/sec: 2.87497\n",
      "I1118 02:31:26.335712 4435920320 tpu_estimator.py:2160] examples/sec: 2.87497\n",
      "INFO:tensorflow:global_step/sec: 0.08934\n",
      "I1118 02:31:37.528508 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08934\n",
      "INFO:tensorflow:examples/sec: 2.85888\n",
      "I1118 02:31:37.528733 4435920320 tpu_estimator.py:2160] examples/sec: 2.85888\n",
      "INFO:tensorflow:global_step/sec: 0.0884085\n",
      "I1118 02:31:48.839640 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884085\n",
      "INFO:tensorflow:examples/sec: 2.82907\n",
      "I1118 02:31:48.839864 4435920320 tpu_estimator.py:2160] examples/sec: 2.82907\n",
      "INFO:tensorflow:global_step/sec: 0.0890816\n",
      "I1118 02:32:00.065306 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890816\n",
      "INFO:tensorflow:examples/sec: 2.85061\n",
      "I1118 02:32:00.065546 4435920320 tpu_estimator.py:2160] examples/sec: 2.85061\n",
      "INFO:tensorflow:global_step/sec: 0.0887454\n",
      "I1118 02:32:11.333514 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887454\n",
      "INFO:tensorflow:examples/sec: 2.83985\n",
      "I1118 02:32:11.333761 4435920320 tpu_estimator.py:2160] examples/sec: 2.83985\n",
      "INFO:tensorflow:global_step/sec: 0.088753\n",
      "I1118 02:32:22.600709 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088753\n",
      "INFO:tensorflow:examples/sec: 2.8401\n",
      "I1118 02:32:22.600945 4435920320 tpu_estimator.py:2160] examples/sec: 2.8401\n",
      "INFO:tensorflow:global_step/sec: 0.0890789\n",
      "I1118 02:32:33.826708 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890789\n",
      "INFO:tensorflow:examples/sec: 2.85053\n",
      "I1118 02:32:33.826925 4435920320 tpu_estimator.py:2160] examples/sec: 2.85053\n",
      "INFO:tensorflow:global_step/sec: 0.0891827\n",
      "I1118 02:32:45.039656 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891827\n",
      "INFO:tensorflow:examples/sec: 2.85384\n",
      "I1118 02:32:45.039875 4435920320 tpu_estimator.py:2160] examples/sec: 2.85384\n",
      "INFO:tensorflow:global_step/sec: 0.0890163\n",
      "I1118 02:32:56.273562 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890163\n",
      "INFO:tensorflow:examples/sec: 2.84852\n",
      "I1118 02:32:56.273788 4435920320 tpu_estimator.py:2160] examples/sec: 2.84852\n",
      "INFO:tensorflow:global_step/sec: 0.0891698\n",
      "I1118 02:33:07.488138 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891698\n",
      "INFO:tensorflow:examples/sec: 2.85343\n",
      "I1118 02:33:07.488359 4435920320 tpu_estimator.py:2160] examples/sec: 2.85343\n",
      "INFO:tensorflow:global_step/sec: 0.089149\n",
      "I1118 02:33:18.705297 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089149\n",
      "INFO:tensorflow:examples/sec: 2.85277\n",
      "I1118 02:33:18.705537 4435920320 tpu_estimator.py:2160] examples/sec: 2.85277\n",
      "INFO:tensorflow:global_step/sec: 0.0892962\n",
      "I1118 02:33:29.903962 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892962\n",
      "INFO:tensorflow:examples/sec: 2.85748\n",
      "I1118 02:33:29.904178 4435920320 tpu_estimator.py:2160] examples/sec: 2.85748\n",
      "INFO:tensorflow:global_step/sec: 0.0897708\n",
      "I1118 02:33:41.043453 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897708\n",
      "INFO:tensorflow:examples/sec: 2.87267\n",
      "I1118 02:33:41.043674 4435920320 tpu_estimator.py:2160] examples/sec: 2.87267\n",
      "INFO:tensorflow:global_step/sec: 0.0892565\n",
      "I1118 02:33:52.247117 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892565\n",
      "INFO:tensorflow:examples/sec: 2.85621\n",
      "I1118 02:33:52.247350 4435920320 tpu_estimator.py:2160] examples/sec: 2.85621\n",
      "INFO:tensorflow:global_step/sec: 0.0891001\n",
      "I1118 02:34:03.470463 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891001\n",
      "INFO:tensorflow:examples/sec: 2.8512\n",
      "I1118 02:34:03.470706 4435920320 tpu_estimator.py:2160] examples/sec: 2.8512\n",
      "INFO:tensorflow:global_step/sec: 0.0888519\n",
      "I1118 02:34:14.725138 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888519\n",
      "INFO:tensorflow:examples/sec: 2.84326\n",
      "I1118 02:34:14.725357 4435920320 tpu_estimator.py:2160] examples/sec: 2.84326\n",
      "INFO:tensorflow:global_step/sec: 0.0892282\n",
      "I1118 02:34:25.932359 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892282\n",
      "INFO:tensorflow:examples/sec: 2.8553\n",
      "I1118 02:34:25.932595 4435920320 tpu_estimator.py:2160] examples/sec: 2.8553\n",
      "INFO:tensorflow:global_step/sec: 0.0897011\n",
      "I1118 02:34:37.080502 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897011\n",
      "INFO:tensorflow:examples/sec: 2.87044\n",
      "I1118 02:34:37.080724 4435920320 tpu_estimator.py:2160] examples/sec: 2.87044\n",
      "INFO:tensorflow:global_step/sec: 0.0894945\n",
      "I1118 02:34:48.254364 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894945\n",
      "INFO:tensorflow:examples/sec: 2.86382\n",
      "I1118 02:34:48.254594 4435920320 tpu_estimator.py:2160] examples/sec: 2.86382\n",
      "INFO:tensorflow:global_step/sec: 0.0892681\n",
      "I1118 02:34:59.456577 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892681\n",
      "INFO:tensorflow:examples/sec: 2.85658\n",
      "I1118 02:34:59.456806 4435920320 tpu_estimator.py:2160] examples/sec: 2.85658\n",
      "INFO:tensorflow:global_step/sec: 0.0898685\n",
      "I1118 02:35:10.583945 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898685\n",
      "INFO:tensorflow:examples/sec: 2.87579\n",
      "I1118 02:35:10.584177 4435920320 tpu_estimator.py:2160] examples/sec: 2.87579\n",
      "INFO:tensorflow:global_step/sec: 0.0889371\n",
      "I1118 02:35:21.827888 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889371\n",
      "INFO:tensorflow:examples/sec: 2.84599\n",
      "I1118 02:35:21.828128 4435920320 tpu_estimator.py:2160] examples/sec: 2.84599\n",
      "INFO:tensorflow:global_step/sec: 0.0893433\n",
      "I1118 02:35:33.020623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893433\n",
      "INFO:tensorflow:examples/sec: 2.85899\n",
      "I1118 02:35:33.020843 4435920320 tpu_estimator.py:2160] examples/sec: 2.85899\n",
      "INFO:tensorflow:global_step/sec: 0.0889303\n",
      "I1118 02:35:44.265391 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889303\n",
      "INFO:tensorflow:examples/sec: 2.84577\n",
      "I1118 02:35:44.265605 4435920320 tpu_estimator.py:2160] examples/sec: 2.84577\n",
      "INFO:tensorflow:global_step/sec: 0.0891178\n",
      "I1118 02:35:55.486479 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891178\n",
      "INFO:tensorflow:examples/sec: 2.85177\n",
      "I1118 02:35:55.486693 4435920320 tpu_estimator.py:2160] examples/sec: 2.85177\n",
      "INFO:tensorflow:global_step/sec: 0.0887391\n",
      "I1118 02:36:06.755479 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887391\n",
      "INFO:tensorflow:examples/sec: 2.83965\n",
      "I1118 02:36:06.755879 4435920320 tpu_estimator.py:2160] examples/sec: 2.83965\n",
      "INFO:tensorflow:global_step/sec: 0.0888115\n",
      "I1118 02:36:18.015268 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888115\n",
      "INFO:tensorflow:examples/sec: 2.84197\n",
      "I1118 02:36:18.015480 4435920320 tpu_estimator.py:2160] examples/sec: 2.84197\n",
      "INFO:tensorflow:global_step/sec: 0.0889256\n",
      "I1118 02:36:29.260623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889256\n",
      "INFO:tensorflow:examples/sec: 2.84562\n",
      "I1118 02:36:29.260839 4435920320 tpu_estimator.py:2160] examples/sec: 2.84562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0886283\n",
      "I1118 02:36:40.543717 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886283\n",
      "INFO:tensorflow:examples/sec: 2.8361\n",
      "I1118 02:36:40.543947 4435920320 tpu_estimator.py:2160] examples/sec: 2.8361\n",
      "INFO:tensorflow:global_step/sec: 0.0889023\n",
      "I1118 02:36:51.792014 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889023\n",
      "INFO:tensorflow:examples/sec: 2.84487\n",
      "I1118 02:36:51.792392 4435920320 tpu_estimator.py:2160] examples/sec: 2.84487\n",
      "INFO:tensorflow:global_step/sec: 0.0883446\n",
      "I1118 02:37:03.111313 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883446\n",
      "INFO:tensorflow:examples/sec: 2.82703\n",
      "I1118 02:37:03.111534 4435920320 tpu_estimator.py:2160] examples/sec: 2.82703\n",
      "INFO:tensorflow:global_step/sec: 0.088908\n",
      "I1118 02:37:14.358919 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088908\n",
      "INFO:tensorflow:examples/sec: 2.84505\n",
      "I1118 02:37:14.359141 4435920320 tpu_estimator.py:2160] examples/sec: 2.84505\n",
      "INFO:tensorflow:global_step/sec: 0.0890074\n",
      "I1118 02:37:25.593936 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890074\n",
      "INFO:tensorflow:examples/sec: 2.84824\n",
      "I1118 02:37:25.594174 4435920320 tpu_estimator.py:2160] examples/sec: 2.84824\n",
      "INFO:tensorflow:global_step/sec: 0.0888034\n",
      "I1118 02:37:36.854750 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888034\n",
      "INFO:tensorflow:examples/sec: 2.84171\n",
      "I1118 02:37:36.854972 4435920320 tpu_estimator.py:2160] examples/sec: 2.84171\n",
      "INFO:tensorflow:global_step/sec: 0.0890147\n",
      "I1118 02:37:48.088850 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890147\n",
      "INFO:tensorflow:examples/sec: 2.84847\n",
      "I1118 02:37:48.089066 4435920320 tpu_estimator.py:2160] examples/sec: 2.84847\n",
      "INFO:tensorflow:global_step/sec: 0.0895067\n",
      "I1118 02:37:59.261198 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895067\n",
      "INFO:tensorflow:examples/sec: 2.86421\n",
      "I1118 02:37:59.261423 4435920320 tpu_estimator.py:2160] examples/sec: 2.86421\n",
      "INFO:tensorflow:global_step/sec: 0.0892801\n",
      "I1118 02:38:10.461926 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892801\n",
      "INFO:tensorflow:examples/sec: 2.85696\n",
      "I1118 02:38:10.462141 4435920320 tpu_estimator.py:2160] examples/sec: 2.85696\n",
      "INFO:tensorflow:global_step/sec: 0.0889319\n",
      "I1118 02:38:21.706464 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889319\n",
      "INFO:tensorflow:examples/sec: 2.84582\n",
      "I1118 02:38:21.706696 4435920320 tpu_estimator.py:2160] examples/sec: 2.84582\n",
      "INFO:tensorflow:global_step/sec: 0.0892157\n",
      "I1118 02:38:32.915262 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892157\n",
      "INFO:tensorflow:examples/sec: 2.8549\n",
      "I1118 02:38:32.915477 4435920320 tpu_estimator.py:2160] examples/sec: 2.8549\n",
      "INFO:tensorflow:global_step/sec: 0.0889685\n",
      "I1118 02:38:44.155178 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889685\n",
      "INFO:tensorflow:examples/sec: 2.84699\n",
      "I1118 02:38:44.155400 4435920320 tpu_estimator.py:2160] examples/sec: 2.84699\n",
      "INFO:tensorflow:global_step/sec: 0.0892159\n",
      "I1118 02:38:55.363965 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892159\n",
      "INFO:tensorflow:examples/sec: 2.85491\n",
      "I1118 02:38:55.364188 4435920320 tpu_estimator.py:2160] examples/sec: 2.85491\n",
      "INFO:tensorflow:global_step/sec: 0.0893834\n",
      "I1118 02:39:06.551692 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893834\n",
      "INFO:tensorflow:examples/sec: 2.86027\n",
      "I1118 02:39:06.551913 4435920320 tpu_estimator.py:2160] examples/sec: 2.86027\n",
      "INFO:tensorflow:global_step/sec: 0.0890964\n",
      "I1118 02:39:17.775513 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890964\n",
      "INFO:tensorflow:examples/sec: 2.85109\n",
      "I1118 02:39:17.775732 4435920320 tpu_estimator.py:2160] examples/sec: 2.85109\n",
      "INFO:tensorflow:global_step/sec: 0.0888907\n",
      "I1118 02:39:29.025256 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888907\n",
      "INFO:tensorflow:examples/sec: 2.8445\n",
      "I1118 02:39:29.025480 4435920320 tpu_estimator.py:2160] examples/sec: 2.8445\n",
      "INFO:tensorflow:global_step/sec: 0.0888413\n",
      "I1118 02:39:40.281257 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888413\n",
      "INFO:tensorflow:examples/sec: 2.84292\n",
      "I1118 02:39:40.281485 4435920320 tpu_estimator.py:2160] examples/sec: 2.84292\n",
      "INFO:tensorflow:global_step/sec: 0.0889332\n",
      "I1118 02:39:51.525671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889332\n",
      "INFO:tensorflow:examples/sec: 2.84586\n",
      "I1118 02:39:51.525893 4435920320 tpu_estimator.py:2160] examples/sec: 2.84586\n",
      "INFO:tensorflow:global_step/sec: 0.0889611\n",
      "I1118 02:40:02.766533 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889611\n",
      "INFO:tensorflow:examples/sec: 2.84676\n",
      "I1118 02:40:02.766755 4435920320 tpu_estimator.py:2160] examples/sec: 2.84676\n",
      "INFO:tensorflow:global_step/sec: 0.0890234\n",
      "I1118 02:40:13.999542 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890234\n",
      "INFO:tensorflow:examples/sec: 2.84875\n",
      "I1118 02:40:13.999782 4435920320 tpu_estimator.py:2160] examples/sec: 2.84875\n",
      "INFO:tensorflow:global_step/sec: 0.0891769\n",
      "I1118 02:40:25.213222 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891769\n",
      "INFO:tensorflow:examples/sec: 2.85366\n",
      "I1118 02:40:25.213439 4435920320 tpu_estimator.py:2160] examples/sec: 2.85366\n",
      "INFO:tensorflow:global_step/sec: 0.0893267\n",
      "I1118 02:40:36.408073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893267\n",
      "INFO:tensorflow:examples/sec: 2.85845\n",
      "I1118 02:40:36.408324 4435920320 tpu_estimator.py:2160] examples/sec: 2.85845\n",
      "INFO:tensorflow:global_step/sec: 0.0888374\n",
      "I1118 02:40:47.664572 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888374\n",
      "INFO:tensorflow:examples/sec: 2.8428\n",
      "I1118 02:40:47.664781 4435920320 tpu_estimator.py:2160] examples/sec: 2.8428\n",
      "INFO:tensorflow:global_step/sec: 0.0884624\n",
      "I1118 02:40:58.968842 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884624\n",
      "INFO:tensorflow:examples/sec: 2.8308\n",
      "I1118 02:40:58.969077 4435920320 tpu_estimator.py:2160] examples/sec: 2.8308\n",
      "INFO:tensorflow:global_step/sec: 0.0889168\n",
      "I1118 02:41:10.215300 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889168\n",
      "INFO:tensorflow:examples/sec: 2.84534\n",
      "I1118 02:41:10.215528 4435920320 tpu_estimator.py:2160] examples/sec: 2.84534\n",
      "INFO:tensorflow:global_step/sec: 0.0895506\n",
      "I1118 02:41:21.382171 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895506\n",
      "INFO:tensorflow:examples/sec: 2.86562\n",
      "I1118 02:41:21.382401 4435920320 tpu_estimator.py:2160] examples/sec: 2.86562\n",
      "INFO:tensorflow:global_step/sec: 0.0899413\n",
      "I1118 02:41:32.500523 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899413\n",
      "INFO:tensorflow:examples/sec: 2.87812\n",
      "I1118 02:41:32.500738 4435920320 tpu_estimator.py:2160] examples/sec: 2.87812\n",
      "INFO:tensorflow:global_step/sec: 0.088645\n",
      "I1118 02:41:43.781472 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088645\n",
      "INFO:tensorflow:examples/sec: 2.83664\n",
      "I1118 02:41:43.781693 4435920320 tpu_estimator.py:2160] examples/sec: 2.83664\n",
      "INFO:tensorflow:global_step/sec: 0.0890264\n",
      "I1118 02:41:55.014101 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890264\n",
      "INFO:tensorflow:examples/sec: 2.84884\n",
      "I1118 02:41:55.014313 4435920320 tpu_estimator.py:2160] examples/sec: 2.84884\n",
      "INFO:tensorflow:global_step/sec: 0.0890422\n",
      "I1118 02:42:06.244738 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890422\n",
      "INFO:tensorflow:examples/sec: 2.84935\n",
      "I1118 02:42:06.244963 4435920320 tpu_estimator.py:2160] examples/sec: 2.84935\n",
      "INFO:tensorflow:global_step/sec: 0.0884504\n",
      "I1118 02:42:17.550497 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884504\n",
      "INFO:tensorflow:examples/sec: 2.83041\n",
      "I1118 02:42:17.550713 4435920320 tpu_estimator.py:2160] examples/sec: 2.83041\n",
      "INFO:tensorflow:global_step/sec: 0.0891865\n",
      "I1118 02:42:28.762975 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891865\n",
      "INFO:tensorflow:examples/sec: 2.85397\n",
      "I1118 02:42:28.763217 4435920320 tpu_estimator.py:2160] examples/sec: 2.85397\n",
      "INFO:tensorflow:global_step/sec: 0.0892789\n",
      "I1118 02:42:39.963829 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892789\n",
      "INFO:tensorflow:examples/sec: 2.85693\n",
      "I1118 02:42:39.964066 4435920320 tpu_estimator.py:2160] examples/sec: 2.85693\n",
      "INFO:tensorflow:global_step/sec: 0.0890685\n",
      "I1118 02:42:51.191123 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890685\n",
      "INFO:tensorflow:examples/sec: 2.85019\n",
      "I1118 02:42:51.191496 4435920320 tpu_estimator.py:2160] examples/sec: 2.85019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0896184\n",
      "I1118 02:43:02.349553 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896184\n",
      "INFO:tensorflow:examples/sec: 2.86779\n",
      "I1118 02:43:02.349780 4435920320 tpu_estimator.py:2160] examples/sec: 2.86779\n",
      "INFO:tensorflow:global_step/sec: 0.0892744\n",
      "I1118 02:43:13.550969 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892744\n",
      "INFO:tensorflow:examples/sec: 2.85678\n",
      "I1118 02:43:13.551185 4435920320 tpu_estimator.py:2160] examples/sec: 2.85678\n",
      "INFO:tensorflow:global_step/sec: 0.0890954\n",
      "I1118 02:43:24.774889 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890954\n",
      "INFO:tensorflow:examples/sec: 2.85105\n",
      "I1118 02:43:24.775109 4435920320 tpu_estimator.py:2160] examples/sec: 2.85105\n",
      "INFO:tensorflow:global_step/sec: 0.0894312\n",
      "I1118 02:43:35.956666 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894312\n",
      "INFO:tensorflow:examples/sec: 2.8618\n",
      "I1118 02:43:35.956892 4435920320 tpu_estimator.py:2160] examples/sec: 2.8618\n",
      "INFO:tensorflow:global_step/sec: 0.0894204\n",
      "I1118 02:43:47.139794 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894204\n",
      "INFO:tensorflow:examples/sec: 2.86145\n",
      "I1118 02:43:47.140015 4435920320 tpu_estimator.py:2160] examples/sec: 2.86145\n",
      "INFO:tensorflow:global_step/sec: 0.0894522\n",
      "I1118 02:43:58.318960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894522\n",
      "INFO:tensorflow:examples/sec: 2.86247\n",
      "I1118 02:43:58.319190 4435920320 tpu_estimator.py:2160] examples/sec: 2.86247\n",
      "INFO:tensorflow:global_step/sec: 0.0880127\n",
      "I1118 02:44:09.680963 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880127\n",
      "INFO:tensorflow:examples/sec: 2.81641\n",
      "I1118 02:44:09.681201 4435920320 tpu_estimator.py:2160] examples/sec: 2.81641\n",
      "INFO:tensorflow:global_step/sec: 0.0888066\n",
      "I1118 02:44:20.941379 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888066\n",
      "INFO:tensorflow:examples/sec: 2.84181\n",
      "I1118 02:44:20.941606 4435920320 tpu_estimator.py:2160] examples/sec: 2.84181\n",
      "INFO:tensorflow:global_step/sec: 0.0891491\n",
      "I1118 02:44:32.158533 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891491\n",
      "INFO:tensorflow:examples/sec: 2.85277\n",
      "I1118 02:44:32.158748 4435920320 tpu_estimator.py:2160] examples/sec: 2.85277\n",
      "INFO:tensorflow:global_step/sec: 0.0892687\n",
      "I1118 02:44:43.360666 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892687\n",
      "INFO:tensorflow:examples/sec: 2.8566\n",
      "I1118 02:44:43.360882 4435920320 tpu_estimator.py:2160] examples/sec: 2.8566\n",
      "INFO:tensorflow:global_step/sec: 0.0893997\n",
      "I1118 02:44:54.546397 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893997\n",
      "INFO:tensorflow:examples/sec: 2.86079\n",
      "I1118 02:44:54.546613 4435920320 tpu_estimator.py:2160] examples/sec: 2.86079\n",
      "INFO:tensorflow:global_step/sec: 0.0889457\n",
      "I1118 02:45:05.789213 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889457\n",
      "INFO:tensorflow:examples/sec: 2.84626\n",
      "I1118 02:45:05.789443 4435920320 tpu_estimator.py:2160] examples/sec: 2.84626\n",
      "INFO:tensorflow:global_step/sec: 0.0888636\n",
      "I1118 02:45:17.042406 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888636\n",
      "INFO:tensorflow:examples/sec: 2.84363\n",
      "I1118 02:45:17.042628 4435920320 tpu_estimator.py:2160] examples/sec: 2.84363\n",
      "INFO:tensorflow:global_step/sec: 0.0881444\n",
      "I1118 02:45:28.387437 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881444\n",
      "INFO:tensorflow:examples/sec: 2.82062\n",
      "I1118 02:45:28.387671 4435920320 tpu_estimator.py:2160] examples/sec: 2.82062\n",
      "INFO:tensorflow:global_step/sec: 0.089205\n",
      "I1118 02:45:39.597568 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089205\n",
      "INFO:tensorflow:examples/sec: 2.85456\n",
      "I1118 02:45:39.598019 4435920320 tpu_estimator.py:2160] examples/sec: 2.85456\n",
      "INFO:tensorflow:global_step/sec: 0.0890407\n",
      "I1118 02:45:50.828404 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890407\n",
      "INFO:tensorflow:examples/sec: 2.8493\n",
      "I1118 02:45:50.828619 4435920320 tpu_estimator.py:2160] examples/sec: 2.8493\n",
      "INFO:tensorflow:global_step/sec: 0.089752\n",
      "I1118 02:46:01.970201 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089752\n",
      "INFO:tensorflow:examples/sec: 2.87206\n",
      "I1118 02:46:01.970436 4435920320 tpu_estimator.py:2160] examples/sec: 2.87206\n",
      "INFO:tensorflow:global_step/sec: 0.0894665\n",
      "I1118 02:46:13.147570 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894665\n",
      "INFO:tensorflow:examples/sec: 2.86293\n",
      "I1118 02:46:13.147809 4435920320 tpu_estimator.py:2160] examples/sec: 2.86293\n",
      "INFO:tensorflow:global_step/sec: 0.0895385\n",
      "I1118 02:46:24.315952 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895385\n",
      "INFO:tensorflow:examples/sec: 2.86523\n",
      "I1118 02:46:24.316188 4435920320 tpu_estimator.py:2160] examples/sec: 2.86523\n",
      "INFO:tensorflow:global_step/sec: 0.0893798\n",
      "I1118 02:46:35.504163 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893798\n",
      "INFO:tensorflow:examples/sec: 2.86015\n",
      "I1118 02:46:35.504402 4435920320 tpu_estimator.py:2160] examples/sec: 2.86015\n",
      "INFO:tensorflow:global_step/sec: 0.0881796\n",
      "I1118 02:46:46.844671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881796\n",
      "INFO:tensorflow:examples/sec: 2.82175\n",
      "I1118 02:46:46.844897 4435920320 tpu_estimator.py:2160] examples/sec: 2.82175\n",
      "INFO:tensorflow:global_step/sec: 0.0889348\n",
      "I1118 02:46:58.088848 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889348\n",
      "INFO:tensorflow:examples/sec: 2.84591\n",
      "I1118 02:46:58.089060 4435920320 tpu_estimator.py:2160] examples/sec: 2.84591\n",
      "INFO:tensorflow:global_step/sec: 0.0881052\n",
      "I1118 02:47:09.438904 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881052\n",
      "INFO:tensorflow:examples/sec: 2.81937\n",
      "I1118 02:47:09.439122 4435920320 tpu_estimator.py:2160] examples/sec: 2.81937\n",
      "INFO:tensorflow:global_step/sec: 0.087494\n",
      "I1118 02:47:20.868282 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087494\n",
      "INFO:tensorflow:examples/sec: 2.79981\n",
      "I1118 02:47:20.868503 4435920320 tpu_estimator.py:2160] examples/sec: 2.79981\n",
      "INFO:tensorflow:global_step/sec: 0.0890132\n",
      "I1118 02:47:32.102556 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890132\n",
      "INFO:tensorflow:examples/sec: 2.84842\n",
      "I1118 02:47:32.102792 4435920320 tpu_estimator.py:2160] examples/sec: 2.84842\n",
      "INFO:tensorflow:global_step/sec: 0.0890606\n",
      "I1118 02:47:43.330843 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890606\n",
      "INFO:tensorflow:examples/sec: 2.84994\n",
      "I1118 02:47:43.331069 4435920320 tpu_estimator.py:2160] examples/sec: 2.84994\n",
      "INFO:tensorflow:global_step/sec: 0.0893073\n",
      "I1118 02:47:54.528146 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893073\n",
      "INFO:tensorflow:examples/sec: 2.85783\n",
      "I1118 02:47:54.528366 4435920320 tpu_estimator.py:2160] examples/sec: 2.85783\n",
      "INFO:tensorflow:global_step/sec: 0.0888027\n",
      "I1118 02:48:05.789066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888027\n",
      "INFO:tensorflow:examples/sec: 2.84169\n",
      "I1118 02:48:05.789286 4435920320 tpu_estimator.py:2160] examples/sec: 2.84169\n",
      "INFO:tensorflow:global_step/sec: 0.089078\n",
      "I1118 02:48:17.015194 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089078\n",
      "INFO:tensorflow:examples/sec: 2.8505\n",
      "I1118 02:48:17.015421 4435920320 tpu_estimator.py:2160] examples/sec: 2.8505\n",
      "INFO:tensorflow:global_step/sec: 0.0892692\n",
      "I1118 02:48:28.217264 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892692\n",
      "INFO:tensorflow:examples/sec: 2.85661\n",
      "I1118 02:48:28.217477 4435920320 tpu_estimator.py:2160] examples/sec: 2.85661\n",
      "INFO:tensorflow:global_step/sec: 0.0888197\n",
      "I1118 02:48:39.476022 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888197\n",
      "INFO:tensorflow:examples/sec: 2.84223\n",
      "I1118 02:48:39.476250 4435920320 tpu_estimator.py:2160] examples/sec: 2.84223\n",
      "INFO:tensorflow:global_step/sec: 0.0888263\n",
      "I1118 02:48:50.733936 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888263\n",
      "INFO:tensorflow:examples/sec: 2.84244\n",
      "I1118 02:48:50.734286 4435920320 tpu_estimator.py:2160] examples/sec: 2.84244\n",
      "INFO:tensorflow:global_step/sec: 0.0879586\n",
      "I1118 02:49:02.102955 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879586\n",
      "INFO:tensorflow:examples/sec: 2.81467\n",
      "I1118 02:49:02.103174 4435920320 tpu_estimator.py:2160] examples/sec: 2.81467\n",
      "INFO:tensorflow:global_step/sec: 0.0883268\n",
      "I1118 02:49:13.424523 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883268\n",
      "INFO:tensorflow:examples/sec: 2.82646\n",
      "I1118 02:49:13.424737 4435920320 tpu_estimator.py:2160] examples/sec: 2.82646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888532\n",
      "I1118 02:49:24.679064 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888532\n",
      "INFO:tensorflow:examples/sec: 2.8433\n",
      "I1118 02:49:24.679326 4435920320 tpu_estimator.py:2160] examples/sec: 2.8433\n",
      "INFO:tensorflow:global_step/sec: 0.0895587\n",
      "I1118 02:49:35.844901 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895587\n",
      "INFO:tensorflow:examples/sec: 2.86588\n",
      "I1118 02:49:35.845125 4435920320 tpu_estimator.py:2160] examples/sec: 2.86588\n",
      "INFO:tensorflow:global_step/sec: 0.0892693\n",
      "I1118 02:49:47.046959 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892693\n",
      "INFO:tensorflow:examples/sec: 2.85662\n",
      "I1118 02:49:47.047173 4435920320 tpu_estimator.py:2160] examples/sec: 2.85662\n",
      "INFO:tensorflow:global_step/sec: 0.0892156\n",
      "I1118 02:49:58.255760 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892156\n",
      "INFO:tensorflow:examples/sec: 2.8549\n",
      "I1118 02:49:58.255980 4435920320 tpu_estimator.py:2160] examples/sec: 2.8549\n",
      "INFO:tensorflow:global_step/sec: 0.0888865\n",
      "I1118 02:50:09.506062 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888865\n",
      "INFO:tensorflow:examples/sec: 2.84437\n",
      "I1118 02:50:09.506283 4435920320 tpu_estimator.py:2160] examples/sec: 2.84437\n",
      "INFO:tensorflow:global_step/sec: 0.088696\n",
      "I1118 02:50:20.780524 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088696\n",
      "INFO:tensorflow:examples/sec: 2.83827\n",
      "I1118 02:50:20.780748 4435920320 tpu_estimator.py:2160] examples/sec: 2.83827\n",
      "INFO:tensorflow:global_step/sec: 0.0889539\n",
      "I1118 02:50:32.022325 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889539\n",
      "INFO:tensorflow:examples/sec: 2.84652\n",
      "I1118 02:50:32.022543 4435920320 tpu_estimator.py:2160] examples/sec: 2.84652\n",
      "INFO:tensorflow:global_step/sec: 0.0886031\n",
      "I1118 02:50:43.308608 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886031\n",
      "INFO:tensorflow:examples/sec: 2.8353\n",
      "I1118 02:50:43.308835 4435920320 tpu_estimator.py:2160] examples/sec: 2.8353\n",
      "INFO:tensorflow:global_step/sec: 0.0892601\n",
      "I1118 02:50:54.511826 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892601\n",
      "INFO:tensorflow:examples/sec: 2.85632\n",
      "I1118 02:50:54.512047 4435920320 tpu_estimator.py:2160] examples/sec: 2.85632\n",
      "INFO:tensorflow:global_step/sec: 0.0882886\n",
      "I1118 02:51:05.838294 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882886\n",
      "INFO:tensorflow:examples/sec: 2.82524\n",
      "I1118 02:51:05.838520 4435920320 tpu_estimator.py:2160] examples/sec: 2.82524\n",
      "INFO:tensorflow:global_step/sec: 0.0884854\n",
      "I1118 02:51:17.139592 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884854\n",
      "INFO:tensorflow:examples/sec: 2.83153\n",
      "I1118 02:51:17.139810 4435920320 tpu_estimator.py:2160] examples/sec: 2.83153\n",
      "INFO:tensorflow:global_step/sec: 0.0900285\n",
      "I1118 02:51:28.247192 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900285\n",
      "INFO:tensorflow:examples/sec: 2.88091\n",
      "I1118 02:51:28.247411 4435920320 tpu_estimator.py:2160] examples/sec: 2.88091\n",
      "INFO:tensorflow:global_step/sec: 0.0893954\n",
      "I1118 02:51:39.433452 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893954\n",
      "INFO:tensorflow:examples/sec: 2.86065\n",
      "I1118 02:51:39.433669 4435920320 tpu_estimator.py:2160] examples/sec: 2.86065\n",
      "INFO:tensorflow:global_step/sec: 0.0893328\n",
      "I1118 02:51:50.627558 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893328\n",
      "INFO:tensorflow:examples/sec: 2.85865\n",
      "I1118 02:51:50.627794 4435920320 tpu_estimator.py:2160] examples/sec: 2.85865\n",
      "INFO:tensorflow:global_step/sec: 0.0890127\n",
      "I1118 02:52:01.861903 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890127\n",
      "INFO:tensorflow:examples/sec: 2.84841\n",
      "I1118 02:52:01.862125 4435920320 tpu_estimator.py:2160] examples/sec: 2.84841\n",
      "INFO:tensorflow:global_step/sec: 0.0892032\n",
      "I1118 02:52:13.072260 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892032\n",
      "INFO:tensorflow:examples/sec: 2.8545\n",
      "I1118 02:52:13.072485 4435920320 tpu_estimator.py:2160] examples/sec: 2.8545\n",
      "INFO:tensorflow:global_step/sec: 0.0888473\n",
      "I1118 02:52:24.327546 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888473\n",
      "INFO:tensorflow:examples/sec: 2.84311\n",
      "I1118 02:52:24.327781 4435920320 tpu_estimator.py:2160] examples/sec: 2.84311\n",
      "INFO:tensorflow:global_step/sec: 0.0894251\n",
      "I1118 02:52:35.510085 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894251\n",
      "INFO:tensorflow:examples/sec: 2.8616\n",
      "I1118 02:52:35.510339 4435920320 tpu_estimator.py:2160] examples/sec: 2.8616\n",
      "INFO:tensorflow:global_step/sec: 0.0884443\n",
      "I1118 02:52:46.816632 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884443\n",
      "INFO:tensorflow:examples/sec: 2.83022\n",
      "I1118 02:52:46.816854 4435920320 tpu_estimator.py:2160] examples/sec: 2.83022\n",
      "INFO:tensorflow:global_step/sec: 0.0887336\n",
      "I1118 02:52:58.086313 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887336\n",
      "INFO:tensorflow:examples/sec: 2.83947\n",
      "I1118 02:52:58.086533 4435920320 tpu_estimator.py:2160] examples/sec: 2.83947\n",
      "INFO:tensorflow:global_step/sec: 0.0891126\n",
      "I1118 02:53:09.308080 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891126\n",
      "INFO:tensorflow:examples/sec: 2.8516\n",
      "I1118 02:53:09.308305 4435920320 tpu_estimator.py:2160] examples/sec: 2.8516\n",
      "INFO:tensorflow:global_step/sec: 0.0896352\n",
      "I1118 02:53:20.464406 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896352\n",
      "INFO:tensorflow:examples/sec: 2.86833\n",
      "I1118 02:53:20.464623 4435920320 tpu_estimator.py:2160] examples/sec: 2.86833\n",
      "INFO:tensorflow:global_step/sec: 0.089281\n",
      "I1118 02:53:31.665006 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089281\n",
      "INFO:tensorflow:examples/sec: 2.85699\n",
      "I1118 02:53:31.665229 4435920320 tpu_estimator.py:2160] examples/sec: 2.85699\n",
      "INFO:tensorflow:global_step/sec: 0.0893713\n",
      "I1118 02:53:42.854269 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893713\n",
      "INFO:tensorflow:examples/sec: 2.85988\n",
      "I1118 02:53:42.854486 4435920320 tpu_estimator.py:2160] examples/sec: 2.85988\n",
      "INFO:tensorflow:global_step/sec: 0.0893678\n",
      "I1118 02:53:54.043985 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893678\n",
      "INFO:tensorflow:examples/sec: 2.85977\n",
      "I1118 02:53:54.044204 4435920320 tpu_estimator.py:2160] examples/sec: 2.85977\n",
      "INFO:tensorflow:global_step/sec: 0.0888055\n",
      "I1118 02:54:05.304584 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888055\n",
      "INFO:tensorflow:examples/sec: 2.84178\n",
      "I1118 02:54:05.304806 4435920320 tpu_estimator.py:2160] examples/sec: 2.84178\n",
      "INFO:tensorflow:global_step/sec: 0.0890943\n",
      "I1118 02:54:16.528619 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890943\n",
      "INFO:tensorflow:examples/sec: 2.85102\n",
      "I1118 02:54:16.528848 4435920320 tpu_estimator.py:2160] examples/sec: 2.85102\n",
      "INFO:tensorflow:global_step/sec: 0.0890036\n",
      "I1118 02:54:27.764127 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890036\n",
      "INFO:tensorflow:examples/sec: 2.84811\n",
      "I1118 02:54:27.764340 4435920320 tpu_estimator.py:2160] examples/sec: 2.84811\n",
      "INFO:tensorflow:global_step/sec: 0.0888989\n",
      "I1118 02:54:39.012859 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888989\n",
      "INFO:tensorflow:examples/sec: 2.84476\n",
      "I1118 02:54:39.013105 4435920320 tpu_estimator.py:2160] examples/sec: 2.84476\n",
      "INFO:tensorflow:global_step/sec: 0.0889357\n",
      "I1118 02:54:50.256928 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889357\n",
      "INFO:tensorflow:examples/sec: 2.84594\n",
      "I1118 02:54:50.257150 4435920320 tpu_estimator.py:2160] examples/sec: 2.84594\n",
      "INFO:tensorflow:global_step/sec: 0.0889285\n",
      "I1118 02:55:01.501929 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889285\n",
      "INFO:tensorflow:examples/sec: 2.84571\n",
      "I1118 02:55:01.502308 4435920320 tpu_estimator.py:2160] examples/sec: 2.84571\n",
      "INFO:tensorflow:global_step/sec: 0.0888271\n",
      "I1118 02:55:12.759752 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888271\n",
      "INFO:tensorflow:examples/sec: 2.84247\n",
      "I1118 02:55:12.759990 4435920320 tpu_estimator.py:2160] examples/sec: 2.84247\n",
      "INFO:tensorflow:global_step/sec: 0.0893541\n",
      "I1118 02:55:23.951179 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893541\n",
      "INFO:tensorflow:examples/sec: 2.85933\n",
      "I1118 02:55:23.951400 4435920320 tpu_estimator.py:2160] examples/sec: 2.85933\n",
      "INFO:tensorflow:global_step/sec: 0.0895399\n",
      "I1118 02:55:35.119390 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895399\n",
      "INFO:tensorflow:examples/sec: 2.86528\n",
      "I1118 02:55:35.119626 4435920320 tpu_estimator.py:2160] examples/sec: 2.86528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0894161\n",
      "I1118 02:55:46.303058 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894161\n",
      "INFO:tensorflow:examples/sec: 2.86132\n",
      "I1118 02:55:46.303294 4435920320 tpu_estimator.py:2160] examples/sec: 2.86132\n",
      "INFO:tensorflow:global_step/sec: 0.089089\n",
      "I1118 02:55:57.527782 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089089\n",
      "INFO:tensorflow:examples/sec: 2.85085\n",
      "I1118 02:55:57.528000 4435920320 tpu_estimator.py:2160] examples/sec: 2.85085\n",
      "INFO:tensorflow:global_step/sec: 0.0880433\n",
      "I1118 02:56:08.885838 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880433\n",
      "INFO:tensorflow:examples/sec: 2.81738\n",
      "I1118 02:56:08.886059 4435920320 tpu_estimator.py:2160] examples/sec: 2.81738\n",
      "INFO:tensorflow:global_step/sec: 0.0886476\n",
      "I1118 02:56:20.166471 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886476\n",
      "INFO:tensorflow:examples/sec: 2.83672\n",
      "I1118 02:56:20.166687 4435920320 tpu_estimator.py:2160] examples/sec: 2.83672\n",
      "INFO:tensorflow:global_step/sec: 0.0893155\n",
      "I1118 02:56:31.362735 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893155\n",
      "INFO:tensorflow:examples/sec: 2.8581\n",
      "I1118 02:56:31.362950 4435920320 tpu_estimator.py:2160] examples/sec: 2.8581\n",
      "INFO:tensorflow:global_step/sec: 0.0887774\n",
      "I1118 02:56:42.626850 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887774\n",
      "INFO:tensorflow:examples/sec: 2.84088\n",
      "I1118 02:56:42.627067 4435920320 tpu_estimator.py:2160] examples/sec: 2.84088\n",
      "INFO:tensorflow:global_step/sec: 0.0892025\n",
      "I1118 02:56:53.837304 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892025\n",
      "INFO:tensorflow:examples/sec: 2.85448\n",
      "I1118 02:56:53.837529 4435920320 tpu_estimator.py:2160] examples/sec: 2.85448\n",
      "INFO:tensorflow:global_step/sec: 0.0893519\n",
      "I1118 02:57:05.028997 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893519\n",
      "INFO:tensorflow:examples/sec: 2.85926\n",
      "I1118 02:57:05.029219 4435920320 tpu_estimator.py:2160] examples/sec: 2.85926\n",
      "INFO:tensorflow:global_step/sec: 0.0894917\n",
      "I1118 02:57:16.203215 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894917\n",
      "INFO:tensorflow:examples/sec: 2.86374\n",
      "I1118 02:57:16.203434 4435920320 tpu_estimator.py:2160] examples/sec: 2.86374\n",
      "INFO:tensorflow:global_step/sec: 0.0894311\n",
      "I1118 02:57:27.385018 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894311\n",
      "INFO:tensorflow:examples/sec: 2.86179\n",
      "I1118 02:57:27.385233 4435920320 tpu_estimator.py:2160] examples/sec: 2.86179\n",
      "INFO:tensorflow:global_step/sec: 0.0890221\n",
      "I1118 02:57:38.618185 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890221\n",
      "INFO:tensorflow:examples/sec: 2.84871\n",
      "I1118 02:57:38.618402 4435920320 tpu_estimator.py:2160] examples/sec: 2.84871\n",
      "INFO:tensorflow:global_step/sec: 0.0889023\n",
      "I1118 02:57:49.866503 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889023\n",
      "INFO:tensorflow:examples/sec: 2.84487\n",
      "I1118 02:57:49.866733 4435920320 tpu_estimator.py:2160] examples/sec: 2.84487\n",
      "INFO:tensorflow:global_step/sec: 0.0894616\n",
      "I1118 02:58:01.044461 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894616\n",
      "INFO:tensorflow:examples/sec: 2.86277\n",
      "I1118 02:58:01.044671 4435920320 tpu_estimator.py:2160] examples/sec: 2.86277\n",
      "INFO:tensorflow:global_step/sec: 0.088804\n",
      "I1118 02:58:12.305247 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088804\n",
      "INFO:tensorflow:examples/sec: 2.84173\n",
      "I1118 02:58:12.305464 4435920320 tpu_estimator.py:2160] examples/sec: 2.84173\n",
      "INFO:tensorflow:global_step/sec: 0.08841\n",
      "I1118 02:58:23.616138 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08841\n",
      "INFO:tensorflow:examples/sec: 2.82912\n",
      "I1118 02:58:23.616357 4435920320 tpu_estimator.py:2160] examples/sec: 2.82912\n",
      "INFO:tensorflow:global_step/sec: 0.0893263\n",
      "I1118 02:58:34.811097 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893263\n",
      "INFO:tensorflow:examples/sec: 2.85844\n",
      "I1118 02:58:34.811327 4435920320 tpu_estimator.py:2160] examples/sec: 2.85844\n",
      "INFO:tensorflow:global_step/sec: 0.0889511\n",
      "I1118 02:58:46.053302 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889511\n",
      "INFO:tensorflow:examples/sec: 2.84643\n",
      "I1118 02:58:46.053555 4435920320 tpu_estimator.py:2160] examples/sec: 2.84643\n",
      "INFO:tensorflow:global_step/sec: 0.0890434\n",
      "I1118 02:58:57.283696 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890434\n",
      "INFO:tensorflow:examples/sec: 2.84939\n",
      "I1118 02:58:57.283911 4435920320 tpu_estimator.py:2160] examples/sec: 2.84939\n",
      "INFO:tensorflow:global_step/sec: 0.0895376\n",
      "I1118 02:59:08.452199 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895376\n",
      "INFO:tensorflow:examples/sec: 2.8652\n",
      "I1118 02:59:08.452435 4435920320 tpu_estimator.py:2160] examples/sec: 2.8652\n",
      "INFO:tensorflow:global_step/sec: 0.0885928\n",
      "I1118 02:59:19.739798 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885928\n",
      "INFO:tensorflow:examples/sec: 2.83497\n",
      "I1118 02:59:19.740015 4435920320 tpu_estimator.py:2160] examples/sec: 2.83497\n",
      "INFO:tensorflow:global_step/sec: 0.0897982\n",
      "I1118 02:59:30.875862 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897982\n",
      "INFO:tensorflow:examples/sec: 2.87354\n",
      "I1118 02:59:30.876148 4435920320 tpu_estimator.py:2160] examples/sec: 2.87354\n",
      "INFO:tensorflow:global_step/sec: 0.089766\n",
      "I1118 02:59:42.015935 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089766\n",
      "INFO:tensorflow:examples/sec: 2.87251\n",
      "I1118 02:59:42.016149 4435920320 tpu_estimator.py:2160] examples/sec: 2.87251\n",
      "INFO:tensorflow:global_step/sec: 0.0894195\n",
      "I1118 02:59:53.199182 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894195\n",
      "INFO:tensorflow:examples/sec: 2.86143\n",
      "I1118 02:59:53.199406 4435920320 tpu_estimator.py:2160] examples/sec: 2.86143\n",
      "INFO:tensorflow:global_step/sec: 0.0894619\n",
      "I1118 03:00:04.377125 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894619\n",
      "INFO:tensorflow:examples/sec: 2.86278\n",
      "I1118 03:00:04.377358 4435920320 tpu_estimator.py:2160] examples/sec: 2.86278\n",
      "INFO:tensorflow:global_step/sec: 0.0888273\n",
      "I1118 03:00:15.634935 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888273\n",
      "INFO:tensorflow:examples/sec: 2.84247\n",
      "I1118 03:00:15.635172 4435920320 tpu_estimator.py:2160] examples/sec: 2.84247\n",
      "INFO:tensorflow:global_step/sec: 0.0892556\n",
      "I1118 03:00:26.838708 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892556\n",
      "INFO:tensorflow:examples/sec: 2.85618\n",
      "I1118 03:00:26.838928 4435920320 tpu_estimator.py:2160] examples/sec: 2.85618\n",
      "INFO:tensorflow:global_step/sec: 0.0894404\n",
      "I1118 03:00:38.019354 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894404\n",
      "INFO:tensorflow:examples/sec: 2.86209\n",
      "I1118 03:00:38.019600 4435920320 tpu_estimator.py:2160] examples/sec: 2.86209\n",
      "INFO:tensorflow:global_step/sec: 0.0896929\n",
      "I1118 03:00:49.168509 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896929\n",
      "INFO:tensorflow:examples/sec: 2.87017\n",
      "I1118 03:00:49.168741 4435920320 tpu_estimator.py:2160] examples/sec: 2.87017\n",
      "INFO:tensorflow:global_step/sec: 0.0889717\n",
      "I1118 03:01:00.408040 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889717\n",
      "INFO:tensorflow:examples/sec: 2.84709\n",
      "I1118 03:01:00.408271 4435920320 tpu_estimator.py:2160] examples/sec: 2.84709\n",
      "INFO:tensorflow:global_step/sec: 0.0887585\n",
      "I1118 03:01:11.674571 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887585\n",
      "INFO:tensorflow:examples/sec: 2.84027\n",
      "I1118 03:01:11.674803 4435920320 tpu_estimator.py:2160] examples/sec: 2.84027\n",
      "INFO:tensorflow:global_step/sec: 0.0891597\n",
      "I1118 03:01:22.890401 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891597\n",
      "INFO:tensorflow:examples/sec: 2.85311\n",
      "I1118 03:01:22.890631 4435920320 tpu_estimator.py:2160] examples/sec: 2.85311\n",
      "INFO:tensorflow:global_step/sec: 0.0885552\n",
      "I1118 03:01:34.182780 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885552\n",
      "INFO:tensorflow:examples/sec: 2.83377\n",
      "I1118 03:01:34.183006 4435920320 tpu_estimator.py:2160] examples/sec: 2.83377\n",
      "INFO:tensorflow:global_step/sec: 0.0892751\n",
      "I1118 03:01:45.384122 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892751\n",
      "INFO:tensorflow:examples/sec: 2.8568\n",
      "I1118 03:01:45.384490 4435920320 tpu_estimator.py:2160] examples/sec: 2.8568\n",
      "INFO:tensorflow:global_step/sec: 0.0892557\n",
      "I1118 03:01:56.587882 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892557\n",
      "INFO:tensorflow:examples/sec: 2.85618\n",
      "I1118 03:01:56.588125 4435920320 tpu_estimator.py:2160] examples/sec: 2.85618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888998\n",
      "I1118 03:02:07.836517 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888998\n",
      "INFO:tensorflow:examples/sec: 2.84479\n",
      "I1118 03:02:07.836753 4435920320 tpu_estimator.py:2160] examples/sec: 2.84479\n",
      "INFO:tensorflow:global_step/sec: 0.0888697\n",
      "I1118 03:02:19.088933 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888697\n",
      "INFO:tensorflow:examples/sec: 2.84383\n",
      "I1118 03:02:19.089173 4435920320 tpu_estimator.py:2160] examples/sec: 2.84383\n",
      "INFO:tensorflow:global_step/sec: 0.0892435\n",
      "I1118 03:02:30.294247 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892435\n",
      "INFO:tensorflow:examples/sec: 2.85579\n",
      "I1118 03:02:30.294487 4435920320 tpu_estimator.py:2160] examples/sec: 2.85579\n",
      "INFO:tensorflow:global_step/sec: 0.0896335\n",
      "I1118 03:02:41.450783 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896335\n",
      "INFO:tensorflow:examples/sec: 2.86827\n",
      "I1118 03:02:41.450989 4435920320 tpu_estimator.py:2160] examples/sec: 2.86827\n",
      "INFO:tensorflow:global_step/sec: 0.0895731\n",
      "I1118 03:02:52.614861 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895731\n",
      "INFO:tensorflow:examples/sec: 2.86634\n",
      "I1118 03:02:52.615097 4435920320 tpu_estimator.py:2160] examples/sec: 2.86634\n",
      "INFO:tensorflow:global_step/sec: 0.0886361\n",
      "I1118 03:03:03.896940 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886361\n",
      "INFO:tensorflow:examples/sec: 2.83636\n",
      "I1118 03:03:03.897175 4435920320 tpu_estimator.py:2160] examples/sec: 2.83636\n",
      "INFO:tensorflow:global_step/sec: 0.0891052\n",
      "I1118 03:03:15.119647 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891052\n",
      "INFO:tensorflow:examples/sec: 2.85137\n",
      "I1118 03:03:15.119866 4435920320 tpu_estimator.py:2160] examples/sec: 2.85137\n",
      "INFO:tensorflow:global_step/sec: 0.0892804\n",
      "I1118 03:03:26.320294 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892804\n",
      "INFO:tensorflow:examples/sec: 2.85697\n",
      "I1118 03:03:26.320513 4435920320 tpu_estimator.py:2160] examples/sec: 2.85697\n",
      "INFO:tensorflow:global_step/sec: 0.0898146\n",
      "I1118 03:03:37.454365 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898146\n",
      "INFO:tensorflow:examples/sec: 2.87407\n",
      "I1118 03:03:37.454605 4435920320 tpu_estimator.py:2160] examples/sec: 2.87407\n",
      "INFO:tensorflow:global_step/sec: 0.0892757\n",
      "I1118 03:03:48.655619 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892757\n",
      "INFO:tensorflow:examples/sec: 2.85682\n",
      "I1118 03:03:48.655855 4435920320 tpu_estimator.py:2160] examples/sec: 2.85682\n",
      "INFO:tensorflow:global_step/sec: 0.0887995\n",
      "I1118 03:03:59.916938 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887995\n",
      "INFO:tensorflow:examples/sec: 2.84158\n",
      "I1118 03:03:59.917157 4435920320 tpu_estimator.py:2160] examples/sec: 2.84158\n",
      "INFO:tensorflow:global_step/sec: 0.0891829\n",
      "I1118 03:04:11.129850 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891829\n",
      "INFO:tensorflow:examples/sec: 2.85385\n",
      "I1118 03:04:11.130127 4435920320 tpu_estimator.py:2160] examples/sec: 2.85385\n",
      "INFO:tensorflow:global_step/sec: 0.0891163\n",
      "I1118 03:04:22.351150 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891163\n",
      "INFO:tensorflow:examples/sec: 2.85172\n",
      "I1118 03:04:22.351372 4435920320 tpu_estimator.py:2160] examples/sec: 2.85172\n",
      "INFO:tensorflow:global_step/sec: 0.0891177\n",
      "I1118 03:04:33.572251 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891177\n",
      "INFO:tensorflow:examples/sec: 2.85177\n",
      "I1118 03:04:33.572478 4435920320 tpu_estimator.py:2160] examples/sec: 2.85177\n",
      "INFO:tensorflow:global_step/sec: 0.0888642\n",
      "I1118 03:04:44.825377 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888642\n",
      "INFO:tensorflow:examples/sec: 2.84365\n",
      "I1118 03:04:44.825598 4435920320 tpu_estimator.py:2160] examples/sec: 2.84365\n",
      "INFO:tensorflow:global_step/sec: 0.0889376\n",
      "I1118 03:04:56.069219 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889376\n",
      "INFO:tensorflow:examples/sec: 2.846\n",
      "I1118 03:04:56.069434 4435920320 tpu_estimator.py:2160] examples/sec: 2.846\n",
      "INFO:tensorflow:global_step/sec: 0.088954\n",
      "I1118 03:05:07.310987 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088954\n",
      "INFO:tensorflow:examples/sec: 2.84653\n",
      "I1118 03:05:07.311205 4435920320 tpu_estimator.py:2160] examples/sec: 2.84653\n",
      "INFO:tensorflow:global_step/sec: 0.0891418\n",
      "I1118 03:05:18.529082 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891418\n",
      "INFO:tensorflow:examples/sec: 2.85254\n",
      "I1118 03:05:18.529313 4435920320 tpu_estimator.py:2160] examples/sec: 2.85254\n",
      "INFO:tensorflow:global_step/sec: 0.0894673\n",
      "I1118 03:05:29.706347 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894673\n",
      "INFO:tensorflow:examples/sec: 2.86295\n",
      "I1118 03:05:29.706585 4435920320 tpu_estimator.py:2160] examples/sec: 2.86295\n",
      "INFO:tensorflow:global_step/sec: 0.0894368\n",
      "I1118 03:05:40.887410 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894368\n",
      "INFO:tensorflow:examples/sec: 2.86198\n",
      "I1118 03:05:40.887632 4435920320 tpu_estimator.py:2160] examples/sec: 2.86198\n",
      "INFO:tensorflow:global_step/sec: 0.0887187\n",
      "I1118 03:05:52.158993 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887187\n",
      "INFO:tensorflow:examples/sec: 2.839\n",
      "I1118 03:05:52.159209 4435920320 tpu_estimator.py:2160] examples/sec: 2.839\n",
      "INFO:tensorflow:global_step/sec: 0.0893885\n",
      "I1118 03:06:03.346117 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893885\n",
      "INFO:tensorflow:examples/sec: 2.86043\n",
      "I1118 03:06:03.346347 4435920320 tpu_estimator.py:2160] examples/sec: 2.86043\n",
      "INFO:tensorflow:global_step/sec: 0.0895769\n",
      "I1118 03:06:14.509702 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895769\n",
      "INFO:tensorflow:examples/sec: 2.86646\n",
      "I1118 03:06:14.509922 4435920320 tpu_estimator.py:2160] examples/sec: 2.86646\n",
      "INFO:tensorflow:global_step/sec: 0.0890752\n",
      "I1118 03:06:25.736207 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890752\n",
      "INFO:tensorflow:examples/sec: 2.85041\n",
      "I1118 03:06:25.736435 4435920320 tpu_estimator.py:2160] examples/sec: 2.85041\n",
      "INFO:tensorflow:global_step/sec: 0.0892165\n",
      "I1118 03:06:36.944881 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892165\n",
      "INFO:tensorflow:examples/sec: 2.85493\n",
      "I1118 03:06:36.945125 4435920320 tpu_estimator.py:2160] examples/sec: 2.85493\n",
      "INFO:tensorflow:global_step/sec: 0.0897306\n",
      "I1118 03:06:48.089343 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897306\n",
      "INFO:tensorflow:examples/sec: 2.87138\n",
      "I1118 03:06:48.089565 4435920320 tpu_estimator.py:2160] examples/sec: 2.87138\n",
      "INFO:tensorflow:global_step/sec: 0.0893078\n",
      "I1118 03:06:59.286571 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893078\n",
      "INFO:tensorflow:examples/sec: 2.85785\n",
      "I1118 03:06:59.286799 4435920320 tpu_estimator.py:2160] examples/sec: 2.85785\n",
      "INFO:tensorflow:global_step/sec: 0.0884747\n",
      "I1118 03:07:10.589236 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884747\n",
      "INFO:tensorflow:examples/sec: 2.83119\n",
      "I1118 03:07:10.589456 4435920320 tpu_estimator.py:2160] examples/sec: 2.83119\n",
      "INFO:tensorflow:global_step/sec: 0.0884673\n",
      "I1118 03:07:21.892847 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884673\n",
      "INFO:tensorflow:examples/sec: 2.83095\n",
      "I1118 03:07:21.893067 4435920320 tpu_estimator.py:2160] examples/sec: 2.83095\n",
      "INFO:tensorflow:global_step/sec: 0.0888783\n",
      "I1118 03:07:33.144191 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888783\n",
      "INFO:tensorflow:examples/sec: 2.84411\n",
      "I1118 03:07:33.144420 4435920320 tpu_estimator.py:2160] examples/sec: 2.84411\n",
      "INFO:tensorflow:global_step/sec: 0.0893505\n",
      "I1118 03:07:44.336086 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893505\n",
      "INFO:tensorflow:examples/sec: 2.85921\n",
      "I1118 03:07:44.336304 4435920320 tpu_estimator.py:2160] examples/sec: 2.85921\n",
      "INFO:tensorflow:global_step/sec: 0.0891723\n",
      "I1118 03:07:55.550314 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891723\n",
      "INFO:tensorflow:examples/sec: 2.85351\n",
      "I1118 03:07:55.550536 4435920320 tpu_estimator.py:2160] examples/sec: 2.85351\n",
      "INFO:tensorflow:global_step/sec: 0.0882565\n",
      "I1118 03:08:06.880929 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882565\n",
      "INFO:tensorflow:examples/sec: 2.82421\n",
      "I1118 03:08:06.881153 4435920320 tpu_estimator.py:2160] examples/sec: 2.82421\n",
      "INFO:tensorflow:global_step/sec: 0.0887197\n",
      "I1118 03:08:18.152383 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887197\n",
      "INFO:tensorflow:examples/sec: 2.83903\n",
      "I1118 03:08:18.152606 4435920320 tpu_estimator.py:2160] examples/sec: 2.83903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0889051\n",
      "I1118 03:08:29.400334 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889051\n",
      "INFO:tensorflow:examples/sec: 2.84496\n",
      "I1118 03:08:29.400568 4435920320 tpu_estimator.py:2160] examples/sec: 2.84496\n",
      "INFO:tensorflow:global_step/sec: 0.088449\n",
      "I1118 03:08:40.706278 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088449\n",
      "INFO:tensorflow:examples/sec: 2.83037\n",
      "I1118 03:08:40.706505 4435920320 tpu_estimator.py:2160] examples/sec: 2.83037\n",
      "INFO:tensorflow:global_step/sec: 0.0887698\n",
      "I1118 03:08:51.971373 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887698\n",
      "INFO:tensorflow:examples/sec: 2.84063\n",
      "I1118 03:08:51.971605 4435920320 tpu_estimator.py:2160] examples/sec: 2.84063\n",
      "INFO:tensorflow:global_step/sec: 0.0891047\n",
      "I1118 03:09:03.194123 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891047\n",
      "INFO:tensorflow:examples/sec: 2.85135\n",
      "I1118 03:09:03.194334 4435920320 tpu_estimator.py:2160] examples/sec: 2.85135\n",
      "INFO:tensorflow:global_step/sec: 0.089381\n",
      "I1118 03:09:14.382158 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089381\n",
      "INFO:tensorflow:examples/sec: 2.86019\n",
      "I1118 03:09:14.382380 4435920320 tpu_estimator.py:2160] examples/sec: 2.86019\n",
      "INFO:tensorflow:global_step/sec: 0.0893784\n",
      "I1118 03:09:25.570548 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893784\n",
      "INFO:tensorflow:examples/sec: 2.86011\n",
      "I1118 03:09:25.570777 4435920320 tpu_estimator.py:2160] examples/sec: 2.86011\n",
      "INFO:tensorflow:global_step/sec: 0.089186\n",
      "I1118 03:09:36.783075 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089186\n",
      "INFO:tensorflow:examples/sec: 2.85395\n",
      "I1118 03:09:36.783300 4435920320 tpu_estimator.py:2160] examples/sec: 2.85395\n",
      "INFO:tensorflow:global_step/sec: 0.0894202\n",
      "I1118 03:09:47.966248 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894202\n",
      "INFO:tensorflow:examples/sec: 2.86145\n",
      "I1118 03:09:47.966468 4435920320 tpu_estimator.py:2160] examples/sec: 2.86145\n",
      "INFO:tensorflow:global_step/sec: 0.0894338\n",
      "I1118 03:09:59.147685 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894338\n",
      "INFO:tensorflow:examples/sec: 2.86188\n",
      "I1118 03:09:59.147907 4435920320 tpu_estimator.py:2160] examples/sec: 2.86188\n",
      "INFO:tensorflow:global_step/sec: 0.089503\n",
      "I1118 03:10:10.320516 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089503\n",
      "INFO:tensorflow:examples/sec: 2.8641\n",
      "I1118 03:10:10.320816 4435920320 tpu_estimator.py:2160] examples/sec: 2.8641\n",
      "INFO:tensorflow:global_step/sec: 0.0896319\n",
      "I1118 03:10:21.477246 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896319\n",
      "INFO:tensorflow:examples/sec: 2.86822\n",
      "I1118 03:10:21.477468 4435920320 tpu_estimator.py:2160] examples/sec: 2.86822\n",
      "INFO:tensorflow:global_step/sec: 0.0893279\n",
      "I1118 03:10:32.671954 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893279\n",
      "INFO:tensorflow:examples/sec: 2.85849\n",
      "I1118 03:10:32.672194 4435920320 tpu_estimator.py:2160] examples/sec: 2.85849\n",
      "INFO:tensorflow:global_step/sec: 0.0887081\n",
      "I1118 03:10:43.944868 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887081\n",
      "INFO:tensorflow:examples/sec: 2.83866\n",
      "I1118 03:10:43.945082 4435920320 tpu_estimator.py:2160] examples/sec: 2.83866\n",
      "INFO:tensorflow:global_step/sec: 0.0888544\n",
      "I1118 03:10:55.199248 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888544\n",
      "INFO:tensorflow:examples/sec: 2.84334\n",
      "I1118 03:10:55.199477 4435920320 tpu_estimator.py:2160] examples/sec: 2.84334\n",
      "INFO:tensorflow:global_step/sec: 0.0886547\n",
      "I1118 03:11:06.478965 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886547\n",
      "INFO:tensorflow:examples/sec: 2.83695\n",
      "I1118 03:11:06.479192 4435920320 tpu_estimator.py:2160] examples/sec: 2.83695\n",
      "INFO:tensorflow:global_step/sec: 0.0893706\n",
      "I1118 03:11:17.668323 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893706\n",
      "INFO:tensorflow:examples/sec: 2.85986\n",
      "I1118 03:11:17.668538 4435920320 tpu_estimator.py:2160] examples/sec: 2.85986\n",
      "INFO:tensorflow:global_step/sec: 0.0889645\n",
      "I1118 03:11:28.908745 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889645\n",
      "INFO:tensorflow:examples/sec: 2.84686\n",
      "I1118 03:11:28.908964 4435920320 tpu_estimator.py:2160] examples/sec: 2.84686\n",
      "INFO:tensorflow:global_step/sec: 0.0894353\n",
      "I1118 03:11:40.090107 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894353\n",
      "INFO:tensorflow:examples/sec: 2.86193\n",
      "I1118 03:11:40.090427 4435920320 tpu_estimator.py:2160] examples/sec: 2.86193\n",
      "INFO:tensorflow:global_step/sec: 0.0894352\n",
      "I1118 03:11:51.271299 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894352\n",
      "INFO:tensorflow:examples/sec: 2.86193\n",
      "I1118 03:11:51.271528 4435920320 tpu_estimator.py:2160] examples/sec: 2.86193\n",
      "INFO:tensorflow:global_step/sec: 0.089217\n",
      "I1118 03:12:02.479944 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089217\n",
      "INFO:tensorflow:examples/sec: 2.85494\n",
      "I1118 03:12:02.480174 4435920320 tpu_estimator.py:2160] examples/sec: 2.85494\n",
      "INFO:tensorflow:global_step/sec: 0.0884852\n",
      "I1118 03:12:13.781253 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884852\n",
      "INFO:tensorflow:examples/sec: 2.83153\n",
      "I1118 03:12:13.781471 4435920320 tpu_estimator.py:2160] examples/sec: 2.83153\n",
      "INFO:tensorflow:global_step/sec: 0.0885966\n",
      "I1118 03:12:25.068367 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885966\n",
      "INFO:tensorflow:examples/sec: 2.83509\n",
      "I1118 03:12:25.068590 4435920320 tpu_estimator.py:2160] examples/sec: 2.83509\n",
      "INFO:tensorflow:global_step/sec: 0.0888335\n",
      "I1118 03:12:36.325450 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888335\n",
      "INFO:tensorflow:examples/sec: 2.84267\n",
      "I1118 03:12:36.325668 4435920320 tpu_estimator.py:2160] examples/sec: 2.84267\n",
      "INFO:tensorflow:global_step/sec: 0.0889222\n",
      "I1118 03:12:47.571164 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889222\n",
      "INFO:tensorflow:examples/sec: 2.84551\n",
      "I1118 03:12:47.571386 4435920320 tpu_estimator.py:2160] examples/sec: 2.84551\n",
      "INFO:tensorflow:global_step/sec: 0.0889389\n",
      "I1118 03:12:58.814861 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889389\n",
      "INFO:tensorflow:examples/sec: 2.84604\n",
      "I1118 03:12:58.815247 4435920320 tpu_estimator.py:2160] examples/sec: 2.84604\n",
      "INFO:tensorflow:global_step/sec: 0.0893136\n",
      "I1118 03:13:10.011344 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893136\n",
      "INFO:tensorflow:examples/sec: 2.85804\n",
      "I1118 03:13:10.011564 4435920320 tpu_estimator.py:2160] examples/sec: 2.85804\n",
      "INFO:tensorflow:global_step/sec: 0.0894657\n",
      "I1118 03:13:21.188811 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894657\n",
      "INFO:tensorflow:examples/sec: 2.8629\n",
      "I1118 03:13:21.189036 4435920320 tpu_estimator.py:2160] examples/sec: 2.8629\n",
      "INFO:tensorflow:global_step/sec: 0.0893507\n",
      "I1118 03:13:32.380682 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893507\n",
      "INFO:tensorflow:examples/sec: 2.85922\n",
      "I1118 03:13:32.380903 4435920320 tpu_estimator.py:2160] examples/sec: 2.85922\n",
      "INFO:tensorflow:global_step/sec: 0.0892954\n",
      "I1118 03:13:43.579442 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892954\n",
      "INFO:tensorflow:examples/sec: 2.85745\n",
      "I1118 03:13:43.579659 4435920320 tpu_estimator.py:2160] examples/sec: 2.85745\n",
      "INFO:tensorflow:global_step/sec: 0.0886632\n",
      "I1118 03:13:54.858095 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886632\n",
      "INFO:tensorflow:examples/sec: 2.83722\n",
      "I1118 03:13:54.858319 4435920320 tpu_estimator.py:2160] examples/sec: 2.83722\n",
      "INFO:tensorflow:global_step/sec: 0.08874\n",
      "I1118 03:14:06.126955 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08874\n",
      "INFO:tensorflow:examples/sec: 2.83968\n",
      "I1118 03:14:06.127167 4435920320 tpu_estimator.py:2160] examples/sec: 2.83968\n",
      "INFO:tensorflow:global_step/sec: 0.0881488\n",
      "I1118 03:14:17.471411 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881488\n",
      "INFO:tensorflow:examples/sec: 2.82076\n",
      "I1118 03:14:17.471633 4435920320 tpu_estimator.py:2160] examples/sec: 2.82076\n",
      "INFO:tensorflow:global_step/sec: 0.0884186\n",
      "I1118 03:14:28.781256 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884186\n",
      "INFO:tensorflow:examples/sec: 2.82939\n",
      "I1118 03:14:28.781475 4435920320 tpu_estimator.py:2160] examples/sec: 2.82939\n",
      "INFO:tensorflow:global_step/sec: 0.0887397\n",
      "I1118 03:14:40.050184 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887397\n",
      "INFO:tensorflow:examples/sec: 2.83967\n",
      "I1118 03:14:40.050415 4435920320 tpu_estimator.py:2160] examples/sec: 2.83967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0890689\n",
      "I1118 03:14:51.277445 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890689\n",
      "INFO:tensorflow:examples/sec: 2.8502\n",
      "I1118 03:14:51.277673 4435920320 tpu_estimator.py:2160] examples/sec: 2.8502\n",
      "INFO:tensorflow:global_step/sec: 0.0893012\n",
      "I1118 03:15:02.475517 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893012\n",
      "INFO:tensorflow:examples/sec: 2.85764\n",
      "I1118 03:15:02.475740 4435920320 tpu_estimator.py:2160] examples/sec: 2.85764\n",
      "INFO:tensorflow:global_step/sec: 0.0884453\n",
      "I1118 03:15:13.781934 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884453\n",
      "INFO:tensorflow:examples/sec: 2.83025\n",
      "I1118 03:15:13.782160 4435920320 tpu_estimator.py:2160] examples/sec: 2.83025\n",
      "INFO:tensorflow:global_step/sec: 0.0890629\n",
      "I1118 03:15:25.009955 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890629\n",
      "INFO:tensorflow:examples/sec: 2.85001\n",
      "I1118 03:15:25.010179 4435920320 tpu_estimator.py:2160] examples/sec: 2.85001\n",
      "INFO:tensorflow:global_step/sec: 0.0891567\n",
      "I1118 03:15:36.226170 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891567\n",
      "INFO:tensorflow:examples/sec: 2.85301\n",
      "I1118 03:15:36.226405 4435920320 tpu_estimator.py:2160] examples/sec: 2.85301\n",
      "INFO:tensorflow:global_step/sec: 0.0896194\n",
      "I1118 03:15:47.384452 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896194\n",
      "INFO:tensorflow:examples/sec: 2.86782\n",
      "I1118 03:15:47.384664 4435920320 tpu_estimator.py:2160] examples/sec: 2.86782\n",
      "INFO:tensorflow:global_step/sec: 0.0895255\n",
      "I1118 03:15:58.554461 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895255\n",
      "INFO:tensorflow:examples/sec: 2.86482\n",
      "I1118 03:15:58.554683 4435920320 tpu_estimator.py:2160] examples/sec: 2.86482\n",
      "INFO:tensorflow:global_step/sec: 0.0884523\n",
      "I1118 03:16:09.859981 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884523\n",
      "INFO:tensorflow:examples/sec: 2.83047\n",
      "I1118 03:16:09.860200 4435920320 tpu_estimator.py:2160] examples/sec: 2.83047\n",
      "INFO:tensorflow:global_step/sec: 0.0894479\n",
      "I1118 03:16:21.039685 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894479\n",
      "INFO:tensorflow:examples/sec: 2.86233\n",
      "I1118 03:16:21.039913 4435920320 tpu_estimator.py:2160] examples/sec: 2.86233\n",
      "INFO:tensorflow:global_step/sec: 0.0896435\n",
      "I1118 03:16:32.195087 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896435\n",
      "INFO:tensorflow:examples/sec: 2.86859\n",
      "I1118 03:16:32.195332 4435920320 tpu_estimator.py:2160] examples/sec: 2.86859\n",
      "INFO:tensorflow:global_step/sec: 0.0895143\n",
      "I1118 03:16:43.366394 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895143\n",
      "INFO:tensorflow:examples/sec: 2.86446\n",
      "I1118 03:16:43.366616 4435920320 tpu_estimator.py:2160] examples/sec: 2.86446\n",
      "INFO:tensorflow:global_step/sec: 0.0878098\n",
      "I1118 03:16:54.754639 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878098\n",
      "INFO:tensorflow:examples/sec: 2.80991\n",
      "I1118 03:16:54.754871 4435920320 tpu_estimator.py:2160] examples/sec: 2.80991\n",
      "INFO:tensorflow:global_step/sec: 0.0881382\n",
      "I1118 03:17:06.100414 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881382\n",
      "INFO:tensorflow:examples/sec: 2.82042\n",
      "I1118 03:17:06.100636 4435920320 tpu_estimator.py:2160] examples/sec: 2.82042\n",
      "INFO:tensorflow:global_step/sec: 0.0887331\n",
      "I1118 03:17:17.370194 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887331\n",
      "INFO:tensorflow:examples/sec: 2.83946\n",
      "I1118 03:17:17.370409 4435920320 tpu_estimator.py:2160] examples/sec: 2.83946\n",
      "INFO:tensorflow:global_step/sec: 0.0891307\n",
      "I1118 03:17:28.589674 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891307\n",
      "INFO:tensorflow:examples/sec: 2.85218\n",
      "I1118 03:17:28.589899 4435920320 tpu_estimator.py:2160] examples/sec: 2.85218\n",
      "INFO:tensorflow:global_step/sec: 0.0890579\n",
      "I1118 03:17:39.818331 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890579\n",
      "INFO:tensorflow:examples/sec: 2.84985\n",
      "I1118 03:17:39.818557 4435920320 tpu_estimator.py:2160] examples/sec: 2.84985\n",
      "INFO:tensorflow:global_step/sec: 0.0887642\n",
      "I1118 03:17:51.084122 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887642\n",
      "INFO:tensorflow:examples/sec: 2.84046\n",
      "I1118 03:17:51.084346 4435920320 tpu_estimator.py:2160] examples/sec: 2.84046\n",
      "INFO:tensorflow:global_step/sec: 0.0888449\n",
      "I1118 03:18:02.339699 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888449\n",
      "INFO:tensorflow:examples/sec: 2.84304\n",
      "I1118 03:18:02.339946 4435920320 tpu_estimator.py:2160] examples/sec: 2.84304\n",
      "INFO:tensorflow:global_step/sec: 0.0890916\n",
      "I1118 03:18:13.564095 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890916\n",
      "INFO:tensorflow:examples/sec: 2.85093\n",
      "I1118 03:18:13.564324 4435920320 tpu_estimator.py:2160] examples/sec: 2.85093\n",
      "INFO:tensorflow:global_step/sec: 0.0882331\n",
      "I1118 03:18:24.897711 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882331\n",
      "INFO:tensorflow:examples/sec: 2.82346\n",
      "I1118 03:18:24.897943 4435920320 tpu_estimator.py:2160] examples/sec: 2.82346\n",
      "INFO:tensorflow:global_step/sec: 0.0889777\n",
      "I1118 03:18:36.136487 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889777\n",
      "INFO:tensorflow:examples/sec: 2.84729\n",
      "I1118 03:18:36.136723 4435920320 tpu_estimator.py:2160] examples/sec: 2.84729\n",
      "INFO:tensorflow:global_step/sec: 0.089319\n",
      "I1118 03:18:47.332306 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089319\n",
      "INFO:tensorflow:examples/sec: 2.85821\n",
      "I1118 03:18:47.332525 4435920320 tpu_estimator.py:2160] examples/sec: 2.85821\n",
      "INFO:tensorflow:global_step/sec: 0.0895898\n",
      "I1118 03:18:58.494282 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895898\n",
      "INFO:tensorflow:examples/sec: 2.86687\n",
      "I1118 03:18:58.494493 4435920320 tpu_estimator.py:2160] examples/sec: 2.86687\n",
      "INFO:tensorflow:global_step/sec: 0.0888342\n",
      "I1118 03:19:09.751216 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888342\n",
      "INFO:tensorflow:examples/sec: 2.84269\n",
      "I1118 03:19:09.751434 4435920320 tpu_estimator.py:2160] examples/sec: 2.84269\n",
      "INFO:tensorflow:global_step/sec: 0.0895924\n",
      "I1118 03:19:20.912885 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895924\n",
      "INFO:tensorflow:examples/sec: 2.86696\n",
      "I1118 03:19:20.913126 4435920320 tpu_estimator.py:2160] examples/sec: 2.86696\n",
      "INFO:tensorflow:global_step/sec: 0.0890613\n",
      "I1118 03:19:32.141090 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890613\n",
      "INFO:tensorflow:examples/sec: 2.84996\n",
      "I1118 03:19:32.141306 4435920320 tpu_estimator.py:2160] examples/sec: 2.84996\n",
      "INFO:tensorflow:global_step/sec: 0.0889564\n",
      "I1118 03:19:43.382567 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889564\n",
      "INFO:tensorflow:examples/sec: 2.8466\n",
      "I1118 03:19:43.382799 4435920320 tpu_estimator.py:2160] examples/sec: 2.8466\n",
      "INFO:tensorflow:global_step/sec: 0.089309\n",
      "I1118 03:19:54.579641 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089309\n",
      "INFO:tensorflow:examples/sec: 2.85789\n",
      "I1118 03:19:54.580015 4435920320 tpu_estimator.py:2160] examples/sec: 2.85789\n",
      "INFO:tensorflow:global_step/sec: 0.0885551\n",
      "I1118 03:20:05.872047 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885551\n",
      "INFO:tensorflow:examples/sec: 2.83376\n",
      "I1118 03:20:05.872276 4435920320 tpu_estimator.py:2160] examples/sec: 2.83376\n",
      "INFO:tensorflow:global_step/sec: 0.089167\n",
      "I1118 03:20:17.086952 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089167\n",
      "INFO:tensorflow:examples/sec: 2.85334\n",
      "I1118 03:20:17.087168 4435920320 tpu_estimator.py:2160] examples/sec: 2.85334\n",
      "INFO:tensorflow:global_step/sec: 0.0891739\n",
      "I1118 03:20:28.300997 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891739\n",
      "INFO:tensorflow:examples/sec: 2.85357\n",
      "I1118 03:20:28.301227 4435920320 tpu_estimator.py:2160] examples/sec: 2.85357\n",
      "INFO:tensorflow:global_step/sec: 0.0890494\n",
      "I1118 03:20:39.530743 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890494\n",
      "INFO:tensorflow:examples/sec: 2.84958\n",
      "I1118 03:20:39.530959 4435920320 tpu_estimator.py:2160] examples/sec: 2.84958\n",
      "INFO:tensorflow:global_step/sec: 0.088714\n",
      "I1118 03:20:50.802915 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088714\n",
      "INFO:tensorflow:examples/sec: 2.83885\n",
      "I1118 03:20:50.803153 4435920320 tpu_estimator.py:2160] examples/sec: 2.83885\n",
      "INFO:tensorflow:global_step/sec: 0.0893998\n",
      "I1118 03:21:01.988627 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893998\n",
      "INFO:tensorflow:examples/sec: 2.86079\n",
      "I1118 03:21:01.988855 4435920320 tpu_estimator.py:2160] examples/sec: 2.86079\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891804\n",
      "I1118 03:21:13.201857 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891804\n",
      "INFO:tensorflow:examples/sec: 2.85377\n",
      "I1118 03:21:13.202072 4435920320 tpu_estimator.py:2160] examples/sec: 2.85377\n",
      "INFO:tensorflow:global_step/sec: 0.0894858\n",
      "I1118 03:21:24.376784 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894858\n",
      "INFO:tensorflow:examples/sec: 2.86355\n",
      "I1118 03:21:24.377007 4435920320 tpu_estimator.py:2160] examples/sec: 2.86355\n",
      "INFO:tensorflow:global_step/sec: 0.0891118\n",
      "I1118 03:21:35.598655 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891118\n",
      "INFO:tensorflow:examples/sec: 2.85158\n",
      "I1118 03:21:35.598894 4435920320 tpu_estimator.py:2160] examples/sec: 2.85158\n",
      "INFO:tensorflow:global_step/sec: 0.0888939\n",
      "I1118 03:21:46.848029 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888939\n",
      "INFO:tensorflow:examples/sec: 2.8446\n",
      "I1118 03:21:46.848260 4435920320 tpu_estimator.py:2160] examples/sec: 2.8446\n",
      "INFO:tensorflow:global_step/sec: 0.0898803\n",
      "I1118 03:21:57.973927 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898803\n",
      "INFO:tensorflow:examples/sec: 2.87617\n",
      "I1118 03:21:57.974146 4435920320 tpu_estimator.py:2160] examples/sec: 2.87617\n",
      "INFO:tensorflow:global_step/sec: 0.0894725\n",
      "I1118 03:22:09.150541 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894725\n",
      "INFO:tensorflow:examples/sec: 2.86312\n",
      "I1118 03:22:09.150760 4435920320 tpu_estimator.py:2160] examples/sec: 2.86312\n",
      "INFO:tensorflow:global_step/sec: 0.0894338\n",
      "I1118 03:22:20.331990 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894338\n",
      "INFO:tensorflow:examples/sec: 2.86188\n",
      "I1118 03:22:20.332207 4435920320 tpu_estimator.py:2160] examples/sec: 2.86188\n",
      "INFO:tensorflow:global_step/sec: 0.0897601\n",
      "I1118 03:22:31.472824 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897601\n",
      "INFO:tensorflow:examples/sec: 2.87232\n",
      "I1118 03:22:31.473039 4435920320 tpu_estimator.py:2160] examples/sec: 2.87232\n",
      "INFO:tensorflow:global_step/sec: 0.0891113\n",
      "I1118 03:22:42.694740 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891113\n",
      "INFO:tensorflow:examples/sec: 2.85156\n",
      "I1118 03:22:42.694972 4435920320 tpu_estimator.py:2160] examples/sec: 2.85156\n",
      "INFO:tensorflow:global_step/sec: 0.0891712\n",
      "I1118 03:22:53.909121 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891712\n",
      "INFO:tensorflow:examples/sec: 2.85348\n",
      "I1118 03:22:53.909360 4435920320 tpu_estimator.py:2160] examples/sec: 2.85348\n",
      "INFO:tensorflow:global_step/sec: 0.0889446\n",
      "I1118 03:23:05.152067 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889446\n",
      "INFO:tensorflow:examples/sec: 2.84623\n",
      "I1118 03:23:05.152286 4435920320 tpu_estimator.py:2160] examples/sec: 2.84623\n",
      "INFO:tensorflow:global_step/sec: 0.0891508\n",
      "I1118 03:23:16.369019 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891508\n",
      "INFO:tensorflow:examples/sec: 2.85282\n",
      "I1118 03:23:16.369422 4435920320 tpu_estimator.py:2160] examples/sec: 2.85282\n",
      "INFO:tensorflow:global_step/sec: 0.0896259\n",
      "I1118 03:23:27.526513 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896259\n",
      "INFO:tensorflow:examples/sec: 2.86803\n",
      "I1118 03:23:27.526746 4435920320 tpu_estimator.py:2160] examples/sec: 2.86803\n",
      "INFO:tensorflow:global_step/sec: 0.0894304\n",
      "I1118 03:23:38.708399 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894304\n",
      "INFO:tensorflow:examples/sec: 2.86177\n",
      "I1118 03:23:38.708606 4435920320 tpu_estimator.py:2160] examples/sec: 2.86177\n",
      "INFO:tensorflow:global_step/sec: 0.0887672\n",
      "I1118 03:23:49.973824 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887672\n",
      "INFO:tensorflow:examples/sec: 2.84055\n",
      "I1118 03:23:49.974045 4435920320 tpu_estimator.py:2160] examples/sec: 2.84055\n",
      "INFO:tensorflow:global_step/sec: 0.0891476\n",
      "I1118 03:24:01.191164 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891476\n",
      "INFO:tensorflow:examples/sec: 2.85272\n",
      "I1118 03:24:01.191393 4435920320 tpu_estimator.py:2160] examples/sec: 2.85272\n",
      "INFO:tensorflow:global_step/sec: 0.089561\n",
      "I1118 03:24:12.356738 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089561\n",
      "INFO:tensorflow:examples/sec: 2.86595\n",
      "I1118 03:24:12.357089 4435920320 tpu_estimator.py:2160] examples/sec: 2.86595\n",
      "INFO:tensorflow:global_step/sec: 0.0890442\n",
      "I1118 03:24:23.587099 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890442\n",
      "INFO:tensorflow:examples/sec: 2.84942\n",
      "I1118 03:24:23.587326 4435920320 tpu_estimator.py:2160] examples/sec: 2.84942\n",
      "INFO:tensorflow:global_step/sec: 0.0885734\n",
      "I1118 03:24:34.877197 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885734\n",
      "INFO:tensorflow:examples/sec: 2.83435\n",
      "I1118 03:24:34.877440 4435920320 tpu_estimator.py:2160] examples/sec: 2.83435\n",
      "INFO:tensorflow:global_step/sec: 0.0885719\n",
      "I1118 03:24:46.167445 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885719\n",
      "INFO:tensorflow:examples/sec: 2.8343\n",
      "I1118 03:24:46.167662 4435920320 tpu_estimator.py:2160] examples/sec: 2.8343\n",
      "INFO:tensorflow:global_step/sec: 0.0892889\n",
      "I1118 03:24:57.367048 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892889\n",
      "INFO:tensorflow:examples/sec: 2.85724\n",
      "I1118 03:24:57.367264 4435920320 tpu_estimator.py:2160] examples/sec: 2.85724\n",
      "INFO:tensorflow:global_step/sec: 0.0891322\n",
      "I1118 03:25:08.586344 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891322\n",
      "INFO:tensorflow:examples/sec: 2.85223\n",
      "I1118 03:25:08.586606 4435920320 tpu_estimator.py:2160] examples/sec: 2.85223\n",
      "INFO:tensorflow:global_step/sec: 0.0890955\n",
      "I1118 03:25:19.810247 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890955\n",
      "INFO:tensorflow:examples/sec: 2.85106\n",
      "I1118 03:25:19.810467 4435920320 tpu_estimator.py:2160] examples/sec: 2.85106\n",
      "INFO:tensorflow:global_step/sec: 0.0886274\n",
      "I1118 03:25:31.093485 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886274\n",
      "INFO:tensorflow:examples/sec: 2.83608\n",
      "I1118 03:25:31.093705 4435920320 tpu_estimator.py:2160] examples/sec: 2.83608\n",
      "INFO:tensorflow:global_step/sec: 0.0892947\n",
      "I1118 03:25:42.292327 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892947\n",
      "INFO:tensorflow:examples/sec: 2.85743\n",
      "I1118 03:25:42.292569 4435920320 tpu_estimator.py:2160] examples/sec: 2.85743\n",
      "INFO:tensorflow:global_step/sec: 0.0886526\n",
      "I1118 03:25:53.572293 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886526\n",
      "INFO:tensorflow:examples/sec: 2.83688\n",
      "I1118 03:25:53.572511 4435920320 tpu_estimator.py:2160] examples/sec: 2.83688\n",
      "INFO:tensorflow:global_step/sec: 0.0882369\n",
      "I1118 03:26:04.905430 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882369\n",
      "INFO:tensorflow:examples/sec: 2.82358\n",
      "I1118 03:26:04.905658 4435920320 tpu_estimator.py:2160] examples/sec: 2.82358\n",
      "INFO:tensorflow:global_step/sec: 0.0891282\n",
      "I1118 03:26:16.125211 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891282\n",
      "INFO:tensorflow:examples/sec: 2.8521\n",
      "I1118 03:26:16.125430 4435920320 tpu_estimator.py:2160] examples/sec: 2.8521\n",
      "INFO:tensorflow:global_step/sec: 0.0891881\n",
      "I1118 03:26:27.337472 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891881\n",
      "INFO:tensorflow:examples/sec: 2.85402\n",
      "I1118 03:26:27.337692 4435920320 tpu_estimator.py:2160] examples/sec: 2.85402\n",
      "INFO:tensorflow:global_step/sec: 0.0891422\n",
      "I1118 03:26:38.555505 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891422\n",
      "INFO:tensorflow:examples/sec: 2.85255\n",
      "I1118 03:26:38.555727 4435920320 tpu_estimator.py:2160] examples/sec: 2.85255\n",
      "INFO:tensorflow:global_step/sec: 0.0890688\n",
      "I1118 03:26:49.782781 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890688\n",
      "INFO:tensorflow:examples/sec: 2.8502\n",
      "I1118 03:26:49.783002 4435920320 tpu_estimator.py:2160] examples/sec: 2.8502\n",
      "INFO:tensorflow:global_step/sec: 0.0888\n",
      "I1118 03:27:01.044037 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888\n",
      "INFO:tensorflow:examples/sec: 2.8416\n",
      "I1118 03:27:01.044256 4435920320 tpu_estimator.py:2160] examples/sec: 2.8416\n",
      "INFO:tensorflow:global_step/sec: 0.0893642\n",
      "I1118 03:27:12.234219 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893642\n",
      "INFO:tensorflow:examples/sec: 2.85965\n",
      "I1118 03:27:12.234452 4435920320 tpu_estimator.py:2160] examples/sec: 2.85965\n",
      "INFO:tensorflow:global_step/sec: 0.0894021\n",
      "I1118 03:27:23.419642 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894021\n",
      "INFO:tensorflow:examples/sec: 2.86087\n",
      "I1118 03:27:23.419892 4435920320 tpu_estimator.py:2160] examples/sec: 2.86087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0890727\n",
      "I1118 03:27:34.646410 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890727\n",
      "INFO:tensorflow:examples/sec: 2.85033\n",
      "I1118 03:27:34.646632 4435920320 tpu_estimator.py:2160] examples/sec: 2.85033\n",
      "INFO:tensorflow:global_step/sec: 0.0888018\n",
      "I1118 03:27:45.907478 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888018\n",
      "INFO:tensorflow:examples/sec: 2.84166\n",
      "I1118 03:27:45.907705 4435920320 tpu_estimator.py:2160] examples/sec: 2.84166\n",
      "INFO:tensorflow:global_step/sec: 0.0890583\n",
      "I1118 03:27:57.136049 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890583\n",
      "INFO:tensorflow:examples/sec: 2.84987\n",
      "I1118 03:27:57.136283 4435920320 tpu_estimator.py:2160] examples/sec: 2.84987\n",
      "INFO:tensorflow:global_step/sec: 0.0891634\n",
      "I1118 03:28:08.351497 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891634\n",
      "INFO:tensorflow:examples/sec: 2.85323\n",
      "I1118 03:28:08.351727 4435920320 tpu_estimator.py:2160] examples/sec: 2.85323\n",
      "INFO:tensorflow:global_step/sec: 0.0892892\n",
      "I1118 03:28:19.550963 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892892\n",
      "INFO:tensorflow:examples/sec: 2.85725\n",
      "I1118 03:28:19.551187 4435920320 tpu_estimator.py:2160] examples/sec: 2.85725\n",
      "INFO:tensorflow:global_step/sec: 0.0891847\n",
      "I1118 03:28:30.763658 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891847\n",
      "INFO:tensorflow:examples/sec: 2.85391\n",
      "I1118 03:28:30.763875 4435920320 tpu_estimator.py:2160] examples/sec: 2.85391\n",
      "INFO:tensorflow:global_step/sec: 0.0888241\n",
      "I1118 03:28:42.021867 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888241\n",
      "INFO:tensorflow:examples/sec: 2.84237\n",
      "I1118 03:28:42.022099 4435920320 tpu_estimator.py:2160] examples/sec: 2.84237\n",
      "INFO:tensorflow:global_step/sec: 0.0900894\n",
      "I1118 03:28:53.121960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900894\n",
      "INFO:tensorflow:examples/sec: 2.88286\n",
      "I1118 03:28:53.122206 4435920320 tpu_estimator.py:2160] examples/sec: 2.88286\n",
      "INFO:tensorflow:global_step/sec: 0.0898539\n",
      "I1118 03:29:04.251105 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898539\n",
      "INFO:tensorflow:examples/sec: 2.87532\n",
      "I1118 03:29:04.251299 4435920320 tpu_estimator.py:2160] examples/sec: 2.87532\n",
      "INFO:tensorflow:global_step/sec: 0.0889102\n",
      "I1118 03:29:15.498443 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889102\n",
      "INFO:tensorflow:examples/sec: 2.84513\n",
      "I1118 03:29:15.498687 4435920320 tpu_estimator.py:2160] examples/sec: 2.84513\n",
      "INFO:tensorflow:global_step/sec: 0.0887323\n",
      "I1118 03:29:26.768280 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887323\n",
      "INFO:tensorflow:examples/sec: 2.83943\n",
      "I1118 03:29:26.768504 4435920320 tpu_estimator.py:2160] examples/sec: 2.83943\n",
      "INFO:tensorflow:global_step/sec: 0.0888552\n",
      "I1118 03:29:38.022529 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888552\n",
      "INFO:tensorflow:examples/sec: 2.84337\n",
      "I1118 03:29:38.022722 4435920320 tpu_estimator.py:2160] examples/sec: 2.84337\n",
      "INFO:tensorflow:global_step/sec: 0.0893477\n",
      "I1118 03:29:49.214776 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893477\n",
      "INFO:tensorflow:examples/sec: 2.85913\n",
      "I1118 03:29:49.214993 4435920320 tpu_estimator.py:2160] examples/sec: 2.85913\n",
      "INFO:tensorflow:global_step/sec: 0.0891983\n",
      "I1118 03:30:00.425901 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891983\n",
      "INFO:tensorflow:examples/sec: 2.85434\n",
      "I1118 03:30:00.426255 4435920320 tpu_estimator.py:2160] examples/sec: 2.85434\n",
      "INFO:tensorflow:global_step/sec: 0.0890811\n",
      "I1118 03:30:11.651482 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890811\n",
      "INFO:tensorflow:examples/sec: 2.8506\n",
      "I1118 03:30:11.651720 4435920320 tpu_estimator.py:2160] examples/sec: 2.8506\n",
      "INFO:tensorflow:global_step/sec: 0.0893493\n",
      "I1118 03:30:22.843514 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893493\n",
      "INFO:tensorflow:examples/sec: 2.85918\n",
      "I1118 03:30:22.843765 4435920320 tpu_estimator.py:2160] examples/sec: 2.85918\n",
      "INFO:tensorflow:global_step/sec: 0.0893394\n",
      "I1118 03:30:34.036774 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893394\n",
      "INFO:tensorflow:examples/sec: 2.85886\n",
      "I1118 03:30:34.036999 4435920320 tpu_estimator.py:2160] examples/sec: 2.85886\n",
      "INFO:tensorflow:global_step/sec: 0.089456\n",
      "I1118 03:30:45.215459 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089456\n",
      "INFO:tensorflow:examples/sec: 2.86259\n",
      "I1118 03:30:45.215684 4435920320 tpu_estimator.py:2160] examples/sec: 2.86259\n",
      "INFO:tensorflow:global_step/sec: 0.0894932\n",
      "I1118 03:30:56.389496 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894932\n",
      "INFO:tensorflow:examples/sec: 2.86378\n",
      "I1118 03:30:56.389718 4435920320 tpu_estimator.py:2160] examples/sec: 2.86378\n",
      "INFO:tensorflow:global_step/sec: 0.0887656\n",
      "I1118 03:31:07.655102 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887656\n",
      "INFO:tensorflow:examples/sec: 2.8405\n",
      "I1118 03:31:07.655317 4435920320 tpu_estimator.py:2160] examples/sec: 2.8405\n",
      "INFO:tensorflow:global_step/sec: 0.088032\n",
      "I1118 03:31:19.014611 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088032\n",
      "INFO:tensorflow:examples/sec: 2.81702\n",
      "I1118 03:31:19.014823 4435920320 tpu_estimator.py:2160] examples/sec: 2.81702\n",
      "INFO:tensorflow:global_step/sec: 0.0892235\n",
      "I1118 03:31:30.222427 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892235\n",
      "INFO:tensorflow:examples/sec: 2.85515\n",
      "I1118 03:31:30.222643 4435920320 tpu_estimator.py:2160] examples/sec: 2.85515\n",
      "INFO:tensorflow:global_step/sec: 0.0898376\n",
      "I1118 03:31:41.353637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898376\n",
      "INFO:tensorflow:examples/sec: 2.8748\n",
      "I1118 03:31:41.353868 4435920320 tpu_estimator.py:2160] examples/sec: 2.8748\n",
      "INFO:tensorflow:global_step/sec: 0.0893136\n",
      "I1118 03:31:52.550136 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893136\n",
      "INFO:tensorflow:examples/sec: 2.85804\n",
      "I1118 03:31:52.550364 4435920320 tpu_estimator.py:2160] examples/sec: 2.85804\n",
      "INFO:tensorflow:global_step/sec: 0.0884364\n",
      "I1118 03:32:03.857725 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884364\n",
      "INFO:tensorflow:examples/sec: 2.82996\n",
      "I1118 03:32:03.857985 4435920320 tpu_estimator.py:2160] examples/sec: 2.82996\n",
      "INFO:tensorflow:global_step/sec: 0.0894202\n",
      "I1118 03:32:15.040859 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894202\n",
      "INFO:tensorflow:examples/sec: 2.86145\n",
      "I1118 03:32:15.041095 4435920320 tpu_estimator.py:2160] examples/sec: 2.86145\n",
      "INFO:tensorflow:global_step/sec: 0.0892249\n",
      "I1118 03:32:26.248480 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892249\n",
      "INFO:tensorflow:examples/sec: 2.8552\n",
      "I1118 03:32:26.248706 4435920320 tpu_estimator.py:2160] examples/sec: 2.8552\n",
      "INFO:tensorflow:global_step/sec: 0.0890955\n",
      "I1118 03:32:37.472409 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890955\n",
      "INFO:tensorflow:examples/sec: 2.85106\n",
      "I1118 03:32:37.472824 4435920320 tpu_estimator.py:2160] examples/sec: 2.85106\n",
      "INFO:tensorflow:global_step/sec: 0.0892086\n",
      "I1118 03:32:48.682066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892086\n",
      "INFO:tensorflow:examples/sec: 2.85467\n",
      "I1118 03:32:48.682276 4435920320 tpu_estimator.py:2160] examples/sec: 2.85467\n",
      "INFO:tensorflow:global_step/sec: 0.0878336\n",
      "I1118 03:33:00.067262 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0878336\n",
      "INFO:tensorflow:examples/sec: 2.81067\n",
      "I1118 03:33:00.067492 4435920320 tpu_estimator.py:2160] examples/sec: 2.81067\n",
      "INFO:tensorflow:global_step/sec: 0.0889251\n",
      "I1118 03:33:11.312671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889251\n",
      "INFO:tensorflow:examples/sec: 2.8456\n",
      "I1118 03:33:11.312902 4435920320 tpu_estimator.py:2160] examples/sec: 2.8456\n",
      "INFO:tensorflow:global_step/sec: 0.0893758\n",
      "I1118 03:33:22.501379 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893758\n",
      "INFO:tensorflow:examples/sec: 2.86003\n",
      "I1118 03:33:22.501592 4435920320 tpu_estimator.py:2160] examples/sec: 2.86003\n",
      "INFO:tensorflow:global_step/sec: 0.0915871\n",
      "I1118 03:33:33.419887 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0915871\n",
      "INFO:tensorflow:examples/sec: 2.93079\n",
      "I1118 03:33:33.420035 4435920320 tpu_estimator.py:2160] examples/sec: 2.93079\n",
      "INFO:tensorflow:global_step/sec: 0.0891803\n",
      "I1118 03:33:44.633124 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891803\n",
      "INFO:tensorflow:examples/sec: 2.85377\n",
      "I1118 03:33:44.633274 4435920320 tpu_estimator.py:2160] examples/sec: 2.85377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.088127\n",
      "I1118 03:33:55.980452 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088127\n",
      "INFO:tensorflow:examples/sec: 2.82006\n",
      "I1118 03:33:55.980666 4435920320 tpu_estimator.py:2160] examples/sec: 2.82006\n",
      "INFO:tensorflow:global_step/sec: 0.0898728\n",
      "I1118 03:34:07.107285 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898728\n",
      "INFO:tensorflow:examples/sec: 2.87593\n",
      "I1118 03:34:07.107507 4435920320 tpu_estimator.py:2160] examples/sec: 2.87593\n",
      "INFO:tensorflow:global_step/sec: 0.0892397\n",
      "I1118 03:34:18.313079 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892397\n",
      "INFO:tensorflow:examples/sec: 2.85567\n",
      "I1118 03:34:18.313305 4435920320 tpu_estimator.py:2160] examples/sec: 2.85567\n",
      "INFO:tensorflow:global_step/sec: 0.0887247\n",
      "I1118 03:34:29.583884 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887247\n",
      "INFO:tensorflow:examples/sec: 2.83919\n",
      "I1118 03:34:29.584119 4435920320 tpu_estimator.py:2160] examples/sec: 2.83919\n",
      "INFO:tensorflow:global_step/sec: 0.0891381\n",
      "I1118 03:34:40.802425 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891381\n",
      "INFO:tensorflow:examples/sec: 2.85242\n",
      "I1118 03:34:40.802648 4435920320 tpu_estimator.py:2160] examples/sec: 2.85242\n",
      "INFO:tensorflow:global_step/sec: 0.0889389\n",
      "I1118 03:34:52.046095 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889389\n",
      "INFO:tensorflow:examples/sec: 2.84605\n",
      "I1118 03:34:52.046318 4435920320 tpu_estimator.py:2160] examples/sec: 2.84605\n",
      "INFO:tensorflow:global_step/sec: 0.0896711\n",
      "I1118 03:35:03.197989 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896711\n",
      "INFO:tensorflow:examples/sec: 2.86947\n",
      "I1118 03:35:03.198202 4435920320 tpu_estimator.py:2160] examples/sec: 2.86947\n",
      "INFO:tensorflow:global_step/sec: 0.0896723\n",
      "I1118 03:35:14.349685 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896723\n",
      "INFO:tensorflow:examples/sec: 2.86952\n",
      "I1118 03:35:14.349916 4435920320 tpu_estimator.py:2160] examples/sec: 2.86952\n",
      "INFO:tensorflow:global_step/sec: 0.0897145\n",
      "I1118 03:35:25.496138 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897145\n",
      "INFO:tensorflow:examples/sec: 2.87086\n",
      "I1118 03:35:25.496357 4435920320 tpu_estimator.py:2160] examples/sec: 2.87086\n",
      "INFO:tensorflow:global_step/sec: 0.0900443\n",
      "I1118 03:35:36.601776 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900443\n",
      "INFO:tensorflow:examples/sec: 2.88142\n",
      "I1118 03:35:36.601994 4435920320 tpu_estimator.py:2160] examples/sec: 2.88142\n",
      "INFO:tensorflow:global_step/sec: 0.0880194\n",
      "I1118 03:35:47.962846 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880194\n",
      "INFO:tensorflow:examples/sec: 2.81662\n",
      "I1118 03:35:47.963062 4435920320 tpu_estimator.py:2160] examples/sec: 2.81662\n",
      "INFO:tensorflow:global_step/sec: 0.0885826\n",
      "I1118 03:35:59.251814 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885826\n",
      "INFO:tensorflow:examples/sec: 2.83464\n",
      "I1118 03:35:59.252026 4435920320 tpu_estimator.py:2160] examples/sec: 2.83464\n",
      "INFO:tensorflow:global_step/sec: 0.0888771\n",
      "I1118 03:36:10.503322 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888771\n",
      "INFO:tensorflow:examples/sec: 2.84407\n",
      "I1118 03:36:10.503555 4435920320 tpu_estimator.py:2160] examples/sec: 2.84407\n",
      "INFO:tensorflow:global_step/sec: 0.0893979\n",
      "I1118 03:36:21.689262 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893979\n",
      "INFO:tensorflow:examples/sec: 2.86073\n",
      "I1118 03:36:21.689491 4435920320 tpu_estimator.py:2160] examples/sec: 2.86073\n",
      "INFO:tensorflow:global_step/sec: 0.0895179\n",
      "I1118 03:36:32.860234 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895179\n",
      "INFO:tensorflow:examples/sec: 2.86457\n",
      "I1118 03:36:32.860456 4435920320 tpu_estimator.py:2160] examples/sec: 2.86457\n",
      "INFO:tensorflow:global_step/sec: 0.0894447\n",
      "I1118 03:36:44.040313 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894447\n",
      "INFO:tensorflow:examples/sec: 2.86223\n",
      "I1118 03:36:44.040552 4435920320 tpu_estimator.py:2160] examples/sec: 2.86223\n",
      "INFO:tensorflow:global_step/sec: 0.0893439\n",
      "I1118 03:36:55.233016 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893439\n",
      "INFO:tensorflow:examples/sec: 2.859\n",
      "I1118 03:36:55.233237 4435920320 tpu_estimator.py:2160] examples/sec: 2.859\n",
      "INFO:tensorflow:global_step/sec: 0.0898751\n",
      "I1118 03:37:06.359550 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898751\n",
      "INFO:tensorflow:examples/sec: 2.876\n",
      "I1118 03:37:06.359758 4435920320 tpu_estimator.py:2160] examples/sec: 2.876\n",
      "INFO:tensorflow:global_step/sec: 0.0884959\n",
      "I1118 03:37:17.659512 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884959\n",
      "INFO:tensorflow:examples/sec: 2.83187\n",
      "I1118 03:37:17.659727 4435920320 tpu_estimator.py:2160] examples/sec: 2.83187\n",
      "INFO:tensorflow:global_step/sec: 0.0881345\n",
      "I1118 03:37:29.005810 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881345\n",
      "INFO:tensorflow:examples/sec: 2.8203\n",
      "I1118 03:37:29.006041 4435920320 tpu_estimator.py:2160] examples/sec: 2.8203\n",
      "INFO:tensorflow:global_step/sec: 0.0895417\n",
      "I1118 03:37:40.173822 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895417\n",
      "INFO:tensorflow:examples/sec: 2.86534\n",
      "I1118 03:37:40.174053 4435920320 tpu_estimator.py:2160] examples/sec: 2.86534\n",
      "INFO:tensorflow:global_step/sec: 0.089673\n",
      "I1118 03:37:51.325421 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089673\n",
      "INFO:tensorflow:examples/sec: 2.86953\n",
      "I1118 03:37:51.325641 4435920320 tpu_estimator.py:2160] examples/sec: 2.86953\n",
      "INFO:tensorflow:global_step/sec: 0.0899895\n",
      "I1118 03:38:02.437831 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899895\n",
      "INFO:tensorflow:examples/sec: 2.87966\n",
      "I1118 03:38:02.438055 4435920320 tpu_estimator.py:2160] examples/sec: 2.87966\n",
      "INFO:tensorflow:global_step/sec: 0.0892264\n",
      "I1118 03:38:13.645285 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892264\n",
      "INFO:tensorflow:examples/sec: 2.85525\n",
      "I1118 03:38:13.645514 4435920320 tpu_estimator.py:2160] examples/sec: 2.85525\n",
      "INFO:tensorflow:global_step/sec: 0.0888423\n",
      "I1118 03:38:24.901174 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888423\n",
      "INFO:tensorflow:examples/sec: 2.84296\n",
      "I1118 03:38:24.901401 4435920320 tpu_estimator.py:2160] examples/sec: 2.84296\n",
      "INFO:tensorflow:global_step/sec: 0.089048\n",
      "I1118 03:38:36.131066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089048\n",
      "INFO:tensorflow:examples/sec: 2.84954\n",
      "I1118 03:38:36.131287 4435920320 tpu_estimator.py:2160] examples/sec: 2.84954\n",
      "INFO:tensorflow:global_step/sec: 0.0889978\n",
      "I1118 03:38:47.367295 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889978\n",
      "INFO:tensorflow:examples/sec: 2.84793\n",
      "I1118 03:38:47.367522 4435920320 tpu_estimator.py:2160] examples/sec: 2.84793\n",
      "INFO:tensorflow:global_step/sec: 0.0897134\n",
      "I1118 03:38:58.513918 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897134\n",
      "INFO:tensorflow:examples/sec: 2.87083\n",
      "I1118 03:38:58.514139 4435920320 tpu_estimator.py:2160] examples/sec: 2.87083\n",
      "INFO:tensorflow:global_step/sec: 0.0891201\n",
      "I1118 03:39:09.734716 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891201\n",
      "INFO:tensorflow:examples/sec: 2.85184\n",
      "I1118 03:39:09.734965 4435920320 tpu_estimator.py:2160] examples/sec: 2.85184\n",
      "INFO:tensorflow:global_step/sec: 0.0888853\n",
      "I1118 03:39:20.985187 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888853\n",
      "INFO:tensorflow:examples/sec: 2.84433\n",
      "I1118 03:39:20.985424 4435920320 tpu_estimator.py:2160] examples/sec: 2.84433\n",
      "INFO:tensorflow:global_step/sec: 0.0889805\n",
      "I1118 03:39:32.223579 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889805\n",
      "INFO:tensorflow:examples/sec: 2.84738\n",
      "I1118 03:39:32.223798 4435920320 tpu_estimator.py:2160] examples/sec: 2.84738\n",
      "INFO:tensorflow:global_step/sec: 0.0887355\n",
      "I1118 03:39:43.493090 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887355\n",
      "INFO:tensorflow:examples/sec: 2.83954\n",
      "I1118 03:39:43.493323 4435920320 tpu_estimator.py:2160] examples/sec: 2.83954\n",
      "INFO:tensorflow:global_step/sec: 0.0888135\n",
      "I1118 03:39:54.752578 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888135\n",
      "INFO:tensorflow:examples/sec: 2.84203\n",
      "I1118 03:39:54.752913 4435920320 tpu_estimator.py:2160] examples/sec: 2.84203\n",
      "INFO:tensorflow:global_step/sec: 0.089215\n",
      "I1118 03:40:05.961480 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089215\n",
      "INFO:tensorflow:examples/sec: 2.85488\n",
      "I1118 03:40:05.961701 4435920320 tpu_estimator.py:2160] examples/sec: 2.85488\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0889913\n",
      "I1118 03:40:17.198517 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889913\n",
      "INFO:tensorflow:examples/sec: 2.84772\n",
      "I1118 03:40:17.198750 4435920320 tpu_estimator.py:2160] examples/sec: 2.84772\n",
      "INFO:tensorflow:global_step/sec: 0.0895719\n",
      "I1118 03:40:28.362736 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895719\n",
      "INFO:tensorflow:examples/sec: 2.8663\n",
      "I1118 03:40:28.362967 4435920320 tpu_estimator.py:2160] examples/sec: 2.8663\n",
      "INFO:tensorflow:global_step/sec: 0.0896977\n",
      "I1118 03:40:39.511301 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896977\n",
      "INFO:tensorflow:examples/sec: 2.87033\n",
      "I1118 03:40:39.511512 4435920320 tpu_estimator.py:2160] examples/sec: 2.87033\n",
      "INFO:tensorflow:global_step/sec: 0.08966\n",
      "I1118 03:40:50.664546 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08966\n",
      "INFO:tensorflow:examples/sec: 2.86912\n",
      "I1118 03:40:50.664761 4435920320 tpu_estimator.py:2160] examples/sec: 2.86912\n",
      "INFO:tensorflow:global_step/sec: 0.0887354\n",
      "I1118 03:41:01.933983 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887354\n",
      "INFO:tensorflow:examples/sec: 2.83953\n",
      "I1118 03:41:01.934223 4435920320 tpu_estimator.py:2160] examples/sec: 2.83953\n",
      "INFO:tensorflow:global_step/sec: 0.088575\n",
      "I1118 03:41:13.223866 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088575\n",
      "INFO:tensorflow:examples/sec: 2.8344\n",
      "I1118 03:41:13.224104 4435920320 tpu_estimator.py:2160] examples/sec: 2.8344\n",
      "INFO:tensorflow:global_step/sec: 0.0891977\n",
      "I1118 03:41:24.434905 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891977\n",
      "INFO:tensorflow:examples/sec: 2.85433\n",
      "I1118 03:41:24.435132 4435920320 tpu_estimator.py:2160] examples/sec: 2.85433\n",
      "INFO:tensorflow:global_step/sec: 0.0890333\n",
      "I1118 03:41:35.666646 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890333\n",
      "INFO:tensorflow:examples/sec: 2.84907\n",
      "I1118 03:41:35.666863 4435920320 tpu_estimator.py:2160] examples/sec: 2.84907\n",
      "INFO:tensorflow:global_step/sec: 0.0891901\n",
      "I1118 03:41:46.878671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891901\n",
      "INFO:tensorflow:examples/sec: 2.85408\n",
      "I1118 03:41:46.878911 4435920320 tpu_estimator.py:2160] examples/sec: 2.85408\n",
      "INFO:tensorflow:global_step/sec: 0.0886521\n",
      "I1118 03:41:58.158704 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886521\n",
      "INFO:tensorflow:examples/sec: 2.83687\n",
      "I1118 03:41:58.158927 4435920320 tpu_estimator.py:2160] examples/sec: 2.83687\n",
      "INFO:tensorflow:global_step/sec: 0.0890936\n",
      "I1118 03:42:09.382849 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890936\n",
      "INFO:tensorflow:examples/sec: 2.85099\n",
      "I1118 03:42:09.383067 4435920320 tpu_estimator.py:2160] examples/sec: 2.85099\n",
      "INFO:tensorflow:global_step/sec: 0.0890601\n",
      "I1118 03:42:20.611219 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890601\n",
      "INFO:tensorflow:examples/sec: 2.84992\n",
      "I1118 03:42:20.611440 4435920320 tpu_estimator.py:2160] examples/sec: 2.84992\n",
      "INFO:tensorflow:global_step/sec: 0.0888333\n",
      "I1118 03:42:31.868264 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888333\n",
      "INFO:tensorflow:examples/sec: 2.84267\n",
      "I1118 03:42:31.868486 4435920320 tpu_estimator.py:2160] examples/sec: 2.84267\n",
      "INFO:tensorflow:global_step/sec: 0.089046\n",
      "I1118 03:42:43.098418 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089046\n",
      "INFO:tensorflow:examples/sec: 2.84947\n",
      "I1118 03:42:43.098641 4435920320 tpu_estimator.py:2160] examples/sec: 2.84947\n",
      "INFO:tensorflow:global_step/sec: 0.0892373\n",
      "I1118 03:42:54.304506 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892373\n",
      "INFO:tensorflow:examples/sec: 2.85559\n",
      "I1118 03:42:54.304738 4435920320 tpu_estimator.py:2160] examples/sec: 2.85559\n",
      "INFO:tensorflow:global_step/sec: 0.0892735\n",
      "I1118 03:43:05.506043 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892735\n",
      "INFO:tensorflow:examples/sec: 2.85675\n",
      "I1118 03:43:05.506266 4435920320 tpu_estimator.py:2160] examples/sec: 2.85675\n",
      "INFO:tensorflow:global_step/sec: 0.089143\n",
      "I1118 03:43:16.723978 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089143\n",
      "INFO:tensorflow:examples/sec: 2.85258\n",
      "I1118 03:43:16.724198 4435920320 tpu_estimator.py:2160] examples/sec: 2.85258\n",
      "INFO:tensorflow:global_step/sec: 0.0890602\n",
      "I1118 03:43:27.952322 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890602\n",
      "INFO:tensorflow:examples/sec: 2.84993\n",
      "I1118 03:43:27.952527 4435920320 tpu_estimator.py:2160] examples/sec: 2.84993\n",
      "INFO:tensorflow:global_step/sec: 0.0894412\n",
      "I1118 03:43:39.132843 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894412\n",
      "INFO:tensorflow:examples/sec: 2.86212\n",
      "I1118 03:43:39.133140 4435920320 tpu_estimator.py:2160] examples/sec: 2.86212\n",
      "INFO:tensorflow:global_step/sec: 0.0888109\n",
      "I1118 03:43:50.392730 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888109\n",
      "INFO:tensorflow:examples/sec: 2.84195\n",
      "I1118 03:43:50.392950 4435920320 tpu_estimator.py:2160] examples/sec: 2.84195\n",
      "INFO:tensorflow:global_step/sec: 0.0896148\n",
      "I1118 03:44:01.551587 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896148\n",
      "INFO:tensorflow:examples/sec: 2.86768\n",
      "I1118 03:44:01.551805 4435920320 tpu_estimator.py:2160] examples/sec: 2.86768\n",
      "INFO:tensorflow:global_step/sec: 0.0879671\n",
      "I1118 03:44:12.919492 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879671\n",
      "INFO:tensorflow:examples/sec: 2.81495\n",
      "I1118 03:44:12.919728 4435920320 tpu_estimator.py:2160] examples/sec: 2.81495\n",
      "INFO:tensorflow:global_step/sec: 0.0897614\n",
      "I1118 03:44:24.060128 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897614\n",
      "INFO:tensorflow:examples/sec: 2.87237\n",
      "I1118 03:44:24.060357 4435920320 tpu_estimator.py:2160] examples/sec: 2.87237\n",
      "INFO:tensorflow:global_step/sec: 0.089798\n",
      "I1118 03:44:35.196226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089798\n",
      "INFO:tensorflow:examples/sec: 2.87354\n",
      "I1118 03:44:35.196443 4435920320 tpu_estimator.py:2160] examples/sec: 2.87354\n",
      "INFO:tensorflow:global_step/sec: 0.0894156\n",
      "I1118 03:44:46.379955 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894156\n",
      "INFO:tensorflow:examples/sec: 2.8613\n",
      "I1118 03:44:46.380172 4435920320 tpu_estimator.py:2160] examples/sec: 2.8613\n",
      "INFO:tensorflow:global_step/sec: 0.0897144\n",
      "I1118 03:44:57.526448 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897144\n",
      "INFO:tensorflow:examples/sec: 2.87086\n",
      "I1118 03:44:57.526665 4435920320 tpu_estimator.py:2160] examples/sec: 2.87086\n",
      "INFO:tensorflow:global_step/sec: 0.0889637\n",
      "I1118 03:45:08.766991 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889637\n",
      "INFO:tensorflow:examples/sec: 2.84684\n",
      "I1118 03:45:08.767379 4435920320 tpu_estimator.py:2160] examples/sec: 2.84684\n",
      "INFO:tensorflow:global_step/sec: 0.0887227\n",
      "I1118 03:45:20.038072 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887227\n",
      "INFO:tensorflow:examples/sec: 2.83913\n",
      "I1118 03:45:20.038292 4435920320 tpu_estimator.py:2160] examples/sec: 2.83913\n",
      "INFO:tensorflow:global_step/sec: 0.0893697\n",
      "I1118 03:45:31.227535 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893697\n",
      "INFO:tensorflow:examples/sec: 2.85983\n",
      "I1118 03:45:31.227766 4435920320 tpu_estimator.py:2160] examples/sec: 2.85983\n",
      "INFO:tensorflow:global_step/sec: 0.0894613\n",
      "I1118 03:45:42.405560 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894613\n",
      "INFO:tensorflow:examples/sec: 2.86276\n",
      "I1118 03:45:42.405778 4435920320 tpu_estimator.py:2160] examples/sec: 2.86276\n",
      "INFO:tensorflow:global_step/sec: 0.0898917\n",
      "I1118 03:45:53.530031 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898917\n",
      "INFO:tensorflow:examples/sec: 2.87653\n",
      "I1118 03:45:53.530245 4435920320 tpu_estimator.py:2160] examples/sec: 2.87653\n",
      "INFO:tensorflow:global_step/sec: 0.0890293\n",
      "I1118 03:46:04.762298 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890293\n",
      "INFO:tensorflow:examples/sec: 2.84894\n",
      "I1118 03:46:04.762521 4435920320 tpu_estimator.py:2160] examples/sec: 2.84894\n",
      "INFO:tensorflow:global_step/sec: 0.0890573\n",
      "I1118 03:46:15.991024 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890573\n",
      "INFO:tensorflow:examples/sec: 2.84983\n",
      "I1118 03:46:15.991245 4435920320 tpu_estimator.py:2160] examples/sec: 2.84983\n",
      "INFO:tensorflow:global_step/sec: 0.0895173\n",
      "I1118 03:46:27.162050 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895173\n",
      "INFO:tensorflow:examples/sec: 2.86455\n",
      "I1118 03:46:27.162268 4435920320 tpu_estimator.py:2160] examples/sec: 2.86455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891051\n",
      "I1118 03:46:38.384752 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891051\n",
      "INFO:tensorflow:examples/sec: 2.85136\n",
      "I1118 03:46:38.384968 4435920320 tpu_estimator.py:2160] examples/sec: 2.85136\n",
      "INFO:tensorflow:global_step/sec: 0.0896288\n",
      "I1118 03:46:49.541860 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896288\n",
      "INFO:tensorflow:examples/sec: 2.86812\n",
      "I1118 03:46:49.542061 4435920320 tpu_estimator.py:2160] examples/sec: 2.86812\n",
      "INFO:tensorflow:global_step/sec: 0.0887211\n",
      "I1118 03:47:00.813160 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887211\n",
      "INFO:tensorflow:examples/sec: 2.83908\n",
      "I1118 03:47:00.813377 4435920320 tpu_estimator.py:2160] examples/sec: 2.83908\n",
      "INFO:tensorflow:global_step/sec: 0.0887272\n",
      "I1118 03:47:12.083663 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887272\n",
      "INFO:tensorflow:examples/sec: 2.83927\n",
      "I1118 03:47:12.083879 4435920320 tpu_estimator.py:2160] examples/sec: 2.83927\n",
      "INFO:tensorflow:global_step/sec: 0.0897082\n",
      "I1118 03:47:23.230911 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897082\n",
      "INFO:tensorflow:examples/sec: 2.87066\n",
      "I1118 03:47:23.231130 4435920320 tpu_estimator.py:2160] examples/sec: 2.87066\n",
      "INFO:tensorflow:global_step/sec: 0.089818\n",
      "I1118 03:47:34.364552 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089818\n",
      "INFO:tensorflow:examples/sec: 2.87417\n",
      "I1118 03:47:34.364776 4435920320 tpu_estimator.py:2160] examples/sec: 2.87417\n",
      "INFO:tensorflow:global_step/sec: 0.0894181\n",
      "I1118 03:47:45.547973 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894181\n",
      "INFO:tensorflow:examples/sec: 2.86138\n",
      "I1118 03:47:45.548200 4435920320 tpu_estimator.py:2160] examples/sec: 2.86138\n",
      "INFO:tensorflow:global_step/sec: 0.0895992\n",
      "I1118 03:47:56.708791 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895992\n",
      "INFO:tensorflow:examples/sec: 2.86717\n",
      "I1118 03:47:56.709020 4435920320 tpu_estimator.py:2160] examples/sec: 2.86717\n",
      "INFO:tensorflow:global_step/sec: 0.0891458\n",
      "I1118 03:48:07.926350 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891458\n",
      "INFO:tensorflow:examples/sec: 2.85267\n",
      "I1118 03:48:07.926563 4435920320 tpu_estimator.py:2160] examples/sec: 2.85267\n",
      "INFO:tensorflow:global_step/sec: 0.0889414\n",
      "I1118 03:48:19.169729 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889414\n",
      "INFO:tensorflow:examples/sec: 2.84612\n",
      "I1118 03:48:19.169954 4435920320 tpu_estimator.py:2160] examples/sec: 2.84612\n",
      "INFO:tensorflow:global_step/sec: 0.0897634\n",
      "I1118 03:48:30.310126 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897634\n",
      "INFO:tensorflow:examples/sec: 2.87243\n",
      "I1118 03:48:30.310354 4435920320 tpu_estimator.py:2160] examples/sec: 2.87243\n",
      "INFO:tensorflow:global_step/sec: 0.089501\n",
      "I1118 03:48:41.483177 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089501\n",
      "INFO:tensorflow:examples/sec: 2.86403\n",
      "I1118 03:48:41.483397 4435920320 tpu_estimator.py:2160] examples/sec: 2.86403\n",
      "INFO:tensorflow:global_step/sec: 0.0895478\n",
      "I1118 03:48:52.650395 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895478\n",
      "INFO:tensorflow:examples/sec: 2.86553\n",
      "I1118 03:48:52.650609 4435920320 tpu_estimator.py:2160] examples/sec: 2.86553\n",
      "INFO:tensorflow:global_step/sec: 0.0890791\n",
      "I1118 03:49:03.876391 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890791\n",
      "INFO:tensorflow:examples/sec: 2.85053\n",
      "I1118 03:49:03.876610 4435920320 tpu_estimator.py:2160] examples/sec: 2.85053\n",
      "INFO:tensorflow:global_step/sec: 0.0886307\n",
      "I1118 03:49:15.159154 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886307\n",
      "INFO:tensorflow:examples/sec: 2.83618\n",
      "I1118 03:49:15.159384 4435920320 tpu_estimator.py:2160] examples/sec: 2.83618\n",
      "INFO:tensorflow:global_step/sec: 0.0888968\n",
      "I1118 03:49:26.408144 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888968\n",
      "INFO:tensorflow:examples/sec: 2.8447\n",
      "I1118 03:49:26.408365 4435920320 tpu_estimator.py:2160] examples/sec: 2.8447\n",
      "INFO:tensorflow:global_step/sec: 0.0872995\n",
      "I1118 03:49:37.862946 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0872995\n",
      "INFO:tensorflow:examples/sec: 2.79359\n",
      "I1118 03:49:37.863177 4435920320 tpu_estimator.py:2160] examples/sec: 2.79359\n",
      "INFO:tensorflow:global_step/sec: 0.0893103\n",
      "I1118 03:49:49.059888 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893103\n",
      "INFO:tensorflow:examples/sec: 2.85793\n",
      "I1118 03:49:49.060127 4435920320 tpu_estimator.py:2160] examples/sec: 2.85793\n",
      "INFO:tensorflow:global_step/sec: 0.0896512\n",
      "I1118 03:50:00.214223 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896512\n",
      "INFO:tensorflow:examples/sec: 2.86884\n",
      "I1118 03:50:00.214463 4435920320 tpu_estimator.py:2160] examples/sec: 2.86884\n",
      "INFO:tensorflow:global_step/sec: 0.0897208\n",
      "I1118 03:50:11.359925 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897208\n",
      "INFO:tensorflow:examples/sec: 2.87106\n",
      "I1118 03:50:11.360143 4435920320 tpu_estimator.py:2160] examples/sec: 2.87106\n",
      "INFO:tensorflow:global_step/sec: 0.0894327\n",
      "I1118 03:50:22.541499 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894327\n",
      "INFO:tensorflow:examples/sec: 2.86185\n",
      "I1118 03:50:22.541722 4435920320 tpu_estimator.py:2160] examples/sec: 2.86185\n",
      "INFO:tensorflow:global_step/sec: 0.0889869\n",
      "I1118 03:50:33.779139 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889869\n",
      "INFO:tensorflow:examples/sec: 2.84758\n",
      "I1118 03:50:33.779354 4435920320 tpu_estimator.py:2160] examples/sec: 2.84758\n",
      "INFO:tensorflow:global_step/sec: 0.0887738\n",
      "I1118 03:50:45.043702 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887738\n",
      "INFO:tensorflow:examples/sec: 2.84076\n",
      "I1118 03:50:45.043930 4435920320 tpu_estimator.py:2160] examples/sec: 2.84076\n",
      "INFO:tensorflow:global_step/sec: 0.0895139\n",
      "I1118 03:50:56.215167 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895139\n",
      "INFO:tensorflow:examples/sec: 2.86444\n",
      "I1118 03:50:56.215386 4435920320 tpu_estimator.py:2160] examples/sec: 2.86444\n",
      "INFO:tensorflow:global_step/sec: 0.0891022\n",
      "I1118 03:51:07.438219 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891022\n",
      "INFO:tensorflow:examples/sec: 2.85127\n",
      "I1118 03:51:07.438445 4435920320 tpu_estimator.py:2160] examples/sec: 2.85127\n",
      "INFO:tensorflow:global_step/sec: 0.0888513\n",
      "I1118 03:51:18.692991 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888513\n",
      "INFO:tensorflow:examples/sec: 2.84324\n",
      "I1118 03:51:18.693219 4435920320 tpu_estimator.py:2160] examples/sec: 2.84324\n",
      "INFO:tensorflow:global_step/sec: 0.0891368\n",
      "I1118 03:51:29.911686 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891368\n",
      "INFO:tensorflow:examples/sec: 2.85238\n",
      "I1118 03:51:29.911906 4435920320 tpu_estimator.py:2160] examples/sec: 2.85238\n",
      "INFO:tensorflow:global_step/sec: 0.0895276\n",
      "I1118 03:51:41.081435 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895276\n",
      "INFO:tensorflow:examples/sec: 2.86488\n",
      "I1118 03:51:41.081668 4435920320 tpu_estimator.py:2160] examples/sec: 2.86488\n",
      "INFO:tensorflow:global_step/sec: 0.0894232\n",
      "I1118 03:51:52.264197 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894232\n",
      "INFO:tensorflow:examples/sec: 2.86154\n",
      "I1118 03:51:52.264415 4435920320 tpu_estimator.py:2160] examples/sec: 2.86154\n",
      "INFO:tensorflow:global_step/sec: 0.0890079\n",
      "I1118 03:52:03.499166 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890079\n",
      "INFO:tensorflow:examples/sec: 2.84825\n",
      "I1118 03:52:03.499397 4435920320 tpu_estimator.py:2160] examples/sec: 2.84825\n",
      "INFO:tensorflow:global_step/sec: 0.0890435\n",
      "I1118 03:52:14.729707 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890435\n",
      "INFO:tensorflow:examples/sec: 2.84939\n",
      "I1118 03:52:14.730037 4435920320 tpu_estimator.py:2160] examples/sec: 2.84939\n",
      "INFO:tensorflow:global_step/sec: 0.0892229\n",
      "I1118 03:52:25.937502 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892229\n",
      "INFO:tensorflow:examples/sec: 2.85513\n",
      "I1118 03:52:25.937722 4435920320 tpu_estimator.py:2160] examples/sec: 2.85513\n",
      "INFO:tensorflow:global_step/sec: 0.0888532\n",
      "I1118 03:52:37.192027 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888532\n",
      "INFO:tensorflow:examples/sec: 2.8433\n",
      "I1118 03:52:37.192254 4435920320 tpu_estimator.py:2160] examples/sec: 2.8433\n",
      "INFO:tensorflow:global_step/sec: 0.0895836\n",
      "I1118 03:52:48.354795 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895836\n",
      "INFO:tensorflow:examples/sec: 2.86668\n",
      "I1118 03:52:48.355014 4435920320 tpu_estimator.py:2160] examples/sec: 2.86668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0892922\n",
      "I1118 03:52:59.553994 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892922\n",
      "INFO:tensorflow:examples/sec: 2.85735\n",
      "I1118 03:52:59.554219 4435920320 tpu_estimator.py:2160] examples/sec: 2.85735\n",
      "INFO:tensorflow:global_step/sec: 0.0892499\n",
      "I1118 03:53:10.758476 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892499\n",
      "INFO:tensorflow:examples/sec: 2.856\n",
      "I1118 03:53:10.758691 4435920320 tpu_estimator.py:2160] examples/sec: 2.856\n",
      "INFO:tensorflow:global_step/sec: 0.0888739\n",
      "I1118 03:53:22.010375 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888739\n",
      "INFO:tensorflow:examples/sec: 2.84397\n",
      "I1118 03:53:22.010619 4435920320 tpu_estimator.py:2160] examples/sec: 2.84397\n",
      "INFO:tensorflow:global_step/sec: 0.0894201\n",
      "I1118 03:53:33.193542 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894201\n",
      "INFO:tensorflow:examples/sec: 2.86144\n",
      "I1118 03:53:33.193758 4435920320 tpu_estimator.py:2160] examples/sec: 2.86144\n",
      "INFO:tensorflow:global_step/sec: 0.0888009\n",
      "I1118 03:53:44.454709 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888009\n",
      "INFO:tensorflow:examples/sec: 2.84163\n",
      "I1118 03:53:44.455105 4435920320 tpu_estimator.py:2160] examples/sec: 2.84163\n",
      "INFO:tensorflow:global_step/sec: 0.0888909\n",
      "I1118 03:53:55.704445 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888909\n",
      "INFO:tensorflow:examples/sec: 2.84451\n",
      "I1118 03:53:55.704837 4435920320 tpu_estimator.py:2160] examples/sec: 2.84451\n",
      "INFO:tensorflow:global_step/sec: 0.0891093\n",
      "I1118 03:54:06.926614 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891093\n",
      "INFO:tensorflow:examples/sec: 2.8515\n",
      "I1118 03:54:06.926961 4435920320 tpu_estimator.py:2160] examples/sec: 2.8515\n",
      "INFO:tensorflow:global_step/sec: 0.0891223\n",
      "I1118 03:54:18.147150 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891223\n",
      "INFO:tensorflow:examples/sec: 2.85191\n",
      "I1118 03:54:18.147377 4435920320 tpu_estimator.py:2160] examples/sec: 2.85191\n",
      "INFO:tensorflow:global_step/sec: 0.0884942\n",
      "I1118 03:54:29.447318 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884942\n",
      "INFO:tensorflow:examples/sec: 2.83181\n",
      "I1118 03:54:29.447546 4435920320 tpu_estimator.py:2160] examples/sec: 2.83181\n",
      "INFO:tensorflow:global_step/sec: 0.0890853\n",
      "I1118 03:54:40.672512 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890853\n",
      "INFO:tensorflow:examples/sec: 2.85073\n",
      "I1118 03:54:40.672739 4435920320 tpu_estimator.py:2160] examples/sec: 2.85073\n",
      "INFO:tensorflow:global_step/sec: 0.089041\n",
      "I1118 03:54:51.903282 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089041\n",
      "INFO:tensorflow:examples/sec: 2.84931\n",
      "I1118 03:54:51.903499 4435920320 tpu_estimator.py:2160] examples/sec: 2.84931\n",
      "INFO:tensorflow:global_step/sec: 0.0894904\n",
      "I1118 03:55:03.077661 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894904\n",
      "INFO:tensorflow:examples/sec: 2.86369\n",
      "I1118 03:55:03.077877 4435920320 tpu_estimator.py:2160] examples/sec: 2.86369\n",
      "INFO:tensorflow:global_step/sec: 0.0891391\n",
      "I1118 03:55:14.296093 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891391\n",
      "INFO:tensorflow:examples/sec: 2.85245\n",
      "I1118 03:55:14.296339 4435920320 tpu_estimator.py:2160] examples/sec: 2.85245\n",
      "INFO:tensorflow:global_step/sec: 0.0894616\n",
      "I1118 03:55:25.474066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894616\n",
      "INFO:tensorflow:examples/sec: 2.86277\n",
      "I1118 03:55:25.474286 4435920320 tpu_estimator.py:2160] examples/sec: 2.86277\n",
      "INFO:tensorflow:global_step/sec: 0.0892726\n",
      "I1118 03:55:36.675720 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892726\n",
      "INFO:tensorflow:examples/sec: 2.85672\n",
      "I1118 03:55:36.676117 4435920320 tpu_estimator.py:2160] examples/sec: 2.85672\n",
      "INFO:tensorflow:global_step/sec: 0.0888894\n",
      "I1118 03:55:47.925662 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888894\n",
      "INFO:tensorflow:examples/sec: 2.84446\n",
      "I1118 03:55:47.925903 4435920320 tpu_estimator.py:2160] examples/sec: 2.84446\n",
      "INFO:tensorflow:global_step/sec: 0.0895159\n",
      "I1118 03:55:59.096858 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895159\n",
      "INFO:tensorflow:examples/sec: 2.86451\n",
      "I1118 03:55:59.097082 4435920320 tpu_estimator.py:2160] examples/sec: 2.86451\n",
      "INFO:tensorflow:global_step/sec: 0.0890525\n",
      "I1118 03:56:10.326202 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890525\n",
      "INFO:tensorflow:examples/sec: 2.84968\n",
      "I1118 03:56:10.326588 4435920320 tpu_estimator.py:2160] examples/sec: 2.84968\n",
      "INFO:tensorflow:global_step/sec: 0.0895203\n",
      "I1118 03:56:21.496846 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895203\n",
      "INFO:tensorflow:examples/sec: 2.86465\n",
      "I1118 03:56:21.497073 4435920320 tpu_estimator.py:2160] examples/sec: 2.86465\n",
      "INFO:tensorflow:global_step/sec: 0.0896628\n",
      "I1118 03:56:32.649762 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896628\n",
      "INFO:tensorflow:examples/sec: 2.86921\n",
      "I1118 03:56:32.649995 4435920320 tpu_estimator.py:2160] examples/sec: 2.86921\n",
      "INFO:tensorflow:global_step/sec: 0.0885681\n",
      "I1118 03:56:43.940507 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885681\n",
      "INFO:tensorflow:examples/sec: 2.83418\n",
      "I1118 03:56:43.940751 4435920320 tpu_estimator.py:2160] examples/sec: 2.83418\n",
      "INFO:tensorflow:global_step/sec: 0.0888731\n",
      "I1118 03:56:55.192491 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888731\n",
      "INFO:tensorflow:examples/sec: 2.84394\n",
      "I1118 03:56:55.192716 4435920320 tpu_estimator.py:2160] examples/sec: 2.84394\n",
      "INFO:tensorflow:global_step/sec: 0.0892071\n",
      "I1118 03:57:06.402368 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892071\n",
      "INFO:tensorflow:examples/sec: 2.85463\n",
      "I1118 03:57:06.402598 4435920320 tpu_estimator.py:2160] examples/sec: 2.85463\n",
      "INFO:tensorflow:global_step/sec: 0.0886064\n",
      "I1118 03:57:17.688210 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886064\n",
      "INFO:tensorflow:examples/sec: 2.83541\n",
      "I1118 03:57:17.688427 4435920320 tpu_estimator.py:2160] examples/sec: 2.83541\n",
      "INFO:tensorflow:global_step/sec: 0.0889738\n",
      "I1118 03:57:28.927495 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889738\n",
      "INFO:tensorflow:examples/sec: 2.84716\n",
      "I1118 03:57:28.927726 4435920320 tpu_estimator.py:2160] examples/sec: 2.84716\n",
      "INFO:tensorflow:global_step/sec: 0.0887096\n",
      "I1118 03:57:40.200223 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887096\n",
      "INFO:tensorflow:examples/sec: 2.83871\n",
      "I1118 03:57:40.200442 4435920320 tpu_estimator.py:2160] examples/sec: 2.83871\n",
      "INFO:tensorflow:global_step/sec: 0.0896374\n",
      "I1118 03:57:51.356290 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896374\n",
      "INFO:tensorflow:examples/sec: 2.8684\n",
      "I1118 03:57:51.356518 4435920320 tpu_estimator.py:2160] examples/sec: 2.8684\n",
      "INFO:tensorflow:global_step/sec: 0.0898065\n",
      "I1118 03:58:02.491337 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898065\n",
      "INFO:tensorflow:examples/sec: 2.87381\n",
      "I1118 03:58:02.491561 4435920320 tpu_estimator.py:2160] examples/sec: 2.87381\n",
      "INFO:tensorflow:global_step/sec: 0.0892221\n",
      "I1118 03:58:13.699322 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892221\n",
      "INFO:tensorflow:examples/sec: 2.85511\n",
      "I1118 03:58:13.699542 4435920320 tpu_estimator.py:2160] examples/sec: 2.85511\n",
      "INFO:tensorflow:global_step/sec: 0.0890457\n",
      "I1118 03:58:24.929526 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890457\n",
      "INFO:tensorflow:examples/sec: 2.84946\n",
      "I1118 03:58:24.929749 4435920320 tpu_estimator.py:2160] examples/sec: 2.84946\n",
      "INFO:tensorflow:global_step/sec: 0.0889844\n",
      "I1118 03:58:36.167454 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889844\n",
      "INFO:tensorflow:examples/sec: 2.8475\n",
      "I1118 03:58:36.167695 4435920320 tpu_estimator.py:2160] examples/sec: 2.8475\n",
      "INFO:tensorflow:global_step/sec: 0.0894862\n",
      "I1118 03:58:47.342341 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894862\n",
      "INFO:tensorflow:examples/sec: 2.86356\n",
      "I1118 03:58:47.342560 4435920320 tpu_estimator.py:2160] examples/sec: 2.86356\n",
      "INFO:tensorflow:global_step/sec: 0.0897769\n",
      "I1118 03:58:58.481059 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897769\n",
      "INFO:tensorflow:examples/sec: 2.87286\n",
      "I1118 03:58:58.481274 4435920320 tpu_estimator.py:2160] examples/sec: 2.87286\n",
      "INFO:tensorflow:global_step/sec: 0.0886609\n",
      "I1118 03:59:09.759989 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886609\n",
      "INFO:tensorflow:examples/sec: 2.83715\n",
      "I1118 03:59:09.760205 4435920320 tpu_estimator.py:2160] examples/sec: 2.83715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888167\n",
      "I1118 03:59:21.019141 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888167\n",
      "INFO:tensorflow:examples/sec: 2.84213\n",
      "I1118 03:59:21.019371 4435920320 tpu_estimator.py:2160] examples/sec: 2.84213\n",
      "INFO:tensorflow:global_step/sec: 0.0890192\n",
      "I1118 03:59:32.252674 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890192\n",
      "INFO:tensorflow:examples/sec: 2.84861\n",
      "I1118 03:59:32.252901 4435920320 tpu_estimator.py:2160] examples/sec: 2.84861\n",
      "INFO:tensorflow:global_step/sec: 0.0893103\n",
      "I1118 03:59:43.449585 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893103\n",
      "INFO:tensorflow:examples/sec: 2.85793\n",
      "I1118 03:59:43.449805 4435920320 tpu_estimator.py:2160] examples/sec: 2.85793\n",
      "INFO:tensorflow:global_step/sec: 0.0890875\n",
      "I1118 03:59:54.674526 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890875\n",
      "INFO:tensorflow:examples/sec: 2.8508\n",
      "I1118 03:59:54.674746 4435920320 tpu_estimator.py:2160] examples/sec: 2.8508\n",
      "INFO:tensorflow:global_step/sec: 0.089508\n",
      "I1118 04:00:05.846689 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089508\n",
      "INFO:tensorflow:examples/sec: 2.86426\n",
      "I1118 04:00:05.846911 4435920320 tpu_estimator.py:2160] examples/sec: 2.86426\n",
      "INFO:tensorflow:global_step/sec: 0.0888647\n",
      "I1118 04:00:17.099750 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888647\n",
      "INFO:tensorflow:examples/sec: 2.84367\n",
      "I1118 04:00:17.099975 4435920320 tpu_estimator.py:2160] examples/sec: 2.84367\n",
      "INFO:tensorflow:global_step/sec: 0.0893808\n",
      "I1118 04:00:28.287828 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893808\n",
      "INFO:tensorflow:examples/sec: 2.86019\n",
      "I1118 04:00:28.288045 4435920320 tpu_estimator.py:2160] examples/sec: 2.86019\n",
      "INFO:tensorflow:global_step/sec: 0.0892899\n",
      "I1118 04:00:39.487298 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892899\n",
      "INFO:tensorflow:examples/sec: 2.85728\n",
      "I1118 04:00:39.487511 4435920320 tpu_estimator.py:2160] examples/sec: 2.85728\n",
      "INFO:tensorflow:global_step/sec: 0.0886796\n",
      "I1118 04:00:50.763855 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886796\n",
      "INFO:tensorflow:examples/sec: 2.83775\n",
      "I1118 04:00:50.764072 4435920320 tpu_estimator.py:2160] examples/sec: 2.83775\n",
      "INFO:tensorflow:global_step/sec: 0.0891696\n",
      "I1118 04:01:01.978458 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891696\n",
      "INFO:tensorflow:examples/sec: 2.85343\n",
      "I1118 04:01:01.978874 4435920320 tpu_estimator.py:2160] examples/sec: 2.85343\n",
      "INFO:tensorflow:global_step/sec: 0.0885248\n",
      "I1118 04:01:13.274725 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885248\n",
      "INFO:tensorflow:examples/sec: 2.83279\n",
      "I1118 04:01:13.274963 4435920320 tpu_estimator.py:2160] examples/sec: 2.83279\n",
      "INFO:tensorflow:global_step/sec: 0.0896793\n",
      "I1118 04:01:24.425587 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896793\n",
      "INFO:tensorflow:examples/sec: 2.86974\n",
      "I1118 04:01:24.425815 4435920320 tpu_estimator.py:2160] examples/sec: 2.86974\n",
      "INFO:tensorflow:global_step/sec: 0.0896365\n",
      "I1118 04:01:35.581743 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896365\n",
      "INFO:tensorflow:examples/sec: 2.86837\n",
      "I1118 04:01:35.581991 4435920320 tpu_estimator.py:2160] examples/sec: 2.86837\n",
      "INFO:tensorflow:global_step/sec: 0.0896117\n",
      "I1118 04:01:46.741080 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896117\n",
      "INFO:tensorflow:examples/sec: 2.86758\n",
      "I1118 04:01:46.741381 4435920320 tpu_estimator.py:2160] examples/sec: 2.86758\n",
      "INFO:tensorflow:global_step/sec: 0.0890081\n",
      "I1118 04:01:57.975920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890081\n",
      "INFO:tensorflow:examples/sec: 2.84826\n",
      "I1118 04:01:57.976152 4435920320 tpu_estimator.py:2160] examples/sec: 2.84826\n",
      "INFO:tensorflow:global_step/sec: 0.0891543\n",
      "I1118 04:02:09.192432 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891543\n",
      "INFO:tensorflow:examples/sec: 2.85294\n",
      "I1118 04:02:09.192664 4435920320 tpu_estimator.py:2160] examples/sec: 2.85294\n",
      "INFO:tensorflow:global_step/sec: 0.0890407\n",
      "I1118 04:02:20.423249 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890407\n",
      "INFO:tensorflow:examples/sec: 2.8493\n",
      "I1118 04:02:20.423475 4435920320 tpu_estimator.py:2160] examples/sec: 2.8493\n",
      "INFO:tensorflow:global_step/sec: 0.0895698\n",
      "I1118 04:02:31.587715 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895698\n",
      "INFO:tensorflow:examples/sec: 2.86623\n",
      "I1118 04:02:31.587930 4435920320 tpu_estimator.py:2160] examples/sec: 2.86623\n",
      "INFO:tensorflow:global_step/sec: 0.0879349\n",
      "I1118 04:02:42.959774 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879349\n",
      "INFO:tensorflow:examples/sec: 2.81392\n",
      "I1118 04:02:42.959994 4435920320 tpu_estimator.py:2160] examples/sec: 2.81392\n",
      "INFO:tensorflow:global_step/sec: 0.0893539\n",
      "I1118 04:02:54.151220 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893539\n",
      "INFO:tensorflow:examples/sec: 2.85933\n",
      "I1118 04:02:54.151439 4435920320 tpu_estimator.py:2160] examples/sec: 2.85933\n",
      "INFO:tensorflow:global_step/sec: 0.0886518\n",
      "I1118 04:03:05.431352 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886518\n",
      "INFO:tensorflow:examples/sec: 2.83686\n",
      "I1118 04:03:05.431586 4435920320 tpu_estimator.py:2160] examples/sec: 2.83686\n",
      "INFO:tensorflow:global_step/sec: 0.0882226\n",
      "I1118 04:03:16.766284 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882226\n",
      "INFO:tensorflow:examples/sec: 2.82312\n",
      "I1118 04:03:16.766543 4435920320 tpu_estimator.py:2160] examples/sec: 2.82312\n",
      "INFO:tensorflow:global_step/sec: 0.0889641\n",
      "I1118 04:03:28.006778 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889641\n",
      "INFO:tensorflow:examples/sec: 2.84685\n",
      "I1118 04:03:28.006994 4435920320 tpu_estimator.py:2160] examples/sec: 2.84685\n",
      "INFO:tensorflow:global_step/sec: 0.0905542\n",
      "I1118 04:03:39.049810 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0905542\n",
      "INFO:tensorflow:examples/sec: 2.89773\n",
      "I1118 04:03:39.049973 4435920320 tpu_estimator.py:2160] examples/sec: 2.89773\n",
      "INFO:tensorflow:global_step/sec: 0.0885496\n",
      "I1118 04:03:50.342998 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885496\n",
      "INFO:tensorflow:examples/sec: 2.83359\n",
      "I1118 04:03:50.343243 4435920320 tpu_estimator.py:2160] examples/sec: 2.83359\n",
      "INFO:tensorflow:global_step/sec: 0.0894491\n",
      "I1118 04:04:01.522526 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894491\n",
      "INFO:tensorflow:examples/sec: 2.86237\n",
      "I1118 04:04:01.522744 4435920320 tpu_estimator.py:2160] examples/sec: 2.86237\n",
      "INFO:tensorflow:global_step/sec: 0.0897747\n",
      "I1118 04:04:12.661534 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897747\n",
      "INFO:tensorflow:examples/sec: 2.87279\n",
      "I1118 04:04:12.661769 4435920320 tpu_estimator.py:2160] examples/sec: 2.87279\n",
      "INFO:tensorflow:global_step/sec: 0.0897218\n",
      "I1118 04:04:23.807084 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897218\n",
      "INFO:tensorflow:examples/sec: 2.8711\n",
      "I1118 04:04:23.807300 4435920320 tpu_estimator.py:2160] examples/sec: 2.8711\n",
      "INFO:tensorflow:global_step/sec: 0.0896103\n",
      "I1118 04:04:34.966516 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896103\n",
      "INFO:tensorflow:examples/sec: 2.86753\n",
      "I1118 04:04:34.966736 4435920320 tpu_estimator.py:2160] examples/sec: 2.86753\n",
      "INFO:tensorflow:global_step/sec: 0.0892491\n",
      "I1118 04:04:46.171116 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892491\n",
      "INFO:tensorflow:examples/sec: 2.85597\n",
      "I1118 04:04:46.171340 4435920320 tpu_estimator.py:2160] examples/sec: 2.85597\n",
      "INFO:tensorflow:global_step/sec: 0.0892976\n",
      "I1118 04:04:57.369615 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892976\n",
      "INFO:tensorflow:examples/sec: 2.85752\n",
      "I1118 04:04:57.370009 4435920320 tpu_estimator.py:2160] examples/sec: 2.85752\n",
      "INFO:tensorflow:global_step/sec: 0.0889813\n",
      "I1118 04:05:08.607924 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889813\n",
      "INFO:tensorflow:examples/sec: 2.8474\n",
      "I1118 04:05:08.608144 4435920320 tpu_estimator.py:2160] examples/sec: 2.8474\n",
      "INFO:tensorflow:global_step/sec: 0.0885417\n",
      "I1118 04:05:19.902038 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885417\n",
      "INFO:tensorflow:examples/sec: 2.83333\n",
      "I1118 04:05:19.902260 4435920320 tpu_estimator.py:2160] examples/sec: 2.83333\n",
      "INFO:tensorflow:global_step/sec: 0.0893763\n",
      "I1118 04:05:31.090705 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893763\n",
      "INFO:tensorflow:examples/sec: 2.86004\n",
      "I1118 04:05:31.090921 4435920320 tpu_estimator.py:2160] examples/sec: 2.86004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893024\n",
      "I1118 04:05:42.288608 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893024\n",
      "INFO:tensorflow:examples/sec: 2.85768\n",
      "I1118 04:05:42.288855 4435920320 tpu_estimator.py:2160] examples/sec: 2.85768\n",
      "INFO:tensorflow:global_step/sec: 0.09005\n",
      "I1118 04:05:53.393651 4435920320 tpu_estimator.py:2159] global_step/sec: 0.09005\n",
      "INFO:tensorflow:examples/sec: 2.8816\n",
      "I1118 04:05:53.393872 4435920320 tpu_estimator.py:2160] examples/sec: 2.8816\n",
      "INFO:tensorflow:global_step/sec: 0.0895248\n",
      "I1118 04:06:04.563638 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895248\n",
      "INFO:tensorflow:examples/sec: 2.86479\n",
      "I1118 04:06:04.563865 4435920320 tpu_estimator.py:2160] examples/sec: 2.86479\n",
      "INFO:tensorflow:global_step/sec: 0.0898408\n",
      "I1118 04:06:15.694443 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898408\n",
      "INFO:tensorflow:examples/sec: 2.87491\n",
      "I1118 04:06:15.694693 4435920320 tpu_estimator.py:2160] examples/sec: 2.87491\n",
      "INFO:tensorflow:global_step/sec: 0.0895414\n",
      "I1118 04:06:26.862459 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895414\n",
      "INFO:tensorflow:examples/sec: 2.86532\n",
      "I1118 04:06:26.862685 4435920320 tpu_estimator.py:2160] examples/sec: 2.86532\n",
      "INFO:tensorflow:global_step/sec: 0.0891021\n",
      "I1118 04:06:38.085538 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891021\n",
      "INFO:tensorflow:examples/sec: 2.85127\n",
      "I1118 04:06:38.085760 4435920320 tpu_estimator.py:2160] examples/sec: 2.85127\n",
      "INFO:tensorflow:global_step/sec: 0.08947\n",
      "I1118 04:06:49.262488 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08947\n",
      "INFO:tensorflow:examples/sec: 2.86304\n",
      "I1118 04:06:49.262706 4435920320 tpu_estimator.py:2160] examples/sec: 2.86304\n",
      "INFO:tensorflow:global_step/sec: 0.0895452\n",
      "I1118 04:07:00.430010 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895452\n",
      "INFO:tensorflow:examples/sec: 2.86545\n",
      "I1118 04:07:00.430231 4435920320 tpu_estimator.py:2160] examples/sec: 2.86545\n",
      "INFO:tensorflow:global_step/sec: 0.0893313\n",
      "I1118 04:07:11.624305 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893313\n",
      "INFO:tensorflow:examples/sec: 2.8586\n",
      "I1118 04:07:11.624526 4435920320 tpu_estimator.py:2160] examples/sec: 2.8586\n",
      "INFO:tensorflow:global_step/sec: 0.0885515\n",
      "I1118 04:07:22.917186 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885515\n",
      "INFO:tensorflow:examples/sec: 2.83365\n",
      "I1118 04:07:22.917589 4435920320 tpu_estimator.py:2160] examples/sec: 2.83365\n",
      "INFO:tensorflow:global_step/sec: 0.0892376\n",
      "I1118 04:07:34.123217 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892376\n",
      "INFO:tensorflow:examples/sec: 2.8556\n",
      "I1118 04:07:34.123445 4435920320 tpu_estimator.py:2160] examples/sec: 2.8556\n",
      "INFO:tensorflow:global_step/sec: 0.0884564\n",
      "I1118 04:07:45.428205 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884564\n",
      "INFO:tensorflow:examples/sec: 2.8306\n",
      "I1118 04:07:45.428412 4435920320 tpu_estimator.py:2160] examples/sec: 2.8306\n",
      "INFO:tensorflow:global_step/sec: 0.0893387\n",
      "I1118 04:07:56.621587 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893387\n",
      "INFO:tensorflow:examples/sec: 2.85884\n",
      "I1118 04:07:56.621818 4435920320 tpu_estimator.py:2160] examples/sec: 2.85884\n",
      "INFO:tensorflow:global_step/sec: 0.0888101\n",
      "I1118 04:08:07.881549 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888101\n",
      "INFO:tensorflow:examples/sec: 2.84192\n",
      "I1118 04:08:07.881790 4435920320 tpu_estimator.py:2160] examples/sec: 2.84192\n",
      "INFO:tensorflow:global_step/sec: 0.0892346\n",
      "I1118 04:08:19.087977 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892346\n",
      "INFO:tensorflow:examples/sec: 2.85551\n",
      "I1118 04:08:19.088206 4435920320 tpu_estimator.py:2160] examples/sec: 2.85551\n",
      "INFO:tensorflow:global_step/sec: 0.0894532\n",
      "I1118 04:08:30.266988 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894532\n",
      "INFO:tensorflow:examples/sec: 2.8625\n",
      "I1118 04:08:30.267205 4435920320 tpu_estimator.py:2160] examples/sec: 2.8625\n",
      "INFO:tensorflow:global_step/sec: 0.0898186\n",
      "I1118 04:08:41.400549 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898186\n",
      "INFO:tensorflow:examples/sec: 2.87419\n",
      "I1118 04:08:41.400768 4435920320 tpu_estimator.py:2160] examples/sec: 2.87419\n",
      "INFO:tensorflow:global_step/sec: 0.0895401\n",
      "I1118 04:08:52.568725 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895401\n",
      "INFO:tensorflow:examples/sec: 2.86528\n",
      "I1118 04:08:52.568943 4435920320 tpu_estimator.py:2160] examples/sec: 2.86528\n",
      "INFO:tensorflow:global_step/sec: 0.0899832\n",
      "I1118 04:09:03.681906 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899832\n",
      "INFO:tensorflow:examples/sec: 2.87946\n",
      "I1118 04:09:03.682124 4435920320 tpu_estimator.py:2160] examples/sec: 2.87946\n",
      "INFO:tensorflow:global_step/sec: 0.0891239\n",
      "I1118 04:09:14.902251 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891239\n",
      "INFO:tensorflow:examples/sec: 2.85197\n",
      "I1118 04:09:14.902479 4435920320 tpu_estimator.py:2160] examples/sec: 2.85197\n",
      "INFO:tensorflow:global_step/sec: 0.0891358\n",
      "I1118 04:09:26.121113 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891358\n",
      "INFO:tensorflow:examples/sec: 2.85234\n",
      "I1118 04:09:26.121342 4435920320 tpu_estimator.py:2160] examples/sec: 2.85234\n",
      "INFO:tensorflow:global_step/sec: 0.0893074\n",
      "I1118 04:09:37.318363 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893074\n",
      "INFO:tensorflow:examples/sec: 2.85784\n",
      "I1118 04:09:37.318578 4435920320 tpu_estimator.py:2160] examples/sec: 2.85784\n",
      "INFO:tensorflow:global_step/sec: 0.0900279\n",
      "I1118 04:09:48.426021 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900279\n",
      "INFO:tensorflow:examples/sec: 2.88089\n",
      "I1118 04:09:48.426232 4435920320 tpu_estimator.py:2160] examples/sec: 2.88089\n",
      "INFO:tensorflow:global_step/sec: 0.0896476\n",
      "I1118 04:09:59.580816 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896476\n",
      "INFO:tensorflow:examples/sec: 2.86872\n",
      "I1118 04:09:59.581034 4435920320 tpu_estimator.py:2160] examples/sec: 2.86872\n",
      "INFO:tensorflow:global_step/sec: 0.0893748\n",
      "I1118 04:10:10.769662 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893748\n",
      "INFO:tensorflow:examples/sec: 2.85999\n",
      "I1118 04:10:10.769889 4435920320 tpu_estimator.py:2160] examples/sec: 2.85999\n",
      "INFO:tensorflow:global_step/sec: 0.0884744\n",
      "I1118 04:10:22.072378 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884744\n",
      "INFO:tensorflow:examples/sec: 2.83118\n",
      "I1118 04:10:22.072612 4435920320 tpu_estimator.py:2160] examples/sec: 2.83118\n",
      "INFO:tensorflow:global_step/sec: 0.0888311\n",
      "I1118 04:10:33.329691 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888311\n",
      "INFO:tensorflow:examples/sec: 2.84259\n",
      "I1118 04:10:33.329930 4435920320 tpu_estimator.py:2160] examples/sec: 2.84259\n",
      "INFO:tensorflow:global_step/sec: 0.0899097\n",
      "I1118 04:10:44.452020 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899097\n",
      "INFO:tensorflow:examples/sec: 2.87711\n",
      "I1118 04:10:44.452262 4435920320 tpu_estimator.py:2160] examples/sec: 2.87711\n",
      "INFO:tensorflow:global_step/sec: 0.0897643\n",
      "I1118 04:10:55.592246 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897643\n",
      "INFO:tensorflow:examples/sec: 2.87246\n",
      "I1118 04:10:55.592469 4435920320 tpu_estimator.py:2160] examples/sec: 2.87246\n",
      "INFO:tensorflow:global_step/sec: 0.0887288\n",
      "I1118 04:11:06.862542 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887288\n",
      "INFO:tensorflow:examples/sec: 2.83932\n",
      "I1118 04:11:06.862761 4435920320 tpu_estimator.py:2160] examples/sec: 2.83932\n",
      "INFO:tensorflow:global_step/sec: 0.0889897\n",
      "I1118 04:11:18.099808 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889897\n",
      "INFO:tensorflow:examples/sec: 2.84767\n",
      "I1118 04:11:18.100043 4435920320 tpu_estimator.py:2160] examples/sec: 2.84767\n",
      "INFO:tensorflow:global_step/sec: 0.0897213\n",
      "I1118 04:11:29.245428 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897213\n",
      "INFO:tensorflow:examples/sec: 2.87108\n",
      "I1118 04:11:29.245645 4435920320 tpu_estimator.py:2160] examples/sec: 2.87108\n",
      "INFO:tensorflow:global_step/sec: 0.0896318\n",
      "I1118 04:11:40.402189 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896318\n",
      "INFO:tensorflow:examples/sec: 2.86822\n",
      "I1118 04:11:40.402417 4435920320 tpu_estimator.py:2160] examples/sec: 2.86822\n",
      "INFO:tensorflow:global_step/sec: 0.0892671\n",
      "I1118 04:11:51.604531 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892671\n",
      "INFO:tensorflow:examples/sec: 2.85655\n",
      "I1118 04:11:51.604760 4435920320 tpu_estimator.py:2160] examples/sec: 2.85655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0890454\n",
      "I1118 04:12:02.834756 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890454\n",
      "INFO:tensorflow:examples/sec: 2.84945\n",
      "I1118 04:12:02.834990 4435920320 tpu_estimator.py:2160] examples/sec: 2.84945\n",
      "INFO:tensorflow:global_step/sec: 0.0886686\n",
      "I1118 04:12:14.112696 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886686\n",
      "INFO:tensorflow:examples/sec: 2.8374\n",
      "I1118 04:12:14.112920 4435920320 tpu_estimator.py:2160] examples/sec: 2.8374\n",
      "INFO:tensorflow:global_step/sec: 0.0894965\n",
      "I1118 04:12:25.286311 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894965\n",
      "INFO:tensorflow:examples/sec: 2.86389\n",
      "I1118 04:12:25.286543 4435920320 tpu_estimator.py:2160] examples/sec: 2.86389\n",
      "INFO:tensorflow:global_step/sec: 0.0896184\n",
      "I1118 04:12:36.444746 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896184\n",
      "INFO:tensorflow:examples/sec: 2.86779\n",
      "I1118 04:12:36.444965 4435920320 tpu_estimator.py:2160] examples/sec: 2.86779\n",
      "INFO:tensorflow:global_step/sec: 0.0892777\n",
      "I1118 04:12:47.645729 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892777\n",
      "INFO:tensorflow:examples/sec: 2.85689\n",
      "I1118 04:12:47.645945 4435920320 tpu_estimator.py:2160] examples/sec: 2.85689\n",
      "INFO:tensorflow:global_step/sec: 0.0883296\n",
      "I1118 04:12:58.966978 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883296\n",
      "INFO:tensorflow:examples/sec: 2.82655\n",
      "I1118 04:12:58.967207 4435920320 tpu_estimator.py:2160] examples/sec: 2.82655\n",
      "INFO:tensorflow:Saving checkpoints for 2000 into ./bert_output/model.ckpt.\n",
      "I1118 04:13:10.258767 4435920320 basic_session_run_hooks.py:606] Saving checkpoints for 2000 into ./bert_output/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.0661896\n",
      "I1118 04:13:14.074990 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0661896\n",
      "INFO:tensorflow:examples/sec: 2.11807\n",
      "I1118 04:13:14.075180 4435920320 tpu_estimator.py:2160] examples/sec: 2.11807\n",
      "INFO:tensorflow:global_step/sec: 0.0890212\n",
      "I1118 04:13:25.308393 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890212\n",
      "INFO:tensorflow:examples/sec: 2.84868\n",
      "I1118 04:13:25.308656 4435920320 tpu_estimator.py:2160] examples/sec: 2.84868\n",
      "INFO:tensorflow:global_step/sec: 0.0892117\n",
      "I1118 04:13:36.517657 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892117\n",
      "INFO:tensorflow:examples/sec: 2.85477\n",
      "I1118 04:13:36.517868 4435920320 tpu_estimator.py:2160] examples/sec: 2.85477\n",
      "INFO:tensorflow:global_step/sec: 0.0895416\n",
      "I1118 04:13:47.685652 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895416\n",
      "INFO:tensorflow:examples/sec: 2.86533\n",
      "I1118 04:13:47.685864 4435920320 tpu_estimator.py:2160] examples/sec: 2.86533\n",
      "INFO:tensorflow:global_step/sec: 0.0893052\n",
      "I1118 04:13:58.883213 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893052\n",
      "INFO:tensorflow:examples/sec: 2.85777\n",
      "I1118 04:13:58.883431 4435920320 tpu_estimator.py:2160] examples/sec: 2.85777\n",
      "INFO:tensorflow:global_step/sec: 0.0891461\n",
      "I1118 04:14:10.100769 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891461\n",
      "INFO:tensorflow:examples/sec: 2.85268\n",
      "I1118 04:14:10.101004 4435920320 tpu_estimator.py:2160] examples/sec: 2.85268\n",
      "INFO:tensorflow:global_step/sec: 0.0896312\n",
      "I1118 04:14:21.257586 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896312\n",
      "INFO:tensorflow:examples/sec: 2.8682\n",
      "I1118 04:14:21.257812 4435920320 tpu_estimator.py:2160] examples/sec: 2.8682\n",
      "INFO:tensorflow:global_step/sec: 0.0895924\n",
      "I1118 04:14:32.419246 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895924\n",
      "INFO:tensorflow:examples/sec: 2.86696\n",
      "I1118 04:14:32.419462 4435920320 tpu_estimator.py:2160] examples/sec: 2.86696\n",
      "INFO:tensorflow:global_step/sec: 0.0896896\n",
      "I1118 04:14:43.568826 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896896\n",
      "INFO:tensorflow:examples/sec: 2.87007\n",
      "I1118 04:14:43.569053 4435920320 tpu_estimator.py:2160] examples/sec: 2.87007\n",
      "INFO:tensorflow:global_step/sec: 0.0893429\n",
      "I1118 04:14:54.761646 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893429\n",
      "INFO:tensorflow:examples/sec: 2.85897\n",
      "I1118 04:14:54.761861 4435920320 tpu_estimator.py:2160] examples/sec: 2.85897\n",
      "INFO:tensorflow:global_step/sec: 0.0897466\n",
      "I1118 04:15:05.904139 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897466\n",
      "INFO:tensorflow:examples/sec: 2.87189\n",
      "I1118 04:15:05.904383 4435920320 tpu_estimator.py:2160] examples/sec: 2.87189\n",
      "INFO:tensorflow:global_step/sec: 0.0893792\n",
      "I1118 04:15:17.092427 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893792\n",
      "INFO:tensorflow:examples/sec: 2.86013\n",
      "I1118 04:15:17.092677 4435920320 tpu_estimator.py:2160] examples/sec: 2.86013\n",
      "INFO:tensorflow:global_step/sec: 0.0883015\n",
      "I1118 04:15:28.417245 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883015\n",
      "INFO:tensorflow:examples/sec: 2.82565\n",
      "I1118 04:15:28.417471 4435920320 tpu_estimator.py:2160] examples/sec: 2.82565\n",
      "INFO:tensorflow:global_step/sec: 0.0894614\n",
      "I1118 04:15:39.595246 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894614\n",
      "INFO:tensorflow:examples/sec: 2.86277\n",
      "I1118 04:15:39.595468 4435920320 tpu_estimator.py:2160] examples/sec: 2.86277\n",
      "INFO:tensorflow:global_step/sec: 0.0895771\n",
      "I1118 04:15:50.758815 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895771\n",
      "INFO:tensorflow:examples/sec: 2.86647\n",
      "I1118 04:15:50.759037 4435920320 tpu_estimator.py:2160] examples/sec: 2.86647\n",
      "INFO:tensorflow:global_step/sec: 0.0892477\n",
      "I1118 04:16:01.963580 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892477\n",
      "INFO:tensorflow:examples/sec: 2.85593\n",
      "I1118 04:16:01.963816 4435920320 tpu_estimator.py:2160] examples/sec: 2.85593\n",
      "INFO:tensorflow:global_step/sec: 0.0895009\n",
      "I1118 04:16:13.136656 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895009\n",
      "INFO:tensorflow:examples/sec: 2.86403\n",
      "I1118 04:16:13.136877 4435920320 tpu_estimator.py:2160] examples/sec: 2.86403\n",
      "INFO:tensorflow:global_step/sec: 0.0895895\n",
      "I1118 04:16:24.298673 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895895\n",
      "INFO:tensorflow:examples/sec: 2.86687\n",
      "I1118 04:16:24.298899 4435920320 tpu_estimator.py:2160] examples/sec: 2.86687\n",
      "INFO:tensorflow:global_step/sec: 0.0890117\n",
      "I1118 04:16:35.533149 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890117\n",
      "INFO:tensorflow:examples/sec: 2.84837\n",
      "I1118 04:16:35.533374 4435920320 tpu_estimator.py:2160] examples/sec: 2.84837\n",
      "INFO:tensorflow:global_step/sec: 0.08892\n",
      "I1118 04:16:46.779217 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08892\n",
      "INFO:tensorflow:examples/sec: 2.84544\n",
      "I1118 04:16:46.779429 4435920320 tpu_estimator.py:2160] examples/sec: 2.84544\n",
      "INFO:tensorflow:global_step/sec: 0.0892897\n",
      "I1118 04:16:57.978718 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892897\n",
      "INFO:tensorflow:examples/sec: 2.85727\n",
      "I1118 04:16:57.978935 4435920320 tpu_estimator.py:2160] examples/sec: 2.85727\n",
      "INFO:tensorflow:global_step/sec: 0.0892747\n",
      "I1118 04:17:09.180101 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892747\n",
      "INFO:tensorflow:examples/sec: 2.85679\n",
      "I1118 04:17:09.180316 4435920320 tpu_estimator.py:2160] examples/sec: 2.85679\n",
      "INFO:tensorflow:global_step/sec: 0.0884424\n",
      "I1118 04:17:20.486921 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884424\n",
      "INFO:tensorflow:examples/sec: 2.83016\n",
      "I1118 04:17:20.487164 4435920320 tpu_estimator.py:2160] examples/sec: 2.83016\n",
      "INFO:tensorflow:global_step/sec: 0.0895563\n",
      "I1118 04:17:31.653088 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895563\n",
      "INFO:tensorflow:examples/sec: 2.8658\n",
      "I1118 04:17:31.653366 4435920320 tpu_estimator.py:2160] examples/sec: 2.8658\n",
      "INFO:tensorflow:global_step/sec: 0.0887828\n",
      "I1118 04:17:42.916492 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887828\n",
      "INFO:tensorflow:examples/sec: 2.84105\n",
      "I1118 04:17:42.916709 4435920320 tpu_estimator.py:2160] examples/sec: 2.84105\n",
      "INFO:tensorflow:global_step/sec: 0.0890315\n",
      "I1118 04:17:54.148468 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890315\n",
      "INFO:tensorflow:examples/sec: 2.84901\n",
      "I1118 04:17:54.148680 4435920320 tpu_estimator.py:2160] examples/sec: 2.84901\n",
      "INFO:tensorflow:global_step/sec: 0.0890548\n",
      "I1118 04:18:05.377521 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890548\n",
      "INFO:tensorflow:examples/sec: 2.84975\n",
      "I1118 04:18:05.377763 4435920320 tpu_estimator.py:2160] examples/sec: 2.84975\n",
      "INFO:tensorflow:global_step/sec: 0.0894361\n",
      "I1118 04:18:16.558680 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894361\n",
      "INFO:tensorflow:examples/sec: 2.86195\n",
      "I1118 04:18:16.558904 4435920320 tpu_estimator.py:2160] examples/sec: 2.86195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0892369\n",
      "I1118 04:18:27.764801 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892369\n",
      "INFO:tensorflow:examples/sec: 2.85558\n",
      "I1118 04:18:27.765016 4435920320 tpu_estimator.py:2160] examples/sec: 2.85558\n",
      "INFO:tensorflow:global_step/sec: 0.0890359\n",
      "I1118 04:18:38.996226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890359\n",
      "INFO:tensorflow:examples/sec: 2.84915\n",
      "I1118 04:18:38.996448 4435920320 tpu_estimator.py:2160] examples/sec: 2.84915\n",
      "INFO:tensorflow:global_step/sec: 0.0893431\n",
      "I1118 04:18:50.189027 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893431\n",
      "INFO:tensorflow:examples/sec: 2.85898\n",
      "I1118 04:18:50.189243 4435920320 tpu_estimator.py:2160] examples/sec: 2.85898\n",
      "INFO:tensorflow:global_step/sec: 0.0889817\n",
      "I1118 04:19:01.427290 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889817\n",
      "INFO:tensorflow:examples/sec: 2.84741\n",
      "I1118 04:19:01.427512 4435920320 tpu_estimator.py:2160] examples/sec: 2.84741\n",
      "INFO:tensorflow:global_step/sec: 0.0885181\n",
      "I1118 04:19:12.724443 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885181\n",
      "INFO:tensorflow:examples/sec: 2.83258\n",
      "I1118 04:19:12.724667 4435920320 tpu_estimator.py:2160] examples/sec: 2.83258\n",
      "INFO:tensorflow:global_step/sec: 0.0882947\n",
      "I1118 04:19:24.050127 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882947\n",
      "INFO:tensorflow:examples/sec: 2.82543\n",
      "I1118 04:19:24.050341 4435920320 tpu_estimator.py:2160] examples/sec: 2.82543\n",
      "INFO:tensorflow:global_step/sec: 0.0892366\n",
      "I1118 04:19:35.256302 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892366\n",
      "INFO:tensorflow:examples/sec: 2.85557\n",
      "I1118 04:19:35.256532 4435920320 tpu_estimator.py:2160] examples/sec: 2.85557\n",
      "INFO:tensorflow:global_step/sec: 0.0893243\n",
      "I1118 04:19:46.451452 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893243\n",
      "INFO:tensorflow:examples/sec: 2.85838\n",
      "I1118 04:19:46.451666 4435920320 tpu_estimator.py:2160] examples/sec: 2.85838\n",
      "INFO:tensorflow:global_step/sec: 0.0892046\n",
      "I1118 04:19:57.661638 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892046\n",
      "INFO:tensorflow:examples/sec: 2.85455\n",
      "I1118 04:19:57.661859 4435920320 tpu_estimator.py:2160] examples/sec: 2.85455\n",
      "INFO:tensorflow:global_step/sec: 0.0885647\n",
      "I1118 04:20:08.952825 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885647\n",
      "INFO:tensorflow:examples/sec: 2.83407\n",
      "I1118 04:20:08.953047 4435920320 tpu_estimator.py:2160] examples/sec: 2.83407\n",
      "INFO:tensorflow:global_step/sec: 0.0886855\n",
      "I1118 04:20:20.228640 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886855\n",
      "INFO:tensorflow:examples/sec: 2.83794\n",
      "I1118 04:20:20.228879 4435920320 tpu_estimator.py:2160] examples/sec: 2.83794\n",
      "INFO:tensorflow:global_step/sec: 0.0892308\n",
      "I1118 04:20:31.435535 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892308\n",
      "INFO:tensorflow:examples/sec: 2.85538\n",
      "I1118 04:20:31.435768 4435920320 tpu_estimator.py:2160] examples/sec: 2.85538\n",
      "INFO:tensorflow:global_step/sec: 0.0899769\n",
      "I1118 04:20:42.549510 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899769\n",
      "INFO:tensorflow:examples/sec: 2.87926\n",
      "I1118 04:20:42.549733 4435920320 tpu_estimator.py:2160] examples/sec: 2.87926\n",
      "INFO:tensorflow:global_step/sec: 0.0891475\n",
      "I1118 04:20:53.766841 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891475\n",
      "INFO:tensorflow:examples/sec: 2.85272\n",
      "I1118 04:20:53.767057 4435920320 tpu_estimator.py:2160] examples/sec: 2.85272\n",
      "INFO:tensorflow:global_step/sec: 0.0886956\n",
      "I1118 04:21:05.041359 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886956\n",
      "INFO:tensorflow:examples/sec: 2.83826\n",
      "I1118 04:21:05.041583 4435920320 tpu_estimator.py:2160] examples/sec: 2.83826\n",
      "INFO:tensorflow:global_step/sec: 0.088962\n",
      "I1118 04:21:16.282096 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088962\n",
      "INFO:tensorflow:examples/sec: 2.84678\n",
      "I1118 04:21:16.282310 4435920320 tpu_estimator.py:2160] examples/sec: 2.84678\n",
      "INFO:tensorflow:global_step/sec: 0.0893188\n",
      "I1118 04:21:27.477969 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893188\n",
      "INFO:tensorflow:examples/sec: 2.8582\n",
      "I1118 04:21:27.478192 4435920320 tpu_estimator.py:2160] examples/sec: 2.8582\n",
      "INFO:tensorflow:global_step/sec: 0.088538\n",
      "I1118 04:21:38.772559 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088538\n",
      "INFO:tensorflow:examples/sec: 2.83322\n",
      "I1118 04:21:38.772821 4435920320 tpu_estimator.py:2160] examples/sec: 2.83322\n",
      "INFO:tensorflow:global_step/sec: 0.0891183\n",
      "I1118 04:21:49.993597 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891183\n",
      "INFO:tensorflow:examples/sec: 2.85178\n",
      "I1118 04:21:49.993818 4435920320 tpu_estimator.py:2160] examples/sec: 2.85178\n",
      "INFO:tensorflow:global_step/sec: 0.0891182\n",
      "I1118 04:22:01.214653 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891182\n",
      "INFO:tensorflow:examples/sec: 2.85178\n",
      "I1118 04:22:01.214885 4435920320 tpu_estimator.py:2160] examples/sec: 2.85178\n",
      "INFO:tensorflow:global_step/sec: 0.0889146\n",
      "I1118 04:22:12.461386 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889146\n",
      "INFO:tensorflow:examples/sec: 2.84527\n",
      "I1118 04:22:12.461605 4435920320 tpu_estimator.py:2160] examples/sec: 2.84527\n",
      "INFO:tensorflow:global_step/sec: 0.0893509\n",
      "I1118 04:22:23.653242 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893509\n",
      "INFO:tensorflow:examples/sec: 2.85923\n",
      "I1118 04:22:23.653640 4435920320 tpu_estimator.py:2160] examples/sec: 2.85923\n",
      "INFO:tensorflow:global_step/sec: 0.0886364\n",
      "I1118 04:22:34.935281 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886364\n",
      "INFO:tensorflow:examples/sec: 2.83636\n",
      "I1118 04:22:34.935516 4435920320 tpu_estimator.py:2160] examples/sec: 2.83636\n",
      "INFO:tensorflow:global_step/sec: 0.0889508\n",
      "I1118 04:22:46.177438 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889508\n",
      "INFO:tensorflow:examples/sec: 2.84643\n",
      "I1118 04:22:46.177656 4435920320 tpu_estimator.py:2160] examples/sec: 2.84643\n",
      "INFO:tensorflow:global_step/sec: 0.0890714\n",
      "I1118 04:22:57.404402 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890714\n",
      "INFO:tensorflow:examples/sec: 2.85029\n",
      "I1118 04:22:57.404628 4435920320 tpu_estimator.py:2160] examples/sec: 2.85029\n",
      "INFO:tensorflow:global_step/sec: 0.08899\n",
      "I1118 04:23:08.641618 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08899\n",
      "INFO:tensorflow:examples/sec: 2.84768\n",
      "I1118 04:23:08.641850 4435920320 tpu_estimator.py:2160] examples/sec: 2.84768\n",
      "INFO:tensorflow:global_step/sec: 0.0882955\n",
      "I1118 04:23:19.967216 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882955\n",
      "INFO:tensorflow:examples/sec: 2.82545\n",
      "I1118 04:23:19.967436 4435920320 tpu_estimator.py:2160] examples/sec: 2.82545\n",
      "INFO:tensorflow:global_step/sec: 0.089026\n",
      "I1118 04:23:31.199892 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089026\n",
      "INFO:tensorflow:examples/sec: 2.84883\n",
      "I1118 04:23:31.200118 4435920320 tpu_estimator.py:2160] examples/sec: 2.84883\n",
      "INFO:tensorflow:global_step/sec: 0.0889017\n",
      "I1118 04:23:42.448281 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889017\n",
      "INFO:tensorflow:examples/sec: 2.84485\n",
      "I1118 04:23:42.448503 4435920320 tpu_estimator.py:2160] examples/sec: 2.84485\n",
      "INFO:tensorflow:global_step/sec: 0.0888046\n",
      "I1118 04:23:53.708956 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888046\n",
      "INFO:tensorflow:examples/sec: 2.84175\n",
      "I1118 04:23:53.709182 4435920320 tpu_estimator.py:2160] examples/sec: 2.84175\n",
      "INFO:tensorflow:global_step/sec: 0.0890059\n",
      "I1118 04:24:04.944175 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890059\n",
      "INFO:tensorflow:examples/sec: 2.84819\n",
      "I1118 04:24:04.944393 4435920320 tpu_estimator.py:2160] examples/sec: 2.84819\n",
      "INFO:tensorflow:global_step/sec: 0.0894016\n",
      "I1118 04:24:16.129657 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894016\n",
      "INFO:tensorflow:examples/sec: 2.86085\n",
      "I1118 04:24:16.129878 4435920320 tpu_estimator.py:2160] examples/sec: 2.86085\n",
      "INFO:tensorflow:global_step/sec: 0.089191\n",
      "I1118 04:24:27.341547 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089191\n",
      "INFO:tensorflow:examples/sec: 2.85411\n",
      "I1118 04:24:27.341765 4435920320 tpu_estimator.py:2160] examples/sec: 2.85411\n",
      "INFO:tensorflow:global_step/sec: 0.0890918\n",
      "I1118 04:24:38.565927 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890918\n",
      "INFO:tensorflow:examples/sec: 2.85094\n",
      "I1118 04:24:38.566143 4435920320 tpu_estimator.py:2160] examples/sec: 2.85094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891361\n",
      "I1118 04:24:49.784729 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891361\n",
      "INFO:tensorflow:examples/sec: 2.85235\n",
      "I1118 04:24:49.784942 4435920320 tpu_estimator.py:2160] examples/sec: 2.85235\n",
      "INFO:tensorflow:global_step/sec: 0.0894608\n",
      "I1118 04:25:00.962812 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894608\n",
      "INFO:tensorflow:examples/sec: 2.86274\n",
      "I1118 04:25:00.963028 4435920320 tpu_estimator.py:2160] examples/sec: 2.86274\n",
      "INFO:tensorflow:global_step/sec: 0.0889375\n",
      "I1118 04:25:12.206661 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889375\n",
      "INFO:tensorflow:examples/sec: 2.846\n",
      "I1118 04:25:12.206882 4435920320 tpu_estimator.py:2160] examples/sec: 2.846\n",
      "INFO:tensorflow:global_step/sec: 0.0884922\n",
      "I1118 04:25:23.507113 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884922\n",
      "INFO:tensorflow:examples/sec: 2.83175\n",
      "I1118 04:25:23.507370 4435920320 tpu_estimator.py:2160] examples/sec: 2.83175\n",
      "INFO:tensorflow:global_step/sec: 0.0896139\n",
      "I1118 04:25:34.666089 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896139\n",
      "INFO:tensorflow:examples/sec: 2.86765\n",
      "I1118 04:25:34.666313 4435920320 tpu_estimator.py:2160] examples/sec: 2.86765\n",
      "INFO:tensorflow:global_step/sec: 0.0886286\n",
      "I1118 04:25:45.949115 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886286\n",
      "INFO:tensorflow:examples/sec: 2.83612\n",
      "I1118 04:25:45.949331 4435920320 tpu_estimator.py:2160] examples/sec: 2.83612\n",
      "INFO:tensorflow:global_step/sec: 0.0886186\n",
      "I1118 04:25:57.233453 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886186\n",
      "INFO:tensorflow:examples/sec: 2.83579\n",
      "I1118 04:25:57.233871 4435920320 tpu_estimator.py:2160] examples/sec: 2.83579\n",
      "INFO:tensorflow:global_step/sec: 0.089118\n",
      "I1118 04:26:08.454525 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089118\n",
      "INFO:tensorflow:examples/sec: 2.85178\n",
      "I1118 04:26:08.454754 4435920320 tpu_estimator.py:2160] examples/sec: 2.85178\n",
      "INFO:tensorflow:global_step/sec: 0.0895244\n",
      "I1118 04:26:19.624665 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895244\n",
      "INFO:tensorflow:examples/sec: 2.86478\n",
      "I1118 04:26:19.625061 4435920320 tpu_estimator.py:2160] examples/sec: 2.86478\n",
      "INFO:tensorflow:global_step/sec: 0.0887547\n",
      "I1118 04:26:30.891664 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887547\n",
      "INFO:tensorflow:examples/sec: 2.84015\n",
      "I1118 04:26:30.891882 4435920320 tpu_estimator.py:2160] examples/sec: 2.84015\n",
      "INFO:tensorflow:global_step/sec: 0.0887295\n",
      "I1118 04:26:42.161911 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887295\n",
      "INFO:tensorflow:examples/sec: 2.83934\n",
      "I1118 04:26:42.162156 4435920320 tpu_estimator.py:2160] examples/sec: 2.83934\n",
      "INFO:tensorflow:global_step/sec: 0.0890648\n",
      "I1118 04:26:53.389666 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890648\n",
      "INFO:tensorflow:examples/sec: 2.85007\n",
      "I1118 04:26:53.389901 4435920320 tpu_estimator.py:2160] examples/sec: 2.85007\n",
      "INFO:tensorflow:global_step/sec: 0.0885239\n",
      "I1118 04:27:04.686047 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885239\n",
      "INFO:tensorflow:examples/sec: 2.83277\n",
      "I1118 04:27:04.686277 4435920320 tpu_estimator.py:2160] examples/sec: 2.83277\n",
      "INFO:tensorflow:global_step/sec: 0.0890916\n",
      "I1118 04:27:15.910423 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890916\n",
      "INFO:tensorflow:examples/sec: 2.85093\n",
      "I1118 04:27:15.910651 4435920320 tpu_estimator.py:2160] examples/sec: 2.85093\n",
      "INFO:tensorflow:global_step/sec: 0.0894637\n",
      "I1118 04:27:27.088161 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894637\n",
      "INFO:tensorflow:examples/sec: 2.86284\n",
      "I1118 04:27:27.088390 4435920320 tpu_estimator.py:2160] examples/sec: 2.86284\n",
      "INFO:tensorflow:global_step/sec: 0.0893855\n",
      "I1118 04:27:38.275656 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893855\n",
      "INFO:tensorflow:examples/sec: 2.86033\n",
      "I1118 04:27:38.275877 4435920320 tpu_estimator.py:2160] examples/sec: 2.86033\n",
      "INFO:tensorflow:global_step/sec: 0.0890523\n",
      "I1118 04:27:49.505010 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890523\n",
      "INFO:tensorflow:examples/sec: 2.84967\n",
      "I1118 04:27:49.505225 4435920320 tpu_estimator.py:2160] examples/sec: 2.84967\n",
      "INFO:tensorflow:global_step/sec: 0.0890617\n",
      "I1118 04:28:00.733184 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890617\n",
      "INFO:tensorflow:examples/sec: 2.84997\n",
      "I1118 04:28:00.733401 4435920320 tpu_estimator.py:2160] examples/sec: 2.84997\n",
      "INFO:tensorflow:global_step/sec: 0.0888577\n",
      "I1118 04:28:11.987142 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888577\n",
      "INFO:tensorflow:examples/sec: 2.84345\n",
      "I1118 04:28:11.987371 4435920320 tpu_estimator.py:2160] examples/sec: 2.84345\n",
      "INFO:tensorflow:global_step/sec: 0.0888813\n",
      "I1118 04:28:23.238097 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888813\n",
      "INFO:tensorflow:examples/sec: 2.8442\n",
      "I1118 04:28:23.238320 4435920320 tpu_estimator.py:2160] examples/sec: 2.8442\n",
      "INFO:tensorflow:global_step/sec: 0.0890407\n",
      "I1118 04:28:34.468921 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890407\n",
      "INFO:tensorflow:examples/sec: 2.8493\n",
      "I1118 04:28:34.469164 4435920320 tpu_estimator.py:2160] examples/sec: 2.8493\n",
      "INFO:tensorflow:global_step/sec: 0.0888297\n",
      "I1118 04:28:45.726423 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888297\n",
      "INFO:tensorflow:examples/sec: 2.84255\n",
      "I1118 04:28:45.726639 4435920320 tpu_estimator.py:2160] examples/sec: 2.84255\n",
      "INFO:tensorflow:global_step/sec: 0.0888396\n",
      "I1118 04:28:56.982648 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888396\n",
      "INFO:tensorflow:examples/sec: 2.84287\n",
      "I1118 04:28:56.982877 4435920320 tpu_estimator.py:2160] examples/sec: 2.84287\n",
      "INFO:tensorflow:global_step/sec: 0.0883869\n",
      "I1118 04:29:08.296528 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883869\n",
      "INFO:tensorflow:examples/sec: 2.82838\n",
      "I1118 04:29:08.296751 4435920320 tpu_estimator.py:2160] examples/sec: 2.82838\n",
      "INFO:tensorflow:global_step/sec: 0.0877059\n",
      "I1118 04:29:19.698286 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0877059\n",
      "INFO:tensorflow:examples/sec: 2.80659\n",
      "I1118 04:29:19.698496 4435920320 tpu_estimator.py:2160] examples/sec: 2.80659\n",
      "INFO:tensorflow:global_step/sec: 0.087992\n",
      "I1118 04:29:31.062936 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087992\n",
      "INFO:tensorflow:examples/sec: 2.81574\n",
      "I1118 04:29:31.063145 4435920320 tpu_estimator.py:2160] examples/sec: 2.81574\n",
      "INFO:tensorflow:global_step/sec: 0.0892858\n",
      "I1118 04:29:42.262938 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892858\n",
      "INFO:tensorflow:examples/sec: 2.85715\n",
      "I1118 04:29:42.263159 4435920320 tpu_estimator.py:2160] examples/sec: 2.85715\n",
      "INFO:tensorflow:global_step/sec: 0.0887803\n",
      "I1118 04:29:53.526713 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887803\n",
      "INFO:tensorflow:examples/sec: 2.84097\n",
      "I1118 04:29:53.526940 4435920320 tpu_estimator.py:2160] examples/sec: 2.84097\n",
      "INFO:tensorflow:global_step/sec: 0.0886619\n",
      "I1118 04:30:04.805505 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886619\n",
      "INFO:tensorflow:examples/sec: 2.83718\n",
      "I1118 04:30:04.805727 4435920320 tpu_estimator.py:2160] examples/sec: 2.83718\n",
      "INFO:tensorflow:global_step/sec: 0.0893172\n",
      "I1118 04:30:16.001559 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893172\n",
      "INFO:tensorflow:examples/sec: 2.85815\n",
      "I1118 04:30:16.001780 4435920320 tpu_estimator.py:2160] examples/sec: 2.85815\n",
      "INFO:tensorflow:global_step/sec: 0.0895035\n",
      "I1118 04:30:27.174299 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895035\n",
      "INFO:tensorflow:examples/sec: 2.86411\n",
      "I1118 04:30:27.174516 4435920320 tpu_estimator.py:2160] examples/sec: 2.86411\n",
      "INFO:tensorflow:global_step/sec: 0.0895799\n",
      "I1118 04:30:38.337519 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895799\n",
      "INFO:tensorflow:examples/sec: 2.86656\n",
      "I1118 04:30:38.337735 4435920320 tpu_estimator.py:2160] examples/sec: 2.86656\n",
      "INFO:tensorflow:global_step/sec: 0.0888966\n",
      "I1118 04:30:49.586550 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888966\n",
      "INFO:tensorflow:examples/sec: 2.84469\n",
      "I1118 04:30:49.586772 4435920320 tpu_estimator.py:2160] examples/sec: 2.84469\n",
      "INFO:tensorflow:global_step/sec: 0.0889054\n",
      "I1118 04:31:00.834464 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889054\n",
      "INFO:tensorflow:examples/sec: 2.84497\n",
      "I1118 04:31:00.834682 4435920320 tpu_estimator.py:2160] examples/sec: 2.84497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888854\n",
      "I1118 04:31:12.084918 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888854\n",
      "INFO:tensorflow:examples/sec: 2.84433\n",
      "I1118 04:31:12.085130 4435920320 tpu_estimator.py:2160] examples/sec: 2.84433\n",
      "INFO:tensorflow:global_step/sec: 0.0889799\n",
      "I1118 04:31:23.323400 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889799\n",
      "INFO:tensorflow:examples/sec: 2.84736\n",
      "I1118 04:31:23.323621 4435920320 tpu_estimator.py:2160] examples/sec: 2.84736\n",
      "INFO:tensorflow:global_step/sec: 0.089759\n",
      "I1118 04:31:34.464346 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089759\n",
      "INFO:tensorflow:examples/sec: 2.87229\n",
      "I1118 04:31:34.464590 4435920320 tpu_estimator.py:2160] examples/sec: 2.87229\n",
      "INFO:tensorflow:global_step/sec: 0.0891582\n",
      "I1118 04:31:45.680389 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891582\n",
      "INFO:tensorflow:examples/sec: 2.85306\n",
      "I1118 04:31:45.680613 4435920320 tpu_estimator.py:2160] examples/sec: 2.85306\n",
      "INFO:tensorflow:global_step/sec: 0.088089\n",
      "I1118 04:31:57.032476 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088089\n",
      "INFO:tensorflow:examples/sec: 2.81885\n",
      "I1118 04:31:57.032684 4435920320 tpu_estimator.py:2160] examples/sec: 2.81885\n",
      "INFO:tensorflow:global_step/sec: 0.0888231\n",
      "I1118 04:32:08.290858 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888231\n",
      "INFO:tensorflow:examples/sec: 2.84234\n",
      "I1118 04:32:08.291095 4435920320 tpu_estimator.py:2160] examples/sec: 2.84234\n",
      "INFO:tensorflow:global_step/sec: 0.0890404\n",
      "I1118 04:32:19.521712 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890404\n",
      "INFO:tensorflow:examples/sec: 2.84929\n",
      "I1118 04:32:19.521929 4435920320 tpu_estimator.py:2160] examples/sec: 2.84929\n",
      "INFO:tensorflow:global_step/sec: 0.0888232\n",
      "I1118 04:32:30.780051 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888232\n",
      "INFO:tensorflow:examples/sec: 2.84234\n",
      "I1118 04:32:30.780277 4435920320 tpu_estimator.py:2160] examples/sec: 2.84234\n",
      "INFO:tensorflow:global_step/sec: 0.0889435\n",
      "I1118 04:32:42.023138 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889435\n",
      "INFO:tensorflow:examples/sec: 2.84619\n",
      "I1118 04:32:42.023365 4435920320 tpu_estimator.py:2160] examples/sec: 2.84619\n",
      "INFO:tensorflow:global_step/sec: 0.0889399\n",
      "I1118 04:32:53.266681 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889399\n",
      "INFO:tensorflow:examples/sec: 2.84608\n",
      "I1118 04:32:53.266898 4435920320 tpu_estimator.py:2160] examples/sec: 2.84608\n",
      "INFO:tensorflow:global_step/sec: 0.0888824\n",
      "I1118 04:33:04.517509 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888824\n",
      "INFO:tensorflow:examples/sec: 2.84424\n",
      "I1118 04:33:04.517755 4435920320 tpu_estimator.py:2160] examples/sec: 2.84424\n",
      "INFO:tensorflow:global_step/sec: 0.0887548\n",
      "I1118 04:33:15.784501 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887548\n",
      "INFO:tensorflow:examples/sec: 2.84016\n",
      "I1118 04:33:15.784717 4435920320 tpu_estimator.py:2160] examples/sec: 2.84016\n",
      "INFO:tensorflow:global_step/sec: 0.0886119\n",
      "I1118 04:33:27.069661 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886119\n",
      "INFO:tensorflow:examples/sec: 2.83558\n",
      "I1118 04:33:27.069881 4435920320 tpu_estimator.py:2160] examples/sec: 2.83558\n",
      "INFO:tensorflow:global_step/sec: 0.0893386\n",
      "I1118 04:33:38.263034 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893386\n",
      "INFO:tensorflow:examples/sec: 2.85883\n",
      "I1118 04:33:38.263252 4435920320 tpu_estimator.py:2160] examples/sec: 2.85883\n",
      "INFO:tensorflow:global_step/sec: 0.0898935\n",
      "I1118 04:33:49.387312 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898935\n",
      "INFO:tensorflow:examples/sec: 2.87659\n",
      "I1118 04:33:49.387534 4435920320 tpu_estimator.py:2160] examples/sec: 2.87659\n",
      "INFO:tensorflow:global_step/sec: 0.089602\n",
      "I1118 04:34:00.547776 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089602\n",
      "INFO:tensorflow:examples/sec: 2.86726\n",
      "I1118 04:34:00.548011 4435920320 tpu_estimator.py:2160] examples/sec: 2.86726\n",
      "INFO:tensorflow:global_step/sec: 0.0887437\n",
      "I1118 04:34:11.816184 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887437\n",
      "INFO:tensorflow:examples/sec: 2.8398\n",
      "I1118 04:34:11.816399 4435920320 tpu_estimator.py:2160] examples/sec: 2.8398\n",
      "INFO:tensorflow:global_step/sec: 0.0892691\n",
      "I1118 04:34:23.018285 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892691\n",
      "INFO:tensorflow:examples/sec: 2.85661\n",
      "I1118 04:34:23.018530 4435920320 tpu_estimator.py:2160] examples/sec: 2.85661\n",
      "INFO:tensorflow:global_step/sec: 0.089049\n",
      "I1118 04:34:34.248075 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089049\n",
      "INFO:tensorflow:examples/sec: 2.84957\n",
      "I1118 04:34:34.248293 4435920320 tpu_estimator.py:2160] examples/sec: 2.84957\n",
      "INFO:tensorflow:global_step/sec: 0.0889925\n",
      "I1118 04:34:45.484960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889925\n",
      "INFO:tensorflow:examples/sec: 2.84776\n",
      "I1118 04:34:45.485186 4435920320 tpu_estimator.py:2160] examples/sec: 2.84776\n",
      "INFO:tensorflow:global_step/sec: 0.0893851\n",
      "I1118 04:34:56.672520 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893851\n",
      "INFO:tensorflow:examples/sec: 2.86032\n",
      "I1118 04:34:56.672751 4435920320 tpu_estimator.py:2160] examples/sec: 2.86032\n",
      "INFO:tensorflow:global_step/sec: 0.0887035\n",
      "I1118 04:35:07.946040 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887035\n",
      "INFO:tensorflow:examples/sec: 2.83851\n",
      "I1118 04:35:07.946274 4435920320 tpu_estimator.py:2160] examples/sec: 2.83851\n",
      "INFO:tensorflow:global_step/sec: 0.0888455\n",
      "I1118 04:35:19.201507 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888455\n",
      "INFO:tensorflow:examples/sec: 2.84305\n",
      "I1118 04:35:19.201705 4435920320 tpu_estimator.py:2160] examples/sec: 2.84305\n",
      "INFO:tensorflow:global_step/sec: 0.0890257\n",
      "I1118 04:35:30.434238 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890257\n",
      "INFO:tensorflow:examples/sec: 2.84882\n",
      "I1118 04:35:30.434459 4435920320 tpu_estimator.py:2160] examples/sec: 2.84882\n",
      "INFO:tensorflow:global_step/sec: 0.0897912\n",
      "I1118 04:35:41.571180 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897912\n",
      "INFO:tensorflow:examples/sec: 2.87332\n",
      "I1118 04:35:41.571479 4435920320 tpu_estimator.py:2160] examples/sec: 2.87332\n",
      "INFO:tensorflow:global_step/sec: 0.0891538\n",
      "I1118 04:35:52.787755 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891538\n",
      "INFO:tensorflow:examples/sec: 2.85292\n",
      "I1118 04:35:52.787961 4435920320 tpu_estimator.py:2160] examples/sec: 2.85292\n",
      "INFO:tensorflow:global_step/sec: 0.0894487\n",
      "I1118 04:36:03.967360 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894487\n",
      "INFO:tensorflow:examples/sec: 2.86236\n",
      "I1118 04:36:03.967589 4435920320 tpu_estimator.py:2160] examples/sec: 2.86236\n",
      "INFO:tensorflow:global_step/sec: 0.0888492\n",
      "I1118 04:36:15.222385 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888492\n",
      "INFO:tensorflow:examples/sec: 2.84317\n",
      "I1118 04:36:15.222604 4435920320 tpu_estimator.py:2160] examples/sec: 2.84317\n",
      "INFO:tensorflow:global_step/sec: 0.089229\n",
      "I1118 04:36:26.429519 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089229\n",
      "INFO:tensorflow:examples/sec: 2.85533\n",
      "I1118 04:36:26.429742 4435920320 tpu_estimator.py:2160] examples/sec: 2.85533\n",
      "INFO:tensorflow:global_step/sec: 0.0894707\n",
      "I1118 04:36:37.606347 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894707\n",
      "INFO:tensorflow:examples/sec: 2.86306\n",
      "I1118 04:36:37.606580 4435920320 tpu_estimator.py:2160] examples/sec: 2.86306\n",
      "INFO:tensorflow:global_step/sec: 0.0889921\n",
      "I1118 04:36:48.843271 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889921\n",
      "INFO:tensorflow:examples/sec: 2.84775\n",
      "I1118 04:36:48.843470 4435920320 tpu_estimator.py:2160] examples/sec: 2.84775\n",
      "INFO:tensorflow:global_step/sec: 0.0889842\n",
      "I1118 04:37:00.081265 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889842\n",
      "INFO:tensorflow:examples/sec: 2.84749\n",
      "I1118 04:37:00.081485 4435920320 tpu_estimator.py:2160] examples/sec: 2.84749\n",
      "INFO:tensorflow:global_step/sec: 0.0891694\n",
      "I1118 04:37:11.295845 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891694\n",
      "INFO:tensorflow:examples/sec: 2.85342\n",
      "I1118 04:37:11.296065 4435920320 tpu_estimator.py:2160] examples/sec: 2.85342\n",
      "INFO:tensorflow:global_step/sec: 0.0891789\n",
      "I1118 04:37:22.509242 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891789\n",
      "INFO:tensorflow:examples/sec: 2.85373\n",
      "I1118 04:37:22.509449 4435920320 tpu_estimator.py:2160] examples/sec: 2.85373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893315\n",
      "I1118 04:37:33.703514 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893315\n",
      "INFO:tensorflow:examples/sec: 2.85861\n",
      "I1118 04:37:33.703735 4435920320 tpu_estimator.py:2160] examples/sec: 2.85861\n",
      "INFO:tensorflow:global_step/sec: 0.0887126\n",
      "I1118 04:37:44.975880 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887126\n",
      "INFO:tensorflow:examples/sec: 2.8388\n",
      "I1118 04:37:44.976119 4435920320 tpu_estimator.py:2160] examples/sec: 2.8388\n",
      "INFO:tensorflow:global_step/sec: 0.0881829\n",
      "I1118 04:37:56.315940 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881829\n",
      "INFO:tensorflow:examples/sec: 2.82185\n",
      "I1118 04:37:56.316159 4435920320 tpu_estimator.py:2160] examples/sec: 2.82185\n",
      "INFO:tensorflow:global_step/sec: 0.0889157\n",
      "I1118 04:38:07.562561 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889157\n",
      "INFO:tensorflow:examples/sec: 2.8453\n",
      "I1118 04:38:07.562786 4435920320 tpu_estimator.py:2160] examples/sec: 2.8453\n",
      "INFO:tensorflow:global_step/sec: 0.0892002\n",
      "I1118 04:38:18.773293 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892002\n",
      "INFO:tensorflow:examples/sec: 2.85441\n",
      "I1118 04:38:18.773522 4435920320 tpu_estimator.py:2160] examples/sec: 2.85441\n",
      "INFO:tensorflow:global_step/sec: 0.0887917\n",
      "I1118 04:38:30.035594 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887917\n",
      "INFO:tensorflow:examples/sec: 2.84134\n",
      "I1118 04:38:30.035809 4435920320 tpu_estimator.py:2160] examples/sec: 2.84134\n",
      "INFO:tensorflow:global_step/sec: 0.0890099\n",
      "I1118 04:38:41.270309 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890099\n",
      "INFO:tensorflow:examples/sec: 2.84832\n",
      "I1118 04:38:41.270543 4435920320 tpu_estimator.py:2160] examples/sec: 2.84832\n",
      "INFO:tensorflow:global_step/sec: 0.0892351\n",
      "I1118 04:38:52.476667 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892351\n",
      "INFO:tensorflow:examples/sec: 2.85552\n",
      "I1118 04:38:52.476896 4435920320 tpu_estimator.py:2160] examples/sec: 2.85552\n",
      "INFO:tensorflow:global_step/sec: 0.0895029\n",
      "I1118 04:39:03.649471 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895029\n",
      "INFO:tensorflow:examples/sec: 2.86409\n",
      "I1118 04:39:03.649689 4435920320 tpu_estimator.py:2160] examples/sec: 2.86409\n",
      "INFO:tensorflow:global_step/sec: 0.0887721\n",
      "I1118 04:39:14.914290 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887721\n",
      "INFO:tensorflow:examples/sec: 2.84071\n",
      "I1118 04:39:14.914522 4435920320 tpu_estimator.py:2160] examples/sec: 2.84071\n",
      "INFO:tensorflow:global_step/sec: 0.0883164\n",
      "I1118 04:39:26.237205 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883164\n",
      "INFO:tensorflow:examples/sec: 2.82612\n",
      "I1118 04:39:26.237421 4435920320 tpu_estimator.py:2160] examples/sec: 2.82612\n",
      "INFO:tensorflow:global_step/sec: 0.0896897\n",
      "I1118 04:39:37.386781 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896897\n",
      "INFO:tensorflow:examples/sec: 2.87007\n",
      "I1118 04:39:37.387287 4435920320 tpu_estimator.py:2160] examples/sec: 2.87007\n",
      "INFO:tensorflow:global_step/sec: 0.0891181\n",
      "I1118 04:39:48.607834 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891181\n",
      "INFO:tensorflow:examples/sec: 2.85178\n",
      "I1118 04:39:48.608053 4435920320 tpu_estimator.py:2160] examples/sec: 2.85178\n",
      "INFO:tensorflow:global_step/sec: 0.0891857\n",
      "I1118 04:39:59.820400 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891857\n",
      "INFO:tensorflow:examples/sec: 2.85394\n",
      "I1118 04:39:59.820634 4435920320 tpu_estimator.py:2160] examples/sec: 2.85394\n",
      "INFO:tensorflow:global_step/sec: 0.088942\n",
      "I1118 04:40:11.063682 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088942\n",
      "INFO:tensorflow:examples/sec: 2.84614\n",
      "I1118 04:40:11.063901 4435920320 tpu_estimator.py:2160] examples/sec: 2.84614\n",
      "INFO:tensorflow:global_step/sec: 0.0891564\n",
      "I1118 04:40:22.279927 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891564\n",
      "INFO:tensorflow:examples/sec: 2.853\n",
      "I1118 04:40:22.280139 4435920320 tpu_estimator.py:2160] examples/sec: 2.853\n",
      "INFO:tensorflow:global_step/sec: 0.0887134\n",
      "I1118 04:40:33.552201 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887134\n",
      "INFO:tensorflow:examples/sec: 2.83883\n",
      "I1118 04:40:33.552489 4435920320 tpu_estimator.py:2160] examples/sec: 2.83883\n",
      "INFO:tensorflow:global_step/sec: 0.0891753\n",
      "I1118 04:40:44.766053 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891753\n",
      "INFO:tensorflow:examples/sec: 2.85361\n",
      "I1118 04:40:44.766278 4435920320 tpu_estimator.py:2160] examples/sec: 2.85361\n",
      "INFO:tensorflow:global_step/sec: 0.0892808\n",
      "I1118 04:40:55.966687 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892808\n",
      "INFO:tensorflow:examples/sec: 2.85698\n",
      "I1118 04:40:55.966923 4435920320 tpu_estimator.py:2160] examples/sec: 2.85698\n",
      "INFO:tensorflow:global_step/sec: 0.0888867\n",
      "I1118 04:41:07.216965 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888867\n",
      "INFO:tensorflow:examples/sec: 2.84437\n",
      "I1118 04:41:07.217202 4435920320 tpu_estimator.py:2160] examples/sec: 2.84437\n",
      "INFO:tensorflow:global_step/sec: 0.0890316\n",
      "I1118 04:41:18.448918 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890316\n",
      "INFO:tensorflow:examples/sec: 2.84901\n",
      "I1118 04:41:18.449142 4435920320 tpu_estimator.py:2160] examples/sec: 2.84901\n",
      "INFO:tensorflow:global_step/sec: 0.0891739\n",
      "I1118 04:41:29.662960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891739\n",
      "INFO:tensorflow:examples/sec: 2.85356\n",
      "I1118 04:41:29.663181 4435920320 tpu_estimator.py:2160] examples/sec: 2.85356\n",
      "INFO:tensorflow:global_step/sec: 0.0888767\n",
      "I1118 04:41:40.914501 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888767\n",
      "INFO:tensorflow:examples/sec: 2.84405\n",
      "I1118 04:41:40.914716 4435920320 tpu_estimator.py:2160] examples/sec: 2.84405\n",
      "INFO:tensorflow:global_step/sec: 0.0888905\n",
      "I1118 04:41:52.164299 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888905\n",
      "INFO:tensorflow:examples/sec: 2.8445\n",
      "I1118 04:41:52.164530 4435920320 tpu_estimator.py:2160] examples/sec: 2.8445\n",
      "INFO:tensorflow:global_step/sec: 0.0894054\n",
      "I1118 04:42:03.349315 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894054\n",
      "INFO:tensorflow:examples/sec: 2.86097\n",
      "I1118 04:42:03.349544 4435920320 tpu_estimator.py:2160] examples/sec: 2.86097\n",
      "INFO:tensorflow:global_step/sec: 0.089728\n",
      "I1118 04:42:14.494097 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089728\n",
      "INFO:tensorflow:examples/sec: 2.8713\n",
      "I1118 04:42:14.494314 4435920320 tpu_estimator.py:2160] examples/sec: 2.8713\n",
      "INFO:tensorflow:global_step/sec: 0.0894179\n",
      "I1118 04:42:25.677539 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894179\n",
      "INFO:tensorflow:examples/sec: 2.86137\n",
      "I1118 04:42:25.677755 4435920320 tpu_estimator.py:2160] examples/sec: 2.86137\n",
      "INFO:tensorflow:global_step/sec: 0.0886424\n",
      "I1118 04:42:36.958833 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886424\n",
      "INFO:tensorflow:examples/sec: 2.83656\n",
      "I1118 04:42:36.959062 4435920320 tpu_estimator.py:2160] examples/sec: 2.83656\n",
      "INFO:tensorflow:global_step/sec: 0.0889719\n",
      "I1118 04:42:48.198351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889719\n",
      "INFO:tensorflow:examples/sec: 2.8471\n",
      "I1118 04:42:48.198597 4435920320 tpu_estimator.py:2160] examples/sec: 2.8471\n",
      "INFO:tensorflow:global_step/sec: 0.0887299\n",
      "I1118 04:42:59.468502 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887299\n",
      "INFO:tensorflow:examples/sec: 2.83936\n",
      "I1118 04:42:59.468730 4435920320 tpu_estimator.py:2160] examples/sec: 2.83936\n",
      "INFO:tensorflow:global_step/sec: 0.0885824\n",
      "I1118 04:43:10.757438 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885824\n",
      "INFO:tensorflow:examples/sec: 2.83464\n",
      "I1118 04:43:10.757657 4435920320 tpu_estimator.py:2160] examples/sec: 2.83464\n",
      "INFO:tensorflow:global_step/sec: 0.0889333\n",
      "I1118 04:43:22.001809 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889333\n",
      "INFO:tensorflow:examples/sec: 2.84587\n",
      "I1118 04:43:22.002040 4435920320 tpu_estimator.py:2160] examples/sec: 2.84587\n",
      "INFO:tensorflow:global_step/sec: 0.0888921\n",
      "I1118 04:43:33.251403 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888921\n",
      "INFO:tensorflow:examples/sec: 2.84455\n",
      "I1118 04:43:33.251630 4435920320 tpu_estimator.py:2160] examples/sec: 2.84455\n",
      "INFO:tensorflow:global_step/sec: 0.0894459\n",
      "I1118 04:43:44.431349 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894459\n",
      "INFO:tensorflow:examples/sec: 2.86227\n",
      "I1118 04:43:44.431589 4435920320 tpu_estimator.py:2160] examples/sec: 2.86227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0895267\n",
      "I1118 04:43:55.601192 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895267\n",
      "INFO:tensorflow:examples/sec: 2.86486\n",
      "I1118 04:43:55.601438 4435920320 tpu_estimator.py:2160] examples/sec: 2.86486\n",
      "INFO:tensorflow:global_step/sec: 0.0886343\n",
      "I1118 04:44:06.883507 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886343\n",
      "INFO:tensorflow:examples/sec: 2.8363\n",
      "I1118 04:44:06.883747 4435920320 tpu_estimator.py:2160] examples/sec: 2.8363\n",
      "INFO:tensorflow:global_step/sec: 0.0885534\n",
      "I1118 04:44:18.176126 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885534\n",
      "INFO:tensorflow:examples/sec: 2.83371\n",
      "I1118 04:44:18.176342 4435920320 tpu_estimator.py:2160] examples/sec: 2.83371\n",
      "INFO:tensorflow:global_step/sec: 0.0886153\n",
      "I1118 04:44:29.460843 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886153\n",
      "INFO:tensorflow:examples/sec: 2.83569\n",
      "I1118 04:44:29.461063 4435920320 tpu_estimator.py:2160] examples/sec: 2.83569\n",
      "INFO:tensorflow:global_step/sec: 0.0892612\n",
      "I1118 04:44:40.663919 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892612\n",
      "INFO:tensorflow:examples/sec: 2.85636\n",
      "I1118 04:44:40.664138 4435920320 tpu_estimator.py:2160] examples/sec: 2.85636\n",
      "INFO:tensorflow:global_step/sec: 0.0889644\n",
      "I1118 04:44:51.904364 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889644\n",
      "INFO:tensorflow:examples/sec: 2.84686\n",
      "I1118 04:44:51.904577 4435920320 tpu_estimator.py:2160] examples/sec: 2.84686\n",
      "INFO:tensorflow:global_step/sec: 0.0889538\n",
      "I1118 04:45:03.146183 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889538\n",
      "INFO:tensorflow:examples/sec: 2.84652\n",
      "I1118 04:45:03.146402 4435920320 tpu_estimator.py:2160] examples/sec: 2.84652\n",
      "INFO:tensorflow:global_step/sec: 0.0893859\n",
      "I1118 04:45:14.333641 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893859\n",
      "INFO:tensorflow:examples/sec: 2.86035\n",
      "I1118 04:45:14.333874 4435920320 tpu_estimator.py:2160] examples/sec: 2.86035\n",
      "INFO:tensorflow:global_step/sec: 0.0890837\n",
      "I1118 04:45:25.559019 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890837\n",
      "INFO:tensorflow:examples/sec: 2.85068\n",
      "I1118 04:45:25.559245 4435920320 tpu_estimator.py:2160] examples/sec: 2.85068\n",
      "INFO:tensorflow:global_step/sec: 0.0892328\n",
      "I1118 04:45:36.765645 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892328\n",
      "INFO:tensorflow:examples/sec: 2.85545\n",
      "I1118 04:45:36.765861 4435920320 tpu_estimator.py:2160] examples/sec: 2.85545\n",
      "INFO:tensorflow:global_step/sec: 0.089107\n",
      "I1118 04:45:47.988112 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089107\n",
      "INFO:tensorflow:examples/sec: 2.85142\n",
      "I1118 04:45:47.988331 4435920320 tpu_estimator.py:2160] examples/sec: 2.85142\n",
      "INFO:tensorflow:global_step/sec: 0.0891447\n",
      "I1118 04:45:59.205828 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891447\n",
      "INFO:tensorflow:examples/sec: 2.85263\n",
      "I1118 04:45:59.206043 4435920320 tpu_estimator.py:2160] examples/sec: 2.85263\n",
      "INFO:tensorflow:global_step/sec: 0.0888072\n",
      "I1118 04:46:10.466181 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888072\n",
      "INFO:tensorflow:examples/sec: 2.84183\n",
      "I1118 04:46:10.466402 4435920320 tpu_estimator.py:2160] examples/sec: 2.84183\n",
      "INFO:tensorflow:global_step/sec: 0.0886593\n",
      "I1118 04:46:21.745337 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886593\n",
      "INFO:tensorflow:examples/sec: 2.8371\n",
      "I1118 04:46:21.745563 4435920320 tpu_estimator.py:2160] examples/sec: 2.8371\n",
      "INFO:tensorflow:global_step/sec: 0.0887452\n",
      "I1118 04:46:33.013553 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887452\n",
      "INFO:tensorflow:examples/sec: 2.83985\n",
      "I1118 04:46:33.013782 4435920320 tpu_estimator.py:2160] examples/sec: 2.83985\n",
      "INFO:tensorflow:global_step/sec: 0.089062\n",
      "I1118 04:46:44.241681 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089062\n",
      "INFO:tensorflow:examples/sec: 2.84998\n",
      "I1118 04:46:44.241914 4435920320 tpu_estimator.py:2160] examples/sec: 2.84998\n",
      "INFO:tensorflow:global_step/sec: 0.0890312\n",
      "I1118 04:46:55.473731 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890312\n",
      "INFO:tensorflow:examples/sec: 2.849\n",
      "I1118 04:46:55.473952 4435920320 tpu_estimator.py:2160] examples/sec: 2.849\n",
      "INFO:tensorflow:global_step/sec: 0.0899485\n",
      "I1118 04:47:06.591181 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899485\n",
      "INFO:tensorflow:examples/sec: 2.87835\n",
      "I1118 04:47:06.591398 4435920320 tpu_estimator.py:2160] examples/sec: 2.87835\n",
      "INFO:tensorflow:global_step/sec: 0.0899484\n",
      "I1118 04:47:17.708658 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899484\n",
      "INFO:tensorflow:examples/sec: 2.87835\n",
      "I1118 04:47:17.708888 4435920320 tpu_estimator.py:2160] examples/sec: 2.87835\n",
      "INFO:tensorflow:global_step/sec: 0.0890375\n",
      "I1118 04:47:28.939876 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890375\n",
      "INFO:tensorflow:examples/sec: 2.8492\n",
      "I1118 04:47:28.940094 4435920320 tpu_estimator.py:2160] examples/sec: 2.8492\n",
      "INFO:tensorflow:global_step/sec: 0.0890867\n",
      "I1118 04:47:40.164920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890867\n",
      "INFO:tensorflow:examples/sec: 2.85077\n",
      "I1118 04:47:40.165142 4435920320 tpu_estimator.py:2160] examples/sec: 2.85077\n",
      "INFO:tensorflow:global_step/sec: 0.0891804\n",
      "I1118 04:47:51.378157 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891804\n",
      "INFO:tensorflow:examples/sec: 2.85377\n",
      "I1118 04:47:51.378381 4435920320 tpu_estimator.py:2160] examples/sec: 2.85377\n",
      "INFO:tensorflow:global_step/sec: 0.0887384\n",
      "I1118 04:48:02.647202 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887384\n",
      "INFO:tensorflow:examples/sec: 2.83963\n",
      "I1118 04:48:02.648044 4435920320 tpu_estimator.py:2160] examples/sec: 2.83963\n",
      "INFO:tensorflow:global_step/sec: 0.0890342\n",
      "I1118 04:48:13.878823 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890342\n",
      "INFO:tensorflow:examples/sec: 2.8491\n",
      "I1118 04:48:13.879036 4435920320 tpu_estimator.py:2160] examples/sec: 2.8491\n",
      "INFO:tensorflow:global_step/sec: 0.0893372\n",
      "I1118 04:48:25.072371 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893372\n",
      "INFO:tensorflow:examples/sec: 2.85879\n",
      "I1118 04:48:25.072577 4435920320 tpu_estimator.py:2160] examples/sec: 2.85879\n",
      "INFO:tensorflow:global_step/sec: 0.089118\n",
      "I1118 04:48:36.293497 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089118\n",
      "INFO:tensorflow:examples/sec: 2.85178\n",
      "I1118 04:48:36.293719 4435920320 tpu_estimator.py:2160] examples/sec: 2.85178\n",
      "INFO:tensorflow:global_step/sec: 0.0897168\n",
      "I1118 04:48:47.439652 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897168\n",
      "INFO:tensorflow:examples/sec: 2.87094\n",
      "I1118 04:48:47.439876 4435920320 tpu_estimator.py:2160] examples/sec: 2.87094\n",
      "INFO:tensorflow:global_step/sec: 0.0894624\n",
      "I1118 04:48:58.617532 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894624\n",
      "INFO:tensorflow:examples/sec: 2.8628\n",
      "I1118 04:48:58.617783 4435920320 tpu_estimator.py:2160] examples/sec: 2.8628\n",
      "INFO:tensorflow:global_step/sec: 0.0892922\n",
      "I1118 04:49:09.816710 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892922\n",
      "INFO:tensorflow:examples/sec: 2.85735\n",
      "I1118 04:49:09.816948 4435920320 tpu_estimator.py:2160] examples/sec: 2.85735\n",
      "INFO:tensorflow:global_step/sec: 0.0891913\n",
      "I1118 04:49:21.028549 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891913\n",
      "INFO:tensorflow:examples/sec: 2.85412\n",
      "I1118 04:49:21.028900 4435920320 tpu_estimator.py:2160] examples/sec: 2.85412\n",
      "INFO:tensorflow:global_step/sec: 0.0889798\n",
      "I1118 04:49:32.267071 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889798\n",
      "INFO:tensorflow:examples/sec: 2.84735\n",
      "I1118 04:49:32.267299 4435920320 tpu_estimator.py:2160] examples/sec: 2.84735\n",
      "INFO:tensorflow:global_step/sec: 0.0892637\n",
      "I1118 04:49:43.469845 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892637\n",
      "INFO:tensorflow:examples/sec: 2.85644\n",
      "I1118 04:49:43.470054 4435920320 tpu_estimator.py:2160] examples/sec: 2.85644\n",
      "INFO:tensorflow:global_step/sec: 0.0897446\n",
      "I1118 04:49:54.612559 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897446\n",
      "INFO:tensorflow:examples/sec: 2.87183\n",
      "I1118 04:49:54.612774 4435920320 tpu_estimator.py:2160] examples/sec: 2.87183\n",
      "INFO:tensorflow:global_step/sec: 0.0891161\n",
      "I1118 04:50:05.833878 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891161\n",
      "INFO:tensorflow:examples/sec: 2.85172\n",
      "I1118 04:50:05.834100 4435920320 tpu_estimator.py:2160] examples/sec: 2.85172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0888782\n",
      "I1118 04:50:17.085238 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888782\n",
      "INFO:tensorflow:examples/sec: 2.8441\n",
      "I1118 04:50:17.085470 4435920320 tpu_estimator.py:2160] examples/sec: 2.8441\n",
      "INFO:tensorflow:global_step/sec: 0.0896433\n",
      "I1118 04:50:28.240579 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896433\n",
      "INFO:tensorflow:examples/sec: 2.86859\n",
      "I1118 04:50:28.240812 4435920320 tpu_estimator.py:2160] examples/sec: 2.86859\n",
      "INFO:tensorflow:global_step/sec: 0.0881744\n",
      "I1118 04:50:39.581671 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0881744\n",
      "INFO:tensorflow:examples/sec: 2.82158\n",
      "I1118 04:50:39.581887 4435920320 tpu_estimator.py:2160] examples/sec: 2.82158\n",
      "INFO:tensorflow:global_step/sec: 0.0884913\n",
      "I1118 04:50:50.882251 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884913\n",
      "INFO:tensorflow:examples/sec: 2.83172\n",
      "I1118 04:50:50.882469 4435920320 tpu_estimator.py:2160] examples/sec: 2.83172\n",
      "INFO:tensorflow:global_step/sec: 0.0888064\n",
      "I1118 04:51:02.142725 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888064\n",
      "INFO:tensorflow:examples/sec: 2.8418\n",
      "I1118 04:51:02.142944 4435920320 tpu_estimator.py:2160] examples/sec: 2.8418\n",
      "INFO:tensorflow:global_step/sec: 0.0893154\n",
      "I1118 04:51:13.338981 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893154\n",
      "INFO:tensorflow:examples/sec: 2.85809\n",
      "I1118 04:51:13.339214 4435920320 tpu_estimator.py:2160] examples/sec: 2.85809\n",
      "INFO:tensorflow:global_step/sec: 0.0893027\n",
      "I1118 04:51:24.536847 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893027\n",
      "INFO:tensorflow:examples/sec: 2.85769\n",
      "I1118 04:51:24.537075 4435920320 tpu_estimator.py:2160] examples/sec: 2.85769\n",
      "INFO:tensorflow:global_step/sec: 0.0897539\n",
      "I1118 04:51:35.678428 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897539\n",
      "INFO:tensorflow:examples/sec: 2.87212\n",
      "I1118 04:51:35.678653 4435920320 tpu_estimator.py:2160] examples/sec: 2.87212\n",
      "INFO:tensorflow:global_step/sec: 0.0895282\n",
      "I1118 04:51:46.848080 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895282\n",
      "INFO:tensorflow:examples/sec: 2.8649\n",
      "I1118 04:51:46.848299 4435920320 tpu_estimator.py:2160] examples/sec: 2.8649\n",
      "INFO:tensorflow:global_step/sec: 0.0887023\n",
      "I1118 04:51:58.121757 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887023\n",
      "INFO:tensorflow:examples/sec: 2.83847\n",
      "I1118 04:51:58.121986 4435920320 tpu_estimator.py:2160] examples/sec: 2.83847\n",
      "INFO:tensorflow:global_step/sec: 0.0886725\n",
      "I1118 04:52:09.399209 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886725\n",
      "INFO:tensorflow:examples/sec: 2.83752\n",
      "I1118 04:52:09.399449 4435920320 tpu_estimator.py:2160] examples/sec: 2.83752\n",
      "INFO:tensorflow:global_step/sec: 0.0880645\n",
      "I1118 04:52:20.754601 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0880645\n",
      "INFO:tensorflow:examples/sec: 2.81806\n",
      "I1118 04:52:20.755087 4435920320 tpu_estimator.py:2160] examples/sec: 2.81806\n",
      "INFO:tensorflow:global_step/sec: 0.0887353\n",
      "I1118 04:52:32.024005 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887353\n",
      "INFO:tensorflow:examples/sec: 2.83953\n",
      "I1118 04:52:32.024218 4435920320 tpu_estimator.py:2160] examples/sec: 2.83953\n",
      "INFO:tensorflow:global_step/sec: 0.089103\n",
      "I1118 04:52:43.246959 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089103\n",
      "INFO:tensorflow:examples/sec: 2.8513\n",
      "I1118 04:52:43.247184 4435920320 tpu_estimator.py:2160] examples/sec: 2.8513\n",
      "INFO:tensorflow:global_step/sec: 0.0887275\n",
      "I1118 04:52:54.517424 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887275\n",
      "INFO:tensorflow:examples/sec: 2.83928\n",
      "I1118 04:52:54.517655 4435920320 tpu_estimator.py:2160] examples/sec: 2.83928\n",
      "INFO:tensorflow:global_step/sec: 0.088991\n",
      "I1118 04:53:05.754517 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088991\n",
      "INFO:tensorflow:examples/sec: 2.84771\n",
      "I1118 04:53:05.754909 4435920320 tpu_estimator.py:2160] examples/sec: 2.84771\n",
      "INFO:tensorflow:global_step/sec: 0.0888711\n",
      "I1118 04:53:17.006770 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888711\n",
      "INFO:tensorflow:examples/sec: 2.84387\n",
      "I1118 04:53:17.007002 4435920320 tpu_estimator.py:2160] examples/sec: 2.84387\n",
      "INFO:tensorflow:global_step/sec: 0.0892373\n",
      "I1118 04:53:28.212841 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892373\n",
      "INFO:tensorflow:examples/sec: 2.85559\n",
      "I1118 04:53:28.213061 4435920320 tpu_estimator.py:2160] examples/sec: 2.85559\n",
      "INFO:tensorflow:global_step/sec: 0.0889554\n",
      "I1118 04:53:39.454437 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889554\n",
      "INFO:tensorflow:examples/sec: 2.84657\n",
      "I1118 04:53:39.454811 4435920320 tpu_estimator.py:2160] examples/sec: 2.84657\n",
      "INFO:tensorflow:global_step/sec: 0.0888907\n",
      "I1118 04:53:50.704209 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888907\n",
      "INFO:tensorflow:examples/sec: 2.8445\n",
      "I1118 04:53:50.704433 4435920320 tpu_estimator.py:2160] examples/sec: 2.8445\n",
      "INFO:tensorflow:global_step/sec: 0.0891166\n",
      "I1118 04:54:01.925466 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891166\n",
      "INFO:tensorflow:examples/sec: 2.85173\n",
      "I1118 04:54:01.925694 4435920320 tpu_estimator.py:2160] examples/sec: 2.85173\n",
      "INFO:tensorflow:global_step/sec: 0.0896251\n",
      "I1118 04:54:13.083066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896251\n",
      "INFO:tensorflow:examples/sec: 2.868\n",
      "I1118 04:54:13.083292 4435920320 tpu_estimator.py:2160] examples/sec: 2.868\n",
      "INFO:tensorflow:global_step/sec: 0.0890588\n",
      "I1118 04:54:24.311631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890588\n",
      "INFO:tensorflow:examples/sec: 2.84988\n",
      "I1118 04:54:24.311854 4435920320 tpu_estimator.py:2160] examples/sec: 2.84988\n",
      "INFO:tensorflow:global_step/sec: 0.0891131\n",
      "I1118 04:54:35.533296 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891131\n",
      "INFO:tensorflow:examples/sec: 2.85162\n",
      "I1118 04:54:35.533514 4435920320 tpu_estimator.py:2160] examples/sec: 2.85162\n",
      "INFO:tensorflow:global_step/sec: 0.0895902\n",
      "I1118 04:54:46.695238 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895902\n",
      "INFO:tensorflow:examples/sec: 2.86689\n",
      "I1118 04:54:46.695466 4435920320 tpu_estimator.py:2160] examples/sec: 2.86689\n",
      "INFO:tensorflow:global_step/sec: 0.0883774\n",
      "I1118 04:54:58.010351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883774\n",
      "INFO:tensorflow:examples/sec: 2.82808\n",
      "I1118 04:54:58.010586 4435920320 tpu_estimator.py:2160] examples/sec: 2.82808\n",
      "INFO:tensorflow:global_step/sec: 0.0890382\n",
      "I1118 04:55:09.241479 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890382\n",
      "INFO:tensorflow:examples/sec: 2.84922\n",
      "I1118 04:55:09.241728 4435920320 tpu_estimator.py:2160] examples/sec: 2.84922\n",
      "INFO:tensorflow:global_step/sec: 0.0893298\n",
      "I1118 04:55:20.435943 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893298\n",
      "INFO:tensorflow:examples/sec: 2.85855\n",
      "I1118 04:55:20.436161 4435920320 tpu_estimator.py:2160] examples/sec: 2.85855\n",
      "INFO:tensorflow:global_step/sec: 0.089494\n",
      "I1118 04:55:31.609872 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089494\n",
      "INFO:tensorflow:examples/sec: 2.86381\n",
      "I1118 04:55:31.610090 4435920320 tpu_estimator.py:2160] examples/sec: 2.86381\n",
      "INFO:tensorflow:global_step/sec: 0.0895327\n",
      "I1118 04:55:42.778985 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895327\n",
      "INFO:tensorflow:examples/sec: 2.86505\n",
      "I1118 04:55:42.779204 4435920320 tpu_estimator.py:2160] examples/sec: 2.86505\n",
      "INFO:tensorflow:global_step/sec: 0.0894541\n",
      "I1118 04:55:53.957897 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894541\n",
      "INFO:tensorflow:examples/sec: 2.86253\n",
      "I1118 04:55:53.958122 4435920320 tpu_estimator.py:2160] examples/sec: 2.86253\n",
      "INFO:tensorflow:global_step/sec: 0.0895414\n",
      "I1118 04:56:05.125911 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895414\n",
      "INFO:tensorflow:examples/sec: 2.86532\n",
      "I1118 04:56:05.126127 4435920320 tpu_estimator.py:2160] examples/sec: 2.86532\n",
      "INFO:tensorflow:global_step/sec: 0.0893351\n",
      "I1118 04:56:16.319734 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893351\n",
      "INFO:tensorflow:examples/sec: 2.85872\n",
      "I1118 04:56:16.320256 4435920320 tpu_estimator.py:2160] examples/sec: 2.85872\n",
      "INFO:tensorflow:global_step/sec: 0.0896376\n",
      "I1118 04:56:27.475769 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896376\n",
      "INFO:tensorflow:examples/sec: 2.8684\n",
      "I1118 04:56:27.475983 4435920320 tpu_estimator.py:2160] examples/sec: 2.8684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0889868\n",
      "I1118 04:56:38.713382 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889868\n",
      "INFO:tensorflow:examples/sec: 2.84758\n",
      "I1118 04:56:38.713602 4435920320 tpu_estimator.py:2160] examples/sec: 2.84758\n",
      "INFO:tensorflow:global_step/sec: 0.0891513\n",
      "I1118 04:56:49.930288 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891513\n",
      "INFO:tensorflow:examples/sec: 2.85284\n",
      "I1118 04:56:49.930506 4435920320 tpu_estimator.py:2160] examples/sec: 2.85284\n",
      "INFO:tensorflow:global_step/sec: 0.088707\n",
      "I1118 04:57:01.203351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088707\n",
      "INFO:tensorflow:examples/sec: 2.83862\n",
      "I1118 04:57:01.203591 4435920320 tpu_estimator.py:2160] examples/sec: 2.83862\n",
      "INFO:tensorflow:global_step/sec: 0.0893404\n",
      "I1118 04:57:12.396475 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893404\n",
      "INFO:tensorflow:examples/sec: 2.85889\n",
      "I1118 04:57:12.396697 4435920320 tpu_estimator.py:2160] examples/sec: 2.85889\n",
      "INFO:tensorflow:global_step/sec: 0.089138\n",
      "I1118 04:57:23.615045 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089138\n",
      "INFO:tensorflow:examples/sec: 2.85242\n",
      "I1118 04:57:23.615429 4435920320 tpu_estimator.py:2160] examples/sec: 2.85242\n",
      "INFO:tensorflow:global_step/sec: 0.0893828\n",
      "I1118 04:57:34.802885 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893828\n",
      "INFO:tensorflow:examples/sec: 2.86025\n",
      "I1118 04:57:34.803120 4435920320 tpu_estimator.py:2160] examples/sec: 2.86025\n",
      "INFO:tensorflow:global_step/sec: 0.0895068\n",
      "I1118 04:57:45.975229 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895068\n",
      "INFO:tensorflow:examples/sec: 2.86422\n",
      "I1118 04:57:45.975452 4435920320 tpu_estimator.py:2160] examples/sec: 2.86422\n",
      "INFO:tensorflow:global_step/sec: 0.0892727\n",
      "I1118 04:57:57.176925 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892727\n",
      "INFO:tensorflow:examples/sec: 2.85673\n",
      "I1118 04:57:57.177161 4435920320 tpu_estimator.py:2160] examples/sec: 2.85673\n",
      "INFO:tensorflow:global_step/sec: 0.0889659\n",
      "I1118 04:58:08.417119 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889659\n",
      "INFO:tensorflow:examples/sec: 2.84691\n",
      "I1118 04:58:08.417344 4435920320 tpu_estimator.py:2160] examples/sec: 2.84691\n",
      "INFO:tensorflow:global_step/sec: 0.0891185\n",
      "I1118 04:58:19.638185 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891185\n",
      "INFO:tensorflow:examples/sec: 2.85179\n",
      "I1118 04:58:19.638403 4435920320 tpu_estimator.py:2160] examples/sec: 2.85179\n",
      "INFO:tensorflow:global_step/sec: 0.0892607\n",
      "I1118 04:58:30.841264 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892607\n",
      "INFO:tensorflow:examples/sec: 2.85634\n",
      "I1118 04:58:30.841498 4435920320 tpu_estimator.py:2160] examples/sec: 2.85634\n",
      "INFO:tensorflow:global_step/sec: 0.0892112\n",
      "I1118 04:58:42.050636 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892112\n",
      "INFO:tensorflow:examples/sec: 2.85476\n",
      "I1118 04:58:42.050872 4435920320 tpu_estimator.py:2160] examples/sec: 2.85476\n",
      "INFO:tensorflow:global_step/sec: 0.0892084\n",
      "I1118 04:58:53.260329 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892084\n",
      "INFO:tensorflow:examples/sec: 2.85467\n",
      "I1118 04:58:53.260555 4435920320 tpu_estimator.py:2160] examples/sec: 2.85467\n",
      "INFO:tensorflow:global_step/sec: 0.0894856\n",
      "I1118 04:59:04.435303 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894856\n",
      "INFO:tensorflow:examples/sec: 2.86354\n",
      "I1118 04:59:04.435518 4435920320 tpu_estimator.py:2160] examples/sec: 2.86354\n",
      "INFO:tensorflow:global_step/sec: 0.0889846\n",
      "I1118 04:59:15.673211 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889846\n",
      "INFO:tensorflow:examples/sec: 2.84751\n",
      "I1118 04:59:15.673449 4435920320 tpu_estimator.py:2160] examples/sec: 2.84751\n",
      "INFO:tensorflow:global_step/sec: 0.0887169\n",
      "I1118 04:59:26.945024 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887169\n",
      "INFO:tensorflow:examples/sec: 2.83894\n",
      "I1118 04:59:26.945253 4435920320 tpu_estimator.py:2160] examples/sec: 2.83894\n",
      "INFO:tensorflow:global_step/sec: 0.0893148\n",
      "I1118 04:59:38.141365 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893148\n",
      "INFO:tensorflow:examples/sec: 2.85807\n",
      "I1118 04:59:38.141581 4435920320 tpu_estimator.py:2160] examples/sec: 2.85807\n",
      "INFO:tensorflow:global_step/sec: 0.0895164\n",
      "I1118 04:59:49.312516 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895164\n",
      "INFO:tensorflow:examples/sec: 2.86452\n",
      "I1118 04:59:49.312740 4435920320 tpu_estimator.py:2160] examples/sec: 2.86452\n",
      "INFO:tensorflow:global_step/sec: 0.0893749\n",
      "I1118 05:00:00.501363 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893749\n",
      "INFO:tensorflow:examples/sec: 2.86\n",
      "I1118 05:00:00.501583 4435920320 tpu_estimator.py:2160] examples/sec: 2.86\n",
      "INFO:tensorflow:global_step/sec: 0.0893026\n",
      "I1118 05:00:11.699226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893026\n",
      "INFO:tensorflow:examples/sec: 2.85768\n",
      "I1118 05:00:11.699458 4435920320 tpu_estimator.py:2160] examples/sec: 2.85768\n",
      "INFO:tensorflow:global_step/sec: 0.0886339\n",
      "I1118 05:00:22.981580 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886339\n",
      "INFO:tensorflow:examples/sec: 2.83629\n",
      "I1118 05:00:22.981822 4435920320 tpu_estimator.py:2160] examples/sec: 2.83629\n",
      "INFO:tensorflow:global_step/sec: 0.0886732\n",
      "I1118 05:00:34.258952 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886732\n",
      "INFO:tensorflow:examples/sec: 2.83754\n",
      "I1118 05:00:34.259200 4435920320 tpu_estimator.py:2160] examples/sec: 2.83754\n",
      "INFO:tensorflow:global_step/sec: 0.0891484\n",
      "I1118 05:00:45.476198 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891484\n",
      "INFO:tensorflow:examples/sec: 2.85275\n",
      "I1118 05:00:45.476470 4435920320 tpu_estimator.py:2160] examples/sec: 2.85275\n",
      "INFO:tensorflow:global_step/sec: 0.0891399\n",
      "I1118 05:00:56.694523 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891399\n",
      "INFO:tensorflow:examples/sec: 2.85248\n",
      "I1118 05:00:56.694766 4435920320 tpu_estimator.py:2160] examples/sec: 2.85248\n",
      "INFO:tensorflow:global_step/sec: 0.0892629\n",
      "I1118 05:01:07.897387 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892629\n",
      "INFO:tensorflow:examples/sec: 2.85641\n",
      "I1118 05:01:07.897615 4435920320 tpu_estimator.py:2160] examples/sec: 2.85641\n",
      "INFO:tensorflow:global_step/sec: 0.0892318\n",
      "I1118 05:01:19.104161 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892318\n",
      "INFO:tensorflow:examples/sec: 2.85542\n",
      "I1118 05:01:19.104393 4435920320 tpu_estimator.py:2160] examples/sec: 2.85542\n",
      "INFO:tensorflow:global_step/sec: 0.0892783\n",
      "I1118 05:01:30.305072 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892783\n",
      "INFO:tensorflow:examples/sec: 2.85691\n",
      "I1118 05:01:30.305292 4435920320 tpu_estimator.py:2160] examples/sec: 2.85691\n",
      "INFO:tensorflow:global_step/sec: 0.0895739\n",
      "I1118 05:01:41.469047 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895739\n",
      "INFO:tensorflow:examples/sec: 2.86637\n",
      "I1118 05:01:41.469274 4435920320 tpu_estimator.py:2160] examples/sec: 2.86637\n",
      "INFO:tensorflow:global_step/sec: 0.0891874\n",
      "I1118 05:01:52.681391 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891874\n",
      "INFO:tensorflow:examples/sec: 2.854\n",
      "I1118 05:01:52.681617 4435920320 tpu_estimator.py:2160] examples/sec: 2.854\n",
      "INFO:tensorflow:global_step/sec: 0.0892182\n",
      "I1118 05:02:03.889863 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892182\n",
      "INFO:tensorflow:examples/sec: 2.85498\n",
      "I1118 05:02:03.890092 4435920320 tpu_estimator.py:2160] examples/sec: 2.85498\n",
      "INFO:tensorflow:global_step/sec: 0.0891532\n",
      "I1118 05:02:15.106504 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891532\n",
      "INFO:tensorflow:examples/sec: 2.8529\n",
      "I1118 05:02:15.106743 4435920320 tpu_estimator.py:2160] examples/sec: 2.8529\n",
      "INFO:tensorflow:global_step/sec: 0.0893458\n",
      "I1118 05:02:26.298970 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893458\n",
      "INFO:tensorflow:examples/sec: 2.85907\n",
      "I1118 05:02:26.299206 4435920320 tpu_estimator.py:2160] examples/sec: 2.85907\n",
      "INFO:tensorflow:global_step/sec: 0.0893831\n",
      "I1118 05:02:37.486766 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893831\n",
      "INFO:tensorflow:examples/sec: 2.86026\n",
      "I1118 05:02:37.486991 4435920320 tpu_estimator.py:2160] examples/sec: 2.86026\n",
      "INFO:tensorflow:global_step/sec: 0.0899298\n",
      "I1118 05:02:48.606565 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899298\n",
      "INFO:tensorflow:examples/sec: 2.87775\n",
      "I1118 05:02:48.606786 4435920320 tpu_estimator.py:2160] examples/sec: 2.87775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0892112\n",
      "I1118 05:02:59.815920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892112\n",
      "INFO:tensorflow:examples/sec: 2.85476\n",
      "I1118 05:02:59.816318 4435920320 tpu_estimator.py:2160] examples/sec: 2.85476\n",
      "INFO:tensorflow:global_step/sec: 0.0892452\n",
      "I1118 05:03:11.021003 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892452\n",
      "INFO:tensorflow:examples/sec: 2.85585\n",
      "I1118 05:03:11.021232 4435920320 tpu_estimator.py:2160] examples/sec: 2.85585\n",
      "INFO:tensorflow:global_step/sec: 0.0894649\n",
      "I1118 05:03:22.198578 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894649\n",
      "INFO:tensorflow:examples/sec: 2.86288\n",
      "I1118 05:03:22.198807 4435920320 tpu_estimator.py:2160] examples/sec: 2.86288\n",
      "INFO:tensorflow:global_step/sec: 0.0887373\n",
      "I1118 05:03:33.467777 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887373\n",
      "INFO:tensorflow:examples/sec: 2.8396\n",
      "I1118 05:03:33.468002 4435920320 tpu_estimator.py:2160] examples/sec: 2.8396\n",
      "INFO:tensorflow:global_step/sec: 0.0893304\n",
      "I1118 05:03:44.662173 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893304\n",
      "INFO:tensorflow:examples/sec: 2.85857\n",
      "I1118 05:03:44.662390 4435920320 tpu_estimator.py:2160] examples/sec: 2.85857\n",
      "INFO:tensorflow:global_step/sec: 0.0892756\n",
      "I1118 05:03:55.863436 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892756\n",
      "INFO:tensorflow:examples/sec: 2.85682\n",
      "I1118 05:03:55.863649 4435920320 tpu_estimator.py:2160] examples/sec: 2.85682\n",
      "INFO:tensorflow:global_step/sec: 0.0893005\n",
      "I1118 05:04:07.061612 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893005\n",
      "INFO:tensorflow:examples/sec: 2.85762\n",
      "I1118 05:04:07.061856 4435920320 tpu_estimator.py:2160] examples/sec: 2.85762\n",
      "INFO:tensorflow:global_step/sec: 0.089046\n",
      "I1118 05:04:18.291749 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089046\n",
      "INFO:tensorflow:examples/sec: 2.84947\n",
      "I1118 05:04:18.291975 4435920320 tpu_estimator.py:2160] examples/sec: 2.84947\n",
      "INFO:tensorflow:global_step/sec: 0.0895416\n",
      "I1118 05:04:29.459733 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895416\n",
      "INFO:tensorflow:examples/sec: 2.86533\n",
      "I1118 05:04:29.459973 4435920320 tpu_estimator.py:2160] examples/sec: 2.86533\n",
      "INFO:tensorflow:global_step/sec: 0.0898916\n",
      "I1118 05:04:40.584249 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898916\n",
      "INFO:tensorflow:examples/sec: 2.87653\n",
      "I1118 05:04:40.584465 4435920320 tpu_estimator.py:2160] examples/sec: 2.87653\n",
      "INFO:tensorflow:global_step/sec: 0.0894385\n",
      "I1118 05:04:51.765095 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894385\n",
      "INFO:tensorflow:examples/sec: 2.86203\n",
      "I1118 05:04:51.765311 4435920320 tpu_estimator.py:2160] examples/sec: 2.86203\n",
      "INFO:tensorflow:global_step/sec: 0.0899634\n",
      "I1118 05:05:02.880740 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899634\n",
      "INFO:tensorflow:examples/sec: 2.87883\n",
      "I1118 05:05:02.880970 4435920320 tpu_estimator.py:2160] examples/sec: 2.87883\n",
      "INFO:tensorflow:global_step/sec: 0.0891043\n",
      "I1118 05:05:14.103524 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891043\n",
      "INFO:tensorflow:examples/sec: 2.85134\n",
      "I1118 05:05:14.103745 4435920320 tpu_estimator.py:2160] examples/sec: 2.85134\n",
      "INFO:tensorflow:global_step/sec: 0.0895073\n",
      "I1118 05:05:25.275794 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895073\n",
      "INFO:tensorflow:examples/sec: 2.86423\n",
      "I1118 05:05:25.276011 4435920320 tpu_estimator.py:2160] examples/sec: 2.86423\n",
      "INFO:tensorflow:global_step/sec: 0.0888158\n",
      "I1118 05:05:36.535066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888158\n",
      "INFO:tensorflow:examples/sec: 2.84211\n",
      "I1118 05:05:36.535299 4435920320 tpu_estimator.py:2160] examples/sec: 2.84211\n",
      "INFO:tensorflow:global_step/sec: 0.0891051\n",
      "I1118 05:05:47.757760 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891051\n",
      "INFO:tensorflow:examples/sec: 2.85136\n",
      "I1118 05:05:47.757965 4435920320 tpu_estimator.py:2160] examples/sec: 2.85136\n",
      "INFO:tensorflow:global_step/sec: 0.0891997\n",
      "I1118 05:05:58.968547 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891997\n",
      "INFO:tensorflow:examples/sec: 2.85439\n",
      "I1118 05:05:58.968763 4435920320 tpu_estimator.py:2160] examples/sec: 2.85439\n",
      "INFO:tensorflow:global_step/sec: 0.0892448\n",
      "I1118 05:06:10.173690 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892448\n",
      "INFO:tensorflow:examples/sec: 2.85583\n",
      "I1118 05:06:10.173911 4435920320 tpu_estimator.py:2160] examples/sec: 2.85583\n",
      "INFO:tensorflow:global_step/sec: 0.089333\n",
      "I1118 05:06:21.367758 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089333\n",
      "INFO:tensorflow:examples/sec: 2.85865\n",
      "I1118 05:06:21.367978 4435920320 tpu_estimator.py:2160] examples/sec: 2.85865\n",
      "INFO:tensorflow:global_step/sec: 0.0895992\n",
      "I1118 05:06:32.528573 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895992\n",
      "INFO:tensorflow:examples/sec: 2.86717\n",
      "I1118 05:06:32.528799 4435920320 tpu_estimator.py:2160] examples/sec: 2.86717\n",
      "INFO:tensorflow:global_step/sec: 0.0896755\n",
      "I1118 05:06:43.679920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896755\n",
      "INFO:tensorflow:examples/sec: 2.86961\n",
      "I1118 05:06:43.680171 4435920320 tpu_estimator.py:2160] examples/sec: 2.86961\n",
      "INFO:tensorflow:global_step/sec: 0.0890118\n",
      "I1118 05:06:54.914367 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890118\n",
      "INFO:tensorflow:examples/sec: 2.84838\n",
      "I1118 05:06:54.914602 4435920320 tpu_estimator.py:2160] examples/sec: 2.84838\n",
      "INFO:tensorflow:global_step/sec: 0.0893578\n",
      "I1118 05:07:06.105323 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893578\n",
      "INFO:tensorflow:examples/sec: 2.85945\n",
      "I1118 05:07:06.105540 4435920320 tpu_estimator.py:2160] examples/sec: 2.85945\n",
      "INFO:tensorflow:global_step/sec: 0.089326\n",
      "I1118 05:07:17.300273 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089326\n",
      "INFO:tensorflow:examples/sec: 2.85843\n",
      "I1118 05:07:17.300502 4435920320 tpu_estimator.py:2160] examples/sec: 2.85843\n",
      "INFO:tensorflow:global_step/sec: 0.0896165\n",
      "I1118 05:07:28.458942 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896165\n",
      "INFO:tensorflow:examples/sec: 2.86773\n",
      "I1118 05:07:28.459177 4435920320 tpu_estimator.py:2160] examples/sec: 2.86773\n",
      "INFO:tensorflow:global_step/sec: 0.0892899\n",
      "I1118 05:07:39.658445 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892899\n",
      "INFO:tensorflow:examples/sec: 2.85728\n",
      "I1118 05:07:39.658660 4435920320 tpu_estimator.py:2160] examples/sec: 2.85728\n",
      "INFO:tensorflow:global_step/sec: 0.0897038\n",
      "I1118 05:07:50.806192 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897038\n",
      "INFO:tensorflow:examples/sec: 2.87052\n",
      "I1118 05:07:50.806394 4435920320 tpu_estimator.py:2160] examples/sec: 2.87052\n",
      "INFO:tensorflow:global_step/sec: 0.088469\n",
      "I1118 05:08:02.109615 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088469\n",
      "INFO:tensorflow:examples/sec: 2.83101\n",
      "I1118 05:08:02.109847 4435920320 tpu_estimator.py:2160] examples/sec: 2.83101\n",
      "INFO:tensorflow:global_step/sec: 0.0888409\n",
      "I1118 05:08:13.365678 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888409\n",
      "INFO:tensorflow:examples/sec: 2.84291\n",
      "I1118 05:08:13.365894 4435920320 tpu_estimator.py:2160] examples/sec: 2.84291\n",
      "INFO:tensorflow:global_step/sec: 0.0893244\n",
      "I1118 05:08:24.560832 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893244\n",
      "INFO:tensorflow:examples/sec: 2.85838\n",
      "I1118 05:08:24.561050 4435920320 tpu_estimator.py:2160] examples/sec: 2.85838\n",
      "INFO:tensorflow:global_step/sec: 0.0889693\n",
      "I1118 05:08:35.800673 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889693\n",
      "INFO:tensorflow:examples/sec: 2.84702\n",
      "I1118 05:08:35.800908 4435920320 tpu_estimator.py:2160] examples/sec: 2.84702\n",
      "INFO:tensorflow:global_step/sec: 0.089123\n",
      "I1118 05:08:47.021106 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089123\n",
      "INFO:tensorflow:examples/sec: 2.85194\n",
      "I1118 05:08:47.021327 4435920320 tpu_estimator.py:2160] examples/sec: 2.85194\n",
      "INFO:tensorflow:global_step/sec: 0.0896923\n",
      "I1118 05:08:58.170353 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896923\n",
      "INFO:tensorflow:examples/sec: 2.87015\n",
      "I1118 05:08:58.170578 4435920320 tpu_estimator.py:2160] examples/sec: 2.87015\n",
      "INFO:tensorflow:global_step/sec: 0.0898995\n",
      "I1118 05:09:09.293869 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898995\n",
      "INFO:tensorflow:examples/sec: 2.87678\n",
      "I1118 05:09:09.294095 4435920320 tpu_estimator.py:2160] examples/sec: 2.87678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0879685\n",
      "I1118 05:09:20.661535 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879685\n",
      "INFO:tensorflow:examples/sec: 2.81499\n",
      "I1118 05:09:20.661741 4435920320 tpu_estimator.py:2160] examples/sec: 2.81499\n",
      "INFO:tensorflow:global_step/sec: 0.0886067\n",
      "I1118 05:09:31.947409 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886067\n",
      "INFO:tensorflow:examples/sec: 2.83541\n",
      "I1118 05:09:31.947632 4435920320 tpu_estimator.py:2160] examples/sec: 2.83541\n",
      "INFO:tensorflow:global_step/sec: 0.0884572\n",
      "I1118 05:09:43.252307 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884572\n",
      "INFO:tensorflow:examples/sec: 2.83063\n",
      "I1118 05:09:43.252525 4435920320 tpu_estimator.py:2160] examples/sec: 2.83063\n",
      "INFO:tensorflow:global_step/sec: 0.0890885\n",
      "I1118 05:09:54.477111 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890885\n",
      "INFO:tensorflow:examples/sec: 2.85083\n",
      "I1118 05:09:54.477338 4435920320 tpu_estimator.py:2160] examples/sec: 2.85083\n",
      "INFO:tensorflow:global_step/sec: 0.0890528\n",
      "I1118 05:10:05.706393 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890528\n",
      "INFO:tensorflow:examples/sec: 2.84969\n",
      "I1118 05:10:05.706617 4435920320 tpu_estimator.py:2160] examples/sec: 2.84969\n",
      "INFO:tensorflow:global_step/sec: 0.0892796\n",
      "I1118 05:10:16.907166 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892796\n",
      "INFO:tensorflow:examples/sec: 2.85695\n",
      "I1118 05:10:16.907410 4435920320 tpu_estimator.py:2160] examples/sec: 2.85695\n",
      "INFO:tensorflow:global_step/sec: 0.0888435\n",
      "I1118 05:10:28.162911 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888435\n",
      "INFO:tensorflow:examples/sec: 2.84299\n",
      "I1118 05:10:28.163132 4435920320 tpu_estimator.py:2160] examples/sec: 2.84299\n",
      "INFO:tensorflow:global_step/sec: 0.0891702\n",
      "I1118 05:10:39.377434 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891702\n",
      "INFO:tensorflow:examples/sec: 2.85345\n",
      "I1118 05:10:39.377667 4435920320 tpu_estimator.py:2160] examples/sec: 2.85345\n",
      "INFO:tensorflow:global_step/sec: 0.0895683\n",
      "I1118 05:10:50.542100 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895683\n",
      "INFO:tensorflow:examples/sec: 2.86619\n",
      "I1118 05:10:50.542310 4435920320 tpu_estimator.py:2160] examples/sec: 2.86619\n",
      "INFO:tensorflow:global_step/sec: 0.0899305\n",
      "I1118 05:11:01.661779 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899305\n",
      "INFO:tensorflow:examples/sec: 2.87778\n",
      "I1118 05:11:01.662008 4435920320 tpu_estimator.py:2160] examples/sec: 2.87778\n",
      "INFO:tensorflow:global_step/sec: 0.0892029\n",
      "I1118 05:11:12.872189 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892029\n",
      "INFO:tensorflow:examples/sec: 2.85449\n",
      "I1118 05:11:12.872416 4435920320 tpu_estimator.py:2160] examples/sec: 2.85449\n",
      "INFO:tensorflow:global_step/sec: 0.0888958\n",
      "I1118 05:11:24.121295 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888958\n",
      "INFO:tensorflow:examples/sec: 2.84467\n",
      "I1118 05:11:24.121520 4435920320 tpu_estimator.py:2160] examples/sec: 2.84467\n",
      "INFO:tensorflow:global_step/sec: 0.0894075\n",
      "I1118 05:11:35.306066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894075\n",
      "INFO:tensorflow:examples/sec: 2.86104\n",
      "I1118 05:11:35.306290 4435920320 tpu_estimator.py:2160] examples/sec: 2.86104\n",
      "INFO:tensorflow:global_step/sec: 0.089503\n",
      "I1118 05:11:46.478920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089503\n",
      "INFO:tensorflow:examples/sec: 2.86409\n",
      "I1118 05:11:46.479141 4435920320 tpu_estimator.py:2160] examples/sec: 2.86409\n",
      "INFO:tensorflow:global_step/sec: 0.0893179\n",
      "I1118 05:11:57.674820 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893179\n",
      "INFO:tensorflow:examples/sec: 2.85817\n",
      "I1118 05:11:57.675051 4435920320 tpu_estimator.py:2160] examples/sec: 2.85817\n",
      "INFO:tensorflow:global_step/sec: 0.0894623\n",
      "I1118 05:12:08.852711 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894623\n",
      "INFO:tensorflow:examples/sec: 2.86279\n",
      "I1118 05:12:08.852951 4435920320 tpu_estimator.py:2160] examples/sec: 2.86279\n",
      "INFO:tensorflow:global_step/sec: 0.0888504\n",
      "I1118 05:12:20.107589 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888504\n",
      "INFO:tensorflow:examples/sec: 2.84321\n",
      "I1118 05:12:20.107820 4435920320 tpu_estimator.py:2160] examples/sec: 2.84321\n",
      "INFO:tensorflow:global_step/sec: 0.0893263\n",
      "I1118 05:12:31.302515 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893263\n",
      "INFO:tensorflow:examples/sec: 2.85844\n",
      "I1118 05:12:31.302738 4435920320 tpu_estimator.py:2160] examples/sec: 2.85844\n",
      "INFO:tensorflow:global_step/sec: 0.0893605\n",
      "I1118 05:12:42.493125 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893605\n",
      "INFO:tensorflow:examples/sec: 2.85954\n",
      "I1118 05:12:42.493355 4435920320 tpu_estimator.py:2160] examples/sec: 2.85954\n",
      "INFO:tensorflow:global_step/sec: 0.0895006\n",
      "I1118 05:12:53.666243 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895006\n",
      "INFO:tensorflow:examples/sec: 2.86402\n",
      "I1118 05:12:53.666471 4435920320 tpu_estimator.py:2160] examples/sec: 2.86402\n",
      "INFO:tensorflow:global_step/sec: 0.0893219\n",
      "I1118 05:13:04.861698 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893219\n",
      "INFO:tensorflow:examples/sec: 2.8583\n",
      "I1118 05:13:04.861914 4435920320 tpu_estimator.py:2160] examples/sec: 2.8583\n",
      "INFO:tensorflow:global_step/sec: 0.0890787\n",
      "I1118 05:13:16.087744 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890787\n",
      "INFO:tensorflow:examples/sec: 2.85052\n",
      "I1118 05:13:16.087972 4435920320 tpu_estimator.py:2160] examples/sec: 2.85052\n",
      "INFO:tensorflow:global_step/sec: 0.0885954\n",
      "I1118 05:13:27.375000 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885954\n",
      "INFO:tensorflow:examples/sec: 2.83505\n",
      "I1118 05:13:27.375225 4435920320 tpu_estimator.py:2160] examples/sec: 2.83505\n",
      "INFO:tensorflow:global_step/sec: 0.0884088\n",
      "I1118 05:13:38.686092 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884088\n",
      "INFO:tensorflow:examples/sec: 2.82908\n",
      "I1118 05:13:38.686321 4435920320 tpu_estimator.py:2160] examples/sec: 2.82908\n",
      "INFO:tensorflow:global_step/sec: 0.0893178\n",
      "I1118 05:13:49.882078 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893178\n",
      "INFO:tensorflow:examples/sec: 2.85817\n",
      "I1118 05:13:49.882508 4435920320 tpu_estimator.py:2160] examples/sec: 2.85817\n",
      "INFO:tensorflow:global_step/sec: 0.0890618\n",
      "I1118 05:14:01.110215 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890618\n",
      "INFO:tensorflow:examples/sec: 2.84998\n",
      "I1118 05:14:01.110431 4435920320 tpu_estimator.py:2160] examples/sec: 2.84998\n",
      "INFO:tensorflow:global_step/sec: 0.0885379\n",
      "I1118 05:14:12.404844 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885379\n",
      "INFO:tensorflow:examples/sec: 2.83321\n",
      "I1118 05:14:12.405066 4435920320 tpu_estimator.py:2160] examples/sec: 2.83321\n",
      "INFO:tensorflow:global_step/sec: 0.0892412\n",
      "I1118 05:14:23.610399 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892412\n",
      "INFO:tensorflow:examples/sec: 2.85572\n",
      "I1118 05:14:23.610614 4435920320 tpu_estimator.py:2160] examples/sec: 2.85572\n",
      "INFO:tensorflow:global_step/sec: 0.0891572\n",
      "I1118 05:14:34.826560 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891572\n",
      "INFO:tensorflow:examples/sec: 2.85303\n",
      "I1118 05:14:34.826918 4435920320 tpu_estimator.py:2160] examples/sec: 2.85303\n",
      "INFO:tensorflow:global_step/sec: 0.0892306\n",
      "I1118 05:14:46.033483 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892306\n",
      "INFO:tensorflow:examples/sec: 2.85538\n",
      "I1118 05:14:46.033735 4435920320 tpu_estimator.py:2160] examples/sec: 2.85538\n",
      "INFO:tensorflow:global_step/sec: 0.088956\n",
      "I1118 05:14:57.274981 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088956\n",
      "INFO:tensorflow:examples/sec: 2.84659\n",
      "I1118 05:14:57.275230 4435920320 tpu_estimator.py:2160] examples/sec: 2.84659\n",
      "INFO:tensorflow:global_step/sec: 0.0894572\n",
      "I1118 05:15:08.453516 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894572\n",
      "INFO:tensorflow:examples/sec: 2.86263\n",
      "I1118 05:15:08.453735 4435920320 tpu_estimator.py:2160] examples/sec: 2.86263\n",
      "INFO:tensorflow:global_step/sec: 0.0889298\n",
      "I1118 05:15:19.698354 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889298\n",
      "INFO:tensorflow:examples/sec: 2.84575\n",
      "I1118 05:15:19.698590 4435920320 tpu_estimator.py:2160] examples/sec: 2.84575\n",
      "INFO:tensorflow:global_step/sec: 0.0891466\n",
      "I1118 05:15:30.915824 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891466\n",
      "INFO:tensorflow:examples/sec: 2.85269\n",
      "I1118 05:15:30.916032 4435920320 tpu_estimator.py:2160] examples/sec: 2.85269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893255\n",
      "I1118 05:15:42.110837 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893255\n",
      "INFO:tensorflow:examples/sec: 2.85841\n",
      "I1118 05:15:42.111063 4435920320 tpu_estimator.py:2160] examples/sec: 2.85841\n",
      "INFO:tensorflow:global_step/sec: 0.0897777\n",
      "I1118 05:15:53.249462 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897777\n",
      "INFO:tensorflow:examples/sec: 2.87289\n",
      "I1118 05:15:53.249675 4435920320 tpu_estimator.py:2160] examples/sec: 2.87289\n",
      "INFO:tensorflow:global_step/sec: 0.0885893\n",
      "I1118 05:16:04.537520 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885893\n",
      "INFO:tensorflow:examples/sec: 2.83486\n",
      "I1118 05:16:04.537786 4435920320 tpu_estimator.py:2160] examples/sec: 2.83486\n",
      "INFO:tensorflow:global_step/sec: 0.0888752\n",
      "I1118 05:16:15.789243 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888752\n",
      "INFO:tensorflow:examples/sec: 2.84401\n",
      "I1118 05:16:15.789467 4435920320 tpu_estimator.py:2160] examples/sec: 2.84401\n",
      "INFO:tensorflow:global_step/sec: 0.0887864\n",
      "I1118 05:16:27.052249 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887864\n",
      "INFO:tensorflow:examples/sec: 2.84116\n",
      "I1118 05:16:27.052472 4435920320 tpu_estimator.py:2160] examples/sec: 2.84116\n",
      "INFO:tensorflow:global_step/sec: 0.0891028\n",
      "I1118 05:16:38.275275 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891028\n",
      "INFO:tensorflow:examples/sec: 2.85129\n",
      "I1118 05:16:38.275496 4435920320 tpu_estimator.py:2160] examples/sec: 2.85129\n",
      "INFO:tensorflow:global_step/sec: 0.0889775\n",
      "I1118 05:16:49.514015 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889775\n",
      "INFO:tensorflow:examples/sec: 2.84728\n",
      "I1118 05:16:49.514229 4435920320 tpu_estimator.py:2160] examples/sec: 2.84728\n",
      "INFO:tensorflow:global_step/sec: 0.0886149\n",
      "I1118 05:17:00.798816 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886149\n",
      "INFO:tensorflow:examples/sec: 2.83568\n",
      "I1118 05:17:00.799067 4435920320 tpu_estimator.py:2160] examples/sec: 2.83568\n",
      "INFO:tensorflow:global_step/sec: 0.0889892\n",
      "I1118 05:17:12.036124 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889892\n",
      "INFO:tensorflow:examples/sec: 2.84765\n",
      "I1118 05:17:12.036339 4435920320 tpu_estimator.py:2160] examples/sec: 2.84765\n",
      "INFO:tensorflow:global_step/sec: 0.0892836\n",
      "I1118 05:17:23.236427 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892836\n",
      "INFO:tensorflow:examples/sec: 2.85707\n",
      "I1118 05:17:23.236654 4435920320 tpu_estimator.py:2160] examples/sec: 2.85707\n",
      "INFO:tensorflow:global_step/sec: 0.089266\n",
      "I1118 05:17:34.438896 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089266\n",
      "INFO:tensorflow:examples/sec: 2.85651\n",
      "I1118 05:17:34.439112 4435920320 tpu_estimator.py:2160] examples/sec: 2.85651\n",
      "INFO:tensorflow:global_step/sec: 0.0895961\n",
      "I1118 05:17:45.600095 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895961\n",
      "INFO:tensorflow:examples/sec: 2.86707\n",
      "I1118 05:17:45.600311 4435920320 tpu_estimator.py:2160] examples/sec: 2.86707\n",
      "INFO:tensorflow:global_step/sec: 0.0892355\n",
      "I1118 05:17:56.806380 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892355\n",
      "INFO:tensorflow:examples/sec: 2.85554\n",
      "I1118 05:17:56.806602 4435920320 tpu_estimator.py:2160] examples/sec: 2.85554\n",
      "INFO:tensorflow:global_step/sec: 0.0896351\n",
      "I1118 05:18:07.962738 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896351\n",
      "INFO:tensorflow:examples/sec: 2.86832\n",
      "I1118 05:18:07.962952 4435920320 tpu_estimator.py:2160] examples/sec: 2.86832\n",
      "INFO:tensorflow:global_step/sec: 0.0893724\n",
      "I1118 05:18:19.151880 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893724\n",
      "INFO:tensorflow:examples/sec: 2.85992\n",
      "I1118 05:18:19.152121 4435920320 tpu_estimator.py:2160] examples/sec: 2.85992\n",
      "INFO:tensorflow:global_step/sec: 0.089852\n",
      "I1118 05:18:30.281274 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089852\n",
      "INFO:tensorflow:examples/sec: 2.87526\n",
      "I1118 05:18:30.281489 4435920320 tpu_estimator.py:2160] examples/sec: 2.87526\n",
      "INFO:tensorflow:global_step/sec: 0.0893451\n",
      "I1118 05:18:41.473844 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893451\n",
      "INFO:tensorflow:examples/sec: 2.85904\n",
      "I1118 05:18:41.474070 4435920320 tpu_estimator.py:2160] examples/sec: 2.85904\n",
      "INFO:tensorflow:global_step/sec: 0.0897265\n",
      "I1118 05:18:52.618818 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897265\n",
      "INFO:tensorflow:examples/sec: 2.87125\n",
      "I1118 05:18:52.619039 4435920320 tpu_estimator.py:2160] examples/sec: 2.87125\n",
      "INFO:tensorflow:global_step/sec: 0.089203\n",
      "I1118 05:19:03.829205 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089203\n",
      "INFO:tensorflow:examples/sec: 2.8545\n",
      "I1118 05:19:03.829418 4435920320 tpu_estimator.py:2160] examples/sec: 2.8545\n",
      "INFO:tensorflow:global_step/sec: 0.0896681\n",
      "I1118 05:19:14.981461 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896681\n",
      "INFO:tensorflow:examples/sec: 2.86938\n",
      "I1118 05:19:14.981688 4435920320 tpu_estimator.py:2160] examples/sec: 2.86938\n",
      "INFO:tensorflow:global_step/sec: 0.091321\n",
      "I1118 05:19:25.931794 4435920320 tpu_estimator.py:2159] global_step/sec: 0.091321\n",
      "INFO:tensorflow:examples/sec: 2.92227\n",
      "I1118 05:19:25.931973 4435920320 tpu_estimator.py:2160] examples/sec: 2.92227\n",
      "INFO:tensorflow:global_step/sec: 0.0870178\n",
      "I1118 05:19:37.423758 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0870178\n",
      "INFO:tensorflow:examples/sec: 2.78457\n",
      "I1118 05:19:37.424138 4435920320 tpu_estimator.py:2160] examples/sec: 2.78457\n",
      "INFO:tensorflow:global_step/sec: 0.0892657\n",
      "I1118 05:19:48.626237 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892657\n",
      "INFO:tensorflow:examples/sec: 2.8565\n",
      "I1118 05:19:48.626451 4435920320 tpu_estimator.py:2160] examples/sec: 2.8565\n",
      "INFO:tensorflow:global_step/sec: 0.0893668\n",
      "I1118 05:19:59.816073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893668\n",
      "INFO:tensorflow:examples/sec: 2.85974\n",
      "I1118 05:19:59.816301 4435920320 tpu_estimator.py:2160] examples/sec: 2.85974\n",
      "INFO:tensorflow:global_step/sec: 0.0894293\n",
      "I1118 05:20:10.998117 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894293\n",
      "INFO:tensorflow:examples/sec: 2.86174\n",
      "I1118 05:20:10.998342 4435920320 tpu_estimator.py:2160] examples/sec: 2.86174\n",
      "INFO:tensorflow:global_step/sec: 0.0897588\n",
      "I1118 05:20:22.139073 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897588\n",
      "INFO:tensorflow:examples/sec: 2.87228\n",
      "I1118 05:20:22.139304 4435920320 tpu_estimator.py:2160] examples/sec: 2.87228\n",
      "INFO:tensorflow:global_step/sec: 0.0894089\n",
      "I1118 05:20:33.323637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894089\n",
      "INFO:tensorflow:examples/sec: 2.86108\n",
      "I1118 05:20:33.323862 4435920320 tpu_estimator.py:2160] examples/sec: 2.86108\n",
      "INFO:tensorflow:global_step/sec: 0.089282\n",
      "I1118 05:20:44.524111 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089282\n",
      "INFO:tensorflow:examples/sec: 2.85702\n",
      "I1118 05:20:44.524348 4435920320 tpu_estimator.py:2160] examples/sec: 2.85702\n",
      "INFO:tensorflow:global_step/sec: 0.0898688\n",
      "I1118 05:20:55.651448 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898688\n",
      "INFO:tensorflow:examples/sec: 2.8758\n",
      "I1118 05:20:55.651679 4435920320 tpu_estimator.py:2160] examples/sec: 2.8758\n",
      "INFO:tensorflow:global_step/sec: 0.089271\n",
      "I1118 05:21:06.853316 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089271\n",
      "INFO:tensorflow:examples/sec: 2.85667\n",
      "I1118 05:21:06.853548 4435920320 tpu_estimator.py:2160] examples/sec: 2.85667\n",
      "INFO:tensorflow:global_step/sec: 0.0893912\n",
      "I1118 05:21:18.040080 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893912\n",
      "INFO:tensorflow:examples/sec: 2.86052\n",
      "I1118 05:21:18.040301 4435920320 tpu_estimator.py:2160] examples/sec: 2.86052\n",
      "INFO:tensorflow:global_step/sec: 0.0894653\n",
      "I1118 05:21:29.217570 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894653\n",
      "INFO:tensorflow:examples/sec: 2.86289\n",
      "I1118 05:21:29.217781 4435920320 tpu_estimator.py:2160] examples/sec: 2.86289\n",
      "INFO:tensorflow:global_step/sec: 0.0874296\n",
      "I1118 05:21:40.655357 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0874296\n",
      "INFO:tensorflow:examples/sec: 2.79775\n",
      "I1118 05:21:40.655761 4435920320 tpu_estimator.py:2160] examples/sec: 2.79775\n",
      "INFO:tensorflow:global_step/sec: 0.0875649\n",
      "I1118 05:21:52.075453 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0875649\n",
      "INFO:tensorflow:examples/sec: 2.80208\n",
      "I1118 05:21:52.075674 4435920320 tpu_estimator.py:2160] examples/sec: 2.80208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0887199\n",
      "I1118 05:22:03.346899 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887199\n",
      "INFO:tensorflow:examples/sec: 2.83904\n",
      "I1118 05:22:03.347141 4435920320 tpu_estimator.py:2160] examples/sec: 2.83904\n",
      "INFO:tensorflow:global_step/sec: 0.0890071\n",
      "I1118 05:22:14.582000 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890071\n",
      "INFO:tensorflow:examples/sec: 2.84823\n",
      "I1118 05:22:14.582293 4435920320 tpu_estimator.py:2160] examples/sec: 2.84823\n",
      "INFO:tensorflow:global_step/sec: 0.0895574\n",
      "I1118 05:22:25.747982 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895574\n",
      "INFO:tensorflow:examples/sec: 2.86584\n",
      "I1118 05:22:25.748216 4435920320 tpu_estimator.py:2160] examples/sec: 2.86584\n",
      "INFO:tensorflow:global_step/sec: 0.0897075\n",
      "I1118 05:22:36.895318 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897075\n",
      "INFO:tensorflow:examples/sec: 2.87064\n",
      "I1118 05:22:36.895536 4435920320 tpu_estimator.py:2160] examples/sec: 2.87064\n",
      "INFO:tensorflow:global_step/sec: 0.0895548\n",
      "I1118 05:22:48.061664 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895548\n",
      "INFO:tensorflow:examples/sec: 2.86575\n",
      "I1118 05:22:48.061895 4435920320 tpu_estimator.py:2160] examples/sec: 2.86575\n",
      "INFO:tensorflow:global_step/sec: 0.0893374\n",
      "I1118 05:22:59.255192 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893374\n",
      "INFO:tensorflow:examples/sec: 2.8588\n",
      "I1118 05:22:59.255410 4435920320 tpu_estimator.py:2160] examples/sec: 2.8588\n",
      "INFO:tensorflow:global_step/sec: 0.088866\n",
      "I1118 05:23:10.508075 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088866\n",
      "INFO:tensorflow:examples/sec: 2.84371\n",
      "I1118 05:23:10.508304 4435920320 tpu_estimator.py:2160] examples/sec: 2.84371\n",
      "INFO:tensorflow:global_step/sec: 0.0894266\n",
      "I1118 05:23:21.690428 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894266\n",
      "INFO:tensorflow:examples/sec: 2.86165\n",
      "I1118 05:23:21.690645 4435920320 tpu_estimator.py:2160] examples/sec: 2.86165\n",
      "INFO:tensorflow:global_step/sec: 0.0884057\n",
      "I1118 05:23:33.001919 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884057\n",
      "INFO:tensorflow:examples/sec: 2.82898\n",
      "I1118 05:23:33.002132 4435920320 tpu_estimator.py:2160] examples/sec: 2.82898\n",
      "INFO:tensorflow:global_step/sec: 0.0889572\n",
      "I1118 05:23:44.243287 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889572\n",
      "INFO:tensorflow:examples/sec: 2.84663\n",
      "I1118 05:23:44.243519 4435920320 tpu_estimator.py:2160] examples/sec: 2.84663\n",
      "INFO:tensorflow:global_step/sec: 0.0890198\n",
      "I1118 05:23:55.476740 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890198\n",
      "INFO:tensorflow:examples/sec: 2.84863\n",
      "I1118 05:23:55.476983 4435920320 tpu_estimator.py:2160] examples/sec: 2.84863\n",
      "INFO:tensorflow:global_step/sec: 0.0888948\n",
      "I1118 05:24:06.726000 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888948\n",
      "INFO:tensorflow:examples/sec: 2.84463\n",
      "I1118 05:24:06.726238 4435920320 tpu_estimator.py:2160] examples/sec: 2.84463\n",
      "INFO:tensorflow:global_step/sec: 0.0888044\n",
      "I1118 05:24:17.986687 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888044\n",
      "INFO:tensorflow:examples/sec: 2.84174\n",
      "I1118 05:24:17.986909 4435920320 tpu_estimator.py:2160] examples/sec: 2.84174\n",
      "INFO:tensorflow:global_step/sec: 0.0894502\n",
      "I1118 05:24:29.166093 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894502\n",
      "INFO:tensorflow:examples/sec: 2.86241\n",
      "I1118 05:24:29.166312 4435920320 tpu_estimator.py:2160] examples/sec: 2.86241\n",
      "INFO:tensorflow:global_step/sec: 0.0892406\n",
      "I1118 05:24:40.371760 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892406\n",
      "INFO:tensorflow:examples/sec: 2.8557\n",
      "I1118 05:24:40.372005 4435920320 tpu_estimator.py:2160] examples/sec: 2.8557\n",
      "INFO:tensorflow:global_step/sec: 0.0890979\n",
      "I1118 05:24:51.595370 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890979\n",
      "INFO:tensorflow:examples/sec: 2.85113\n",
      "I1118 05:24:51.595602 4435920320 tpu_estimator.py:2160] examples/sec: 2.85113\n",
      "INFO:tensorflow:global_step/sec: 0.0890037\n",
      "I1118 05:25:02.830857 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890037\n",
      "INFO:tensorflow:examples/sec: 2.84812\n",
      "I1118 05:25:02.831089 4435920320 tpu_estimator.py:2160] examples/sec: 2.84812\n",
      "INFO:tensorflow:global_step/sec: 0.0894362\n",
      "I1118 05:25:14.012008 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894362\n",
      "INFO:tensorflow:examples/sec: 2.86196\n",
      "I1118 05:25:14.012229 4435920320 tpu_estimator.py:2160] examples/sec: 2.86196\n",
      "INFO:tensorflow:global_step/sec: 0.0899078\n",
      "I1118 05:25:25.134505 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899078\n",
      "INFO:tensorflow:examples/sec: 2.87705\n",
      "I1118 05:25:25.134725 4435920320 tpu_estimator.py:2160] examples/sec: 2.87705\n",
      "INFO:tensorflow:global_step/sec: 0.0900179\n",
      "I1118 05:25:36.243429 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900179\n",
      "INFO:tensorflow:examples/sec: 2.88057\n",
      "I1118 05:25:36.243653 4435920320 tpu_estimator.py:2160] examples/sec: 2.88057\n",
      "INFO:tensorflow:global_step/sec: 0.0895545\n",
      "I1118 05:25:47.409797 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895545\n",
      "INFO:tensorflow:examples/sec: 2.86574\n",
      "I1118 05:25:47.410006 4435920320 tpu_estimator.py:2160] examples/sec: 2.86574\n",
      "INFO:tensorflow:global_step/sec: 0.089019\n",
      "I1118 05:25:58.643362 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089019\n",
      "INFO:tensorflow:examples/sec: 2.84861\n",
      "I1118 05:25:58.643579 4435920320 tpu_estimator.py:2160] examples/sec: 2.84861\n",
      "INFO:tensorflow:global_step/sec: 0.0890872\n",
      "I1118 05:26:09.868319 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890872\n",
      "INFO:tensorflow:examples/sec: 2.85079\n",
      "I1118 05:26:09.868530 4435920320 tpu_estimator.py:2160] examples/sec: 2.85079\n",
      "INFO:tensorflow:global_step/sec: 0.0893909\n",
      "I1118 05:26:21.055155 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893909\n",
      "INFO:tensorflow:examples/sec: 2.86051\n",
      "I1118 05:26:21.055392 4435920320 tpu_estimator.py:2160] examples/sec: 2.86051\n",
      "INFO:tensorflow:global_step/sec: 0.0897021\n",
      "I1118 05:26:32.203171 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897021\n",
      "INFO:tensorflow:examples/sec: 2.87047\n",
      "I1118 05:26:32.203407 4435920320 tpu_estimator.py:2160] examples/sec: 2.87047\n",
      "INFO:tensorflow:global_step/sec: 0.089214\n",
      "I1118 05:26:43.412176 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089214\n",
      "INFO:tensorflow:examples/sec: 2.85485\n",
      "I1118 05:26:43.412429 4435920320 tpu_estimator.py:2160] examples/sec: 2.85485\n",
      "INFO:tensorflow:global_step/sec: 0.0893826\n",
      "I1118 05:26:54.600030 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893826\n",
      "INFO:tensorflow:examples/sec: 2.86024\n",
      "I1118 05:26:54.600277 4435920320 tpu_estimator.py:2160] examples/sec: 2.86024\n",
      "INFO:tensorflow:global_step/sec: 0.0892815\n",
      "I1118 05:27:05.800561 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892815\n",
      "INFO:tensorflow:examples/sec: 2.85701\n",
      "I1118 05:27:05.800780 4435920320 tpu_estimator.py:2160] examples/sec: 2.85701\n",
      "INFO:tensorflow:global_step/sec: 0.0892122\n",
      "I1118 05:27:17.009779 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892122\n",
      "INFO:tensorflow:examples/sec: 2.85479\n",
      "I1118 05:27:17.009999 4435920320 tpu_estimator.py:2160] examples/sec: 2.85479\n",
      "INFO:tensorflow:global_step/sec: 0.0896362\n",
      "I1118 05:27:28.166007 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896362\n",
      "INFO:tensorflow:examples/sec: 2.86836\n",
      "I1118 05:27:28.166232 4435920320 tpu_estimator.py:2160] examples/sec: 2.86836\n",
      "INFO:tensorflow:global_step/sec: 0.0894001\n",
      "I1118 05:27:39.351659 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894001\n",
      "INFO:tensorflow:examples/sec: 2.8608\n",
      "I1118 05:27:39.351902 4435920320 tpu_estimator.py:2160] examples/sec: 2.8608\n",
      "INFO:tensorflow:global_step/sec: 0.0897739\n",
      "I1118 05:27:50.490751 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897739\n",
      "INFO:tensorflow:examples/sec: 2.87276\n",
      "I1118 05:27:50.491206 4435920320 tpu_estimator.py:2160] examples/sec: 2.87276\n",
      "INFO:tensorflow:global_step/sec: 0.0889384\n",
      "I1118 05:28:01.734451 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889384\n",
      "INFO:tensorflow:examples/sec: 2.84603\n",
      "I1118 05:28:01.734675 4435920320 tpu_estimator.py:2160] examples/sec: 2.84603\n",
      "INFO:tensorflow:global_step/sec: 0.0895197\n",
      "I1118 05:28:12.905210 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895197\n",
      "INFO:tensorflow:examples/sec: 2.86463\n",
      "I1118 05:28:12.905436 4435920320 tpu_estimator.py:2160] examples/sec: 2.86463\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0887004\n",
      "I1118 05:28:24.179123 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887004\n",
      "INFO:tensorflow:examples/sec: 2.83841\n",
      "I1118 05:28:24.179337 4435920320 tpu_estimator.py:2160] examples/sec: 2.83841\n",
      "INFO:tensorflow:global_step/sec: 0.0892623\n",
      "I1118 05:28:35.382053 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892623\n",
      "INFO:tensorflow:examples/sec: 2.85639\n",
      "I1118 05:28:35.382278 4435920320 tpu_estimator.py:2160] examples/sec: 2.85639\n",
      "INFO:tensorflow:global_step/sec: 0.0893522\n",
      "I1118 05:28:46.573714 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893522\n",
      "INFO:tensorflow:examples/sec: 2.85927\n",
      "I1118 05:28:46.573933 4435920320 tpu_estimator.py:2160] examples/sec: 2.85927\n",
      "INFO:tensorflow:global_step/sec: 0.0893423\n",
      "I1118 05:28:57.766623 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893423\n",
      "INFO:tensorflow:examples/sec: 2.85895\n",
      "I1118 05:28:57.766845 4435920320 tpu_estimator.py:2160] examples/sec: 2.85895\n",
      "INFO:tensorflow:global_step/sec: 0.0888788\n",
      "I1118 05:29:09.017893 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888788\n",
      "INFO:tensorflow:examples/sec: 2.84412\n",
      "I1118 05:29:09.018100 4435920320 tpu_estimator.py:2160] examples/sec: 2.84412\n",
      "INFO:tensorflow:global_step/sec: 0.0885363\n",
      "I1118 05:29:20.312719 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885363\n",
      "INFO:tensorflow:examples/sec: 2.83316\n",
      "I1118 05:29:20.312924 4435920320 tpu_estimator.py:2160] examples/sec: 2.83316\n",
      "INFO:tensorflow:global_step/sec: 0.0890175\n",
      "I1118 05:29:31.546463 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890175\n",
      "INFO:tensorflow:examples/sec: 2.84856\n",
      "I1118 05:29:31.546694 4435920320 tpu_estimator.py:2160] examples/sec: 2.84856\n",
      "INFO:tensorflow:global_step/sec: 0.0888383\n",
      "I1118 05:29:42.802857 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888383\n",
      "INFO:tensorflow:examples/sec: 2.84283\n",
      "I1118 05:29:42.803103 4435920320 tpu_estimator.py:2160] examples/sec: 2.84283\n",
      "INFO:tensorflow:global_step/sec: 0.0890553\n",
      "I1118 05:29:54.031830 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890553\n",
      "INFO:tensorflow:examples/sec: 2.84977\n",
      "I1118 05:29:54.032100 4435920320 tpu_estimator.py:2160] examples/sec: 2.84977\n",
      "INFO:tensorflow:global_step/sec: 0.0895055\n",
      "I1118 05:30:05.204326 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895055\n",
      "INFO:tensorflow:examples/sec: 2.86418\n",
      "I1118 05:30:05.204549 4435920320 tpu_estimator.py:2160] examples/sec: 2.86418\n",
      "INFO:tensorflow:global_step/sec: 0.0895867\n",
      "I1118 05:30:16.366705 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895867\n",
      "INFO:tensorflow:examples/sec: 2.86677\n",
      "I1118 05:30:16.366927 4435920320 tpu_estimator.py:2160] examples/sec: 2.86677\n",
      "INFO:tensorflow:global_step/sec: 0.0897371\n",
      "I1118 05:30:27.510377 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897371\n",
      "INFO:tensorflow:examples/sec: 2.87159\n",
      "I1118 05:30:27.510607 4435920320 tpu_estimator.py:2160] examples/sec: 2.87159\n",
      "INFO:tensorflow:global_step/sec: 0.0896544\n",
      "I1118 05:30:38.664325 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896544\n",
      "INFO:tensorflow:examples/sec: 2.86894\n",
      "I1118 05:30:38.664569 4435920320 tpu_estimator.py:2160] examples/sec: 2.86894\n",
      "INFO:tensorflow:global_step/sec: 0.0889155\n",
      "I1118 05:30:49.910949 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889155\n",
      "INFO:tensorflow:examples/sec: 2.84529\n",
      "I1118 05:30:49.911163 4435920320 tpu_estimator.py:2160] examples/sec: 2.84529\n",
      "INFO:tensorflow:global_step/sec: 0.0890302\n",
      "I1118 05:31:01.143109 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890302\n",
      "INFO:tensorflow:examples/sec: 2.84897\n",
      "I1118 05:31:01.143333 4435920320 tpu_estimator.py:2160] examples/sec: 2.84897\n",
      "INFO:tensorflow:global_step/sec: 0.0890383\n",
      "I1118 05:31:12.374210 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890383\n",
      "INFO:tensorflow:examples/sec: 2.84923\n",
      "I1118 05:31:12.375658 4435920320 tpu_estimator.py:2160] examples/sec: 2.84923\n",
      "INFO:tensorflow:global_step/sec: 0.0895214\n",
      "I1118 05:31:23.544730 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895214\n",
      "INFO:tensorflow:examples/sec: 2.86468\n",
      "I1118 05:31:23.544960 4435920320 tpu_estimator.py:2160] examples/sec: 2.86468\n",
      "INFO:tensorflow:global_step/sec: 0.0892732\n",
      "I1118 05:31:34.746284 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892732\n",
      "INFO:tensorflow:examples/sec: 2.85674\n",
      "I1118 05:31:34.746501 4435920320 tpu_estimator.py:2160] examples/sec: 2.85674\n",
      "INFO:tensorflow:global_step/sec: 0.0895296\n",
      "I1118 05:31:45.915770 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895296\n",
      "INFO:tensorflow:examples/sec: 2.86495\n",
      "I1118 05:31:45.915990 4435920320 tpu_estimator.py:2160] examples/sec: 2.86495\n",
      "INFO:tensorflow:global_step/sec: 0.0889951\n",
      "I1118 05:31:57.152371 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889951\n",
      "INFO:tensorflow:examples/sec: 2.84784\n",
      "I1118 05:31:57.152588 4435920320 tpu_estimator.py:2160] examples/sec: 2.84784\n",
      "INFO:tensorflow:global_step/sec: 0.089166\n",
      "I1118 05:32:08.367397 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089166\n",
      "INFO:tensorflow:examples/sec: 2.85331\n",
      "I1118 05:32:08.367690 4435920320 tpu_estimator.py:2160] examples/sec: 2.85331\n",
      "INFO:tensorflow:global_step/sec: 0.0895914\n",
      "I1118 05:32:19.529181 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895914\n",
      "INFO:tensorflow:examples/sec: 2.86693\n",
      "I1118 05:32:19.529408 4435920320 tpu_estimator.py:2160] examples/sec: 2.86693\n",
      "INFO:tensorflow:global_step/sec: 0.0898763\n",
      "I1118 05:32:30.655581 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898763\n",
      "INFO:tensorflow:examples/sec: 2.87604\n",
      "I1118 05:32:30.655814 4435920320 tpu_estimator.py:2160] examples/sec: 2.87604\n",
      "INFO:tensorflow:global_step/sec: 0.0889019\n",
      "I1118 05:32:41.903920 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889019\n",
      "INFO:tensorflow:examples/sec: 2.84486\n",
      "I1118 05:32:41.904143 4435920320 tpu_estimator.py:2160] examples/sec: 2.84486\n",
      "INFO:tensorflow:global_step/sec: 0.0887113\n",
      "I1118 05:32:53.176449 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887113\n",
      "INFO:tensorflow:examples/sec: 2.83876\n",
      "I1118 05:32:53.176689 4435920320 tpu_estimator.py:2160] examples/sec: 2.83876\n",
      "INFO:tensorflow:global_step/sec: 0.0892627\n",
      "I1118 05:33:04.379346 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892627\n",
      "INFO:tensorflow:examples/sec: 2.85641\n",
      "I1118 05:33:04.379580 4435920320 tpu_estimator.py:2160] examples/sec: 2.85641\n",
      "INFO:tensorflow:global_step/sec: 0.0895881\n",
      "I1118 05:33:15.541526 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895881\n",
      "INFO:tensorflow:examples/sec: 2.86682\n",
      "I1118 05:33:15.541743 4435920320 tpu_estimator.py:2160] examples/sec: 2.86682\n",
      "INFO:tensorflow:global_step/sec: 0.0889546\n",
      "I1118 05:33:26.783232 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889546\n",
      "INFO:tensorflow:examples/sec: 2.84655\n",
      "I1118 05:33:26.783454 4435920320 tpu_estimator.py:2160] examples/sec: 2.84655\n",
      "INFO:tensorflow:global_step/sec: 0.0888536\n",
      "I1118 05:33:38.037711 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888536\n",
      "INFO:tensorflow:examples/sec: 2.84331\n",
      "I1118 05:33:38.037925 4435920320 tpu_estimator.py:2160] examples/sec: 2.84331\n",
      "INFO:tensorflow:global_step/sec: 0.0896904\n",
      "I1118 05:33:49.187210 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896904\n",
      "INFO:tensorflow:examples/sec: 2.87009\n",
      "I1118 05:33:49.187433 4435920320 tpu_estimator.py:2160] examples/sec: 2.87009\n",
      "INFO:tensorflow:global_step/sec: 0.0893618\n",
      "I1118 05:34:00.377626 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893618\n",
      "INFO:tensorflow:examples/sec: 2.85958\n",
      "I1118 05:34:00.377846 4435920320 tpu_estimator.py:2160] examples/sec: 2.85958\n",
      "INFO:tensorflow:global_step/sec: 0.0898946\n",
      "I1118 05:34:11.501770 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898946\n",
      "INFO:tensorflow:examples/sec: 2.87663\n",
      "I1118 05:34:11.502000 4435920320 tpu_estimator.py:2160] examples/sec: 2.87663\n",
      "INFO:tensorflow:global_step/sec: 0.0895836\n",
      "I1118 05:34:22.664517 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895836\n",
      "INFO:tensorflow:examples/sec: 2.86668\n",
      "I1118 05:34:22.664735 4435920320 tpu_estimator.py:2160] examples/sec: 2.86668\n",
      "INFO:tensorflow:global_step/sec: 0.0894888\n",
      "I1118 05:34:33.839117 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894888\n",
      "INFO:tensorflow:examples/sec: 2.86364\n",
      "I1118 05:34:33.839331 4435920320 tpu_estimator.py:2160] examples/sec: 2.86364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893703\n",
      "I1118 05:34:45.028506 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893703\n",
      "INFO:tensorflow:examples/sec: 2.85985\n",
      "I1118 05:34:45.028723 4435920320 tpu_estimator.py:2160] examples/sec: 2.85985\n",
      "INFO:tensorflow:global_step/sec: 0.0895223\n",
      "I1118 05:34:56.198904 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895223\n",
      "INFO:tensorflow:examples/sec: 2.86471\n",
      "I1118 05:34:56.199128 4435920320 tpu_estimator.py:2160] examples/sec: 2.86471\n",
      "INFO:tensorflow:global_step/sec: 0.0892388\n",
      "I1118 05:35:07.404793 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892388\n",
      "INFO:tensorflow:examples/sec: 2.85564\n",
      "I1118 05:35:07.405015 4435920320 tpu_estimator.py:2160] examples/sec: 2.85564\n",
      "INFO:tensorflow:global_step/sec: 0.0889252\n",
      "I1118 05:35:18.650213 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889252\n",
      "INFO:tensorflow:examples/sec: 2.84561\n",
      "I1118 05:35:18.650447 4435920320 tpu_estimator.py:2160] examples/sec: 2.84561\n",
      "INFO:tensorflow:global_step/sec: 0.0891215\n",
      "I1118 05:35:29.870836 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891215\n",
      "INFO:tensorflow:examples/sec: 2.85189\n",
      "I1118 05:35:29.871051 4435920320 tpu_estimator.py:2160] examples/sec: 2.85189\n",
      "INFO:tensorflow:global_step/sec: 0.089156\n",
      "I1118 05:35:41.087162 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089156\n",
      "INFO:tensorflow:examples/sec: 2.85299\n",
      "I1118 05:35:41.087399 4435920320 tpu_estimator.py:2160] examples/sec: 2.85299\n",
      "INFO:tensorflow:global_step/sec: 0.0893707\n",
      "I1118 05:35:52.276480 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893707\n",
      "INFO:tensorflow:examples/sec: 2.85986\n",
      "I1118 05:35:52.276700 4435920320 tpu_estimator.py:2160] examples/sec: 2.85986\n",
      "INFO:tensorflow:global_step/sec: 0.0893846\n",
      "I1118 05:36:03.464094 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893846\n",
      "INFO:tensorflow:examples/sec: 2.86031\n",
      "I1118 05:36:03.464316 4435920320 tpu_estimator.py:2160] examples/sec: 2.86031\n",
      "INFO:tensorflow:global_step/sec: 0.0894565\n",
      "I1118 05:36:14.642699 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894565\n",
      "INFO:tensorflow:examples/sec: 2.86261\n",
      "I1118 05:36:14.642910 4435920320 tpu_estimator.py:2160] examples/sec: 2.86261\n",
      "INFO:tensorflow:global_step/sec: 0.0894277\n",
      "I1118 05:36:25.824924 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894277\n",
      "INFO:tensorflow:examples/sec: 2.86169\n",
      "I1118 05:36:25.825139 4435920320 tpu_estimator.py:2160] examples/sec: 2.86169\n",
      "INFO:tensorflow:global_step/sec: 0.0894739\n",
      "I1118 05:36:37.001377 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894739\n",
      "INFO:tensorflow:examples/sec: 2.86316\n",
      "I1118 05:36:37.001611 4435920320 tpu_estimator.py:2160] examples/sec: 2.86316\n",
      "INFO:tensorflow:global_step/sec: 0.0893782\n",
      "I1118 05:36:48.189786 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893782\n",
      "INFO:tensorflow:examples/sec: 2.8601\n",
      "I1118 05:36:48.190015 4435920320 tpu_estimator.py:2160] examples/sec: 2.8601\n",
      "INFO:tensorflow:global_step/sec: 0.0894931\n",
      "I1118 05:36:59.363840 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894931\n",
      "INFO:tensorflow:examples/sec: 2.86378\n",
      "I1118 05:36:59.364070 4435920320 tpu_estimator.py:2160] examples/sec: 2.86378\n",
      "INFO:tensorflow:global_step/sec: 0.0897866\n",
      "I1118 05:37:10.501357 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897866\n",
      "INFO:tensorflow:examples/sec: 2.87317\n",
      "I1118 05:37:10.501589 4435920320 tpu_estimator.py:2160] examples/sec: 2.87317\n",
      "INFO:tensorflow:global_step/sec: 0.0895633\n",
      "I1118 05:37:21.666636 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895633\n",
      "INFO:tensorflow:examples/sec: 2.86602\n",
      "I1118 05:37:21.666858 4435920320 tpu_estimator.py:2160] examples/sec: 2.86602\n",
      "INFO:tensorflow:global_step/sec: 0.08959\n",
      "I1118 05:37:32.828593 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08959\n",
      "INFO:tensorflow:examples/sec: 2.86688\n",
      "I1118 05:37:32.828803 4435920320 tpu_estimator.py:2160] examples/sec: 2.86688\n",
      "INFO:tensorflow:global_step/sec: 0.0887545\n",
      "I1118 05:37:44.095642 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887545\n",
      "INFO:tensorflow:examples/sec: 2.84014\n",
      "I1118 05:37:44.096013 4435920320 tpu_estimator.py:2160] examples/sec: 2.84014\n",
      "INFO:tensorflow:global_step/sec: 0.0889387\n",
      "I1118 05:37:55.339348 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889387\n",
      "INFO:tensorflow:examples/sec: 2.84604\n",
      "I1118 05:37:55.339599 4435920320 tpu_estimator.py:2160] examples/sec: 2.84604\n",
      "INFO:tensorflow:global_step/sec: 0.0896427\n",
      "I1118 05:38:06.494760 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896427\n",
      "INFO:tensorflow:examples/sec: 2.86857\n",
      "I1118 05:38:06.494997 4435920320 tpu_estimator.py:2160] examples/sec: 2.86857\n",
      "INFO:tensorflow:global_step/sec: 0.0890331\n",
      "I1118 05:38:17.726515 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890331\n",
      "INFO:tensorflow:examples/sec: 2.84906\n",
      "I1118 05:38:17.726735 4435920320 tpu_estimator.py:2160] examples/sec: 2.84906\n",
      "INFO:tensorflow:global_step/sec: 0.0888214\n",
      "I1118 05:38:28.985061 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888214\n",
      "INFO:tensorflow:examples/sec: 2.84228\n",
      "I1118 05:38:28.985277 4435920320 tpu_estimator.py:2160] examples/sec: 2.84228\n",
      "INFO:tensorflow:global_step/sec: 0.0889631\n",
      "I1118 05:38:40.225682 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889631\n",
      "INFO:tensorflow:examples/sec: 2.84682\n",
      "I1118 05:38:40.225898 4435920320 tpu_estimator.py:2160] examples/sec: 2.84682\n",
      "INFO:tensorflow:global_step/sec: 0.0891001\n",
      "I1118 05:38:51.449016 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891001\n",
      "INFO:tensorflow:examples/sec: 2.8512\n",
      "I1118 05:38:51.449232 4435920320 tpu_estimator.py:2160] examples/sec: 2.8512\n",
      "INFO:tensorflow:global_step/sec: 0.0887437\n",
      "I1118 05:39:02.717432 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887437\n",
      "INFO:tensorflow:examples/sec: 2.8398\n",
      "I1118 05:39:02.717656 4435920320 tpu_estimator.py:2160] examples/sec: 2.8398\n",
      "INFO:tensorflow:global_step/sec: 0.0892594\n",
      "I1118 05:39:13.920730 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892594\n",
      "INFO:tensorflow:examples/sec: 2.8563\n",
      "I1118 05:39:13.920963 4435920320 tpu_estimator.py:2160] examples/sec: 2.8563\n",
      "INFO:tensorflow:global_step/sec: 0.0893105\n",
      "I1118 05:39:25.117645 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893105\n",
      "INFO:tensorflow:examples/sec: 2.85794\n",
      "I1118 05:39:25.117866 4435920320 tpu_estimator.py:2160] examples/sec: 2.85794\n",
      "INFO:tensorflow:global_step/sec: 0.0893596\n",
      "I1118 05:39:36.308361 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893596\n",
      "INFO:tensorflow:examples/sec: 2.85951\n",
      "I1118 05:39:36.308733 4435920320 tpu_estimator.py:2160] examples/sec: 2.85951\n",
      "INFO:tensorflow:global_step/sec: 0.0895174\n",
      "I1118 05:39:47.479358 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895174\n",
      "INFO:tensorflow:examples/sec: 2.86456\n",
      "I1118 05:39:47.479565 4435920320 tpu_estimator.py:2160] examples/sec: 2.86456\n",
      "INFO:tensorflow:global_step/sec: 0.0894246\n",
      "I1118 05:39:58.661982 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894246\n",
      "INFO:tensorflow:examples/sec: 2.86159\n",
      "I1118 05:39:58.662199 4435920320 tpu_estimator.py:2160] examples/sec: 2.86159\n",
      "INFO:tensorflow:global_step/sec: 0.089091\n",
      "I1118 05:40:09.886479 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089091\n",
      "INFO:tensorflow:examples/sec: 2.85091\n",
      "I1118 05:40:09.886721 4435920320 tpu_estimator.py:2160] examples/sec: 2.85091\n",
      "INFO:tensorflow:global_step/sec: 0.0896034\n",
      "I1118 05:40:21.046756 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896034\n",
      "INFO:tensorflow:examples/sec: 2.86731\n",
      "I1118 05:40:21.046977 4435920320 tpu_estimator.py:2160] examples/sec: 2.86731\n",
      "INFO:tensorflow:global_step/sec: 0.0893909\n",
      "I1118 05:40:32.233597 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893909\n",
      "INFO:tensorflow:examples/sec: 2.86051\n",
      "I1118 05:40:32.233819 4435920320 tpu_estimator.py:2160] examples/sec: 2.86051\n",
      "INFO:tensorflow:global_step/sec: 0.0895349\n",
      "I1118 05:40:43.402431 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895349\n",
      "INFO:tensorflow:examples/sec: 2.86512\n",
      "I1118 05:40:43.402652 4435920320 tpu_estimator.py:2160] examples/sec: 2.86512\n",
      "INFO:tensorflow:global_step/sec: 0.0895523\n",
      "I1118 05:40:54.569069 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895523\n",
      "INFO:tensorflow:examples/sec: 2.86567\n",
      "I1118 05:40:54.569286 4435920320 tpu_estimator.py:2160] examples/sec: 2.86567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0895176\n",
      "I1118 05:41:05.740077 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895176\n",
      "INFO:tensorflow:examples/sec: 2.86456\n",
      "I1118 05:41:05.740292 4435920320 tpu_estimator.py:2160] examples/sec: 2.86456\n",
      "INFO:tensorflow:global_step/sec: 0.0889188\n",
      "I1118 05:41:16.986274 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889188\n",
      "INFO:tensorflow:examples/sec: 2.8454\n",
      "I1118 05:41:16.986495 4435920320 tpu_estimator.py:2160] examples/sec: 2.8454\n",
      "INFO:tensorflow:global_step/sec: 0.089095\n",
      "I1118 05:41:28.210249 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089095\n",
      "INFO:tensorflow:examples/sec: 2.85104\n",
      "I1118 05:41:28.210471 4435920320 tpu_estimator.py:2160] examples/sec: 2.85104\n",
      "INFO:tensorflow:global_step/sec: 0.0889034\n",
      "I1118 05:41:39.458434 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889034\n",
      "INFO:tensorflow:examples/sec: 2.84491\n",
      "I1118 05:41:39.458664 4435920320 tpu_estimator.py:2160] examples/sec: 2.84491\n",
      "INFO:tensorflow:global_step/sec: 0.0895686\n",
      "I1118 05:41:50.623106 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895686\n",
      "INFO:tensorflow:examples/sec: 2.8662\n",
      "I1118 05:41:50.623327 4435920320 tpu_estimator.py:2160] examples/sec: 2.8662\n",
      "INFO:tensorflow:global_step/sec: 0.089134\n",
      "I1118 05:42:01.842101 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089134\n",
      "INFO:tensorflow:examples/sec: 2.85229\n",
      "I1118 05:42:01.842507 4435920320 tpu_estimator.py:2160] examples/sec: 2.85229\n",
      "INFO:tensorflow:global_step/sec: 0.0892366\n",
      "I1118 05:42:13.048279 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892366\n",
      "INFO:tensorflow:examples/sec: 2.85557\n",
      "I1118 05:42:13.048509 4435920320 tpu_estimator.py:2160] examples/sec: 2.85557\n",
      "INFO:tensorflow:global_step/sec: 0.089601\n",
      "I1118 05:42:24.208858 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089601\n",
      "INFO:tensorflow:examples/sec: 2.86723\n",
      "I1118 05:42:24.209081 4435920320 tpu_estimator.py:2160] examples/sec: 2.86723\n",
      "INFO:tensorflow:global_step/sec: 0.0894983\n",
      "I1118 05:42:35.382244 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894983\n",
      "INFO:tensorflow:examples/sec: 2.86395\n",
      "I1118 05:42:35.382462 4435920320 tpu_estimator.py:2160] examples/sec: 2.86395\n",
      "INFO:tensorflow:global_step/sec: 0.0904246\n",
      "I1118 05:42:46.441199 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0904246\n",
      "INFO:tensorflow:examples/sec: 2.89359\n",
      "I1118 05:42:46.441434 4435920320 tpu_estimator.py:2160] examples/sec: 2.89359\n",
      "INFO:tensorflow:global_step/sec: 0.0901746\n",
      "I1118 05:42:57.530778 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0901746\n",
      "INFO:tensorflow:examples/sec: 2.88559\n",
      "I1118 05:42:57.530993 4435920320 tpu_estimator.py:2160] examples/sec: 2.88559\n",
      "INFO:tensorflow:global_step/sec: 0.0897723\n",
      "I1118 05:43:08.670079 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897723\n",
      "INFO:tensorflow:examples/sec: 2.87272\n",
      "I1118 05:43:08.670305 4435920320 tpu_estimator.py:2160] examples/sec: 2.87272\n",
      "INFO:tensorflow:global_step/sec: 0.0894353\n",
      "I1118 05:43:19.851339 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894353\n",
      "INFO:tensorflow:examples/sec: 2.86193\n",
      "I1118 05:43:19.851549 4435920320 tpu_estimator.py:2160] examples/sec: 2.86193\n",
      "INFO:tensorflow:global_step/sec: 0.0888396\n",
      "I1118 05:43:31.107712 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888396\n",
      "INFO:tensorflow:examples/sec: 2.84287\n",
      "I1118 05:43:31.108382 4435920320 tpu_estimator.py:2160] examples/sec: 2.84287\n",
      "INFO:tensorflow:global_step/sec: 0.0893133\n",
      "I1118 05:43:42.304130 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893133\n",
      "INFO:tensorflow:examples/sec: 2.85802\n",
      "I1118 05:43:42.304363 4435920320 tpu_estimator.py:2160] examples/sec: 2.85802\n",
      "INFO:tensorflow:global_step/sec: 0.0891293\n",
      "I1118 05:43:53.523802 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891293\n",
      "INFO:tensorflow:examples/sec: 2.85214\n",
      "I1118 05:43:53.524030 4435920320 tpu_estimator.py:2160] examples/sec: 2.85214\n",
      "INFO:tensorflow:global_step/sec: 0.0898323\n",
      "I1118 05:44:04.655630 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898323\n",
      "INFO:tensorflow:examples/sec: 2.87463\n",
      "I1118 05:44:04.655848 4435920320 tpu_estimator.py:2160] examples/sec: 2.87463\n",
      "INFO:tensorflow:global_step/sec: 0.0889336\n",
      "I1118 05:44:15.899983 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889336\n",
      "INFO:tensorflow:examples/sec: 2.84587\n",
      "I1118 05:44:15.900213 4435920320 tpu_estimator.py:2160] examples/sec: 2.84587\n",
      "INFO:tensorflow:global_step/sec: 0.0894628\n",
      "I1118 05:44:27.077811 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894628\n",
      "INFO:tensorflow:examples/sec: 2.86281\n",
      "I1118 05:44:27.078031 4435920320 tpu_estimator.py:2160] examples/sec: 2.86281\n",
      "INFO:tensorflow:global_step/sec: 0.0894442\n",
      "I1118 05:44:38.257972 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894442\n",
      "INFO:tensorflow:examples/sec: 2.86222\n",
      "I1118 05:44:38.258201 4435920320 tpu_estimator.py:2160] examples/sec: 2.86222\n",
      "INFO:tensorflow:global_step/sec: 0.0893131\n",
      "I1118 05:44:49.454547 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893131\n",
      "INFO:tensorflow:examples/sec: 2.85802\n",
      "I1118 05:44:49.454778 4435920320 tpu_estimator.py:2160] examples/sec: 2.85802\n",
      "INFO:tensorflow:global_step/sec: 0.0896028\n",
      "I1118 05:45:00.614899 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896028\n",
      "INFO:tensorflow:examples/sec: 2.86729\n",
      "I1118 05:45:00.615114 4435920320 tpu_estimator.py:2160] examples/sec: 2.86729\n",
      "INFO:tensorflow:global_step/sec: 0.0892915\n",
      "I1118 05:45:11.814187 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892915\n",
      "INFO:tensorflow:examples/sec: 2.85733\n",
      "I1118 05:45:11.814593 4435920320 tpu_estimator.py:2160] examples/sec: 2.85733\n",
      "INFO:tensorflow:global_step/sec: 0.0888136\n",
      "I1118 05:45:23.073725 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888136\n",
      "INFO:tensorflow:examples/sec: 2.84203\n",
      "I1118 05:45:23.073961 4435920320 tpu_estimator.py:2160] examples/sec: 2.84203\n",
      "INFO:tensorflow:global_step/sec: 0.0891781\n",
      "I1118 05:45:34.287237 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891781\n",
      "INFO:tensorflow:examples/sec: 2.8537\n",
      "I1118 05:45:34.287443 4435920320 tpu_estimator.py:2160] examples/sec: 2.8537\n",
      "INFO:tensorflow:global_step/sec: 0.0893509\n",
      "I1118 05:45:45.479066 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893509\n",
      "INFO:tensorflow:examples/sec: 2.85923\n",
      "I1118 05:45:45.479409 4435920320 tpu_estimator.py:2160] examples/sec: 2.85923\n",
      "INFO:tensorflow:global_step/sec: 0.0893792\n",
      "I1118 05:45:56.667334 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893792\n",
      "INFO:tensorflow:examples/sec: 2.86013\n",
      "I1118 05:45:56.667557 4435920320 tpu_estimator.py:2160] examples/sec: 2.86013\n",
      "INFO:tensorflow:global_step/sec: 0.0890632\n",
      "I1118 05:46:07.895314 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890632\n",
      "INFO:tensorflow:examples/sec: 2.85002\n",
      "I1118 05:46:07.895524 4435920320 tpu_estimator.py:2160] examples/sec: 2.85002\n",
      "INFO:tensorflow:global_step/sec: 0.0883627\n",
      "I1118 05:46:19.212306 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883627\n",
      "INFO:tensorflow:examples/sec: 2.82761\n",
      "I1118 05:46:19.212522 4435920320 tpu_estimator.py:2160] examples/sec: 2.82761\n",
      "INFO:tensorflow:global_step/sec: 0.0891264\n",
      "I1118 05:46:30.432336 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891264\n",
      "INFO:tensorflow:examples/sec: 2.85204\n",
      "I1118 05:46:30.432570 4435920320 tpu_estimator.py:2160] examples/sec: 2.85204\n",
      "INFO:tensorflow:global_step/sec: 0.0893697\n",
      "I1118 05:46:41.621793 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893697\n",
      "INFO:tensorflow:examples/sec: 2.85983\n",
      "I1118 05:46:41.622016 4435920320 tpu_estimator.py:2160] examples/sec: 2.85983\n",
      "INFO:tensorflow:global_step/sec: 0.0894209\n",
      "I1118 05:46:52.804872 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894209\n",
      "INFO:tensorflow:examples/sec: 2.86147\n",
      "I1118 05:46:52.805102 4435920320 tpu_estimator.py:2160] examples/sec: 2.86147\n",
      "INFO:tensorflow:global_step/sec: 0.089439\n",
      "I1118 05:47:03.985682 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089439\n",
      "INFO:tensorflow:examples/sec: 2.86205\n",
      "I1118 05:47:03.985908 4435920320 tpu_estimator.py:2160] examples/sec: 2.86205\n",
      "INFO:tensorflow:global_step/sec: 0.0889381\n",
      "I1118 05:47:15.229465 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889381\n",
      "INFO:tensorflow:examples/sec: 2.84602\n",
      "I1118 05:47:15.229683 4435920320 tpu_estimator.py:2160] examples/sec: 2.84602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0900222\n",
      "I1118 05:47:26.337840 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900222\n",
      "INFO:tensorflow:examples/sec: 2.88071\n",
      "I1118 05:47:26.338060 4435920320 tpu_estimator.py:2160] examples/sec: 2.88071\n",
      "INFO:tensorflow:global_step/sec: 0.0896848\n",
      "I1118 05:47:37.488015 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896848\n",
      "INFO:tensorflow:examples/sec: 2.86991\n",
      "I1118 05:47:37.488315 4435920320 tpu_estimator.py:2160] examples/sec: 2.86991\n",
      "INFO:tensorflow:global_step/sec: 0.0896521\n",
      "I1118 05:47:48.642205 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896521\n",
      "INFO:tensorflow:examples/sec: 2.86887\n",
      "I1118 05:47:48.642421 4435920320 tpu_estimator.py:2160] examples/sec: 2.86887\n",
      "INFO:tensorflow:global_step/sec: 0.0893411\n",
      "I1118 05:47:59.835274 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893411\n",
      "INFO:tensorflow:examples/sec: 2.85891\n",
      "I1118 05:47:59.835495 4435920320 tpu_estimator.py:2160] examples/sec: 2.85891\n",
      "INFO:tensorflow:global_step/sec: 0.0899452\n",
      "I1118 05:48:10.953160 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899452\n",
      "INFO:tensorflow:examples/sec: 2.87825\n",
      "I1118 05:48:10.953390 4435920320 tpu_estimator.py:2160] examples/sec: 2.87825\n",
      "INFO:tensorflow:global_step/sec: 0.0895643\n",
      "I1118 05:48:22.118311 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895643\n",
      "INFO:tensorflow:examples/sec: 2.86606\n",
      "I1118 05:48:22.118525 4435920320 tpu_estimator.py:2160] examples/sec: 2.86606\n",
      "INFO:tensorflow:global_step/sec: 0.0892244\n",
      "I1118 05:48:33.326010 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892244\n",
      "INFO:tensorflow:examples/sec: 2.85518\n",
      "I1118 05:48:33.326220 4435920320 tpu_estimator.py:2160] examples/sec: 2.85518\n",
      "INFO:tensorflow:global_step/sec: 0.0892267\n",
      "I1118 05:48:44.533421 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892267\n",
      "INFO:tensorflow:examples/sec: 2.85525\n",
      "I1118 05:48:44.533647 4435920320 tpu_estimator.py:2160] examples/sec: 2.85525\n",
      "INFO:tensorflow:global_step/sec: 0.0898102\n",
      "I1118 05:48:55.668015 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898102\n",
      "INFO:tensorflow:examples/sec: 2.87393\n",
      "I1118 05:48:55.668429 4435920320 tpu_estimator.py:2160] examples/sec: 2.87393\n",
      "INFO:tensorflow:global_step/sec: 0.0893392\n",
      "I1118 05:49:06.861304 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893392\n",
      "INFO:tensorflow:examples/sec: 2.85885\n",
      "I1118 05:49:06.861526 4435920320 tpu_estimator.py:2160] examples/sec: 2.85885\n",
      "INFO:tensorflow:global_step/sec: 0.0894905\n",
      "I1118 05:49:18.035683 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894905\n",
      "INFO:tensorflow:examples/sec: 2.8637\n",
      "I1118 05:49:18.035909 4435920320 tpu_estimator.py:2160] examples/sec: 2.8637\n",
      "INFO:tensorflow:global_step/sec: 0.0891816\n",
      "I1118 05:49:29.248771 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891816\n",
      "INFO:tensorflow:examples/sec: 2.85381\n",
      "I1118 05:49:29.249050 4435920320 tpu_estimator.py:2160] examples/sec: 2.85381\n",
      "INFO:tensorflow:global_step/sec: 0.0890829\n",
      "I1118 05:49:40.474248 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890829\n",
      "INFO:tensorflow:examples/sec: 2.85065\n",
      "I1118 05:49:40.474470 4435920320 tpu_estimator.py:2160] examples/sec: 2.85065\n",
      "INFO:tensorflow:global_step/sec: 0.0893363\n",
      "I1118 05:49:51.667924 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893363\n",
      "INFO:tensorflow:examples/sec: 2.85876\n",
      "I1118 05:49:51.668200 4435920320 tpu_estimator.py:2160] examples/sec: 2.85876\n",
      "INFO:tensorflow:global_step/sec: 0.0892445\n",
      "I1118 05:50:02.873076 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892445\n",
      "INFO:tensorflow:examples/sec: 2.85583\n",
      "I1118 05:50:02.873308 4435920320 tpu_estimator.py:2160] examples/sec: 2.85583\n",
      "INFO:tensorflow:global_step/sec: 0.0891941\n",
      "I1118 05:50:14.084605 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891941\n",
      "INFO:tensorflow:examples/sec: 2.85421\n",
      "I1118 05:50:14.084825 4435920320 tpu_estimator.py:2160] examples/sec: 2.85421\n",
      "INFO:tensorflow:global_step/sec: 0.0896782\n",
      "I1118 05:50:25.235548 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896782\n",
      "INFO:tensorflow:examples/sec: 2.8697\n",
      "I1118 05:50:25.235767 4435920320 tpu_estimator.py:2160] examples/sec: 2.8697\n",
      "INFO:tensorflow:global_step/sec: 0.0897111\n",
      "I1118 05:50:36.382437 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897111\n",
      "INFO:tensorflow:examples/sec: 2.87076\n",
      "I1118 05:50:36.382661 4435920320 tpu_estimator.py:2160] examples/sec: 2.87076\n",
      "INFO:tensorflow:global_step/sec: 0.089132\n",
      "I1118 05:50:47.601755 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089132\n",
      "INFO:tensorflow:examples/sec: 2.85222\n",
      "I1118 05:50:47.601970 4435920320 tpu_estimator.py:2160] examples/sec: 2.85222\n",
      "INFO:tensorflow:global_step/sec: 0.08933\n",
      "I1118 05:50:58.796136 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08933\n",
      "INFO:tensorflow:examples/sec: 2.85856\n",
      "I1118 05:50:58.796293 4435920320 tpu_estimator.py:2160] examples/sec: 2.85856\n",
      "INFO:tensorflow:global_step/sec: 0.0886933\n",
      "I1118 05:51:10.071007 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886933\n",
      "INFO:tensorflow:examples/sec: 2.83819\n",
      "I1118 05:51:10.071227 4435920320 tpu_estimator.py:2160] examples/sec: 2.83819\n",
      "INFO:tensorflow:global_step/sec: 0.0894138\n",
      "I1118 05:51:21.254955 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894138\n",
      "INFO:tensorflow:examples/sec: 2.86124\n",
      "I1118 05:51:21.255178 4435920320 tpu_estimator.py:2160] examples/sec: 2.86124\n",
      "INFO:tensorflow:global_step/sec: 0.0896887\n",
      "I1118 05:51:32.404631 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896887\n",
      "INFO:tensorflow:examples/sec: 2.87004\n",
      "I1118 05:51:32.404848 4435920320 tpu_estimator.py:2160] examples/sec: 2.87004\n",
      "INFO:tensorflow:global_step/sec: 0.0890684\n",
      "I1118 05:51:43.631962 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890684\n",
      "INFO:tensorflow:examples/sec: 2.85019\n",
      "I1118 05:51:43.632193 4435920320 tpu_estimator.py:2160] examples/sec: 2.85019\n",
      "INFO:tensorflow:global_step/sec: 0.0887012\n",
      "I1118 05:51:54.905773 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887012\n",
      "INFO:tensorflow:examples/sec: 2.83844\n",
      "I1118 05:51:54.906007 4435920320 tpu_estimator.py:2160] examples/sec: 2.83844\n",
      "INFO:tensorflow:global_step/sec: 0.0885401\n",
      "I1118 05:52:06.200083 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885401\n",
      "INFO:tensorflow:examples/sec: 2.83328\n",
      "I1118 05:52:06.200304 4435920320 tpu_estimator.py:2160] examples/sec: 2.83328\n",
      "INFO:tensorflow:global_step/sec: 0.0892206\n",
      "I1118 05:52:17.408261 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892206\n",
      "INFO:tensorflow:examples/sec: 2.85506\n",
      "I1118 05:52:17.408506 4435920320 tpu_estimator.py:2160] examples/sec: 2.85506\n",
      "INFO:tensorflow:global_step/sec: 0.0897461\n",
      "I1118 05:52:28.550796 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897461\n",
      "INFO:tensorflow:examples/sec: 2.87188\n",
      "I1118 05:52:28.551021 4435920320 tpu_estimator.py:2160] examples/sec: 2.87188\n",
      "INFO:tensorflow:global_step/sec: 0.0892425\n",
      "I1118 05:52:39.756232 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892425\n",
      "INFO:tensorflow:examples/sec: 2.85576\n",
      "I1118 05:52:39.756484 4435920320 tpu_estimator.py:2160] examples/sec: 2.85576\n",
      "INFO:tensorflow:global_step/sec: 0.0892385\n",
      "I1118 05:52:50.962167 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892385\n",
      "INFO:tensorflow:examples/sec: 2.85563\n",
      "I1118 05:52:50.962385 4435920320 tpu_estimator.py:2160] examples/sec: 2.85563\n",
      "INFO:tensorflow:global_step/sec: 0.0891158\n",
      "I1118 05:53:02.183516 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891158\n",
      "INFO:tensorflow:examples/sec: 2.85171\n",
      "I1118 05:53:02.183738 4435920320 tpu_estimator.py:2160] examples/sec: 2.85171\n",
      "INFO:tensorflow:global_step/sec: 0.0899188\n",
      "I1118 05:53:13.304641 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899188\n",
      "INFO:tensorflow:examples/sec: 2.8774\n",
      "I1118 05:53:13.304858 4435920320 tpu_estimator.py:2160] examples/sec: 2.8774\n",
      "INFO:tensorflow:global_step/sec: 0.0893504\n",
      "I1118 05:53:24.496525 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893504\n",
      "INFO:tensorflow:examples/sec: 2.85921\n",
      "I1118 05:53:24.496737 4435920320 tpu_estimator.py:2160] examples/sec: 2.85921\n",
      "INFO:tensorflow:global_step/sec: 0.0891098\n",
      "I1118 05:53:35.718635 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891098\n",
      "INFO:tensorflow:examples/sec: 2.85151\n",
      "I1118 05:53:35.718854 4435920320 tpu_estimator.py:2160] examples/sec: 2.85151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893022\n",
      "I1118 05:53:46.916571 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893022\n",
      "INFO:tensorflow:examples/sec: 2.85767\n",
      "I1118 05:53:46.916789 4435920320 tpu_estimator.py:2160] examples/sec: 2.85767\n",
      "INFO:tensorflow:global_step/sec: 0.0888149\n",
      "I1118 05:53:58.175928 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888149\n",
      "INFO:tensorflow:examples/sec: 2.84208\n",
      "I1118 05:53:58.176137 4435920320 tpu_estimator.py:2160] examples/sec: 2.84208\n",
      "INFO:tensorflow:global_step/sec: 0.0873394\n",
      "I1118 05:54:09.625525 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0873394\n",
      "INFO:tensorflow:examples/sec: 2.79486\n",
      "I1118 05:54:09.625743 4435920320 tpu_estimator.py:2160] examples/sec: 2.79486\n",
      "INFO:tensorflow:global_step/sec: 0.088994\n",
      "I1118 05:54:20.862244 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088994\n",
      "INFO:tensorflow:examples/sec: 2.84781\n",
      "I1118 05:54:20.862466 4435920320 tpu_estimator.py:2160] examples/sec: 2.84781\n",
      "INFO:tensorflow:global_step/sec: 0.0895957\n",
      "I1118 05:54:32.023491 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895957\n",
      "INFO:tensorflow:examples/sec: 2.86706\n",
      "I1118 05:54:32.023710 4435920320 tpu_estimator.py:2160] examples/sec: 2.86706\n",
      "INFO:tensorflow:global_step/sec: 0.0897705\n",
      "I1118 05:54:43.163011 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897705\n",
      "INFO:tensorflow:examples/sec: 2.87266\n",
      "I1118 05:54:43.163233 4435920320 tpu_estimator.py:2160] examples/sec: 2.87266\n",
      "INFO:tensorflow:global_step/sec: 0.0893807\n",
      "I1118 05:54:54.351106 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893807\n",
      "INFO:tensorflow:examples/sec: 2.86018\n",
      "I1118 05:54:54.351322 4435920320 tpu_estimator.py:2160] examples/sec: 2.86018\n",
      "INFO:tensorflow:global_step/sec: 0.0891882\n",
      "I1118 05:55:05.563353 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891882\n",
      "INFO:tensorflow:examples/sec: 2.85402\n",
      "I1118 05:55:05.563571 4435920320 tpu_estimator.py:2160] examples/sec: 2.85402\n",
      "INFO:tensorflow:global_step/sec: 0.0897839\n",
      "I1118 05:55:16.701201 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897839\n",
      "INFO:tensorflow:examples/sec: 2.87309\n",
      "I1118 05:55:16.701417 4435920320 tpu_estimator.py:2160] examples/sec: 2.87309\n",
      "INFO:tensorflow:global_step/sec: 0.0898293\n",
      "I1118 05:55:27.833422 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898293\n",
      "INFO:tensorflow:examples/sec: 2.87454\n",
      "I1118 05:55:27.833639 4435920320 tpu_estimator.py:2160] examples/sec: 2.87454\n",
      "INFO:tensorflow:global_step/sec: 0.0885169\n",
      "I1118 05:55:39.130704 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885169\n",
      "INFO:tensorflow:examples/sec: 2.83254\n",
      "I1118 05:55:39.130922 4435920320 tpu_estimator.py:2160] examples/sec: 2.83254\n",
      "INFO:tensorflow:global_step/sec: 0.0890208\n",
      "I1118 05:55:50.364062 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890208\n",
      "INFO:tensorflow:examples/sec: 2.84866\n",
      "I1118 05:55:50.365414 4435920320 tpu_estimator.py:2160] examples/sec: 2.84866\n",
      "INFO:tensorflow:global_step/sec: 0.0891204\n",
      "I1118 05:56:01.584814 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891204\n",
      "INFO:tensorflow:examples/sec: 2.85185\n",
      "I1118 05:56:01.585033 4435920320 tpu_estimator.py:2160] examples/sec: 2.85185\n",
      "INFO:tensorflow:global_step/sec: 0.0895669\n",
      "I1118 05:56:12.749657 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895669\n",
      "INFO:tensorflow:examples/sec: 2.86614\n",
      "I1118 05:56:12.749891 4435920320 tpu_estimator.py:2160] examples/sec: 2.86614\n",
      "INFO:tensorflow:global_step/sec: 0.0896863\n",
      "I1118 05:56:23.899616 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896863\n",
      "INFO:tensorflow:examples/sec: 2.86996\n",
      "I1118 05:56:23.899986 4435920320 tpu_estimator.py:2160] examples/sec: 2.86996\n",
      "INFO:tensorflow:global_step/sec: 0.0895678\n",
      "I1118 05:56:35.064359 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895678\n",
      "INFO:tensorflow:examples/sec: 2.86617\n",
      "I1118 05:56:35.064588 4435920320 tpu_estimator.py:2160] examples/sec: 2.86617\n",
      "INFO:tensorflow:global_step/sec: 0.0891502\n",
      "I1118 05:56:46.281388 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891502\n",
      "INFO:tensorflow:examples/sec: 2.85281\n",
      "I1118 05:56:46.281640 4435920320 tpu_estimator.py:2160] examples/sec: 2.85281\n",
      "INFO:tensorflow:global_step/sec: 0.0894932\n",
      "I1118 05:56:57.455401 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894932\n",
      "INFO:tensorflow:examples/sec: 2.86378\n",
      "I1118 05:56:57.455620 4435920320 tpu_estimator.py:2160] examples/sec: 2.86378\n",
      "INFO:tensorflow:global_step/sec: 0.0896318\n",
      "I1118 05:57:08.612172 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896318\n",
      "INFO:tensorflow:examples/sec: 2.86822\n",
      "I1118 05:57:08.612391 4435920320 tpu_estimator.py:2160] examples/sec: 2.86822\n",
      "INFO:tensorflow:global_step/sec: 0.0895431\n",
      "I1118 05:57:19.779962 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895431\n",
      "INFO:tensorflow:examples/sec: 2.86538\n",
      "I1118 05:57:19.780177 4435920320 tpu_estimator.py:2160] examples/sec: 2.86538\n",
      "INFO:tensorflow:global_step/sec: 0.0898422\n",
      "I1118 05:57:30.910586 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898422\n",
      "INFO:tensorflow:examples/sec: 2.87495\n",
      "I1118 05:57:30.910802 4435920320 tpu_estimator.py:2160] examples/sec: 2.87495\n",
      "INFO:tensorflow:global_step/sec: 0.0885586\n",
      "I1118 05:57:42.202555 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885586\n",
      "INFO:tensorflow:examples/sec: 2.83387\n",
      "I1118 05:57:42.202780 4435920320 tpu_estimator.py:2160] examples/sec: 2.83387\n",
      "INFO:tensorflow:global_step/sec: 0.0889876\n",
      "I1118 05:57:53.440097 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889876\n",
      "INFO:tensorflow:examples/sec: 2.8476\n",
      "I1118 05:57:53.440317 4435920320 tpu_estimator.py:2160] examples/sec: 2.8476\n",
      "INFO:tensorflow:global_step/sec: 0.0894451\n",
      "I1118 05:58:04.620110 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894451\n",
      "INFO:tensorflow:examples/sec: 2.86224\n",
      "I1118 05:58:04.620332 4435920320 tpu_estimator.py:2160] examples/sec: 2.86224\n",
      "INFO:tensorflow:global_step/sec: 0.0893447\n",
      "I1118 05:58:15.812716 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893447\n",
      "INFO:tensorflow:examples/sec: 2.85903\n",
      "I1118 05:58:15.812937 4435920320 tpu_estimator.py:2160] examples/sec: 2.85903\n",
      "INFO:tensorflow:global_step/sec: 0.0888892\n",
      "I1118 05:58:27.062683 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888892\n",
      "INFO:tensorflow:examples/sec: 2.84446\n",
      "I1118 05:58:27.062919 4435920320 tpu_estimator.py:2160] examples/sec: 2.84446\n",
      "INFO:tensorflow:global_step/sec: 0.0890458\n",
      "I1118 05:58:38.292848 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890458\n",
      "INFO:tensorflow:examples/sec: 2.84947\n",
      "I1118 05:58:38.293077 4435920320 tpu_estimator.py:2160] examples/sec: 2.84947\n",
      "INFO:tensorflow:global_step/sec: 0.0894407\n",
      "I1118 05:58:49.473453 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894407\n",
      "INFO:tensorflow:examples/sec: 2.8621\n",
      "I1118 05:58:49.473690 4435920320 tpu_estimator.py:2160] examples/sec: 2.8621\n",
      "INFO:tensorflow:global_step/sec: 0.0899924\n",
      "I1118 05:59:00.585476 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899924\n",
      "INFO:tensorflow:examples/sec: 2.87976\n",
      "I1118 05:59:00.585689 4435920320 tpu_estimator.py:2160] examples/sec: 2.87976\n",
      "INFO:tensorflow:global_step/sec: 0.0889584\n",
      "I1118 05:59:11.826709 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889584\n",
      "INFO:tensorflow:examples/sec: 2.84667\n",
      "I1118 05:59:11.826939 4435920320 tpu_estimator.py:2160] examples/sec: 2.84667\n",
      "INFO:tensorflow:global_step/sec: 0.0893967\n",
      "I1118 05:59:23.012807 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893967\n",
      "INFO:tensorflow:examples/sec: 2.86069\n",
      "I1118 05:59:23.013036 4435920320 tpu_estimator.py:2160] examples/sec: 2.86069\n",
      "INFO:tensorflow:global_step/sec: 0.089291\n",
      "I1118 05:59:34.212131 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089291\n",
      "INFO:tensorflow:examples/sec: 2.85731\n",
      "I1118 05:59:34.212346 4435920320 tpu_estimator.py:2160] examples/sec: 2.85731\n",
      "INFO:tensorflow:global_step/sec: 0.0892712\n",
      "I1118 05:59:45.413960 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892712\n",
      "INFO:tensorflow:examples/sec: 2.85668\n",
      "I1118 05:59:45.414186 4435920320 tpu_estimator.py:2160] examples/sec: 2.85668\n",
      "INFO:tensorflow:global_step/sec: 0.0896256\n",
      "I1118 05:59:56.571491 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896256\n",
      "INFO:tensorflow:examples/sec: 2.86802\n",
      "I1118 05:59:56.571725 4435920320 tpu_estimator.py:2160] examples/sec: 2.86802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0894478\n",
      "I1118 06:00:07.751188 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894478\n",
      "INFO:tensorflow:examples/sec: 2.86233\n",
      "I1118 06:00:07.751417 4435920320 tpu_estimator.py:2160] examples/sec: 2.86233\n",
      "INFO:tensorflow:global_step/sec: 0.0887612\n",
      "I1118 06:00:19.017371 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887612\n",
      "INFO:tensorflow:examples/sec: 2.84036\n",
      "I1118 06:00:19.017590 4435920320 tpu_estimator.py:2160] examples/sec: 2.84036\n",
      "INFO:tensorflow:global_step/sec: 0.088991\n",
      "I1118 06:00:30.254469 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088991\n",
      "INFO:tensorflow:examples/sec: 2.84771\n",
      "I1118 06:00:30.254698 4435920320 tpu_estimator.py:2160] examples/sec: 2.84771\n",
      "INFO:tensorflow:global_step/sec: 0.0891571\n",
      "I1118 06:00:41.470609 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891571\n",
      "INFO:tensorflow:examples/sec: 2.85303\n",
      "I1118 06:00:41.470831 4435920320 tpu_estimator.py:2160] examples/sec: 2.85303\n",
      "INFO:tensorflow:global_step/sec: 0.0893827\n",
      "I1118 06:00:52.658459 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893827\n",
      "INFO:tensorflow:examples/sec: 2.86025\n",
      "I1118 06:00:52.658678 4435920320 tpu_estimator.py:2160] examples/sec: 2.86025\n",
      "INFO:tensorflow:global_step/sec: 0.0894757\n",
      "I1118 06:01:03.834672 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894757\n",
      "INFO:tensorflow:examples/sec: 2.86322\n",
      "I1118 06:01:03.835037 4435920320 tpu_estimator.py:2160] examples/sec: 2.86322\n",
      "INFO:tensorflow:global_step/sec: 0.0892478\n",
      "I1118 06:01:15.039439 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892478\n",
      "INFO:tensorflow:examples/sec: 2.85593\n",
      "I1118 06:01:15.039672 4435920320 tpu_estimator.py:2160] examples/sec: 2.85593\n",
      "INFO:tensorflow:global_step/sec: 0.0898976\n",
      "I1118 06:01:26.163199 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898976\n",
      "INFO:tensorflow:examples/sec: 2.87672\n",
      "I1118 06:01:26.163413 4435920320 tpu_estimator.py:2160] examples/sec: 2.87672\n",
      "INFO:tensorflow:global_step/sec: 0.0893802\n",
      "I1118 06:01:37.351361 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893802\n",
      "INFO:tensorflow:examples/sec: 2.86017\n",
      "I1118 06:01:37.351589 4435920320 tpu_estimator.py:2160] examples/sec: 2.86017\n",
      "INFO:tensorflow:global_step/sec: 0.0897358\n",
      "I1118 06:01:48.495187 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897358\n",
      "INFO:tensorflow:examples/sec: 2.87155\n",
      "I1118 06:01:48.495408 4435920320 tpu_estimator.py:2160] examples/sec: 2.87155\n",
      "INFO:tensorflow:global_step/sec: 0.0893677\n",
      "I1118 06:01:59.684917 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893677\n",
      "INFO:tensorflow:examples/sec: 2.85977\n",
      "I1118 06:01:59.685162 4435920320 tpu_estimator.py:2160] examples/sec: 2.85977\n",
      "INFO:tensorflow:global_step/sec: 0.089093\n",
      "I1118 06:02:10.909139 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089093\n",
      "INFO:tensorflow:examples/sec: 2.85098\n",
      "I1118 06:02:10.909381 4435920320 tpu_estimator.py:2160] examples/sec: 2.85098\n",
      "INFO:tensorflow:global_step/sec: 0.0891157\n",
      "I1118 06:02:22.130501 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891157\n",
      "INFO:tensorflow:examples/sec: 2.8517\n",
      "I1118 06:02:22.130719 4435920320 tpu_estimator.py:2160] examples/sec: 2.8517\n",
      "INFO:tensorflow:global_step/sec: 0.0892521\n",
      "I1118 06:02:33.334735 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892521\n",
      "INFO:tensorflow:examples/sec: 2.85607\n",
      "I1118 06:02:33.334970 4435920320 tpu_estimator.py:2160] examples/sec: 2.85607\n",
      "INFO:tensorflow:global_step/sec: 0.0891384\n",
      "I1118 06:02:44.553256 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891384\n",
      "INFO:tensorflow:examples/sec: 2.85243\n",
      "I1118 06:02:44.553483 4435920320 tpu_estimator.py:2160] examples/sec: 2.85243\n",
      "INFO:tensorflow:global_step/sec: 0.0895457\n",
      "I1118 06:02:55.720716 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895457\n",
      "INFO:tensorflow:examples/sec: 2.86546\n",
      "I1118 06:02:55.720937 4435920320 tpu_estimator.py:2160] examples/sec: 2.86546\n",
      "INFO:tensorflow:global_step/sec: 0.0896515\n",
      "I1118 06:03:06.875020 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896515\n",
      "INFO:tensorflow:examples/sec: 2.86885\n",
      "I1118 06:03:06.875239 4435920320 tpu_estimator.py:2160] examples/sec: 2.86885\n",
      "INFO:tensorflow:global_step/sec: 0.0898352\n",
      "I1118 06:03:18.006492 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898352\n",
      "INFO:tensorflow:examples/sec: 2.87473\n",
      "I1118 06:03:18.006700 4435920320 tpu_estimator.py:2160] examples/sec: 2.87473\n",
      "INFO:tensorflow:global_step/sec: 0.0892932\n",
      "I1118 06:03:29.205580 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892932\n",
      "INFO:tensorflow:examples/sec: 2.85738\n",
      "I1118 06:03:29.205819 4435920320 tpu_estimator.py:2160] examples/sec: 2.85738\n",
      "INFO:tensorflow:global_step/sec: 0.0890773\n",
      "I1118 06:03:40.431787 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890773\n",
      "INFO:tensorflow:examples/sec: 2.85047\n",
      "I1118 06:03:40.432017 4435920320 tpu_estimator.py:2160] examples/sec: 2.85047\n",
      "INFO:tensorflow:global_step/sec: 0.0894343\n",
      "I1118 06:03:51.613194 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894343\n",
      "INFO:tensorflow:examples/sec: 2.8619\n",
      "I1118 06:03:51.613437 4435920320 tpu_estimator.py:2160] examples/sec: 2.8619\n",
      "INFO:tensorflow:global_step/sec: 0.0893694\n",
      "I1118 06:04:02.802689 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893694\n",
      "INFO:tensorflow:examples/sec: 2.85982\n",
      "I1118 06:04:02.802911 4435920320 tpu_estimator.py:2160] examples/sec: 2.85982\n",
      "INFO:tensorflow:global_step/sec: 0.0890541\n",
      "I1118 06:04:14.031831 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890541\n",
      "INFO:tensorflow:examples/sec: 2.84973\n",
      "I1118 06:04:14.032063 4435920320 tpu_estimator.py:2160] examples/sec: 2.84973\n",
      "INFO:tensorflow:global_step/sec: 0.08962\n",
      "I1118 06:04:25.190057 4435920320 tpu_estimator.py:2159] global_step/sec: 0.08962\n",
      "INFO:tensorflow:examples/sec: 2.86784\n",
      "I1118 06:04:25.190290 4435920320 tpu_estimator.py:2160] examples/sec: 2.86784\n",
      "INFO:tensorflow:global_step/sec: 0.0903003\n",
      "I1118 06:04:36.264206 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0903003\n",
      "INFO:tensorflow:examples/sec: 2.88961\n",
      "I1118 06:04:36.264426 4435920320 tpu_estimator.py:2160] examples/sec: 2.88961\n",
      "INFO:tensorflow:global_step/sec: 0.0897353\n",
      "I1118 06:04:47.408127 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897353\n",
      "INFO:tensorflow:examples/sec: 2.87153\n",
      "I1118 06:04:47.408354 4435920320 tpu_estimator.py:2160] examples/sec: 2.87153\n",
      "INFO:tensorflow:global_step/sec: 0.089549\n",
      "I1118 06:04:58.575173 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089549\n",
      "INFO:tensorflow:examples/sec: 2.86557\n",
      "I1118 06:04:58.575417 4435920320 tpu_estimator.py:2160] examples/sec: 2.86557\n",
      "INFO:tensorflow:global_step/sec: 0.0896951\n",
      "I1118 06:05:09.724047 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896951\n",
      "INFO:tensorflow:examples/sec: 2.87024\n",
      "I1118 06:05:09.724266 4435920320 tpu_estimator.py:2160] examples/sec: 2.87024\n",
      "INFO:tensorflow:global_step/sec: 0.0891707\n",
      "I1118 06:05:20.938462 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891707\n",
      "INFO:tensorflow:examples/sec: 2.85346\n",
      "I1118 06:05:20.938678 4435920320 tpu_estimator.py:2160] examples/sec: 2.85346\n",
      "INFO:tensorflow:global_step/sec: 0.0892004\n",
      "I1118 06:05:32.149214 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892004\n",
      "INFO:tensorflow:examples/sec: 2.85441\n",
      "I1118 06:05:32.149436 4435920320 tpu_estimator.py:2160] examples/sec: 2.85441\n",
      "INFO:tensorflow:global_step/sec: 0.0887434\n",
      "I1118 06:05:43.417647 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887434\n",
      "INFO:tensorflow:examples/sec: 2.83979\n",
      "I1118 06:05:43.417866 4435920320 tpu_estimator.py:2160] examples/sec: 2.83979\n",
      "INFO:tensorflow:global_step/sec: 0.0895789\n",
      "I1118 06:05:54.581012 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895789\n",
      "INFO:tensorflow:examples/sec: 2.86652\n",
      "I1118 06:05:54.581268 4435920320 tpu_estimator.py:2160] examples/sec: 2.86652\n",
      "INFO:tensorflow:global_step/sec: 0.0896136\n",
      "I1118 06:06:05.740023 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896136\n",
      "INFO:tensorflow:examples/sec: 2.86763\n",
      "I1118 06:06:05.740249 4435920320 tpu_estimator.py:2160] examples/sec: 2.86763\n",
      "INFO:tensorflow:global_step/sec: 0.0896993\n",
      "I1118 06:06:16.888365 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896993\n",
      "INFO:tensorflow:examples/sec: 2.87038\n",
      "I1118 06:06:16.888586 4435920320 tpu_estimator.py:2160] examples/sec: 2.87038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0895759\n",
      "I1118 06:06:28.052100 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895759\n",
      "INFO:tensorflow:examples/sec: 2.86643\n",
      "I1118 06:06:28.052344 4435920320 tpu_estimator.py:2160] examples/sec: 2.86643\n",
      "INFO:tensorflow:global_step/sec: 0.0892412\n",
      "I1118 06:06:39.257683 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892412\n",
      "INFO:tensorflow:examples/sec: 2.85572\n",
      "I1118 06:06:39.257913 4435920320 tpu_estimator.py:2160] examples/sec: 2.85572\n",
      "INFO:tensorflow:global_step/sec: 0.0896482\n",
      "I1118 06:06:50.412392 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896482\n",
      "INFO:tensorflow:examples/sec: 2.86874\n",
      "I1118 06:06:50.412621 4435920320 tpu_estimator.py:2160] examples/sec: 2.86874\n",
      "INFO:tensorflow:global_step/sec: 0.0892108\n",
      "I1118 06:07:01.621783 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892108\n",
      "INFO:tensorflow:examples/sec: 2.85475\n",
      "I1118 06:07:01.622014 4435920320 tpu_estimator.py:2160] examples/sec: 2.85475\n",
      "INFO:tensorflow:global_step/sec: 0.089269\n",
      "I1118 06:07:12.823894 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089269\n",
      "INFO:tensorflow:examples/sec: 2.85661\n",
      "I1118 06:07:12.824129 4435920320 tpu_estimator.py:2160] examples/sec: 2.85661\n",
      "INFO:tensorflow:global_step/sec: 0.0892445\n",
      "I1118 06:07:24.029054 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892445\n",
      "INFO:tensorflow:examples/sec: 2.85582\n",
      "I1118 06:07:24.029288 4435920320 tpu_estimator.py:2160] examples/sec: 2.85582\n",
      "INFO:tensorflow:global_step/sec: 0.089283\n",
      "I1118 06:07:35.229391 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089283\n",
      "INFO:tensorflow:examples/sec: 2.85706\n",
      "I1118 06:07:35.229605 4435920320 tpu_estimator.py:2160] examples/sec: 2.85706\n",
      "INFO:tensorflow:global_step/sec: 0.0885327\n",
      "I1118 06:07:46.524655 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0885327\n",
      "INFO:tensorflow:examples/sec: 2.83305\n",
      "I1118 06:07:46.524870 4435920320 tpu_estimator.py:2160] examples/sec: 2.83305\n",
      "INFO:tensorflow:global_step/sec: 0.0889137\n",
      "I1118 06:07:57.771522 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889137\n",
      "INFO:tensorflow:examples/sec: 2.84524\n",
      "I1118 06:07:57.771741 4435920320 tpu_estimator.py:2160] examples/sec: 2.84524\n",
      "INFO:tensorflow:global_step/sec: 0.0892243\n",
      "I1118 06:08:08.979244 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892243\n",
      "INFO:tensorflow:examples/sec: 2.85518\n",
      "I1118 06:08:08.979479 4435920320 tpu_estimator.py:2160] examples/sec: 2.85518\n",
      "INFO:tensorflow:global_step/sec: 0.0882803\n",
      "I1118 06:08:20.306795 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882803\n",
      "INFO:tensorflow:examples/sec: 2.82497\n",
      "I1118 06:08:20.307234 4435920320 tpu_estimator.py:2160] examples/sec: 2.82497\n",
      "INFO:tensorflow:global_step/sec: 0.0890353\n",
      "I1118 06:08:31.538308 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890353\n",
      "INFO:tensorflow:examples/sec: 2.84913\n",
      "I1118 06:08:31.538703 4435920320 tpu_estimator.py:2160] examples/sec: 2.84913\n",
      "INFO:tensorflow:global_step/sec: 0.0893684\n",
      "I1118 06:08:42.727951 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893684\n",
      "INFO:tensorflow:examples/sec: 2.85979\n",
      "I1118 06:08:42.728195 4435920320 tpu_estimator.py:2160] examples/sec: 2.85979\n",
      "INFO:tensorflow:global_step/sec: 0.089498\n",
      "I1118 06:08:53.901369 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089498\n",
      "INFO:tensorflow:examples/sec: 2.86394\n",
      "I1118 06:08:53.901588 4435920320 tpu_estimator.py:2160] examples/sec: 2.86394\n",
      "INFO:tensorflow:global_step/sec: 0.0899215\n",
      "I1118 06:09:05.022169 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899215\n",
      "INFO:tensorflow:examples/sec: 2.87749\n",
      "I1118 06:09:05.022390 4435920320 tpu_estimator.py:2160] examples/sec: 2.87749\n",
      "INFO:tensorflow:global_step/sec: 0.0894784\n",
      "I1118 06:09:16.198064 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894784\n",
      "INFO:tensorflow:examples/sec: 2.86331\n",
      "I1118 06:09:16.198298 4435920320 tpu_estimator.py:2160] examples/sec: 2.86331\n",
      "INFO:tensorflow:global_step/sec: 0.0897481\n",
      "I1118 06:09:27.340353 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897481\n",
      "INFO:tensorflow:examples/sec: 2.87194\n",
      "I1118 06:09:27.340586 4435920320 tpu_estimator.py:2160] examples/sec: 2.87194\n",
      "INFO:tensorflow:global_step/sec: 0.0890609\n",
      "I1118 06:09:38.568624 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890609\n",
      "INFO:tensorflow:examples/sec: 2.84995\n",
      "I1118 06:09:38.568883 4435920320 tpu_estimator.py:2160] examples/sec: 2.84995\n",
      "INFO:tensorflow:global_step/sec: 0.0890316\n",
      "I1118 06:09:49.800591 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890316\n",
      "INFO:tensorflow:examples/sec: 2.84901\n",
      "I1118 06:09:49.800807 4435920320 tpu_estimator.py:2160] examples/sec: 2.84901\n",
      "INFO:tensorflow:global_step/sec: 0.0892649\n",
      "I1118 06:10:01.003226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892649\n",
      "INFO:tensorflow:examples/sec: 2.85648\n",
      "I1118 06:10:01.003447 4435920320 tpu_estimator.py:2160] examples/sec: 2.85648\n",
      "INFO:tensorflow:global_step/sec: 0.0893441\n",
      "I1118 06:10:12.195891 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893441\n",
      "INFO:tensorflow:examples/sec: 2.85901\n",
      "I1118 06:10:12.196110 4435920320 tpu_estimator.py:2160] examples/sec: 2.85901\n",
      "INFO:tensorflow:global_step/sec: 0.0893187\n",
      "I1118 06:10:23.391764 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893187\n",
      "INFO:tensorflow:examples/sec: 2.8582\n",
      "I1118 06:10:23.392005 4435920320 tpu_estimator.py:2160] examples/sec: 2.8582\n",
      "INFO:tensorflow:global_step/sec: 0.0897423\n",
      "I1118 06:10:34.534772 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897423\n",
      "INFO:tensorflow:examples/sec: 2.87175\n",
      "I1118 06:10:34.534992 4435920320 tpu_estimator.py:2160] examples/sec: 2.87175\n",
      "INFO:tensorflow:global_step/sec: 0.0900583\n",
      "I1118 06:10:45.638693 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900583\n",
      "INFO:tensorflow:examples/sec: 2.88187\n",
      "I1118 06:10:45.638925 4435920320 tpu_estimator.py:2160] examples/sec: 2.88187\n",
      "INFO:tensorflow:global_step/sec: 0.0896349\n",
      "I1118 06:10:56.795058 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896349\n",
      "INFO:tensorflow:examples/sec: 2.86832\n",
      "I1118 06:10:56.795288 4435920320 tpu_estimator.py:2160] examples/sec: 2.86832\n",
      "INFO:tensorflow:global_step/sec: 0.0899821\n",
      "I1118 06:11:07.908360 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899821\n",
      "INFO:tensorflow:examples/sec: 2.87943\n",
      "I1118 06:11:07.908572 4435920320 tpu_estimator.py:2160] examples/sec: 2.87943\n",
      "INFO:tensorflow:global_step/sec: 0.0895016\n",
      "I1118 06:11:19.081350 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895016\n",
      "INFO:tensorflow:examples/sec: 2.86405\n",
      "I1118 06:11:19.081572 4435920320 tpu_estimator.py:2160] examples/sec: 2.86405\n",
      "INFO:tensorflow:global_step/sec: 0.0897058\n",
      "I1118 06:11:30.228901 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897058\n",
      "INFO:tensorflow:examples/sec: 2.87059\n",
      "I1118 06:11:30.229120 4435920320 tpu_estimator.py:2160] examples/sec: 2.87059\n",
      "INFO:tensorflow:global_step/sec: 0.0883206\n",
      "I1118 06:11:41.551292 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883206\n",
      "INFO:tensorflow:examples/sec: 2.82626\n",
      "I1118 06:11:41.551522 4435920320 tpu_estimator.py:2160] examples/sec: 2.82626\n",
      "INFO:tensorflow:global_step/sec: 0.0886185\n",
      "I1118 06:11:52.835606 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886185\n",
      "INFO:tensorflow:examples/sec: 2.83579\n",
      "I1118 06:11:52.835828 4435920320 tpu_estimator.py:2160] examples/sec: 2.83579\n",
      "INFO:tensorflow:global_step/sec: 0.0893856\n",
      "I1118 06:12:04.023102 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893856\n",
      "INFO:tensorflow:examples/sec: 2.86034\n",
      "I1118 06:12:04.023308 4435920320 tpu_estimator.py:2160] examples/sec: 2.86034\n",
      "INFO:tensorflow:global_step/sec: 0.089629\n",
      "I1118 06:12:15.180198 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089629\n",
      "INFO:tensorflow:examples/sec: 2.86813\n",
      "I1118 06:12:15.180414 4435920320 tpu_estimator.py:2160] examples/sec: 2.86813\n",
      "INFO:tensorflow:global_step/sec: 0.0897164\n",
      "I1118 06:12:26.326442 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897164\n",
      "INFO:tensorflow:examples/sec: 2.87093\n",
      "I1118 06:12:26.326678 4435920320 tpu_estimator.py:2160] examples/sec: 2.87093\n",
      "INFO:tensorflow:global_step/sec: 0.0895894\n",
      "I1118 06:12:37.488484 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895894\n",
      "INFO:tensorflow:examples/sec: 2.86686\n",
      "I1118 06:12:37.488711 4435920320 tpu_estimator.py:2160] examples/sec: 2.86686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0893919\n",
      "I1118 06:12:48.675179 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893919\n",
      "INFO:tensorflow:examples/sec: 2.86054\n",
      "I1118 06:12:48.675397 4435920320 tpu_estimator.py:2160] examples/sec: 2.86054\n",
      "INFO:tensorflow:global_step/sec: 0.0896779\n",
      "I1118 06:12:59.826186 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896779\n",
      "INFO:tensorflow:examples/sec: 2.86969\n",
      "I1118 06:12:59.826426 4435920320 tpu_estimator.py:2160] examples/sec: 2.86969\n",
      "INFO:tensorflow:global_step/sec: 0.0892388\n",
      "I1118 06:13:11.032057 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892388\n",
      "INFO:tensorflow:examples/sec: 2.85564\n",
      "I1118 06:13:11.032274 4435920320 tpu_estimator.py:2160] examples/sec: 2.85564\n",
      "INFO:tensorflow:global_step/sec: 0.0891857\n",
      "I1118 06:13:22.244617 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891857\n",
      "INFO:tensorflow:examples/sec: 2.85394\n",
      "I1118 06:13:22.244848 4435920320 tpu_estimator.py:2160] examples/sec: 2.85394\n",
      "INFO:tensorflow:global_step/sec: 0.0896486\n",
      "I1118 06:13:33.399280 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896486\n",
      "INFO:tensorflow:examples/sec: 2.86876\n",
      "I1118 06:13:33.399502 4435920320 tpu_estimator.py:2160] examples/sec: 2.86876\n",
      "INFO:tensorflow:global_step/sec: 0.0891214\n",
      "I1118 06:13:44.619935 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891214\n",
      "INFO:tensorflow:examples/sec: 2.85189\n",
      "I1118 06:13:44.620165 4435920320 tpu_estimator.py:2160] examples/sec: 2.85189\n",
      "INFO:tensorflow:global_step/sec: 0.0898653\n",
      "I1118 06:13:55.747775 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898653\n",
      "INFO:tensorflow:examples/sec: 2.87569\n",
      "I1118 06:13:55.748020 4435920320 tpu_estimator.py:2160] examples/sec: 2.87569\n",
      "INFO:tensorflow:global_step/sec: 0.0888595\n",
      "I1118 06:14:07.001351 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888595\n",
      "INFO:tensorflow:examples/sec: 2.8435\n",
      "I1118 06:14:07.001503 4435920320 tpu_estimator.py:2160] examples/sec: 2.8435\n",
      "INFO:tensorflow:global_step/sec: 0.0887462\n",
      "I1118 06:14:18.269498 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887462\n",
      "INFO:tensorflow:examples/sec: 2.83988\n",
      "I1118 06:14:18.269720 4435920320 tpu_estimator.py:2160] examples/sec: 2.83988\n",
      "INFO:tensorflow:global_step/sec: 0.0894066\n",
      "I1118 06:14:29.454360 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894066\n",
      "INFO:tensorflow:examples/sec: 2.86101\n",
      "I1118 06:14:29.454587 4435920320 tpu_estimator.py:2160] examples/sec: 2.86101\n",
      "INFO:tensorflow:global_step/sec: 0.0893976\n",
      "I1118 06:14:40.640326 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893976\n",
      "INFO:tensorflow:examples/sec: 2.86072\n",
      "I1118 06:14:40.640546 4435920320 tpu_estimator.py:2160] examples/sec: 2.86072\n",
      "INFO:tensorflow:global_step/sec: 0.0895285\n",
      "I1118 06:14:51.809978 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895285\n",
      "INFO:tensorflow:examples/sec: 2.86491\n",
      "I1118 06:14:51.810192 4435920320 tpu_estimator.py:2160] examples/sec: 2.86491\n",
      "INFO:tensorflow:global_step/sec: 0.0891389\n",
      "I1118 06:15:03.028412 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891389\n",
      "INFO:tensorflow:examples/sec: 2.85244\n",
      "I1118 06:15:03.028633 4435920320 tpu_estimator.py:2160] examples/sec: 2.85244\n",
      "INFO:tensorflow:global_step/sec: 0.0893551\n",
      "I1118 06:15:14.219736 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893551\n",
      "INFO:tensorflow:examples/sec: 2.85936\n",
      "I1118 06:15:14.219975 4435920320 tpu_estimator.py:2160] examples/sec: 2.85936\n",
      "INFO:tensorflow:global_step/sec: 0.0897162\n",
      "I1118 06:15:25.365983 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897162\n",
      "INFO:tensorflow:examples/sec: 2.87092\n",
      "I1118 06:15:25.366216 4435920320 tpu_estimator.py:2160] examples/sec: 2.87092\n",
      "INFO:tensorflow:global_step/sec: 0.089958\n",
      "I1118 06:15:36.482285 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089958\n",
      "INFO:tensorflow:examples/sec: 2.87866\n",
      "I1118 06:15:36.482510 4435920320 tpu_estimator.py:2160] examples/sec: 2.87866\n",
      "INFO:tensorflow:global_step/sec: 0.0896212\n",
      "I1118 06:15:47.640367 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896212\n",
      "INFO:tensorflow:examples/sec: 2.86788\n",
      "I1118 06:15:47.640591 4435920320 tpu_estimator.py:2160] examples/sec: 2.86788\n",
      "INFO:tensorflow:global_step/sec: 0.0890947\n",
      "I1118 06:15:58.864365 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890947\n",
      "INFO:tensorflow:examples/sec: 2.85103\n",
      "I1118 06:15:58.864578 4435920320 tpu_estimator.py:2160] examples/sec: 2.85103\n",
      "INFO:tensorflow:global_step/sec: 0.0895493\n",
      "I1118 06:16:10.031410 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895493\n",
      "INFO:tensorflow:examples/sec: 2.86558\n",
      "I1118 06:16:10.031642 4435920320 tpu_estimator.py:2160] examples/sec: 2.86558\n",
      "INFO:tensorflow:global_step/sec: 0.0896581\n",
      "I1118 06:16:21.184922 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896581\n",
      "INFO:tensorflow:examples/sec: 2.86906\n",
      "I1118 06:16:21.185128 4435920320 tpu_estimator.py:2160] examples/sec: 2.86906\n",
      "INFO:tensorflow:global_step/sec: 0.0895079\n",
      "I1118 06:16:32.357093 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895079\n",
      "INFO:tensorflow:examples/sec: 2.86425\n",
      "I1118 06:16:32.357331 4435920320 tpu_estimator.py:2160] examples/sec: 2.86425\n",
      "INFO:tensorflow:global_step/sec: 0.0889371\n",
      "I1118 06:16:43.600983 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889371\n",
      "INFO:tensorflow:examples/sec: 2.84599\n",
      "I1118 06:16:43.601202 4435920320 tpu_estimator.py:2160] examples/sec: 2.84599\n",
      "INFO:tensorflow:global_step/sec: 0.0887282\n",
      "I1118 06:16:54.871371 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887282\n",
      "INFO:tensorflow:examples/sec: 2.8393\n",
      "I1118 06:16:54.871604 4435920320 tpu_estimator.py:2160] examples/sec: 2.8393\n",
      "INFO:tensorflow:global_step/sec: 0.089148\n",
      "I1118 06:17:06.088656 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089148\n",
      "INFO:tensorflow:examples/sec: 2.85274\n",
      "I1118 06:17:06.088876 4435920320 tpu_estimator.py:2160] examples/sec: 2.85274\n",
      "INFO:tensorflow:global_step/sec: 0.0896353\n",
      "I1118 06:17:17.245010 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896353\n",
      "INFO:tensorflow:examples/sec: 2.86833\n",
      "I1118 06:17:17.245242 4435920320 tpu_estimator.py:2160] examples/sec: 2.86833\n",
      "INFO:tensorflow:global_step/sec: 0.0897249\n",
      "I1118 06:17:28.390161 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897249\n",
      "INFO:tensorflow:examples/sec: 2.8712\n",
      "I1118 06:17:28.390388 4435920320 tpu_estimator.py:2160] examples/sec: 2.8712\n",
      "INFO:tensorflow:global_step/sec: 0.0897379\n",
      "I1118 06:17:39.533712 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897379\n",
      "INFO:tensorflow:examples/sec: 2.87161\n",
      "I1118 06:17:39.533928 4435920320 tpu_estimator.py:2160] examples/sec: 2.87161\n",
      "INFO:tensorflow:global_step/sec: 0.0893984\n",
      "I1118 06:17:50.719594 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893984\n",
      "INFO:tensorflow:examples/sec: 2.86075\n",
      "I1118 06:17:50.719810 4435920320 tpu_estimator.py:2160] examples/sec: 2.86075\n",
      "INFO:tensorflow:global_step/sec: 0.0898474\n",
      "I1118 06:18:01.849581 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898474\n",
      "INFO:tensorflow:examples/sec: 2.87512\n",
      "I1118 06:18:01.849813 4435920320 tpu_estimator.py:2160] examples/sec: 2.87512\n",
      "INFO:tensorflow:global_step/sec: 0.0898908\n",
      "I1118 06:18:12.974199 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898908\n",
      "INFO:tensorflow:examples/sec: 2.8765\n",
      "I1118 06:18:12.974427 4435920320 tpu_estimator.py:2160] examples/sec: 2.8765\n",
      "INFO:tensorflow:global_step/sec: 0.0892899\n",
      "I1118 06:18:24.173669 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892899\n",
      "INFO:tensorflow:examples/sec: 2.85728\n",
      "I1118 06:18:24.173902 4435920320 tpu_estimator.py:2160] examples/sec: 2.85728\n",
      "INFO:tensorflow:global_step/sec: 0.0894836\n",
      "I1118 06:18:35.348889 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894836\n",
      "INFO:tensorflow:examples/sec: 2.86348\n",
      "I1118 06:18:35.349128 4435920320 tpu_estimator.py:2160] examples/sec: 2.86348\n",
      "INFO:tensorflow:global_step/sec: 0.0890835\n",
      "I1118 06:18:46.574341 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890835\n",
      "INFO:tensorflow:examples/sec: 2.85067\n",
      "I1118 06:18:46.574559 4435920320 tpu_estimator.py:2160] examples/sec: 2.85067\n",
      "INFO:tensorflow:global_step/sec: 0.0882554\n",
      "I1118 06:18:57.905062 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0882554\n",
      "INFO:tensorflow:examples/sec: 2.82417\n",
      "I1118 06:18:57.905325 4435920320 tpu_estimator.py:2160] examples/sec: 2.82417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0886538\n",
      "I1118 06:19:09.184911 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886538\n",
      "INFO:tensorflow:examples/sec: 2.83692\n",
      "I1118 06:19:09.185125 4435920320 tpu_estimator.py:2160] examples/sec: 2.83692\n",
      "INFO:tensorflow:global_step/sec: 0.0893161\n",
      "I1118 06:19:20.381094 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893161\n",
      "INFO:tensorflow:examples/sec: 2.85812\n",
      "I1118 06:19:20.381321 4435920320 tpu_estimator.py:2160] examples/sec: 2.85812\n",
      "INFO:tensorflow:global_step/sec: 0.0894889\n",
      "I1118 06:19:31.555662 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894889\n",
      "INFO:tensorflow:examples/sec: 2.86365\n",
      "I1118 06:19:31.555915 4435920320 tpu_estimator.py:2160] examples/sec: 2.86365\n",
      "INFO:tensorflow:global_step/sec: 0.0892394\n",
      "I1118 06:19:42.761468 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892394\n",
      "INFO:tensorflow:examples/sec: 2.85566\n",
      "I1118 06:19:42.761686 4435920320 tpu_estimator.py:2160] examples/sec: 2.85566\n",
      "INFO:tensorflow:global_step/sec: 0.0895556\n",
      "I1118 06:19:53.927771 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895556\n",
      "INFO:tensorflow:examples/sec: 2.86578\n",
      "I1118 06:19:53.928064 4435920320 tpu_estimator.py:2160] examples/sec: 2.86578\n",
      "INFO:tensorflow:global_step/sec: 0.0897429\n",
      "I1118 06:20:05.070674 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897429\n",
      "INFO:tensorflow:examples/sec: 2.87177\n",
      "I1118 06:20:05.070906 4435920320 tpu_estimator.py:2160] examples/sec: 2.87177\n",
      "INFO:tensorflow:global_step/sec: 0.0895014\n",
      "I1118 06:20:16.243687 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895014\n",
      "INFO:tensorflow:examples/sec: 2.86404\n",
      "I1118 06:20:16.243920 4435920320 tpu_estimator.py:2160] examples/sec: 2.86404\n",
      "INFO:tensorflow:global_step/sec: 0.0894687\n",
      "I1118 06:20:27.420781 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894687\n",
      "INFO:tensorflow:examples/sec: 2.863\n",
      "I1118 06:20:27.421000 4435920320 tpu_estimator.py:2160] examples/sec: 2.863\n",
      "INFO:tensorflow:global_step/sec: 0.0893259\n",
      "I1118 06:20:38.615731 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893259\n",
      "INFO:tensorflow:examples/sec: 2.85843\n",
      "I1118 06:20:38.615967 4435920320 tpu_estimator.py:2160] examples/sec: 2.85843\n",
      "INFO:tensorflow:global_step/sec: 0.0897849\n",
      "I1118 06:20:49.753455 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897849\n",
      "INFO:tensorflow:examples/sec: 2.87312\n",
      "I1118 06:20:49.753673 4435920320 tpu_estimator.py:2160] examples/sec: 2.87312\n",
      "INFO:tensorflow:global_step/sec: 0.0892781\n",
      "I1118 06:21:00.954432 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892781\n",
      "INFO:tensorflow:examples/sec: 2.8569\n",
      "I1118 06:21:00.954662 4435920320 tpu_estimator.py:2160] examples/sec: 2.8569\n",
      "INFO:tensorflow:global_step/sec: 0.0895881\n",
      "I1118 06:21:12.116620 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895881\n",
      "INFO:tensorflow:examples/sec: 2.86682\n",
      "I1118 06:21:12.116852 4435920320 tpu_estimator.py:2160] examples/sec: 2.86682\n",
      "INFO:tensorflow:global_step/sec: 0.089748\n",
      "I1118 06:21:23.258929 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089748\n",
      "INFO:tensorflow:examples/sec: 2.87194\n",
      "I1118 06:21:23.259162 4435920320 tpu_estimator.py:2160] examples/sec: 2.87194\n",
      "INFO:tensorflow:global_step/sec: 0.0895636\n",
      "I1118 06:21:34.424176 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895636\n",
      "INFO:tensorflow:examples/sec: 2.86604\n",
      "I1118 06:21:34.424413 4435920320 tpu_estimator.py:2160] examples/sec: 2.86604\n",
      "INFO:tensorflow:global_step/sec: 0.0891758\n",
      "I1118 06:21:45.637971 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891758\n",
      "INFO:tensorflow:examples/sec: 2.85363\n",
      "I1118 06:21:45.638192 4435920320 tpu_estimator.py:2160] examples/sec: 2.85363\n",
      "INFO:tensorflow:global_step/sec: 0.0888023\n",
      "I1118 06:21:56.898938 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888023\n",
      "INFO:tensorflow:examples/sec: 2.84167\n",
      "I1118 06:21:56.899153 4435920320 tpu_estimator.py:2160] examples/sec: 2.84167\n",
      "INFO:tensorflow:global_step/sec: 0.0876754\n",
      "I1118 06:22:08.304645 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0876754\n",
      "INFO:tensorflow:examples/sec: 2.80561\n",
      "I1118 06:22:08.304863 4435920320 tpu_estimator.py:2160] examples/sec: 2.80561\n",
      "INFO:tensorflow:global_step/sec: 0.0894171\n",
      "I1118 06:22:19.488196 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894171\n",
      "INFO:tensorflow:examples/sec: 2.86135\n",
      "I1118 06:22:19.488421 4435920320 tpu_estimator.py:2160] examples/sec: 2.86135\n",
      "INFO:tensorflow:global_step/sec: 0.0898964\n",
      "I1118 06:22:30.612113 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898964\n",
      "INFO:tensorflow:examples/sec: 2.87668\n",
      "I1118 06:22:30.612346 4435920320 tpu_estimator.py:2160] examples/sec: 2.87668\n",
      "INFO:tensorflow:global_step/sec: 0.0900066\n",
      "I1118 06:22:41.722409 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900066\n",
      "INFO:tensorflow:examples/sec: 2.88021\n",
      "I1118 06:22:41.722635 4435920320 tpu_estimator.py:2160] examples/sec: 2.88021\n",
      "INFO:tensorflow:global_step/sec: 0.0894118\n",
      "I1118 06:22:52.906634 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894118\n",
      "INFO:tensorflow:examples/sec: 2.86118\n",
      "I1118 06:22:52.906864 4435920320 tpu_estimator.py:2160] examples/sec: 2.86118\n",
      "INFO:tensorflow:global_step/sec: 0.0898471\n",
      "I1118 06:23:04.036618 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898471\n",
      "INFO:tensorflow:examples/sec: 2.87511\n",
      "I1118 06:23:04.036834 4435920320 tpu_estimator.py:2160] examples/sec: 2.87511\n",
      "INFO:tensorflow:global_step/sec: 0.0901982\n",
      "I1118 06:23:15.123347 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0901982\n",
      "INFO:tensorflow:examples/sec: 2.88634\n",
      "I1118 06:23:15.123588 4435920320 tpu_estimator.py:2160] examples/sec: 2.88634\n",
      "INFO:tensorflow:global_step/sec: 0.0894176\n",
      "I1118 06:23:26.306800 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894176\n",
      "INFO:tensorflow:examples/sec: 2.86136\n",
      "I1118 06:23:26.307025 4435920320 tpu_estimator.py:2160] examples/sec: 2.86136\n",
      "INFO:tensorflow:global_step/sec: 0.0898947\n",
      "I1118 06:23:37.430932 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898947\n",
      "INFO:tensorflow:examples/sec: 2.87663\n",
      "I1118 06:23:37.431150 4435920320 tpu_estimator.py:2160] examples/sec: 2.87663\n",
      "INFO:tensorflow:global_step/sec: 0.0895267\n",
      "I1118 06:23:48.600783 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895267\n",
      "INFO:tensorflow:examples/sec: 2.86485\n",
      "I1118 06:23:48.601013 4435920320 tpu_estimator.py:2160] examples/sec: 2.86485\n",
      "INFO:tensorflow:global_step/sec: 0.089607\n",
      "I1118 06:23:59.760607 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089607\n",
      "INFO:tensorflow:examples/sec: 2.86742\n",
      "I1118 06:23:59.760825 4435920320 tpu_estimator.py:2160] examples/sec: 2.86742\n",
      "INFO:tensorflow:global_step/sec: 0.0889873\n",
      "I1118 06:24:10.998177 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889873\n",
      "INFO:tensorflow:examples/sec: 2.84759\n",
      "I1118 06:24:10.998394 4435920320 tpu_estimator.py:2160] examples/sec: 2.84759\n",
      "INFO:tensorflow:global_step/sec: 0.0879827\n",
      "I1118 06:24:22.364077 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0879827\n",
      "INFO:tensorflow:examples/sec: 2.81545\n",
      "I1118 06:24:22.364300 4435920320 tpu_estimator.py:2160] examples/sec: 2.81545\n",
      "INFO:tensorflow:global_step/sec: 0.0890119\n",
      "I1118 06:24:33.598508 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890119\n",
      "INFO:tensorflow:examples/sec: 2.84838\n",
      "I1118 06:24:33.598725 4435920320 tpu_estimator.py:2160] examples/sec: 2.84838\n",
      "INFO:tensorflow:global_step/sec: 0.0894967\n",
      "I1118 06:24:44.772119 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894967\n",
      "INFO:tensorflow:examples/sec: 2.8639\n",
      "I1118 06:24:44.772340 4435920320 tpu_estimator.py:2160] examples/sec: 2.8639\n",
      "INFO:tensorflow:global_step/sec: 0.0899973\n",
      "I1118 06:24:55.883541 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899973\n",
      "INFO:tensorflow:examples/sec: 2.87991\n",
      "I1118 06:24:55.883754 4435920320 tpu_estimator.py:2160] examples/sec: 2.87991\n",
      "INFO:tensorflow:global_step/sec: 0.0893603\n",
      "I1118 06:25:07.074226 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893603\n",
      "INFO:tensorflow:examples/sec: 2.85953\n",
      "I1118 06:25:07.074444 4435920320 tpu_estimator.py:2160] examples/sec: 2.85953\n",
      "INFO:tensorflow:global_step/sec: 0.0895658\n",
      "I1118 06:25:18.239183 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895658\n",
      "INFO:tensorflow:examples/sec: 2.86611\n",
      "I1118 06:25:18.239429 4435920320 tpu_estimator.py:2160] examples/sec: 2.86611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0898385\n",
      "I1118 06:25:29.370255 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898385\n",
      "INFO:tensorflow:examples/sec: 2.87483\n",
      "I1118 06:25:29.370471 4435920320 tpu_estimator.py:2160] examples/sec: 2.87483\n",
      "INFO:tensorflow:global_step/sec: 0.0890592\n",
      "I1118 06:25:40.598746 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890592\n",
      "INFO:tensorflow:examples/sec: 2.8499\n",
      "I1118 06:25:40.598981 4435920320 tpu_estimator.py:2160] examples/sec: 2.8499\n",
      "INFO:tensorflow:global_step/sec: 0.0883049\n",
      "I1118 06:25:51.923167 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0883049\n",
      "INFO:tensorflow:examples/sec: 2.82576\n",
      "I1118 06:25:51.923386 4435920320 tpu_estimator.py:2160] examples/sec: 2.82576\n",
      "INFO:tensorflow:global_step/sec: 0.0895746\n",
      "I1118 06:26:03.087028 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895746\n",
      "INFO:tensorflow:examples/sec: 2.86639\n",
      "I1118 06:26:03.087258 4435920320 tpu_estimator.py:2160] examples/sec: 2.86639\n",
      "INFO:tensorflow:global_step/sec: 0.0891592\n",
      "I1118 06:26:14.302908 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891592\n",
      "INFO:tensorflow:examples/sec: 2.85309\n",
      "I1118 06:26:14.303127 4435920320 tpu_estimator.py:2160] examples/sec: 2.85309\n",
      "INFO:tensorflow:global_step/sec: 0.0900269\n",
      "I1118 06:26:25.410706 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0900269\n",
      "INFO:tensorflow:examples/sec: 2.88086\n",
      "I1118 06:26:25.410933 4435920320 tpu_estimator.py:2160] examples/sec: 2.88086\n",
      "INFO:tensorflow:global_step/sec: 0.0899224\n",
      "I1118 06:26:36.531413 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899224\n",
      "INFO:tensorflow:examples/sec: 2.87752\n",
      "I1118 06:26:36.531641 4435920320 tpu_estimator.py:2160] examples/sec: 2.87752\n",
      "INFO:tensorflow:global_step/sec: 0.0897347\n",
      "I1118 06:26:47.675379 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897347\n",
      "INFO:tensorflow:examples/sec: 2.87151\n",
      "I1118 06:26:47.675616 4435920320 tpu_estimator.py:2160] examples/sec: 2.87151\n",
      "INFO:tensorflow:global_step/sec: 0.0896137\n",
      "I1118 06:26:58.834388 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896137\n",
      "INFO:tensorflow:examples/sec: 2.86764\n",
      "I1118 06:26:58.834624 4435920320 tpu_estimator.py:2160] examples/sec: 2.86764\n",
      "INFO:tensorflow:global_step/sec: 0.0890286\n",
      "I1118 06:27:10.066720 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890286\n",
      "INFO:tensorflow:examples/sec: 2.84892\n",
      "I1118 06:27:10.066932 4435920320 tpu_estimator.py:2160] examples/sec: 2.84892\n",
      "INFO:tensorflow:global_step/sec: 0.0896898\n",
      "I1118 06:27:21.216269 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896898\n",
      "INFO:tensorflow:examples/sec: 2.87008\n",
      "I1118 06:27:21.216496 4435920320 tpu_estimator.py:2160] examples/sec: 2.87008\n",
      "INFO:tensorflow:global_step/sec: 0.0889189\n",
      "I1118 06:27:32.462461 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0889189\n",
      "INFO:tensorflow:examples/sec: 2.8454\n",
      "I1118 06:27:32.462687 4435920320 tpu_estimator.py:2160] examples/sec: 2.8454\n",
      "INFO:tensorflow:global_step/sec: 0.0892222\n",
      "I1118 06:27:43.670451 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892222\n",
      "INFO:tensorflow:examples/sec: 2.85511\n",
      "I1118 06:27:43.670696 4435920320 tpu_estimator.py:2160] examples/sec: 2.85511\n",
      "INFO:tensorflow:global_step/sec: 0.0894232\n",
      "I1118 06:27:54.853215 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894232\n",
      "INFO:tensorflow:examples/sec: 2.86154\n",
      "I1118 06:27:54.853434 4435920320 tpu_estimator.py:2160] examples/sec: 2.86154\n",
      "INFO:tensorflow:global_step/sec: 0.0897894\n",
      "I1118 06:28:05.990397 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897894\n",
      "INFO:tensorflow:examples/sec: 2.87326\n",
      "I1118 06:28:05.990616 4435920320 tpu_estimator.py:2160] examples/sec: 2.87326\n",
      "INFO:tensorflow:global_step/sec: 0.0896217\n",
      "I1118 06:28:17.148393 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896217\n",
      "INFO:tensorflow:examples/sec: 2.86789\n",
      "I1118 06:28:17.148621 4435920320 tpu_estimator.py:2160] examples/sec: 2.86789\n",
      "INFO:tensorflow:global_step/sec: 0.0891044\n",
      "I1118 06:28:28.371195 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891044\n",
      "INFO:tensorflow:examples/sec: 2.85134\n",
      "I1118 06:28:28.371427 4435920320 tpu_estimator.py:2160] examples/sec: 2.85134\n",
      "INFO:tensorflow:global_step/sec: 0.0890784\n",
      "I1118 06:28:39.597259 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890784\n",
      "INFO:tensorflow:examples/sec: 2.85051\n",
      "I1118 06:28:39.597495 4435920320 tpu_estimator.py:2160] examples/sec: 2.85051\n",
      "INFO:tensorflow:global_step/sec: 0.0894951\n",
      "I1118 06:28:50.771063 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894951\n",
      "INFO:tensorflow:examples/sec: 2.86384\n",
      "I1118 06:28:50.771278 4435920320 tpu_estimator.py:2160] examples/sec: 2.86384\n",
      "INFO:tensorflow:global_step/sec: 0.0894688\n",
      "I1118 06:29:01.948138 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894688\n",
      "INFO:tensorflow:examples/sec: 2.863\n",
      "I1118 06:29:01.948389 4435920320 tpu_estimator.py:2160] examples/sec: 2.863\n",
      "INFO:tensorflow:global_step/sec: 0.0884213\n",
      "I1118 06:29:13.257637 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0884213\n",
      "INFO:tensorflow:examples/sec: 2.82948\n",
      "I1118 06:29:13.257855 4435920320 tpu_estimator.py:2160] examples/sec: 2.82948\n",
      "INFO:tensorflow:global_step/sec: 0.0896083\n",
      "I1118 06:29:24.417289 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896083\n",
      "INFO:tensorflow:examples/sec: 2.86747\n",
      "I1118 06:29:24.417512 4435920320 tpu_estimator.py:2160] examples/sec: 2.86747\n",
      "INFO:tensorflow:global_step/sec: 0.0894725\n",
      "I1118 06:29:35.593912 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894725\n",
      "INFO:tensorflow:examples/sec: 2.86312\n",
      "I1118 06:29:35.594131 4435920320 tpu_estimator.py:2160] examples/sec: 2.86312\n",
      "INFO:tensorflow:global_step/sec: 0.0895333\n",
      "I1118 06:29:46.762946 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895333\n",
      "INFO:tensorflow:examples/sec: 2.86506\n",
      "I1118 06:29:46.763186 4435920320 tpu_estimator.py:2160] examples/sec: 2.86506\n",
      "INFO:tensorflow:global_step/sec: 0.0895794\n",
      "I1118 06:29:57.926215 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895794\n",
      "INFO:tensorflow:examples/sec: 2.86654\n",
      "I1118 06:29:57.926440 4435920320 tpu_estimator.py:2160] examples/sec: 2.86654\n",
      "INFO:tensorflow:global_step/sec: 0.089231\n",
      "I1118 06:30:09.133105 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089231\n",
      "INFO:tensorflow:examples/sec: 2.85539\n",
      "I1118 06:30:09.133338 4435920320 tpu_estimator.py:2160] examples/sec: 2.85539\n",
      "INFO:tensorflow:global_step/sec: 0.0899489\n",
      "I1118 06:30:20.250509 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899489\n",
      "INFO:tensorflow:examples/sec: 2.87837\n",
      "I1118 06:30:20.250726 4435920320 tpu_estimator.py:2160] examples/sec: 2.87837\n",
      "INFO:tensorflow:global_step/sec: 0.0896451\n",
      "I1118 06:30:31.405622 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896451\n",
      "INFO:tensorflow:examples/sec: 2.86864\n",
      "I1118 06:30:31.406031 4435920320 tpu_estimator.py:2160] examples/sec: 2.86864\n",
      "INFO:tensorflow:global_step/sec: 0.089435\n",
      "I1118 06:30:42.586911 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089435\n",
      "INFO:tensorflow:examples/sec: 2.86192\n",
      "I1118 06:30:42.587129 4435920320 tpu_estimator.py:2160] examples/sec: 2.86192\n",
      "INFO:tensorflow:global_step/sec: 0.0891672\n",
      "I1118 06:30:53.801819 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891672\n",
      "INFO:tensorflow:examples/sec: 2.85335\n",
      "I1118 06:30:53.802040 4435920320 tpu_estimator.py:2160] examples/sec: 2.85335\n",
      "INFO:tensorflow:global_step/sec: 0.0892278\n",
      "I1118 06:31:05.009065 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892278\n",
      "INFO:tensorflow:examples/sec: 2.85529\n",
      "I1118 06:31:05.009284 4435920320 tpu_estimator.py:2160] examples/sec: 2.85529\n",
      "INFO:tensorflow:global_step/sec: 0.088995\n",
      "I1118 06:31:16.245666 4435920320 tpu_estimator.py:2159] global_step/sec: 0.088995\n",
      "INFO:tensorflow:examples/sec: 2.84784\n",
      "I1118 06:31:16.246010 4435920320 tpu_estimator.py:2160] examples/sec: 2.84784\n",
      "INFO:tensorflow:global_step/sec: 0.0899885\n",
      "I1118 06:31:27.358186 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899885\n",
      "INFO:tensorflow:examples/sec: 2.87963\n",
      "I1118 06:31:27.358404 4435920320 tpu_estimator.py:2160] examples/sec: 2.87963\n",
      "INFO:tensorflow:global_step/sec: 0.0897197\n",
      "I1118 06:31:38.504005 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897197\n",
      "INFO:tensorflow:examples/sec: 2.87103\n",
      "I1118 06:31:38.504225 4435920320 tpu_estimator.py:2160] examples/sec: 2.87103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0891821\n",
      "I1118 06:31:49.717036 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891821\n",
      "INFO:tensorflow:examples/sec: 2.85383\n",
      "I1118 06:31:49.717272 4435920320 tpu_estimator.py:2160] examples/sec: 2.85383\n",
      "INFO:tensorflow:global_step/sec: 0.0896536\n",
      "I1118 06:32:00.871082 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896536\n",
      "INFO:tensorflow:examples/sec: 2.86891\n",
      "I1118 06:32:00.871333 4435920320 tpu_estimator.py:2160] examples/sec: 2.86891\n",
      "INFO:tensorflow:global_step/sec: 0.0888733\n",
      "I1118 06:32:12.123049 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0888733\n",
      "INFO:tensorflow:examples/sec: 2.84394\n",
      "I1118 06:32:12.123265 4435920320 tpu_estimator.py:2160] examples/sec: 2.84394\n",
      "INFO:tensorflow:global_step/sec: 0.0890047\n",
      "I1118 06:32:23.358402 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890047\n",
      "INFO:tensorflow:examples/sec: 2.84815\n",
      "I1118 06:32:23.358619 4435920320 tpu_estimator.py:2160] examples/sec: 2.84815\n",
      "INFO:tensorflow:global_step/sec: 0.0893239\n",
      "I1118 06:32:34.553626 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893239\n",
      "INFO:tensorflow:examples/sec: 2.85836\n",
      "I1118 06:32:34.553850 4435920320 tpu_estimator.py:2160] examples/sec: 2.85836\n",
      "INFO:tensorflow:global_step/sec: 0.089631\n",
      "I1118 06:32:45.710467 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089631\n",
      "INFO:tensorflow:examples/sec: 2.86819\n",
      "I1118 06:32:45.710682 4435920320 tpu_estimator.py:2160] examples/sec: 2.86819\n",
      "INFO:tensorflow:global_step/sec: 0.0896094\n",
      "I1118 06:32:56.870023 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0896094\n",
      "INFO:tensorflow:examples/sec: 2.8675\n",
      "I1118 06:32:56.870249 4435920320 tpu_estimator.py:2160] examples/sec: 2.8675\n",
      "INFO:tensorflow:global_step/sec: 0.0894086\n",
      "I1118 06:33:08.054630 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894086\n",
      "INFO:tensorflow:examples/sec: 2.86107\n",
      "I1118 06:33:08.054864 4435920320 tpu_estimator.py:2160] examples/sec: 2.86107\n",
      "INFO:tensorflow:global_step/sec: 0.0897468\n",
      "I1118 06:33:19.197101 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897468\n",
      "INFO:tensorflow:examples/sec: 2.8719\n",
      "I1118 06:33:19.197324 4435920320 tpu_estimator.py:2160] examples/sec: 2.8719\n",
      "INFO:tensorflow:global_step/sec: 0.0895664\n",
      "I1118 06:33:30.361971 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895664\n",
      "INFO:tensorflow:examples/sec: 2.86612\n",
      "I1118 06:33:30.362200 4435920320 tpu_estimator.py:2160] examples/sec: 2.86612\n",
      "INFO:tensorflow:global_step/sec: 0.0892914\n",
      "I1118 06:33:41.561253 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892914\n",
      "INFO:tensorflow:examples/sec: 2.85733\n",
      "I1118 06:33:41.561634 4435920320 tpu_estimator.py:2160] examples/sec: 2.85733\n",
      "INFO:tensorflow:global_step/sec: 0.0850599\n",
      "I1118 06:33:53.317672 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0850599\n",
      "INFO:tensorflow:examples/sec: 2.72192\n",
      "I1118 06:33:53.317878 4435920320 tpu_estimator.py:2160] examples/sec: 2.72192\n",
      "INFO:tensorflow:global_step/sec: 0.087676\n",
      "I1118 06:34:04.723325 4435920320 tpu_estimator.py:2159] global_step/sec: 0.087676\n",
      "INFO:tensorflow:examples/sec: 2.80563\n",
      "I1118 06:34:04.723558 4435920320 tpu_estimator.py:2160] examples/sec: 2.80563\n",
      "INFO:tensorflow:global_step/sec: 0.0890527\n",
      "I1118 06:34:15.952622 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890527\n",
      "INFO:tensorflow:examples/sec: 2.84969\n",
      "I1118 06:34:15.952845 4435920320 tpu_estimator.py:2160] examples/sec: 2.84969\n",
      "INFO:tensorflow:global_step/sec: 0.0893491\n",
      "I1118 06:34:27.144681 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893491\n",
      "INFO:tensorflow:examples/sec: 2.85917\n",
      "I1118 06:34:27.144901 4435920320 tpu_estimator.py:2160] examples/sec: 2.85917\n",
      "INFO:tensorflow:global_step/sec: 0.0897029\n",
      "I1118 06:34:38.292584 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0897029\n",
      "INFO:tensorflow:examples/sec: 2.87049\n",
      "I1118 06:34:38.292804 4435920320 tpu_estimator.py:2160] examples/sec: 2.87049\n",
      "INFO:tensorflow:global_step/sec: 0.0898429\n",
      "I1118 06:34:49.423145 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898429\n",
      "INFO:tensorflow:examples/sec: 2.87497\n",
      "I1118 06:34:49.423368 4435920320 tpu_estimator.py:2160] examples/sec: 2.87497\n",
      "INFO:tensorflow:global_step/sec: 0.0899176\n",
      "I1118 06:35:00.544418 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0899176\n",
      "INFO:tensorflow:examples/sec: 2.87736\n",
      "I1118 06:35:00.544643 4435920320 tpu_estimator.py:2160] examples/sec: 2.87736\n",
      "INFO:tensorflow:global_step/sec: 0.0898006\n",
      "I1118 06:35:11.680202 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898006\n",
      "INFO:tensorflow:examples/sec: 2.87362\n",
      "I1118 06:35:11.680416 4435920320 tpu_estimator.py:2160] examples/sec: 2.87362\n",
      "INFO:tensorflow:global_step/sec: 0.0894578\n",
      "I1118 06:35:22.858669 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894578\n",
      "INFO:tensorflow:examples/sec: 2.86265\n",
      "I1118 06:35:22.858900 4435920320 tpu_estimator.py:2160] examples/sec: 2.86265\n",
      "INFO:tensorflow:global_step/sec: 0.089001\n",
      "I1118 06:35:34.094492 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089001\n",
      "INFO:tensorflow:examples/sec: 2.84803\n",
      "I1118 06:35:34.094715 4435920320 tpu_estimator.py:2160] examples/sec: 2.84803\n",
      "INFO:tensorflow:global_step/sec: 0.0887565\n",
      "I1118 06:35:45.361273 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887565\n",
      "INFO:tensorflow:examples/sec: 2.84021\n",
      "I1118 06:35:45.361490 4435920320 tpu_estimator.py:2160] examples/sec: 2.84021\n",
      "INFO:tensorflow:global_step/sec: 0.0893453\n",
      "I1118 06:35:56.553815 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893453\n",
      "INFO:tensorflow:examples/sec: 2.85905\n",
      "I1118 06:35:56.554039 4435920320 tpu_estimator.py:2160] examples/sec: 2.85905\n",
      "INFO:tensorflow:global_step/sec: 0.0894524\n",
      "I1118 06:36:07.732948 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894524\n",
      "INFO:tensorflow:examples/sec: 2.86248\n",
      "I1118 06:36:07.733178 4435920320 tpu_estimator.py:2160] examples/sec: 2.86248\n",
      "INFO:tensorflow:global_step/sec: 0.089205\n",
      "I1118 06:36:18.943089 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089205\n",
      "INFO:tensorflow:examples/sec: 2.85456\n",
      "I1118 06:36:18.943305 4435920320 tpu_estimator.py:2160] examples/sec: 2.85456\n",
      "INFO:tensorflow:global_step/sec: 0.0893245\n",
      "I1118 06:36:30.138241 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893245\n",
      "INFO:tensorflow:examples/sec: 2.85838\n",
      "I1118 06:36:30.138453 4435920320 tpu_estimator.py:2160] examples/sec: 2.85838\n",
      "INFO:tensorflow:global_step/sec: 0.0898001\n",
      "I1118 06:36:41.274063 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0898001\n",
      "INFO:tensorflow:examples/sec: 2.8736\n",
      "I1118 06:36:41.274296 4435920320 tpu_estimator.py:2160] examples/sec: 2.8736\n",
      "INFO:tensorflow:global_step/sec: 0.0894815\n",
      "I1118 06:36:52.449564 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894815\n",
      "INFO:tensorflow:examples/sec: 2.86341\n",
      "I1118 06:36:52.449801 4435920320 tpu_estimator.py:2160] examples/sec: 2.86341\n",
      "INFO:tensorflow:global_step/sec: 0.0892686\n",
      "I1118 06:37:03.651703 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892686\n",
      "INFO:tensorflow:examples/sec: 2.8566\n",
      "I1118 06:37:03.651934 4435920320 tpu_estimator.py:2160] examples/sec: 2.8566\n",
      "INFO:tensorflow:global_step/sec: 0.0892847\n",
      "I1118 06:37:14.851823 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892847\n",
      "INFO:tensorflow:examples/sec: 2.85711\n",
      "I1118 06:37:14.852046 4435920320 tpu_estimator.py:2160] examples/sec: 2.85711\n",
      "INFO:tensorflow:global_step/sec: 0.0886564\n",
      "I1118 06:37:26.131319 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0886564\n",
      "INFO:tensorflow:examples/sec: 2.83701\n",
      "I1118 06:37:26.131541 4435920320 tpu_estimator.py:2160] examples/sec: 2.83701\n",
      "INFO:tensorflow:global_step/sec: 0.0893813\n",
      "I1118 06:37:37.319336 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893813\n",
      "INFO:tensorflow:examples/sec: 2.8602\n",
      "I1118 06:37:37.319555 4435920320 tpu_estimator.py:2160] examples/sec: 2.8602\n",
      "INFO:tensorflow:global_step/sec: 0.0890202\n",
      "I1118 06:37:48.552737 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0890202\n",
      "INFO:tensorflow:examples/sec: 2.84865\n",
      "I1118 06:37:48.552944 4435920320 tpu_estimator.py:2160] examples/sec: 2.84865\n",
      "INFO:tensorflow:global_step/sec: 0.0895889\n",
      "I1118 06:37:59.714841 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0895889\n",
      "INFO:tensorflow:examples/sec: 2.86685\n",
      "I1118 06:37:59.715059 4435920320 tpu_estimator.py:2160] examples/sec: 2.86685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 0.0892186\n",
      "I1118 06:38:10.923285 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892186\n",
      "INFO:tensorflow:examples/sec: 2.85499\n",
      "I1118 06:38:10.923511 4435920320 tpu_estimator.py:2160] examples/sec: 2.85499\n",
      "INFO:tensorflow:global_step/sec: 0.089244\n",
      "I1118 06:38:22.128506 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089244\n",
      "INFO:tensorflow:examples/sec: 2.85581\n",
      "I1118 06:38:22.128756 4435920320 tpu_estimator.py:2160] examples/sec: 2.85581\n",
      "INFO:tensorflow:global_step/sec: 0.089343\n",
      "I1118 06:38:33.321315 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089343\n",
      "INFO:tensorflow:examples/sec: 2.85898\n",
      "I1118 06:38:33.321533 4435920320 tpu_estimator.py:2160] examples/sec: 2.85898\n",
      "INFO:tensorflow:global_step/sec: 0.089566\n",
      "I1118 06:38:44.486266 4435920320 tpu_estimator.py:2159] global_step/sec: 0.089566\n",
      "INFO:tensorflow:examples/sec: 2.86611\n",
      "I1118 06:38:44.486640 4435920320 tpu_estimator.py:2160] examples/sec: 2.86611\n",
      "INFO:tensorflow:global_step/sec: 0.0894711\n",
      "I1118 06:38:55.663076 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0894711\n",
      "INFO:tensorflow:examples/sec: 2.86308\n",
      "I1118 06:38:55.663298 4435920320 tpu_estimator.py:2160] examples/sec: 2.86308\n",
      "INFO:tensorflow:global_step/sec: 0.0893046\n",
      "I1118 06:39:06.860702 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0893046\n",
      "INFO:tensorflow:examples/sec: 2.85775\n",
      "I1118 06:39:06.860939 4435920320 tpu_estimator.py:2160] examples/sec: 2.85775\n",
      "INFO:tensorflow:global_step/sec: 0.0891747\n",
      "I1118 06:39:18.074648 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0891747\n",
      "INFO:tensorflow:examples/sec: 2.85359\n",
      "I1118 06:39:18.074868 4435920320 tpu_estimator.py:2160] examples/sec: 2.85359\n",
      "INFO:tensorflow:global_step/sec: 0.0887596\n",
      "I1118 06:39:29.341037 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0887596\n",
      "INFO:tensorflow:examples/sec: 2.84031\n",
      "I1118 06:39:29.341289 4435920320 tpu_estimator.py:2160] examples/sec: 2.84031\n",
      "INFO:tensorflow:global_step/sec: 0.0892581\n",
      "I1118 06:39:40.544512 4435920320 tpu_estimator.py:2159] global_step/sec: 0.0892581\n",
      "INFO:tensorflow:examples/sec: 2.85626\n",
      "I1118 06:39:40.544730 4435920320 tpu_estimator.py:2160] examples/sec: 2.85626\n",
      "INFO:tensorflow:Saving checkpoints for 2784 into ./bert_output/model.ckpt.\n",
      "I1118 06:39:40.545506 4435920320 basic_session_run_hooks.py:606] Saving checkpoints for 2784 into ./bert_output/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.29137778.\n",
      "I1118 06:39:45.178905 4435920320 estimator.py:368] Loss for final step: 0.29137778.\n",
      "INFO:tensorflow:training_loop marked as finished\n",
      "I1118 06:39:45.181236 4435920320 error_handling.py:96] training_loop marked as finished\n",
      "INFO:tensorflow:Writing example 0 of 300\n",
      "I1118 06:39:45.190603 4435920320 run_classifier.py:487] Writing example 0 of 300\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:39:45.191450 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-0\n",
      "I1118 06:39:45.191544 4435920320 run_classifier.py:462] guid: dev-0\n",
      "INFO:tensorflow:tokens: [CLS] whisky is a kind of popular wine [SEP]\n",
      "I1118 06:39:45.191647 4435920320 run_classifier.py:464] tokens: [CLS] whisky is a kind of popular wine [SEP]\n",
      "INFO:tensorflow:input_ids: 101 21265 2003 1037 2785 1997 2759 4511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.191766 4435920320 run_classifier.py:465] input_ids: 101 21265 2003 1037 2785 1997 2759 4511 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.191877 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.191973 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1118 06:39:45.192037 4435920320 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:39:45.192992 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-1\n",
      "I1118 06:39:45.193088 4435920320 run_classifier.py:462] guid: dev-1\n",
      "INFO:tensorflow:tokens: [CLS] ships have no wheels . [SEP]\n",
      "I1118 06:39:45.193161 4435920320 run_classifier.py:464] tokens: [CLS] ships have no wheels . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 3719 2031 2053 7787 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.193256 4435920320 run_classifier.py:465] input_ids: 101 3719 2031 2053 7787 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.193372 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.193467 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 06:39:45.193530 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:39:45.194952 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-2\n",
      "I1118 06:39:45.195085 4435920320 run_classifier.py:462] guid: dev-2\n",
      "INFO:tensorflow:tokens: [CLS] my friend was happy when celebrating 10th birthday [SEP]\n",
      "I1118 06:39:45.195162 4435920320 run_classifier.py:464] tokens: [CLS] my friend was happy when celebrating 10th birthday [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2026 2767 2001 3407 2043 12964 6049 5798 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.195261 4435920320 run_classifier.py:465] input_ids: 101 2026 2767 2001 3407 2043 12964 6049 5798 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.195359 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.195453 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1118 06:39:45.195608 4435920320 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:39:45.196435 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-3\n",
      "I1118 06:39:45.196527 4435920320 run_classifier.py:462] guid: dev-3\n",
      "INFO:tensorflow:tokens: [CLS] bathing improves the quality of the sleep in the bedroom [SEP]\n",
      "I1118 06:39:45.196601 4435920320 run_classifier.py:464] tokens: [CLS] bathing improves the quality of the sleep in the bedroom [SEP]\n",
      "INFO:tensorflow:input_ids: 101 17573 24840 1996 3737 1997 1996 3637 1999 1996 5010 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.196695 4435920320 run_classifier.py:465] input_ids: 101 17573 24840 1996 3737 1997 1996 3637 1999 1996 5010 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.196789 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.196883 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 1 (id = 1)\n",
      "I1118 06:39:45.197448 4435920320 run_classifier.py:468] label: 1 (id = 1)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:39:45.198386 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: dev-4\n",
      "I1118 06:39:45.198497 4435920320 run_classifier.py:462] guid: dev-4\n",
      "INFO:tensorflow:tokens: [CLS] structure of the bottle is not suitable for stir - fry ##ing . [SEP]\n",
      "I1118 06:39:45.198573 4435920320 run_classifier.py:464] tokens: [CLS] structure of the bottle is not suitable for stir - fry ##ing . [SEP]\n",
      "INFO:tensorflow:input_ids: 101 3252 1997 1996 5835 2003 2025 7218 2005 16130 1011 14744 2075 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.198673 4435920320 run_classifier.py:465] input_ids: 101 3252 1997 1996 5835 2003 2025 7218 2005 16130 1011 14744 2075 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.198770 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:39:45.198987 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 06:39:45.199080 4435920320 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running evaluation *****\n",
      "I1118 06:39:45.311100 4435920320 run_classifier.py:898] ***** Running evaluation *****\n",
      "INFO:tensorflow:  Num examples = 300 (300 actual, 0 padding)\n",
      "I1118 06:39:45.311226 4435920320 run_classifier.py:901]   Num examples = 300 (300 actual, 0 padding)\n",
      "INFO:tensorflow:  Batch size = 8\n",
      "I1118 06:39:45.311296 4435920320 run_classifier.py:902]   Batch size = 8\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I1118 06:39:45.375272 4435920320 estimator.py:1145] Calling model_fn.\n",
      "INFO:tensorflow:Running eval on CPU\n",
      "I1118 06:39:45.375565 4435920320 tpu_estimator.py:2965] Running eval on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I1118 06:39:45.375972 4435920320 run_classifier.py:627] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
      "I1118 06:39:45.376092 4435920320 run_classifier.py:629]   name = input_ids, shape = (?, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
      "I1118 06:39:45.376190 4435920320 run_classifier.py:629]   name = input_mask, shape = (?, 128)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
      "I1118 06:39:45.376276 4435920320 run_classifier.py:629]   name = is_real_example, shape = (?,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
      "I1118 06:39:45.376357 4435920320 run_classifier.py:629]   name = label_ids, shape = (?,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
      "I1118 06:39:45.376442 4435920320 run_classifier.py:629]   name = segment_ids, shape = (?, 128)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:45.592229 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:45.664916 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:45.731070 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:45.811468 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bc50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:45.892915 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bf2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bf2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:45.968436 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bf2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bf2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.060960 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.126399 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.191381 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92bbe0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.272802 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b6d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a92b6d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.349857 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.423688 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.505043 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.575399 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.641663 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.723150 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1470>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1470>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.800282 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.872549 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a959cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:46.954810 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.021862 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.090555 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3acf0e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.172846 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3ab9de10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.251119 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.321697 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.401918 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.469716 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.536870 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b4cac18>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.621047 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.697234 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9597b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9597b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.766379 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9597b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9597b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.850698 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.922756 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:47.989717 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b593cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b593cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.071215 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b593cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b593cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9592b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9592b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.149757 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9592b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9592b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.221215 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.304058 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.374279 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.441874 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b829d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7be898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7be898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.526313 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7be898>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7be898>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.604506 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a29b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a29b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.676653 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9598d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.757858 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.825949 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.890979 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7bee80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b9490f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b9490f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:48.973518 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b9490f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b9490f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.053072 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.124675 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b465fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.204848 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.271301 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.337100 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b8e4f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.419542 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.497184 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.569850 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.650964 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.717095 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.782620 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd3d198>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.864094 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23780>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb23780>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:49.941967 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2e80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2e80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b880208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b880208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.013626 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b880208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b880208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.094771 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bce2438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.160573 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.227989 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bd19f98>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bee26a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bee26a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.309104 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bee26a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bee26a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.387174 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.460702 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3abb1ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.544970 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.612262 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.678965 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb2e7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c0835c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c0835c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.760631 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c0835c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c0835c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.836616 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9bfdd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:50.908122 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3b7f0240>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d37d9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d37d9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:39:51.027525 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d37d9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d37d9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "I1118 06:39:51.430489 4435920320 run_classifier.py:663] **** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.430603 4435920320 run_classifier.py:669]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.430685 4435920320 run_classifier.py:669]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.430751 4435920320 run_classifier.py:669]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.430812 4435920320 run_classifier.py:669]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.430871 4435920320 run_classifier.py:669]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431024 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431100 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431165 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431227 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431283 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431341 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431396 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431451 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431505 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.431957 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432040 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432113 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432176 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432239 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432294 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432350 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432405 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432463 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432518 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432575 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432630 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432687 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432741 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432798 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432852 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432907 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.432960 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433017 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433072 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433130 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433183 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433238 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433290 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433349 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433403 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433460 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433515 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433573 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433626 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433685 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433738 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433792 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433845 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433904 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.433959 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434016 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434071 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434123 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434178 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434234 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434288 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434345 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434398 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434455 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434509 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434565 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434619 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434672 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434725 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434782 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434838 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434895 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.434949 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435002 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435056 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435113 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435168 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435226 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435279 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435335 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435390 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435446 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435501 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435554 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435609 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435667 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435720 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435779 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435834 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435888 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435941 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.435998 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436053 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436110 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436163 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436220 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436273 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436330 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436428 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436509 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436574 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436639 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436696 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436753 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436809 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436861 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436914 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.436972 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437026 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437081 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437134 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437189 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437242 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437297 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437349 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437402 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437453 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437510 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437562 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437618 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437669 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437722 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437775 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437830 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437882 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437938 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.437990 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438045 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438097 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438150 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438203 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438255 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438307 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438362 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438414 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438469 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438522 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438574 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438626 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438682 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438735 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438790 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438842 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438899 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.438949 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439004 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439057 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439109 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439161 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439216 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439269 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439323 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439375 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439428 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439481 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439536 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439589 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439643 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439696 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439751 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439802 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439857 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439909 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.439961 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440011 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440067 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440119 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440174 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440225 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440275 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440327 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440385 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440435 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440491 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440542 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440598 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440650 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440706 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440757 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440809 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440860 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440916 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.440968 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441024 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441074 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441126 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441178 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441233 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441285 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441341 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441393 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441489 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441572 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441647 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441709 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441765 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441822 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441881 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441935 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.441994 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.442049 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.442104 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.442157 4435920320 run_classifier.py:669]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:39:51.442214 4435920320 run_classifier.py:669]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
      "I1118 06:39:51.442285 4435920320 run_classifier.py:669]   name = output_weights:0, shape = (2, 768)\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
      "I1118 06:39:51.442353 4435920320 run_classifier.py:669]   name = output_bias:0, shape = (2,)\n",
      "WARNING:tensorflow:From run_classifier.py:686: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\n",
      "W1118 06:39:51.445497 4435920320 deprecation_wrapper.py:119] From run_classifier.py:686: The name tf.metrics.accuracy is deprecated. Please use tf.compat.v1.metrics.accuracy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From run_classifier.py:688: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
      "\n",
      "W1118 06:39:51.484217 4435920320 deprecation_wrapper.py:119] From run_classifier.py:688: The name tf.metrics.mean is deprecated. Please use tf.compat.v1.metrics.mean instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I1118 06:39:51.520801 4435920320 estimator.py:1147] Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-18T06:39:51Z\n",
      "I1118 06:39:51.533347 4435920320 evaluation.py:255] Starting evaluation at 2019-11-18T06:39:51Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I1118 06:39:52.505803 4435920320 monitored_session.py:240] Graph was finalized.\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W1118 06:39:52.506084 4435920320 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-2784\n",
      "I1118 06:39:52.506938 4435920320 saver.py:1280] Restoring parameters from ./bert_output/model.ckpt-2784\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I1118 06:39:53.022974 4435920320 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I1118 06:39:53.076276 4435920320 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-18-06:40:24\n",
      "I1118 06:40:24.131103 4435920320 evaluation.py:275] Finished evaluation at 2019-11-18-06:40:24\n",
      "INFO:tensorflow:Saving dict for global step 2784: eval_accuracy = 0.79333335, eval_loss = 0.49343324, global_step = 2784, loss = 0.49889097\n",
      "I1118 06:40:24.131297 4435920320 estimator.py:2039] Saving dict for global step 2784: eval_accuracy = 0.79333335, eval_loss = 0.49343324, global_step = 2784, loss = 0.49889097\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 2784: ./bert_output/model.ckpt-2784\n",
      "I1118 06:40:24.717236 4435920320 estimator.py:2099] Saving 'checkpoint_path' summary for global step 2784: ./bert_output/model.ckpt-2784\n",
      "INFO:tensorflow:evaluation_loop marked as finished\n",
      "I1118 06:40:24.717658 4435920320 error_handling.py:96] evaluation_loop marked as finished\n",
      "INFO:tensorflow:***** Eval results *****\n",
      "I1118 06:40:24.717808 4435920320 run_classifier.py:923] ***** Eval results *****\n",
      "INFO:tensorflow:  eval_accuracy = 0.79333335\n",
      "I1118 06:40:24.717882 4435920320 run_classifier.py:925]   eval_accuracy = 0.79333335\n",
      "INFO:tensorflow:  eval_loss = 0.49343324\n",
      "I1118 06:40:24.718060 4435920320 run_classifier.py:925]   eval_loss = 0.49343324\n",
      "INFO:tensorflow:  global_step = 2784\n",
      "I1118 06:40:24.718138 4435920320 run_classifier.py:925]   global_step = 2784\n",
      "INFO:tensorflow:  loss = 0.49889097\n",
      "I1118 06:40:24.718205 4435920320 run_classifier.py:925]   loss = 0.49889097\n",
      "INFO:tensorflow:Writing example 0 of 6038\n",
      "I1118 06:40:24.745589 4435920320 run_classifier.py:487] Writing example 0 of 6038\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:40:24.745891 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-1\n",
      "I1118 06:40:24.745959 4435920320 run_classifier.py:462] guid: test-1\n",
      "INFO:tensorflow:tokens: [CLS] stone is usually in round - shapes [SEP]\n",
      "I1118 06:40:24.746022 4435920320 run_classifier.py:464] tokens: [CLS] stone is usually in round - shapes [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2962 2003 2788 1999 2461 1011 10466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.746106 4435920320 run_classifier.py:465] input_ids: 101 2962 2003 2788 1999 2461 1011 10466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.746857 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.746973 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 06:40:24.747220 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:40:24.747765 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-2\n",
      "I1118 06:40:24.747843 4435920320 run_classifier.py:462] guid: test-2\n",
      "INFO:tensorflow:tokens: [CLS] no one can get stars and sell them now [SEP]\n",
      "I1118 06:40:24.747909 4435920320 run_classifier.py:464] tokens: [CLS] no one can get stars and sell them now [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2053 2028 2064 2131 3340 1998 5271 2068 2085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.747995 4435920320 run_classifier.py:465] input_ids: 101 2053 2028 2064 2131 3340 1998 5271 2068 2085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.748080 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.748162 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 06:40:24.748218 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:40:24.748681 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-3\n",
      "I1118 06:40:24.748756 4435920320 run_classifier.py:462] guid: test-3\n",
      "INFO:tensorflow:tokens: [CLS] new york is not the capital of usa [SEP]\n",
      "I1118 06:40:24.748818 4435920320 run_classifier.py:464] tokens: [CLS] new york is not the capital of usa [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2047 2259 2003 2025 1996 3007 1997 3915 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.748901 4435920320 run_classifier.py:465] input_ids: 101 2047 2259 2003 2025 1996 3007 1997 3915 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.748984 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.749063 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 06:40:24.749192 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:40:24.749768 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-4\n",
      "I1118 06:40:24.749848 4435920320 run_classifier.py:462] guid: test-4\n",
      "INFO:tensorflow:tokens: [CLS] the sun is much brighter than the moon and stars in the day time [SEP]\n",
      "I1118 06:40:24.749912 4435920320 run_classifier.py:464] tokens: [CLS] the sun is much brighter than the moon and stars in the day time [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 3103 2003 2172 16176 2084 1996 4231 1998 3340 1999 1996 2154 2051 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.749997 4435920320 run_classifier.py:465] input_ids: 101 1996 3103 2003 2172 16176 2084 1996 4231 1998 3340 1999 1996 2154 2051 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.750082 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.750164 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 06:40:24.750300 4435920320 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 06:40:24.750824 4435920320 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-5\n",
      "I1118 06:40:24.750901 4435920320 run_classifier.py:462] guid: test-5\n",
      "INFO:tensorflow:tokens: [CLS] a restaurant does not have doctors or medical equipment [SEP]\n",
      "I1118 06:40:24.750964 4435920320 run_classifier.py:464] tokens: [CLS] a restaurant does not have doctors or medical equipment [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1037 4825 2515 2025 2031 7435 2030 2966 3941 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.751049 4435920320 run_classifier.py:465] input_ids: 101 1037 4825 2515 2025 2031 7435 2030 2966 3941 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.751132 4435920320 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 06:40:24.751215 4435920320 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 06:40:24.751338 4435920320 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running prediction*****\n",
      "I1118 06:40:26.770174 4435920320 run_classifier.py:944] ***** Running prediction*****\n",
      "INFO:tensorflow:  Num examples = 6038 (6038 actual, 0 padding)\n",
      "I1118 06:40:26.770287 4435920320 run_classifier.py:947]   Num examples = 6038 (6038 actual, 0 padding)\n",
      "INFO:tensorflow:  Batch size = 8\n",
      "I1118 06:40:26.770347 4435920320 run_classifier.py:948]   Batch size = 8\n",
      "INFO:tensorflow:***** Predict results *****\n",
      "I1118 06:40:26.770447 4435920320 run_classifier.py:962] ***** Predict results *****\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I1118 06:40:26.794986 4435920320 estimator.py:1145] Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "I1118 06:40:26.795124 4435920320 tpu_estimator.py:2965] Running infer on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I1118 06:40:26.795320 4435920320 run_classifier.py:627] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
      "I1118 06:40:26.795402 4435920320 run_classifier.py:629]   name = input_ids, shape = (?, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
      "I1118 06:40:26.795466 4435920320 run_classifier.py:629]   name = input_mask, shape = (?, 128)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
      "I1118 06:40:26.795525 4435920320 run_classifier.py:629]   name = is_real_example, shape = (?,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
      "I1118 06:40:26.795579 4435920320 run_classifier.py:629]   name = label_ids, shape = (?,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
      "I1118 06:40:26.795636 4435920320 run_classifier.py:629]   name = segment_ids, shape = (?, 128)\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:26.919667 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:26.988901 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.054152 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fdfd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fde80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fde80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.133769 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fde80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fde80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.210607 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd7b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd7b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.286437 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.367117 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.433867 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.501885 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.582263 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.658943 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.729671 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.811504 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.880197 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:27.952399 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a350ef320>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.036038 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a355fd940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bc88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.117348 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bc88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3579bc88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.192322 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.281071 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.353628 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.418565 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a351034a8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a36015588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a36015588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.498917 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a36015588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a36015588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.577419 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.648454 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.734004 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.799472 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.866003 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a360156d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc98cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc98cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:28.946138 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc98cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc98cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.024621 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.100591 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.181861 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.250431 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.319666 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a356d46d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a37e676d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a37e676d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.565485 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a37e676d8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a37e676d8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.648221 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.718593 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c772cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c772cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.798818 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c772cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c772cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.866987 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:29.933124 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3454ec88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34bd0978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34bd0978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.014639 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34bd0978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a34bd0978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.093400 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a655ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a655ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.163576 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a655ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a655ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.245235 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.312018 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.380570 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a39d5f748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.463361 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c165860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb70ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb70ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.539885 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb70ba8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3bb70ba8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.610721 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3513e1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.690964 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.756638 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.823085 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d96b588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.909296 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a9a2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:30.987735 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c244ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c244ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.059489 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c244ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c244ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.144758 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.210310 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.276744 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d3a7ef0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc65748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc65748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.359013 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc65748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3cc65748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.438665 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.511759 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a79c5c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35825978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35825978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.593777 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35825978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35825978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.661351 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.728430 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a33a46d68>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1afa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1afa20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.808924 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1afa20>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d1afa20>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.885976 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:31.958927 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720b70>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720b70>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:32.040455 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:32.109476 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:32.176560 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c38c908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c252c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c252c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:32.257082 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c252c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3c252c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:32.334801 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a35720ac8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:32.405476 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3d0a5550>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a7b7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a7b7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 06:40:32.612818 4435920320 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a7b7cf8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3a7b7cf8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "I1118 06:40:33.007917 4435920320 run_classifier.py:663] **** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008037 4435920320 run_classifier.py:669]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008121 4435920320 run_classifier.py:669]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008186 4435920320 run_classifier.py:669]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008249 4435920320 run_classifier.py:669]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008308 4435920320 run_classifier.py:669]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008519 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008596 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008661 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008724 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008780 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008838 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008893 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.008950 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009005 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009057 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009109 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009167 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009221 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009276 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009330 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009382 4435920320 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009436 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009495 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009547 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009605 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009657 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009712 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009765 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009823 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009876 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009927 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.009980 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010036 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010089 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010147 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010200 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010252 4435920320 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010306 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010363 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010416 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010473 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010526 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010582 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010636 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010692 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010746 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010797 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010851 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010907 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.010959 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011016 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011070 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011122 4435920320 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011175 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011233 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011285 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011343 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011394 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011450 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011504 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011559 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011612 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011665 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011718 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011775 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011827 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011883 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011937 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.011991 4435920320 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012043 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012101 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012154 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012210 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012262 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012319 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012372 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012428 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012482 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012533 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012587 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012643 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012696 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012753 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012805 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012858 4435920320 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012912 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.012969 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013021 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013077 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013129 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013187 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013240 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013296 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013349 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013402 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013455 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013510 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013563 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013619 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013674 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013726 4435920320 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013778 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013836 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013889 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013945 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.013998 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014053 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014106 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014163 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014218 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014270 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014321 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014379 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014432 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014490 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014543 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014595 4435920320 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014647 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014704 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014757 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014812 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014866 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014923 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.014976 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.015033 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.015086 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.015139 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.015192 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 06:40:33.015249 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.015301 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.015360 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.015413 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.015468 4435920320 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.015521 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.016479 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.016551 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.016655 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.016714 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017197 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017267 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017338 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017396 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017452 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017507 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017565 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017618 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017674 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017729 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017781 4435920320 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017832 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017889 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017943 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.017999 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.018086 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.018174 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.018244 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.018329 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.018400 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.018454 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.018507 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019398 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019474 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019541 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019599 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019655 4435920320 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019711 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019770 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019824 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019884 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019939 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.019995 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020049 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020107 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020162 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020217 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020271 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020328 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020385 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020442 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020498 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020551 4435920320 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020604 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020668 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020727 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020784 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020838 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020895 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.020951 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021008 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021062 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021117 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021172 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021229 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021285 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021342 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021398 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021451 4435920320 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021505 4435920320 run_classifier.py:669]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "I1118 06:40:33.021562 4435920320 run_classifier.py:669]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\r\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\r\n",
      "I1118 06:40:33.021639 4435920320 run_classifier.py:669]   name = output_weights:0, shape = (2, 768)\r\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,)\r\n",
      "I1118 06:40:33.021708 4435920320 run_classifier.py:669]   name = output_bias:0, shape = (2,)\r\n",
      "INFO:tensorflow:Done calling model_fn.\r\n",
      "I1118 06:40:33.021979 4435920320 estimator.py:1147] Done calling model_fn.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "I1118 06:40:33.328733 4435920320 monitored_session.py:240] Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-2784\n",
      "I1118 06:40:33.329920 4435920320 saver.py:1280] Restoring parameters from ./bert_output/model.ckpt-2784\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I1118 06:40:33.836291 4435920320 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I1118 06:40:33.881639 4435920320 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I1118 06:51:01.015840 4435920320 error_handling.py:96] prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I1118 06:51:01.016016 4435920320 error_handling.py:96] prediction_loop marked as finished\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "#!python run_classifier.py \\\n",
    "#      --task_name=cola \\\n",
    "#      --do_train=true \\\n",
    "#      --do_eval=true \\\n",
    "#      --data_dir=./task2_data/ \\\n",
    "#      --vocab_file=./uncased_L-12_H-768_A-12/vocab.txt \\\n",
    "#      --bert_config_file=./uncased_L-12_H-768_A-12/bert_config.json \\\n",
    "#      --init_checkpoint=./uncased_L-12_H-768_A-12/bert_model.ckpt \\\n",
    "#      --max_seq_length=128 /\n",
    "#      --train_batch_size=32 /\n",
    "#      --learning_rate=2e-5 /\n",
    "#      --num_train_epochs=3.0 /\n",
    "#      --output_dir=./bert_output/ /\n",
    "#      --do_lower_case=True /"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:981: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:784: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "W1118 11:05:06.820677 4467508672 deprecation_wrapper.py:119] From run_classifier.py:784: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:784: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "W1118 11:05:06.820794 4467508672 deprecation_wrapper.py:119] From run_classifier.py:784: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1118 11:05:06.821140 4467508672 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:93: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:808: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "W1118 11:05:06.821733 4467508672 deprecation_wrapper.py:119] From run_classifier.py:808: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W1118 11:05:07.870614 4467508672 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0xb2c729598>) includes params argument, but params are not passed to Estimator.\n",
      "W1118 11:05:07.871120 4467508672 estimator.py:1984] Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0xb2c729598>) includes params argument, but params are not passed to Estimator.\n",
      "INFO:tensorflow:Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a30449278>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "I1118 11:05:07.871733 4467508672 estimator.py:209] Using config: {'_model_dir': './bert_output/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a30449278>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=1000, num_shards=8, num_cores_per_replica=None, per_host_input_for_training=3, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "I1118 11:05:07.872143 4467508672 tpu_context.py:209] _TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "W1118 11:05:07.872269 4467508672 tpu_context.py:211] eval_on_tpu ignored because use_tpu is False.\n",
      "WARNING:tensorflow:From run_classifier.py:199: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "W1118 11:05:07.872409 4467508672 deprecation_wrapper.py:119] From run_classifier.py:199: The name tf.gfile.Open is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:483: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "W1118 11:05:07.901531 4467508672 deprecation_wrapper.py:119] From run_classifier.py:483: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:487: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "W1118 11:05:07.902409 4467508672 deprecation_wrapper.py:119] From run_classifier.py:487: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Writing example 0 of 6063\n",
      "I1118 11:05:07.902555 4467508672 run_classifier.py:487] Writing example 0 of 6063\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 11:05:07.902884 4467508672 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-1\n",
      "I1118 11:05:07.902976 4467508672 run_classifier.py:462] guid: test-1\n",
      "INFO:tensorflow:tokens: [CLS] an elephant is much bigger than a fridge [SEP]\n",
      "I1118 11:05:07.903053 4467508672 run_classifier.py:464] tokens: [CLS] an elephant is much bigger than a fridge [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2019 10777 2003 2172 7046 2084 1037 16716 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.903141 4467508672 run_classifier.py:465] input_ids: 101 2019 10777 2003 2172 7046 2084 1037 16716 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.903258 4467508672 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.903351 4467508672 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 11:05:07.903414 4467508672 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 11:05:07.904274 4467508672 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-2\n",
      "I1118 11:05:07.904374 4467508672 run_classifier.py:462] guid: test-2\n",
      "INFO:tensorflow:tokens: [CLS] stone is usually in round - shapes [SEP]\n",
      "I1118 11:05:07.904447 4467508672 run_classifier.py:464] tokens: [CLS] stone is usually in round - shapes [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2962 2003 2788 1999 2461 1011 10466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.904531 4467508672 run_classifier.py:465] input_ids: 101 2962 2003 2788 1999 2461 1011 10466 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.904614 4467508672 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.904743 4467508672 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 11:05:07.904831 4467508672 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 11:05:07.905343 4467508672 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-3\n",
      "I1118 11:05:07.905423 4467508672 run_classifier.py:462] guid: test-3\n",
      "INFO:tensorflow:tokens: [CLS] no one can get stars and sell them now [SEP]\n",
      "I1118 11:05:07.905510 4467508672 run_classifier.py:464] tokens: [CLS] no one can get stars and sell them now [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2053 2028 2064 2131 3340 1998 5271 2068 2085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.905614 4467508672 run_classifier.py:465] input_ids: 101 2053 2028 2064 2131 3340 1998 5271 2068 2085 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.905699 4467508672 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.905869 4467508672 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 11:05:07.905946 4467508672 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 11:05:07.906424 4467508672 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-4\n",
      "I1118 11:05:07.906501 4467508672 run_classifier.py:462] guid: test-4\n",
      "INFO:tensorflow:tokens: [CLS] new york is not the capital of usa [SEP]\n",
      "I1118 11:05:07.906586 4467508672 run_classifier.py:464] tokens: [CLS] new york is not the capital of usa [SEP]\n",
      "INFO:tensorflow:input_ids: 101 2047 2259 2003 2025 1996 3007 1997 3915 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.906669 4467508672 run_classifier.py:465] input_ids: 101 2047 2259 2003 2025 1996 3007 1997 3915 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.906752 4467508672 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.906885 4467508672 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 11:05:07.906966 4467508672 run_classifier.py:468] label: 0 (id = 0)\n",
      "INFO:tensorflow:*** Example ***\n",
      "I1118 11:05:07.907534 4467508672 run_classifier.py:461] *** Example ***\n",
      "INFO:tensorflow:guid: test-5\n",
      "I1118 11:05:07.907610 4467508672 run_classifier.py:462] guid: test-5\n",
      "INFO:tensorflow:tokens: [CLS] the sun is much brighter than the moon and stars in the day time [SEP]\n",
      "I1118 11:05:07.907674 4467508672 run_classifier.py:464] tokens: [CLS] the sun is much brighter than the moon and stars in the day time [SEP]\n",
      "INFO:tensorflow:input_ids: 101 1996 3103 2003 2172 16176 2084 1996 4231 1998 3340 1999 1996 2154 2051 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.907757 4467508672 run_classifier.py:465] input_ids: 101 1996 3103 2003 2172 16176 2084 1996 4231 1998 3340 1999 1996 2154 2051 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.907839 4467508672 run_classifier.py:466] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "I1118 11:05:07.907919 4467508672 run_classifier.py:467] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:label: 0 (id = 0)\n",
      "I1118 11:05:07.908013 4467508672 run_classifier.py:468] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running prediction*****\n",
      "I1118 11:05:09.899074 4467508672 run_classifier.py:944] ***** Running prediction*****\n",
      "INFO:tensorflow:  Num examples = 6063 (6063 actual, 0 padding)\n",
      "I1118 11:05:09.899188 4467508672 run_classifier.py:947]   Num examples = 6063 (6063 actual, 0 padding)\n",
      "INFO:tensorflow:  Batch size = 8\n",
      "I1118 11:05:09.899247 4467508672 run_classifier.py:948]   Batch size = 8\n",
      "WARNING:tensorflow:From run_classifier.py:514: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "W1118 11:05:09.899410 4467508672 deprecation_wrapper.py:119] From run_classifier.py:514: The name tf.FixedLenFeature is deprecated. Please use tf.io.FixedLenFeature instead.\n",
      "\n",
      "INFO:tensorflow:***** Predict results *****\n",
      "I1118 11:05:09.899516 4467508672 run_classifier.py:962] ***** Predict results *****\n",
      "WARNING:tensorflow:From run_classifier.py:550: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "W1118 11:05:09.931133 4467508672 deprecation.py:323] From run_classifier.py:550: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "W1118 11:05:09.931282 4467508672 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/contrib/data/python/ops/batching.py:273: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n",
      "WARNING:tensorflow:From run_classifier.py:523: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "W1118 11:05:09.932570 4467508672 deprecation_wrapper.py:119] From run_classifier.py:523: The name tf.parse_single_example is deprecated. Please use tf.io.parse_single_example instead.\n",
      "\n",
      "WARNING:tensorflow:From run_classifier.py:530: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "W1118 11:05:09.937057 4467508672 deprecation.py:323] From run_classifier.py:530: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "I1118 11:05:09.951781 4467508672 estimator.py:1145] Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "I1118 11:05:09.951970 4467508672 tpu_estimator.py:2965] Running infer on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "I1118 11:05:09.952284 4467508672 run_classifier.py:627] *** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (?, 128)\n",
      "I1118 11:05:09.952429 4467508672 run_classifier.py:629]   name = input_ids, shape = (?, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (?, 128)\n",
      "I1118 11:05:09.952530 4467508672 run_classifier.py:629]   name = input_mask, shape = (?, 128)\n",
      "INFO:tensorflow:  name = is_real_example, shape = (?,)\n",
      "I1118 11:05:09.952608 4467508672 run_classifier.py:629]   name = is_real_example, shape = (?,)\n",
      "INFO:tensorflow:  name = label_ids, shape = (?,)\n",
      "I1118 11:05:09.952759 4467508672 run_classifier.py:629]   name = label_ids, shape = (?,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (?, 128)\n",
      "I1118 11:05:09.952872 4467508672 run_classifier.py:629]   name = segment_ids, shape = (?, 128)\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W1118 11:05:09.956170 4467508672 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:171: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W1118 11:05:09.957370 4467508672 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:409: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "W1118 11:05:09.982799 4467508672 deprecation_wrapper.py:119] From /Users/kelson/repository/personal/github/bert/modeling.py:490: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:From /Users/kelson/repository/personal/github/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W1118 11:05:10.030960 4467508672 deprecation.py:323] From /Users/kelson/repository/personal/github/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.119155 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.201003 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.280687 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30877860>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fac88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fac88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.398707 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fac88>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fac88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.492804 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa630>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa630>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.577342 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.667367 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.850288 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:10.933161 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3090af28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.021006 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.108728 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e71d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e71d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.185739 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e71d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e71d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b643c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b643c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.272785 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b643c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b643c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.350198 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.423928 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30c091d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.517443 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a308fa588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3097a668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3097a668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.596047 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3097a668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3097a668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e72b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e72b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.674988 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e72b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e72b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.768783 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.853061 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:11.930613 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b2d390>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30d13e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30d13e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.019379 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30d13e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30d13e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b63dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b63dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.118122 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b63dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30b63dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.215869 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.320548 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.403223 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ef0940>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30f5de80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30f5de80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.494844 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30f5de80>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30f5de80>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.594127 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7438>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ce3278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ce3278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.675238 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ce3278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ce3278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e6f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e6f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.747005 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e6f160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e6f160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31028080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31028080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.836311 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31028080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31028080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:12.917453 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.004696 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a310913c8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ffaf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ffaf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.102573 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ffaf28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ffaf28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ca9be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ca9be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.183732 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ca9be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30ca9be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e57f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e57f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.270223 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e57f28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30e57f28>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.375394 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.451224 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.527834 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a311d54e0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3116f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3116f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.620189 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3116f668>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3116f668>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.784169 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.859504 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:13.952176 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.027837 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.121051 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3133a710>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31254cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31254cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.229454 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31254cc0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31254cc0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.322227 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309b3978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309b3978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.413800 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309b3978>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309b3978>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3153c2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3153c2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.514010 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3153c2b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3153c2b0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.584314 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.659814 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3156b1d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31467d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31467d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.757848 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31467d30>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31467d30>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.849642 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30af01d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30af01d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:14.933707 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30af01d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30af01d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.018063 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.085171 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.162000 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a316036a0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31390a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31390a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.261242 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31390a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31390a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3160e358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3160e358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.359817 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3160e358>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3160e358>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a314677b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a314677b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.451647 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a314677b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a314677b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3159d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3159d160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.548963 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3159d160>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a3159d160>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.618054 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.697257 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a318138d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.793303 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31153a90>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.884883 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a309e7208>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:15.970888 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30a54080>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:16.061175 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:16.135225 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:16.206579 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31906e48>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a313bb588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a313bb588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:16.293157 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a313bb588>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a313bb588>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31769e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31769e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:16.379452 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31769e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a31769e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a317a9dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a317a9dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:16.467005 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a317a9dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a317a9dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30497c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30497c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "W1118 11:05:16.607635 4467508672 ag_logging.py:145] Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30497c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x1a30497c50>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From run_classifier.py:661: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "W1118 11:05:16.627466 4467508672 deprecation_wrapper.py:119] From run_classifier.py:661: The name tf.train.init_from_checkpoint is deprecated. Please use tf.compat.v1.train.init_from_checkpoint instead.\n",
      "\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "I1118 11:05:17.113034 4467508672 run_classifier.py:663] **** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113293 4467508672 run_classifier.py:669]   name = bert/embeddings/word_embeddings:0, shape = (30522, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113451 4467508672 run_classifier.py:669]   name = bert/embeddings/token_type_embeddings:0, shape = (2, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113520 4467508672 run_classifier.py:669]   name = bert/embeddings/position_embeddings:0, shape = (512, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113582 4467508672 run_classifier.py:669]   name = bert/embeddings/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113672 4467508672 run_classifier.py:669]   name = bert/embeddings/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113775 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113850 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.113953 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114009 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114224 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114335 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114416 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114477 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114533 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114588 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114640 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114696 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114749 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114807 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114861 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.114953 4467508672 run_classifier.py:669]   name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115006 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115064 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115118 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115173 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115226 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115281 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115334 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115390 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115442 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115493 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115545 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115602 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115654 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115711 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115763 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115817 4467508672 run_classifier.py:669]   name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115869 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115923 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.115977 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116033 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116085 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116141 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116193 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116249 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116302 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116355 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116482 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116554 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116607 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116665 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116718 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116770 4467508672 run_classifier.py:669]   name = bert/encoder/layer_2/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116824 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116880 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116935 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.116991 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117044 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117102 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117155 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117210 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117264 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117319 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117372 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117430 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117485 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117541 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117594 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117647 4467508672 run_classifier.py:669]   name = bert/encoder/layer_3/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117700 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117755 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117808 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117864 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117918 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.117974 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118026 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118083 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118136 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118189 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118241 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118298 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118351 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118407 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118460 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118512 4467508672 run_classifier.py:669]   name = bert/encoder/layer_4/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118565 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118623 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118675 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118731 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118786 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118842 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118896 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.118952 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119004 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119057 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119110 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119168 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119220 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119277 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119331 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119384 4467508672 run_classifier.py:669]   name = bert/encoder/layer_5/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119436 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119493 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119546 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119603 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119658 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119714 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119768 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119823 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119876 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119928 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.119981 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120037 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120090 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120146 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120198 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120250 4467508672 run_classifier.py:669]   name = bert/encoder/layer_6/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120304 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120360 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120412 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120470 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120522 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120579 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120633 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120688 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120741 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120794 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120847 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120903 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.120957 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121015 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121068 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121120 4467508672 run_classifier.py:669]   name = bert/encoder/layer_7/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121174 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121231 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121284 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121340 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121393 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121477 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121547 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121604 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121656 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121709 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121762 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121818 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121871 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121928 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.121981 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122033 4467508672 run_classifier.py:669]   name = bert/encoder/layer_8/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122087 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122143 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122196 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122255 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122309 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122364 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122418 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122474 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122529 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122581 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122635 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122691 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122744 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122800 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122852 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122905 4467508672 run_classifier.py:669]   name = bert/encoder/layer_9/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.122958 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123014 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123067 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123125 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123178 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123236 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123289 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123344 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123399 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123452 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123505 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123563 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123616 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123674 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123726 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123779 4467508672 run_classifier.py:669]   name = bert/encoder/layer_10/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123831 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123888 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/query/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.123942 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124000 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/key/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124053 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124109 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/self/value/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124163 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124219 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124274 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124327 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/attention/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124379 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/kernel:0, shape = (768, 3072), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124437 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/intermediate/dense/bias:0, shape = (3072,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124490 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/kernel:0, shape = (3072, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124547 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/output/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124601 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/beta:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124653 4467508672 run_classifier.py:669]   name = bert/encoder/layer_11/output/LayerNorm/gamma:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124706 4467508672 run_classifier.py:669]   name = bert/pooler/dense/kernel:0, shape = (768, 768), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "I1118 11:05:17.124763 4467508672 run_classifier.py:669]   name = bert/pooler/dense/bias:0, shape = (768,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 768)\n",
      "I1118 11:05:17.124837 4467508672 run_classifier.py:669]   name = output_weights:0, shape = (2, 768)\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,)\n",
      "I1118 11:05:17.124906 4467508672 run_classifier.py:669]   name = output_bias:0, shape = (2,)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "I1118 11:05:17.125185 4467508672 estimator.py:1147] Done calling model_fn.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W1118 11:05:17.239002 4467508672 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:1354: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "I1118 11:05:17.464623 4467508672 monitored_session.py:240] Graph was finalized.\n",
      "2019-11-18 11:05:17.469420: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "WARNING:tensorflow:From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W1118 11:05:17.470504 4467508672 deprecation.py:323] From /Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from ./bert_output/model.ckpt-2784\n",
      "I1118 11:05:17.471282 4467508672 saver.py:1280] Restoring parameters from ./bert_output/model.ckpt-2784\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "I1118 11:05:18.454233 4467508672 session_manager.py:500] Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "I1118 11:05:18.515225 4467508672 session_manager.py:502] Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I1118 11:16:13.483642 4467508672 error_handling.py:96] prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "I1118 11:16:13.483824 4467508672 error_handling.py:96] prediction_loop marked as finished\n"
     ]
    }
   ],
   "source": [
    "# PREDICT\n",
    "\n",
    "#! python run_classifier.py --task_name=cola --do_train=false --do_eval=false --do_predict=true --data_dir=./task2_data/ --vocab_file=./uncased_L-12_H-768_A-12/vocab.txt --bert_config_file=./uncased_L-12_H-768_A-12/bert_config.json --init_checkpoint=./uncased_L-12_H-768_A-12/bert_model.ckpt --output_dir=./bert_output/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explaining Bert Output - Task 2\n",
    "\n",
    "Similar to Task 1, Task 2 prediction values were generated for each sentence, which is the probability of the sentence being part of the target class 0 or 1.\n",
    "\n",
    "The big difference between Task 1 and 2 in the evaluation is that the output will be pre-processed so transform each sentence into the target label (A, B or C).\n",
    "\n",
    "This transformation is done by taking the which of the 2 options for each sentence is the one with highest probability. In total there were 6063 results, but we as the results are an extension of the 2021 sentences, only 2021 entries are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          id predlab\n",
      "0        1.0       A\n",
      "1        2.0       C\n",
      "2        3.0       B\n",
      "3        4.0       B\n",
      "4        5.0       A\n",
      "...      ...     ...\n",
      "2016  2017.0       A\n",
      "2017  2018.0       A\n",
      "2018  2019.0       A\n",
      "2019  2020.0       A\n",
      "2020  2021.0       C\n",
      "\n",
      "[2021 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Preparing BERT prediction output for Task 2\n",
    "task2_result_df = pd.read_csv('task2_data/test_results_task2.tsv', sep='\\t', header=None)\n",
    "\n",
    "to_predict=int(task2_result_df.shape[0] / 3) # 3 is the number of target\n",
    "\n",
    "pred_list = [None] * to_predict * 3 # 3 classes\n",
    "\n",
    "for i in range(to_predict):\n",
    "    \n",
    "    v1 = task2_result_df.loc[i][0] if task2_result_df.loc[i][0] < task2_result_df.loc[i][1] else task2_result_df.loc[i][1]\n",
    "    v2 = task2_result_df.loc[i + to_predict][0] if task2_result_df.loc[ i +to_predict][0] < task2_result_df.loc[i + to_predict][1] else task2_result_df.loc[ i + to_predict][1]\n",
    "    v3 = task2_result_df.loc[i + to_predict * 2][0] if task2_result_df.loc[ i +to_predict * 2][0] < task2_result_df.loc[i + to_predict * 2][1] else task2_result_df.loc[ i + to_predict * 2][1]    \n",
    "    \n",
    "    values_list = [v1,v2,v3]\n",
    "    predicted = values_list.index(max(values_list))\n",
    "    \n",
    "    if predicted == 0:\n",
    "        predicted = 'A'\n",
    "    elif predicted == 1:\n",
    "        predicted = 'B'\n",
    "    else:\n",
    "        predicted = 'C'\n",
    "        \n",
    "    pred_list[i] = predicted\n",
    "    \n",
    "    task2_result_df.loc[i][0]=i+1\n",
    "    \n",
    "#     task2_result_df.loc[i][1]=predicted\n",
    "       \n",
    "task2_result_df['pred'] = pred_list\n",
    "\n",
    "task2_result_df.columns = ['id', 'predn','predlab'] \n",
    "\n",
    "task2_result_df = task2_result_df[:2021]\n",
    "task2_result_df = task2_result_df.drop('predn', axis=1)\n",
    "print(task2_result_df)\n",
    "\n",
    "task2_result_df.to_csv('evaluation_tools/task2_my_result.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 60.1188%\r\n"
     ]
    }
   ],
   "source": [
    "! python evaluation_tools/taskB_scorer.py --gold-labels evaluation_tools/taskB_trial_answer.csv --pred-labels evaluation_tools/task2_my_result.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2 Accuracy: 60.1188%\n",
    "\n",
    "The final output for Task 2 was taken through the integration of the evaluation tools https://github.com/wangcunxiang/SemEval2020-Task4-Commonsense-Validation-and-Explanation\n",
    "\n",
    "\n",
    "## Task 3\n",
    "\n",
    "For task 3, GPT2 pre-trainned was uses, by making use of the transformers library: https://github.com/huggingface/transformers\n",
    "\n",
    "\n",
    "### Pre-Processing Task 3 Data\n",
    "\n",
    "The trainning data for Task 3 contained 3 explanation sentences for each sentence that do not make sense.\n",
    "\n",
    "This data was manually extracted in such a way to create a single file with each line containing these 3 \"explanation sentences\" in one single line.\n",
    "\n",
    "![task3_processed](images/task3_img1.png)\n",
    "\n",
    "\n",
    "### Below is the step to Fine-Tune GPT2 with the pre-processed data for Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'transformers/examples/'\n",
      "/Users/kelson/repository/personal/github/bert/transformers/examples\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kelson/anaconda3/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n",
      "11/19/2019 19:15:54 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
      "11/19/2019 19:15:55 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /Users/kelson/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "11/19/2019 19:15:55 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"num_labels\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "11/19/2019 19:15:56 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /Users/kelson/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "11/19/2019 19:15:56 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /Users/kelson/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "11/19/2019 19:15:57 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /Users/kelson/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "11/19/2019 19:16:00 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir='', config_name='', device=device(type='cpu'), do_eval=True, do_lower_case=False, do_train=True, eval_all_checkpoints=False, eval_data_file='../../task3_data/taskC_test.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=0, no_cuda=False, num_train_epochs=1.0, output_dir='output', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=50, save_total_limit=None, seed=42, server_ip='', server_port='', tokenizer_name='', train_data_file='../../task3_data/taskC_train.txt', warmup_steps=0, weight_decay=0.0)\n",
      "11/19/2019 19:16:00 - INFO - __main__ -   Creating features from dataset file at ../../task3_data\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   Token indices sequence length is longer than the specified maximum sequence length for this model (63606 > 1024). Running this sequence through the model will result in indexing errors\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - WARNING - transformers.tokenization_utils -   This tokenizer does not make use of special tokens. Input is returned with no modification.\n",
      "11/19/2019 19:16:01 - INFO - __main__ -   Saving features into cached file ../../task3_data/gpt2_cached_lm_1024_taskC_train.txt\n",
      "Traceback (most recent call last):\n",
      "  File \"run_lm_finetuning.py\", line 566, in <module>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    main()\r\n",
      "  File \"run_lm_finetuning.py\", line 518, in main\r\n",
      "    global_step, tr_loss = train(args, train_dataset, model, tokenizer)\r\n",
      "  File \"run_lm_finetuning.py\", line 199, in train\r\n",
      "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total)\r\n",
      "TypeError: __init__() got an unexpected keyword argument 'num_warmup_steps'\r\n"
     ]
    }
   ],
   "source": [
    "%cd transformers/examples/\n",
    "\n",
    "\n",
    "# ! python run_lm_finetuning.py \\\n",
    "#     --output_dir=output \\\n",
    "#     --model_type=gpt2 \\\n",
    "#     --model_name_or_path=\"gpt2\" \\\n",
    "#     --do_train \\\n",
    "#     --train_data_file=\"../../task3_data/taskC_train.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 - Prediction\n",
    "\n",
    "Having GPT2 pre-trained, the next step is to provide the logic to generate the automatic explanation for each sentence in the Test data.\n",
    "\n",
    "### Transformers Git Pull Required\n",
    "`pip install git+https://github.com/huggingface/transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'transformers/examples/'\n",
      "/Users/kelson/repository/personal/github/bert/transformers/examples\n",
      "2021\n",
      "Predicting Input : 0 - he put an elephant into the fridge.\n",
      "Predicted Output: In fact, these Cane are very sturdy. No paper was necessary and they don't\n",
      "\n",
      "Predicting Input : 1 - my sister eats a stone after breakfast every day.\n",
      "Predicted Output: You see something important, not your death.\"\n",
      "\n",
      "Predicting Input : 2 - money can be used for buying stars.\n",
      "Predicted Output: When first conceived of, uninspiring bids are generally accepted to reduce prices. In order\n",
      "\n",
      "Predicting Input : 3 - USA is located in the northeastern part of New York.\n",
      "Predicted Output: Archers gather at a football field in 1905. Photo by George Lazar Credit: Getty\n",
      "\n",
      "Predicting Input : 4 - a man can better see stars and the moon in daytime.\n",
      "Predicted Output: There are numerous categories to choose from to qualify for insurance coverage. Many insurance companies will offer\n",
      "\n",
      "Predicting Input : 5 - he was sent to a restaurant for treatment after a car crash.\n",
      "Predicted Output: He said family members, members of the Durant family and friends were attacked by an unknown assailant\n",
      "\n",
      "Predicting Input : 6 - his mother became angry after he got good grades in the math exam.\n",
      "Predicted Output: In short: Her starchy face was trending in favor of people over pictures of her best\n",
      "\n",
      "Predicting Input : 7 - cans are usually made of gold.\n",
      "Predicted Output: An astrologer, who has called mad or foolish men in rich lands\n",
      "\n",
      "Predicting Input : 8 - I put my desktop into my suitcase before departure.\n",
      "Predicted Output: The manager handed me a piece of paper explaining in detail how to rent out the room to\n",
      "\n",
      "Predicting Input : 9 - I walk to the Moon.\n",
      "Predicted Output: Thinks—career for Harshu: What were you meant to do?\n",
      "\n",
      "Predicting Input : 10 - I work 25 hours a day.\n",
      "Predicted Output: We're studying online, but we're studying on paper.\n",
      "\n",
      "Predicting Input : 11 - I'm hungry for water.\n",
      "Predicted Output: Incessantly cooking, don't let cooking get too heavy. Do not try to cook\n",
      "\n",
      "Predicting Input : 12 - this basket can hold one gallon of water.\n",
      "Predicted Output: But there are too many water apps to manage. Some save you water but get you wrong\n",
      "\n",
      "Predicting Input : 13 - this man can jump across a mountain easily.\n",
      "Predicted Output: This man cannot possibly be within the reach of the rocks and any branches or trees.\n",
      "\n",
      "Predicting Input : 14 - I put the rubbish into a mailbox.\n",
      "Predicted Output: MEMORY YOU'RE AN SPIDER? So what's your reward for being true\n",
      "\n",
      "Predicting Input : 15 - most children hate candies.\n",
      "Predicted Output: You know why this a bit hard to nail on #MakeUselessFood cbr so\n",
      "\n",
      "Predicting Input : 16 - most people become wiser after drinking a lot alcohol.\n",
      "Predicted Output: You can't skip an episode of Huckleberry Finn. If\n",
      "\n",
      "Predicting Input : 17 - a salad usually contains grass.\n",
      "Predicted Output: For less kale you have to use long slender legs for row to row or pot to pot\n",
      "\n",
      "Predicting Input : 18 - computers are everywhere in a forest.\n",
      "Predicted Output: There are countless trees, each with its own personal architecture. It will give you a chance\n",
      "\n",
      "Predicting Input : 19 - my family used to watch radio together after dinner.\n",
      "Predicted Output: In recent months a plumber's son suffered a stroke that left him paralyzed.\n",
      "\n",
      "Predicting Input : 20 - I became sleepy after a whole day starvation.\n",
      "Predicted Output: The hospital wardens, who were called upon to serve him the presents with food, began\n",
      "\n",
      "Predicting Input : 21 - he cured after taking poisons.\n",
      "Predicted Output: 2650 244 Phys. Dec. 9 Phys. Dec. 9 Normal Normal Road 821\n",
      "\n",
      "Predicting Input : 22 - I changed my car when passing a crossroads.\n",
      "Predicted Output: The therapist who helps me do something special at the road\n",
      "\n",
      "Predicting Input : 23 - I had a sweet dream when I was awake yesterday.\n",
      "Predicted Output: Mental reality mixed with a scene of action making you sad\n",
      "\n",
      "Predicting Input : 24 - my dad grows a tin of cola everyday.\n",
      "Predicted Output: He doesn't mind, but he does worry that some old days after morning he will turn\n",
      "\n",
      "Predicting Input : 25 - Alice became joyful after felt left out by her friends.\n",
      "Predicted Output: The topic is one she likes to talk about\n",
      "\n",
      "Predicting Input : 26 - Bob played hard to prepare for the examination.\n",
      "Predicted Output: Rambwig turned a fresh face to stare at Malek. There were still some tears\n",
      "\n",
      "Predicting Input : 27 - I was humiliated by him so I had better friendship with him.\n",
      "Predicted Output: The opportunity suddenly presented itself to me. Each time he sat at my desk and read me\n",
      "\n",
      "Predicting Input : 28 - I cooked my meal at the restaurant.\n",
      "Predicted Output: It was lovely and the staff was kind enough to stop by to serve us some food.\n",
      "\n",
      "Predicting Input : 29 - Bob loves to play tennis at the sports store.\n",
      "Predicted Output: He doesn't let a father like him compete in tennis.\n",
      "\n",
      "Predicting Input : 30 - I love eating seafood so I love eating chicken.\n",
      "Predicted Output: But there are still a few things I hate that don't make my pierogi more delicious\n",
      "\n",
      "Predicting Input : 31 - I am short so I have an advantage in basketball.\n",
      "Predicted Output: Q: Very nice! But it's snowing when you play basketball with other people.\n",
      "\n",
      "Predicting Input : 32 - mother sings a rock-and-roll to send her baby asleep.\n",
      "Predicted Output: For well over 10,000 years, Brahms wrote lines in varying shapes and sizes to\n",
      "\n",
      "Predicting Input : 33 - this apple is a beautiful light blue color.\n",
      "Predicted Output: You will notice that a small amount of citrus (green or brown in colour) is added\n",
      "\n",
      "Predicting Input : 34 - i call my mum with a drum.\n",
      "Predicted Output: Yukimoto In-roo is nervous. She seems to think she can't understand\n",
      "\n",
      "Predicting Input : 35 - i like drinking mercury after dinner.\n",
      "Predicted Output: You know something about a medical condition and wouldn't love to know how much you had to\n",
      "\n",
      "Predicting Input : 36 - i put a bar of soap on a birthday cake.\n",
      "Predicted Output: But when Malcolm understood, he could not marry the principal and his younger sister\n",
      "\n",
      "Predicting Input : 37 - plastic is the main ingredient in candy.\n",
      "Predicted Output: You will notice that a large amount of pulp is removed from the soap making up the mold\n",
      "\n",
      "Predicting Input : 38 - he put some soy sauce to make the robot taste better.\n",
      "Predicted Output: He left behind her, which was then recycled to travel with her after having already done her\n",
      "\n",
      "Predicting Input : 39 - he is drunk soon after drinking some water.\n",
      "Predicted Output: He got assaulted by a building manager and stabbed to death in the stomach as he did not\n",
      "\n",
      "Predicting Input : 40 - walking is an expensive hobby.\n",
      "Predicted Output: There are approximately 26,000 hours of programmed sound played by human beings each year. When\n",
      "\n",
      "Predicting Input : 41 - managers greet with each other and begin to bark.\n",
      "Predicted Output: We've talked about a lot of things recently, so this is going to be a short\n",
      "\n",
      "Predicting Input : 42 - teachers teach sports with a globe.\n",
      "Predicted Output: There are machines for the earth. The Soviets also taught sports in steel and steel don't\n",
      "\n",
      "Predicting Input : 43 - a funeral is a place for laughter.\n",
      "Predicted Output: He says family members, friends, and coworkers can collect personal items so they can find them\n",
      "\n",
      "Predicting Input : 44 - children have fun in the cemetery.\n",
      "Predicted Output: If you enter the school with a white supremacist in mind, you'll probably want to continue\n",
      "\n",
      "Predicting Input : 45 - Mary eats at a restaurant to do the dishes.\n",
      "Predicted Output: The giraffe is known for its distinctive dancing head.\n",
      "\n",
      "Predicting Input : 46 - wool is a form of eating material.\n",
      "Predicted Output: You will eat paper, paper with anything cooked. Not trees, fruits or vegetables.\n",
      "\n",
      "Predicting Input : 47 - mum is cooking a delicious meal in the bedroom.\n",
      "Predicted Output: You may wonder why. This is because Jasmine usually drinks up beer when she's outside\n",
      "\n",
      "Predicting Input : 48 - cars on the railway multiply during holidays.\n",
      "Predicted Output: There are 67 roads, 15 motorways and 7 airports on the highways and 25 areas on\n",
      "\n",
      "Predicting Input : 49 - beef cooked on the drill tastes delicious.\n",
      "Predicted Output: An expert strategist says: \"In one gym, 'gender may change without getting into trouble\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 50 - Jerry takes a tissue from the rolodex to keep in touch with Tom.\n",
      "Predicted Output: If there were just a single word in Japanese, Tom would have loved it.\n",
      "\n",
      "Predicting Input : 51 - pictures can be kept on films eternally.\n",
      "Predicted Output: 23. Trailer artists, put on skydivers and train for expeditions\n",
      "\n",
      "Predicting Input : 52 - we use a penknife to cut wood.\n",
      "Predicted Output: You will hear during a letter that you cannot read if you are carrying too much water.\n",
      "\n",
      "Predicting Input : 53 - human beings can travel to the moon by bus.\n",
      "Predicted Output: There are 120 moon-related species in existence. Some are known only from human books.\n",
      "\n",
      "Predicting Input : 54 - she is sad to play with her friends.\n",
      "Predicted Output: If you haven't had the chance to spend the weekend with her yet, what are you\n",
      "\n",
      "Predicting Input : 55 - smartphones nowadays are huge.\n",
      "Predicted Output: And when YouTube starts the increase in popular multiplayer games will come with huge bugs.\n",
      "\n",
      "Predicting Input : 56 - we took television sets to commemorate important days.\n",
      "Predicted Output: Strict supervision was a common approach in museums. During our visit we managed to keep our\n",
      "\n",
      "Predicting Input : 57 - i park my car in the living room.\n",
      "Predicted Output: It was sunny just a few days ago when I arrived at the lady at the High School\n",
      "\n",
      "Predicting Input : 58 - he turns on the light to listen to music.\n",
      "Predicted Output: If you haven't had the chance to listen to music through the SWITCH open world game\n",
      "\n",
      "Predicting Input : 59 - Lily opened the wall to let some fresh air in.\n",
      "Predicted Output: \"What are we doing, taking you from Santa?\" asked Lily. \"Please don't fall\n",
      "\n",
      "Predicting Input : 60 - a giant panda is a black and white bird found in Asia.\n",
      "Predicted Output: If you eat any panda in New Guinea, or even in Denmark, you will find\n",
      "\n",
      "Predicting Input : 61 - people put on summer dresses when winter comes.\n",
      "Predicted Output: If you wait until the middle of the winter to wear these custom dresses you could get yourself\n",
      "\n",
      "Predicting Input : 62 - smoking is good for health.\n",
      "Predicted Output: Forget smashing things! In my body right now I'm going to crash up and throw\n",
      "\n",
      "Predicting Input : 63 - a power socket has a copper housing for safety.\n",
      "Predicted Output: There are definitely trade-offs to such cosmetic needs in military space if they have to use\n",
      "\n",
      "Predicting Input : 64 - it takes him an hour to tear himself apart.\n",
      "Predicted Output: But there are things. He can't tear himself apart by himself. They make his brain\n",
      "\n",
      "Predicting Input : 65 - i rip a letter to tell her i am fine.\n",
      "Predicted Output: [40:52]Cheshire werewolves are dogs that go along with humans. They\n",
      "\n",
      "Predicting Input : 66 - a dining room is a place where people sleep.\n",
      "Predicted Output: You may wonder why. It's because guests can't reach the dining room if they're\n",
      "\n",
      "Predicting Input : 67 - he breaks the iron board with a bare hand.\n",
      "Predicted Output: The Home Depot gift-giving event is celebrated on Saturday and Sunday from 12:00 -\n",
      "\n",
      "Predicting Input : 68 - supermarkets rise prices for a year-end clearance.\n",
      "Predicted Output: But over 40 years, one in four shoppers are unable to pay them back.\n",
      "\n",
      "Predicting Input : 69 - the farm is destroyed by a tomato.\n",
      "Predicted Output: This event brings together a large number of firefighters to destroy an entire village's flag in support\n",
      "\n",
      "Predicting Input : 70 - he is so famous that nobody has heard of him.\n",
      "Predicted Output: He has difficulty sleeping, but he has breakfast. This way he gets sleep with his wife\n",
      "\n",
      "Predicting Input : 71 - i like her songs because she is a good cook.\n",
      "Predicted Output: You know best when a girl's food sucks. They don't eat food for or against\n",
      "\n",
      "Predicting Input : 72 - there are many trees in the pond.\n",
      "Predicted Output:  Tailors only lift water, but we go swimming in water,\" Javanese spoke.\n",
      "\n",
      "Predicting Input : 73 - the moon is a planet of the solar system.\n",
      "Predicted Output: The brightness of the moon is equivalent to the length of\n",
      "\n",
      "Predicting Input : 74 - we stopped at the power plant because we needed gasoline.\n",
      "Predicted Output: [62] See, e.g., United States v. Grimm (Del. 1986\n",
      "\n",
      "Predicting Input : 75 - Lily goes to the cinema to listen to a concert.\n",
      "Predicted Output: He gets interrupted by a box of movies before he begins his study and spots the music under\n",
      "\n",
      "Predicting Input : 76 - centigrade is a measure of acidity.\n",
      "Predicted Output: There are plenty of \"passive\" cucumbers that add their nutrition to your diet.\n",
      "\n",
      "Predicting Input : 77 - the ancients used a satellite as a guidance.\n",
      "Predicted Output: If you notice any of the stuff on top of your desk, give it a read.\n",
      "\n",
      "Predicting Input : 78 - the fire was caused by a glass of water.\n",
      "Predicted Output: The French firefighters wanted a further four hours before they needed to let off fire engines.\n",
      "\n",
      "Predicting Input : 79 - December is the 13th month of a year.\n",
      "Predicted Output: Why not remove ice from the sky\n",
      "\n",
      "Predicting Input : 80 - a prince is a piece used in the game of chess.\n",
      "Predicted Output: He has normally cut a talismanic platinum game piece from a chess piece to make it\n",
      "\n",
      "Predicting Input : 81 - maid is an important member of a family.\n",
      "Predicted Output: You may wonder why. It's because pregnant women must tell their husbands when they are pregnant\n",
      "\n",
      "Predicting Input : 82 - he opens the door with a lock.\n",
      "Predicted Output: Sara bows her head to the outside wall. The glass had cracked when she saw it\n",
      "\n",
      "Predicting Input : 83 - a sea is a large body of sweet water.\n",
      "Predicted Output: But when sperm break the glass, it separates from it and no sperm will ever see it\n",
      "\n",
      "Predicting Input : 84 - a rooster can lay eggs.\n",
      "Predicted Output: He should raise one-half of the chicks in $17.75 balls to $24\n",
      "\n",
      "Predicting Input : 85 - farmers are growing mushrooms under the sun.\n",
      "Predicted Output: He says mushrooms would be too bad to harvest in Mexico as it would harm our food supply\n",
      "\n",
      "Predicting Input : 86 - Tom buys his girlfriend a bunch of carnations on Valentine's Day.\n",
      "Predicted Output: What one dude makes? A woman's perfume. (Who would buy perfume with this stuff\n",
      "\n",
      "Predicting Input : 87 - soup is best eaten with a fork.\n",
      "Predicted Output: There are plenty of beets in all varieties\n",
      "\n",
      "Predicting Input : 88 - there are many fishes in the desert.\n",
      "Predicted Output:  Sock diggers prefer larger, larger fish to chew on when trying to survive.\" I know this\n",
      "\n",
      "Predicting Input : 89 - i fly to Paris to see the Statue of Liberty.\n",
      "Predicted Output: On March 26, a woman was killed while standing by the Union soldier during a battle with\n",
      "\n",
      "Predicting Input : 90 - nissan is a foreign plane manufacturer.\n",
      "Predicted Output: The 21 years in pictures jove into 2011? I'm\n",
      "\n",
      "Predicting Input : 91 - food is stored in the pan.\n",
      "Predicted Output: There are plenty of on-the-grid options (link to houses).\n",
      "\n",
      "Predicting Input : 92 - the image of the cat seems to be reflected many times in the glass.\n",
      "Predicted Output: This isn't just a safety issue. Engineers are concerned about it because light isn't released\n",
      "\n",
      "Predicting Input : 93 - she goes traveling on weekdays.\n",
      "Predicted Output: But when Ali arrived, he was told there was only one place he could go. He\n",
      "\n",
      "Predicting Input : 94 - laughter is a sign of sorrow.\n",
      "Predicted Output: If you lose something, you get no consolation. If you find yourself upset, you don\n",
      "\n",
      "Predicting Input : 95 - Mary sits at the closet and begins reading.\n",
      "Predicted Output: The picture shifts from a cool look to darker. Her hair is pale and welled with\n",
      "\n",
      "Predicting Input : 96 - Jim practices running on the bed every morning.\n",
      "Predicted Output: If you haven't had to look at clothes in 5 minutes, here are some ways that\n",
      "\n",
      "Predicting Input : 97 - he climbs up the mountain with the help of a ribbon.\n",
      "Predicted Output: It's crazy so, not that it matters. She won't carry along with us.\n",
      "\n",
      "Predicting Input : 98 - he opens a storybook and begins to cook.\n",
      "Predicted Output: He sets down his pajamas to wrap his legs up. He rolls his eyes and\n",
      "\n",
      "Predicting Input : 99 - i put some chili powder to create a sweet flavor.\n",
      "Predicted Output: He doesn't order a ton of Thai basil. We'll add things like orange or coconut\n",
      "\n",
      "Predicting Input : 100 - Bob looks up a word in a shopping list.\n",
      "Predicted Output: By Tom Davidson\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 101 - she begins working for relaxation.\n",
      "Predicted Output: The train jumps over a wire with the conductor on her side. Or leave it on the\n",
      "\n",
      "Predicting Input : 102 - people usually work on Sundays.\n",
      "Predicted Output: There are numerous websites, often where you will find detailed travel news from places like Japan.\n",
      "\n",
      "Predicting Input : 103 - Santa Claus pretends to be a parent to give children presents.\n",
      "Predicted Output: When says chocolate day, you can also taste it by taste. You may need to enjoy\n",
      "\n",
      "Predicting Input : 104 - he sells something to eat to the convenient store.\n",
      "Predicted Output: You may wonder why. It's only convenient to eat something in order to watch the movie\n",
      "\n",
      "Predicting Input : 105 - he killed the driver to thank him for a free ride.\n",
      "Predicted Output: If you wish to be present for the turkey's funeral in your hotel room, you may\n",
      "\n",
      "Predicting Input : 106 - she feels ill and goes to see a teacher.\n",
      "Predicted Output: But when Ana tries to tell her not to get angry at her teacher with her friend and\n",
      "\n",
      "Predicting Input : 107 - i buy a CD to listen to the latest advertisements.\n",
      "Predicted Output: And there are others, like ex-girlfriends who always say 'Ahh, ad\n",
      "\n",
      "Predicting Input : 108 - it takes me 3 seconds to read the book.\n",
      "Predicted Output: But if your intelligence and writing ability are lacking, then you will fail at reading the book\n",
      "\n",
      "Predicting Input : 109 - he is allergic to cats so he keeps a cat as a pet.\n",
      "Predicted Output: He first touches our T and we don't know how it went so we don't know\n",
      "\n",
      "Predicting Input : 110 - he runs faster than no one and wins the champion.\n",
      "Predicted Output: And when Craig asks, \"What was she got into?\" he asks about her having don\n",
      "\n",
      "Predicting Input : 111 - i wear a belfry to be informed of the time.\n",
      "Predicted Output: If you wear any of the following during pregnancy, your doctor will recommend certain methods to prevent\n",
      "\n",
      "Predicting Input : 112 - a plane beats its wings and flies off.\n",
      "Predicted Output: You know something about a plane? He flew to Pennsylvania and would sometimes fly in the sky\n",
      "\n",
      "Predicting Input : 113 - Jim invites Amy to dance in a pool.\n",
      "Predicted Output: Rebecca observes their BFF and tells Amy that she'd like to dance on the beach\n",
      "\n",
      "Predicting Input : 114 - people lived in houses made of wool in the past.\n",
      "Predicted Output: This record dates back to the 15th century. Earliest known dates can be found in\n",
      "\n",
      "Predicting Input : 115 - it is rainy so they go camping.\n",
      "Predicted Output: You will notice you are home for the holidays. Don't put money into your TV or\n",
      "\n",
      "Predicting Input : 116 - the regular meeting is held at 2 o'clock in the morning.\n",
      "Predicted Output: 18 years ago we are always here to celebrate the anniversary of the Shoah. I'm\n",
      "\n",
      "Predicting Input : 117 - heat the drink with some ice, please.\n",
      "Predicted Output: For much cheaper served, up to 7 servings of drinks can be purchased by making a glass\n",
      "\n",
      "Predicting Input : 118 - i eat apples because of my illness.\n",
      "Predicted Output: But my potions don't help a lot because I'm low on fluids and may not want\n",
      "\n",
      "Predicting Input : 119 - he catches a subway train in a bus stop.\n",
      "Predicted Output: As part of his \"Off the Record\", Vandy dropped in some balls and got into\n",
      "\n",
      "Predicting Input : 120 - Mary asks questions when taking an exam to get a high score.\n",
      "Predicted Output: He makes promises that the students will like him. But if he keeps saying what he needs\n",
      "\n",
      "Predicting Input : 121 - human beings shake tails to express their happiness.\n",
      "Predicted Output: You may prefer your BIFY to hurt you or perform your duties if you have no\n",
      "\n",
      "Predicting Input : 122 - he had his hair cut at the butcher's.\n",
      "Predicted Output: Zombies had been hunting thousands of non-\n",
      "\n",
      "Predicting Input : 123 - i drink some glass.\n",
      "Predicted Output: He sent Mary out, but not by dressing in black\n",
      "\n",
      "Predicting Input : 124 - Tim and Mary got married on February 31st.\n",
      "Predicted Output: When Dr Holliday and Mary first met Mary, she had been dating Paul Humpher\n",
      "\n",
      "Predicting Input : 125 - she writes diaries in a magazine.\n",
      "Predicted Output: It's unlikely his \"friend\" will admit to writing about his affair with Brzezinski\n",
      "\n",
      "Predicting Input : 126 - the girl next to Mary in the photo is her nephew.\n",
      "Predicted Output: We made contact with a local account in Malta. She got in touch with us and told\n",
      "\n",
      "Predicting Input : 127 - i spend a dollar to buy a diamond ring.\n",
      "Predicted Output: You know better than to try to control negative thoughts or feelings. Read many books and journal\n",
      "\n",
      "Predicting Input : 128 - Cinderella lives a happy life before she meets the prince.\n",
      "Predicted Output: 22 Dandelion-The Black Rose Rabbit is once again in charge of that land.\n",
      "\n",
      "Predicting Input : 129 - children take music classes in the gym.\n",
      "Predicted Output: There are estimates that a national price for opera is probably around $500 for it.\n",
      "\n",
      "Predicting Input : 130 - Obama has served as President of the USA for twelve years.\n",
      "Predicted Output: And like Ronald Reagan, he was also briefly on New Day. That day was for President\n",
      "\n",
      "Predicting Input : 131 - children like flying planes in the spring.\n",
      "Predicted Output: This company aims to be an environment that attracts people from other areas and languages.\n",
      "\n",
      "Predicting Input : 132 - people must have a driver's license to travel abroad.\n",
      "Predicted Output: But what happens when a couple are born overseas? That's not easy to figure out.\n",
      "\n",
      "Predicting Input : 133 - it is snowy and very hot outside.\n",
      "Predicted Output: There are layers of cactus in the mushroom man cave in the canteen.\n",
      "\n",
      "Predicting Input : 134 - the car stops at the green light.\n",
      "Predicted Output: In 10 apps it is possible to change directions to cause specific parts of the car to stop\n",
      "\n",
      "Predicting Input : 135 - people take off clothes when they feel cold.\n",
      "Predicted Output: There are thin spaces, which are hard for us to reach. Our muscles work and support\n",
      "\n",
      "Predicting Input : 136 - Trump was Emperor of the USA in 2018.\n",
      "Predicted Output: We just voted for Donald Trump. That was all.\n",
      "\n",
      "Predicting Input : 137 - he puts the radio next to his mouth to hear more clearly.\n",
      "Predicted Output: Oban shrugs, doesn't take note of what you're saying. \"It's\n",
      "\n",
      "Predicting Input : 138 - he is honored for vandalizing cultural heritages.\n",
      "Predicted Output: He did so after a South African staff member made himself known to someone close to his father\n",
      "\n",
      "Predicting Input : 139 - i buy a toy for my small brother for a shell.\n",
      "Predicted Output: You may wonder why. It's because adults have nothing to do with toys. It's\n",
      "\n",
      "Predicting Input : 140 - they have to live in a mansion because of poverty.\n",
      "Predicted Output: We've always been lucky to have more friends\n",
      "\n",
      "Predicting Input : 141 - it is a pity that she only comes the first in the competition.\n",
      "Predicted Output: Nobody who knows how to write should don't read\n",
      "\n",
      "Predicting Input : 142 - they get out of an air-raid shelter to avoid bombs.\n",
      "Predicted Output: If you throw something, you can't flee. There's no escape into the air.\n",
      "\n",
      "Predicting Input : 143 - the Taj Mahal is made of white maples.\n",
      "Predicted Output: Even a standard sense of humour is meant to be authentic in most cases.\n",
      "\n",
      "Predicting Input : 144 - they speak in c++ in order to communicate with each other.\n",
      "Predicted Output: For example with classes, you can use dispatch to communicate with all classes (with one exception\n",
      "\n",
      "Predicting Input : 145 - Stephen William Hawking was a great actor.\n",
      "Predicted Output: And what happened is, it's all fiction. That's all right. It's time\n",
      "\n",
      "Predicting Input : 146 - it is really painful for a father to give birth to his baby.\n",
      "Predicted Output: But there are few of us who can endure to watch our future collapse into absolute darkness.\n",
      "\n",
      "Predicting Input : 147 - they sleep in a hotel while camping in the forest.\n",
      "Predicted Output: He says both families, who have no pets, try to keep away from each other in\n",
      "\n",
      "Predicting Input : 148 - i water my mp3 player two times a week.\n",
      "Predicted Output: my channel credits when a player has two clicks\n",
      "\n",
      "Predicting Input : 149 - Mongolia is mass surrounded by water.\n",
      "Predicted Output: If you eat grass, you can look deep for flowers; your stomach cannot respond to running\n",
      "\n",
      "Predicting Input : 150 - she wears clothes because of nearsightedness.\n",
      "Predicted Output: He used normal clothing, so he also wears three glasses\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 151 - Tim buys a pair of socks to prevent his bike from being stolen.\n",
      "Predicted Output: He runs straight into a wall to try and open it in his toilet to close the door\n",
      "\n",
      "Predicting Input : 152 - the cook treats his patients carefully.\n",
      "Predicted Output: It's reasonable to not care for which specialty is worse for you if you're not willing\n",
      "\n",
      "Predicting Input : 153 - the Titanic struck a life buoy and sank.\n",
      "Predicted Output: The wooden boat he uses was likely to have\n",
      "\n",
      "Predicting Input : 154 - he paints the ceiling with cola.\n",
      "Predicted Output: But when Shaw tells a live audience that audiences are unable to read Wall Street Journal because they\n",
      "\n",
      "Predicting Input : 155 - she writes to her pen pal because they meet each other every day.\n",
      "Predicted Output: \"One day I received an email from my\n",
      "\n",
      "Predicting Input : 156 - a lemon tastes sour when it goes bad.\n",
      "Predicted Output: You may drink fruit, water, or cocoa in homemade cookies. While none of these require\n",
      "\n",
      "Predicting Input : 157 - pumpkins can be made into carriages for people to ride on.\n",
      "Predicted Output: There are houses with a double door in front of them and in front of an open window\n",
      "\n",
      "Predicting Input : 158 - she is cut by a carpet accidentally.\n",
      "Predicted Output: There was never any \"foam propelling\" effect on the literal translation of the French\n",
      "\n",
      "Predicting Input : 159 - i use pencils to cut paper cuts.\n",
      "Predicted Output: 30½-inch cheese is definitely larger than on big\n",
      "\n",
      "Predicting Input : 160 - people burnt ice for warmth th the past.\n",
      "Predicted Output: 14 0???. we all look terrible\n",
      "\n",
      "Predicting Input : 161 - Tony gives his girlfriend chocolates to break up.\n",
      "Predicted Output: You spend more time getting things done (like food or\n",
      "\n",
      "Predicting Input : 162 - she brings a scarf because of the rain.\n",
      "Predicted Output: The turban is allowed for women.\n",
      "\n",
      "Predicting Input : 163 - i will pick you up on the CD disk.\n",
      "Predicted Output: There are generally three \"rows\" of CDs to choose from. Some artists will only record\n",
      "\n",
      "Predicting Input : 164 - we had lunch together on March 33rd.\n",
      "Predicted Output: We later ate dinner. The main goal was to expand our relationship with Yuya. We\n",
      "\n",
      "Predicting Input : 165 - he practices running on noodles every morning.\n",
      "Predicted Output: But some trainers still are critical of his skipping a workout on the grind with other athletes.\n",
      "\n",
      "Predicting Input : 166 - the dog sits there without even a mew.\n",
      "Predicted Output: Transparent work has\n",
      "\n",
      "Predicting Input : 167 - he walks in from the wall.\n",
      "Predicted Output: Yoshimura suddenly gazes at me from the opposite wall. It seems he's aware\n",
      "\n",
      "Predicting Input : 168 - she rides a chair to work.\n",
      "Predicted Output: It's incredibly busy, but it's funny. So much so that people call it one\n",
      "\n",
      "Predicting Input : 169 - he writes his homework on the dew.\n",
      "Predicted Output: He wrote several letters, some of which ended with both sort words and sharp puns.\n",
      "\n",
      "Predicting Input : 170 - a rabbit likes eating meat.\n",
      "Predicted Output: This type of rabbit, which is found primarily in Oregon and near Alaska, can't eat\n",
      "\n",
      "Predicting Input : 171 - the lake will be frozen in summer.\n",
      "Predicted Output: This means winter does not mean for your dinosaurs to survive in the polar regions.\n",
      "\n",
      "Predicting Input : 172 - she washes her computer with warm water.\n",
      "Predicted Output: The green tint has a nice red band across the surface of the keyboard and has a ton\n",
      "\n",
      "Predicting Input : 173 - i cut a piece of cloth with chopsticks.\n",
      "Predicted Output: 16. Grip one to one of the palms to raise one's gaze at the subject.\n",
      "\n",
      "Predicting Input : 174 - Mary adds some detergent to her bread.\n",
      "Predicted Output: This isn't how a piece of bread tastes. There's no fatty substance in it.\n",
      "\n",
      "Predicting Input : 175 - i am preparing for the trip yesterday.\n",
      "Predicted Output: France is supposed to be Germany's culture heartland\n",
      "\n",
      "Predicting Input : 176 - i begin walking as soon as the light turns purple.\n",
      "Predicted Output: We turn away from the path to where Takata goes in the mirror and turn in order\n",
      "\n",
      "Predicting Input : 177 - i drink coke with a chopstick.\n",
      "Predicted Output: He'd hoped he was too lost to reply. He'd even hoped he'd have no\n",
      "\n",
      "Predicting Input : 178 - the knitting wool enjoys playing cats.\n",
      "Predicted Output: But some fencing wouldn't go as well if it comes from the garbage bin.\n",
      "\n",
      "Predicting Input : 179 - she takes attractive selfies with her headphone.\n",
      "Predicted Output: You also notice K-pop will want less airtime on their Instagram posts.\n",
      "\n",
      "Predicting Input : 180 - i buy a boyfriend for my sister.\n",
      "Predicted Output: He went away with a girl to'molest her', not pull her off a plane\n",
      "\n",
      "Predicting Input : 181 - His father got excited because he failed the math exam.\n",
      "Predicted Output: I told him he is 5'6″2″ and he agreed to take the math\n",
      "\n",
      "Predicting Input : 182 - I go to the museum to get my clothes cleaned.\n",
      "Predicted Output: WIRINGMAN'S THING THEY DO NOT KNOW\n",
      "\n",
      "Predicting Input : 183 - Bob bought chocolate for his enemy on Valentine's Day.\n",
      "Predicted Output: An employee suffered second-degree burns in 2009\n",
      "\n",
      "Predicting Input : 184 - French fries are made of oranges.\n",
      "Predicted Output: There are cheese soups and miso burgers.\n",
      "\n",
      "Predicting Input : 185 - Mars is the most suitable homeland for human beings.\n",
      "Predicted Output: As your astronauts travel, you will need resources to maintain your spaceflight capacity. The elements\n",
      "\n",
      "Predicting Input : 186 - Harry stoops down to throw the basketball into the basket.\n",
      "Predicted Output: Wanda strides back, quickly coming out from under her basket. She catches on to his\n",
      "\n",
      "Predicting Input : 187 - I'm thirsty so I drink seawater.\n",
      "Predicted Output: It's ok for a simple 50s stereotype. So many people today talk about what they\n",
      "\n",
      "Predicting Input : 188 - Alice is running on the sea.\n",
      "Predicted Output: He told Jillie, \"I don't know how you would figure out if it was\n",
      "\n",
      "Predicting Input : 189 - Bob looks delightful because he hurts his fingers.\n",
      "Predicted Output: Two hours from turn\n",
      "\n",
      "Predicting Input : 190 - Maria coughs when she breathes in fresh air.\n",
      "Predicted Output: You should prepare air-conditioning and ventilation of your house. Air conditioning is not necessary\n",
      "\n",
      "Predicting Input : 191 - African lions enjoy eating grass.\n",
      "Predicted Output: 10. Harlem Lions' Landscapes Were Not Fine\n",
      "\n",
      "Predicting Input : 192 - I graduated from college at the age of 3.\n",
      "Predicted Output: He left university at a young age to pursue a master's degree in psychology.\n",
      "\n",
      "Predicting Input : 193 - Bears stay underwater during hibernation.\n",
      "Predicted Output: As well as returning a blue heart to bear in storm conditions, bears love to release items\n",
      "\n",
      "Predicting Input : 194 - Alice go to the bathroom for repletion.\n",
      "Predicted Output: This day brings war, so you may wish to burn off it first.\n",
      "\n",
      "Predicting Input : 195 - I went underwater and took a deep breath.\n",
      "Predicted Output: I didn't realize what the writing was about until I fell asleep. There were only two feet\n",
      "\n",
      "Predicting Input : 196 - Dogs can feed blood to their young ones.\n",
      "Predicted Output: 22. Eggs don't grow in long worms. The roots of worms grow at the bottom\n",
      "\n",
      "Predicting Input : 197 - Frank goes to the bank to save people.\n",
      "Predicted Output: He uses kitchen knives, when he doesn't have money to pay bills\n",
      "\n",
      "Predicting Input : 198 - Jack can read books before learning basic words.\n",
      "Predicted Output: For example: if a school will place restrictions on books that are primarily designed for children.\n",
      "\n",
      "Predicting Input : 199 - Kids fly kites in a windless day.\n",
      "Predicted Output: When they discover they are too young to ride the mountains.\n",
      "\n",
      "Predicting Input : 200 - People get together to fight on Christmas Eve.\n",
      "Predicted Output: But some Easter greetings are more common because of Christmas maladies than traditional church services.\n",
      "\n",
      "Predicting Input : 201 - Jason put a tea bag into his cup to make a cup of coffee.\n",
      "Predicted Output: Ummm…Well it's easy for me to cut\n",
      "\n",
      "Predicting Input : 202 - Harry feels grieved seeing his old friends.\n",
      "Predicted Output: ~ —Forgive their hunch. 『The birds were here before.\"】\n",
      "\n",
      "Predicting Input : 203 - Gloria determines to lose weight so she eats more.\n",
      "Predicted Output: There are plenty of not-so-intuitive states that show up when talking about men in\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 204 - Saying \"Hello\", he walked out of the classroom and went home.\n",
      "Predicted Output: In my classroom he is still there. Because I didn't want him to see his brother\n",
      "\n",
      "Predicting Input : 205 - Danis put his smartphone into the washing machine.\n",
      "Predicted Output: The owner threw his own boot on the sidewalk. The shop manager stepped into the store and\n",
      "\n",
      "Predicting Input : 206 - Billy rides an ant to cross the bridge.\n",
      "Predicted Output: If you ride on a bike, you wouldn't hear your hands stuck down on the bridge\n",
      "\n",
      "Predicting Input : 207 - Bell takes a raft rushing up the waterfall.\n",
      "Predicted Output: We've talked about a lot of things above. First of all, X's main plans\n",
      "\n",
      "Predicting Input : 208 - George put a broom in his mouth to brush teeth.\n",
      "Predicted Output: eishiba felt a deep pain in her face and looked up at Miura.\n",
      "\n",
      "Predicting Input : 209 - I closed my mouth against the bright light.\n",
      "Predicted Output: \"You're bleeding like a baby. You cannot stand.\"\n",
      "\n",
      "Predicting Input : 210 - My heart beats slower when I quicken my pace.\n",
      "Predicted Output: My \"Drinkers dont know how to keep themselves calm and open.\"\n",
      "\n",
      "Predicting Input : 211 - Bob plays basketball with his cute dog.\n",
      "Predicted Output: Phuto Says His Bunch of Football Pants Is Made Of Snacking Gum\n",
      "\n",
      "Predicting Input : 212 - Fitz uses a pen to open the lock.\n",
      "Predicted Output: This means two pages, one on the backs of his chest. If you've never seen\n",
      "\n",
      "Predicting Input : 213 - Frank had s stomachache so he drank more cold water.\n",
      "Predicted Output: Xia went outside and the door was opened so he found Yuigahama doing his cooking\n",
      "\n",
      "Predicting Input : 214 - Eric told a lie and his mother praised him.\n",
      "Predicted Output: In case anyone needed a reminder, this reporter can watch his video below:<|endoftext|>10 Big\n",
      "\n",
      "Predicting Input : 215 - Henry flew to the USA sitting in a car.\n",
      "Predicted Output: He got upset but the mother is very cooperative. She seems to love him and his education\n",
      "\n",
      "Predicting Input : 216 - Carter opened the window of the airplane to get some fresh air.\n",
      "Predicted Output: The plane blew up, but he used only to fly so much as 15 minutes. The\n",
      "\n",
      "Predicting Input : 217 - On my birthday, we lighted up birthday cakes for celebration.\n",
      "Predicted Output: This way around was a little different. Each morning we didn't eat cake and one might\n",
      "\n",
      "Predicting Input : 218 - the solar system is located in Earth.\n",
      "Predicted Output: As not totally alien, this would not qualify as alien.\"\n",
      "\n",
      "Predicting Input : 219 - When you smile at others, they will cry to you.\n",
      "Predicted Output: You need words to catch fire.\n",
      "\n",
      "Predicting Input : 220 - His favorite color is purple, the same color as the sky.\n",
      "Predicted Output: Chrome rocks are a common color in caves. They can be quite harsh and could cause\n",
      "\n",
      "Predicting Input : 221 - Jenny ordered a hot dog in a pet store.\n",
      "Predicted Output: In early 2000, a woman at a dentist's office started getting bronchitis. She\n",
      "\n",
      "Predicting Input : 222 - I dried my hair before using shampoo.\n",
      "Predicted Output: a japanese landscape, no way to clarify a description of the landscape.. they are looking\n",
      "\n",
      "Predicting Input : 223 - The man in black cured him with a bullet.\n",
      "Predicted Output: The killers were sitting on the sidewalk near the building when\n",
      "\n",
      "Predicting Input : 224 - She boiled the egg and the egg turned into liquid.\n",
      "Predicted Output: Temptation Springs took a short time to hatch. There were no shaking plants in the spring\n",
      "\n",
      "Predicting Input : 225 - Elisa was terrified bitten by a book.\n",
      "Predicted Output: In those moments when a local media crew filmed the encounter, they managed to contain the horror\n",
      "\n",
      "Predicting Input : 226 - In the dark, I turn off the light to read a book.\n",
      "Predicted Output: s On one foot (2 to 3 feet tall) my hair is crossed up in the bed\n",
      "\n",
      "Predicting Input : 227 - Atlantic is the biggest continent in the world.\n",
      "Predicted Output: There are 101 registered mowers in the Scottish region of Scotland. On average, they represent\n",
      "\n",
      "Predicting Input : 228 - The cat is sitting on the blanket, because she is warm.\n",
      "Predicted Output: The cat sits on a warm blanket. Coldly on the bed in front of you.\n",
      "\n",
      "Predicting Input : 229 - Riko drops a steel cup from the desk and the cup breaks into pieces.\n",
      "Predicted Output: Riko climbs over a chair to open her hair and covers it with her hair. R\n",
      "\n",
      "Predicting Input : 230 - Human will get eventually out of power if they are unplugged.\n",
      "Predicted Output: An angry cellphone user, who had been surfing the internet for more than 25 years, recently\n",
      "\n",
      "Predicting Input : 231 - She decided to eat more healthy foods like hamburgers.\n",
      "Predicted Output: In different diets and the same food form might not cause major health problems in each other.\n",
      "\n",
      "Predicting Input : 232 - Max stays close to the campfire to keep cool.\n",
      "Predicted Output: An excellent pillow case, which is used only in windows\n",
      "\n",
      "Predicting Input : 233 - Chloe got concentrated when she was bored.\n",
      "Predicted Output: Mili took Kimakre's hand in hers. \"They look smart like you with your\n",
      "\n",
      "Predicting Input : 234 - He got a cold and laughed from time to time.\n",
      "Predicted Output: Pitchfork replied in a statement:\n",
      "\n",
      "Predicting Input : 235 - Mark ate a big bitter cherry pie.\n",
      "Predicted Output: The bartender had appeared at the corner with the bar employees\n",
      "\n",
      "Predicting Input : 236 - Gloria wears a cat on her head.\n",
      "Predicted Output: – Cat fetishism, how it has progressed\n",
      "\n",
      "Predicting Input : 237 - Harry went to the barbershop to have his glasses repaired.\n",
      "Predicted Output: Rights group blows up, pressurizing whiskey to dissapear through nose\n",
      "\n",
      "Predicting Input : 238 - Reilly is sleeping on the window.\n",
      "Predicted Output: Hawaiian surfers dream about meeting his Santa\n",
      "\n",
      "Predicting Input : 239 - I have a desk on my lamp.\n",
      "Predicted Output: In my lamp as a chair, I sleep in my chair. I stay by it and\n",
      "\n",
      "Predicting Input : 240 - Eric stalled for a while so we ended up early.\n",
      "Predicted Output: This still represents one the best cost four triggers in Magic: The Gathering: even if you\n",
      "\n",
      "Predicting Input : 241 - Mark called his friends with his pencil.\n",
      "Predicted Output: \"So shall we. You will do exactly as we say. Ok?\"\n",
      "\n",
      "Predicting Input : 242 - My friend is cooking a meal on a stone.\n",
      "Predicted Output: We've never watched one of our cats make a dance\n",
      "\n",
      "Predicting Input : 243 - Riko made a cup of banana milkshake with apples and milk.\n",
      "Predicted Output: We'd missed work, but we still hung out at my house for hours.\n",
      "\n",
      "Predicting Input : 244 - My mother was preparing dinner when I woke up in the morning.\n",
      "Predicted Output: \"When she picked me up, I rushed to get her in front of me. And\n",
      "\n",
      "Predicting Input : 245 - Mike drank too much wine so he feels sober now.\n",
      "Predicted Output: He thought too much, so he makes jokes. He runs his fingers down his hair to\n",
      "\n",
      "Predicting Input : 246 - My mother went to an artist to check her teeth.\n",
      "Predicted Output: Paintbrush artist, former wife of Shay's neighbor who says she beat her not her\n",
      "\n",
      "Predicting Input : 247 - Mike cried after hearing an interesting joke.\n",
      "Predicted Output: We've talked about a lot of things related to happiness and things like eating well. So\n",
      "\n",
      "Predicting Input : 248 - She sharpened the pencil with an eraser.\n",
      "Predicted Output: \"We will finish and write.\"\n",
      "\n",
      "Predicting Input : 249 - Being involved in a car accident can ease your life.\n",
      "Predicted Output: If you haven't stopped doing something important then you will never\n",
      "\n",
      "Predicting Input : 250 - It is dark outside because it is ten am.\n",
      "Predicted Output: Tales of astronomy have a general sense of sunset. Here are some typical tales of people who\n",
      "\n",
      "Predicting Input : 251 - Harry was relaxed because of the nightmare.\n",
      "Predicted Output: Ablaster glanced over. \"We need him to assist us in stabilizing the situation.\"\n",
      "\n",
      "Predicting Input : 252 - Astrid kissed him out of pure hatred.\n",
      "Predicted Output: Qin Chen frowned with a surprise.\n",
      "\n",
      "Predicting Input : 253 - His son took the school bus to go to work.\n",
      "Predicted Output: In North Cork with a Red Cross card holder, artist Murray knew no guard on the side\n",
      "\n",
      "Predicting Input : 254 - Throwing stones at cats is treating them nicely.\n",
      "Predicted Output: You probably know our Cat People already know who we are\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 255 - Society demands reward for criminals.\n",
      "Predicted Output: – Decriminalization, most often only removes \"crime from the background,\" so it would\n",
      "\n",
      "Predicting Input : 256 - He saw a car floating at sea.\n",
      "Predicted Output: \"He jumped up, like a kid jumping from the sky and landed at his feet,\"\n",
      "\n",
      "Predicting Input : 257 - People can buy potatoes at a fruit shop.\n",
      "Predicted Output: And when potatoes are in so many places then you don't get McDonald's for free.\n",
      "\n",
      "Predicting Input : 258 - I should take off my underpants when swimming.\n",
      "Predicted Output: Rötzen : No ego or mental ability\n",
      "\n",
      "Predicting Input : 259 - My son needs bra after growing up.\n",
      "Predicted Output: But why not give him a lift up to the sky\n",
      "\n",
      "Predicting Input : 260 - I choked on water.\n",
      "Predicted Output: WHAT IS THIS MEANERS OVER HERE?\n",
      "\n",
      "Predicting Input : 261 - If you feel cool you may sweat.\n",
      "Predicted Output: There are plenty of \"power women\" around. Some see them as athletic and even famous\n",
      "\n",
      "Predicting Input : 262 - I go to school for playing the computer game.\n",
      "Predicted Output: Swell walking my go to school on Sundays. A girl will walk when she is absolutely\n",
      "\n",
      "Predicting Input : 263 - I slipped on a rock.\n",
      "Predicted Output: Fully clothed, sat on the beach at the house of\n",
      "\n",
      "Predicting Input : 264 - I study eight days a week.\n",
      "Predicted Output: 10. Freeze hair, don't pick pots.\n",
      "\n",
      "Predicting Input : 265 - I see a mute chatting with my son.\n",
      "Predicted Output: 'He does need more. He was always accustomed to watching him with my brother and we've\n",
      "\n",
      "Predicting Input : 266 - a deaf person took part in a concert.\n",
      "Predicted Output: We found absolutely nothing, so we found anyone with hearing loss. So no one was hurt\n",
      "\n",
      "Predicting Input : 267 - I see a cripple running.\n",
      "Predicted Output: I see the starving guy, or the poor crippled man\n",
      "\n",
      "Predicting Input : 268 - I saw an egg walk on the road.\n",
      "Predicted Output: I took it home and the woman said I shouldn't remember this. It didn't take her\n",
      "\n",
      "Predicting Input : 269 - a dog is probable to beat a tiger.\n",
      "Predicted Output: 13. Somehow DeBord was already suicidal.\n",
      "\n",
      "Predicting Input : 270 - I go to school with a desktop computer.\n",
      "Predicted Output: read more<|endoftext|>Tempe's probably not holding on to its former glory — not with its\n",
      "\n",
      "Predicting Input : 271 - now I plan to sleep so I take a coffee.\n",
      "Predicted Output: 30 9 Aug 2012, 12:29 pm I came back to pet older brother to check\n",
      "\n",
      "Predicting Input : 272 - I travel to the united states by rocket.\n",
      "Predicted Output: 22=Steel Rod, Will not let hole in steel allow me to survive\n",
      "\n",
      "Predicting Input : 273 - we should borrow things after returning them.\n",
      "Predicted Output: If you decide to not return the key instead, when you should check if it is needed\n",
      "\n",
      "Predicting Input : 274 - I want to have light food such as steaks.\n",
      "Predicted Output: If you eat milk, don't throw milk in your mouth. If you drink milk in\n",
      "\n",
      "Predicting Input : 275 - I like swimming in the hole.\n",
      "Predicted Output: If you eat half a cup of fish during a meal the likely survival rate will be lower\n",
      "\n",
      "Predicting Input : 276 - I want world war.\n",
      "Predicted Output: The physical dimension of space is so distant from it\n",
      "\n",
      "Predicting Input : 277 - a snowman will exist forever.\n",
      "Predicted Output: If you're not human\n",
      "\n",
      "Predicting Input : 278 - I like his toy so I can grab it.\n",
      "Predicted Output: [monks slam tree, rock]\n",
      "\n",
      "Predicting Input : 279 - patients are cured by nurses.\n",
      "Predicted Output: The common misconception is that patients at risk for diabetes are patients\n",
      "\n",
      "Predicting Input : 280 - flowers can help you do homework.\n",
      "Predicted Output: That they aren't a source of sleep hormone is probably because they grow late to our birth\n",
      "\n",
      "Predicting Input : 281 - we should eat more sugar to protect teeth.\n",
      "Predicted Output: There are basically three different types of sugar products\n",
      "\n",
      "Predicting Input : 282 - we should take the ice when we catch a cold.\n",
      "Predicted Output: If you haven't had the chance to observe the phenomenon then you might want to take the\n",
      "\n",
      "Predicting Input : 283 - in summer people wear more clothes than in winter.\n",
      "Predicted Output: For campfires and the hot summer days where you don't have fire trucks to support your\n",
      "\n",
      "Predicting Input : 284 - we can skate on snow.\n",
      "Predicted Output: You will lose focus, they will say something to you\n",
      "\n",
      "Predicting Input : 285 - we can eat dirt.\n",
      "Predicted Output: But if only so, we would have avoided it all in the future: by following my\n",
      "\n",
      "Predicting Input : 286 - mobile phones don't need charging.\n",
      "Predicted Output: So what happens when a phone with a charger can't charge? Your Galaxy S5 does\n",
      "\n",
      "Predicting Input : 287 - if we are going to be late we should walk.\n",
      "Predicted Output: 12:54 How to win a party based on listening to unruly guests\n",
      "\n",
      "Predicting Input : 288 - you can go to the moon by subway.\n",
      "Predicted Output: As with garbage receptacles, you can trash in front of an altar or within a hotel\n",
      "\n",
      "Predicting Input : 289 - I see a dog playing a computer.\n",
      "Predicted Output: I see cat and dogs playing with toys in the living room\n",
      "\n",
      "Predicting Input : 290 - we have an ice ball fight in winter.\n",
      "Predicted Output: Wouldn't you want that shit I am talking\n",
      "\n",
      "Predicting Input : 291 - milk is made by chicken.\n",
      "Predicted Output: There are numerous cats, including my own sisters, which can be grown from meat. If\n",
      "\n",
      "Predicting Input : 292 - On Mars, there have many countries.\n",
      "Predicted Output: There are literally hundreds, if not thousands of times more citizens in Mars than we are in\n",
      "\n",
      "Predicting Input : 293 - A student can't learn his course.\n",
      "Predicted Output: What will college students be doing on their campuses?\n",
      "\n",
      "Predicting Input : 294 - Someone can live who don't eat anything.\n",
      "Predicted Output: But just because he knows things doesn't mean\n",
      "\n",
      "Predicting Input : 295 - A man can see on a dark night.\n",
      "Predicted Output: It's okay to cry\n",
      "\n",
      "Predicting Input : 296 - It is always cold in July in the northern earth.\n",
      "Predicted Output: As we climb some of the upper reaches of the Mt. Hayden Ranch in Washington state\n",
      "\n",
      "Predicting Input : 297 - To get a service or good, you don't have to pay money.\n",
      "Predicted Output: For example: It is too easy to justify that many places are harder for people to find\n",
      "\n",
      "Predicting Input : 298 - Cancer can bring happiness to us.\n",
      "Predicted Output: There are numerous articles, books, bookshelves and clothing that focus on working with cancer\n",
      "\n",
      "Predicting Input : 299 - people can only feel happiness.\n",
      "Predicted Output: As they observe each other in the club gym, every moment is loaded with routine. One\n",
      "\n",
      "Predicting Input : 300 - Fish can live in a land without water.\n",
      "Predicted Output: And what happens when a cat can't hunt? An angry cat doesn't expect to fall\n",
      "\n",
      "Predicting Input : 301 - Computers can eat food.\n",
      "Predicted Output: An extension program such as Mr. Costanza's solutions will make animals suitable for human consumption\n",
      "\n",
      "Predicting Input : 302 - Everyone lives in the country.\n",
      "Predicted Output: If you travel abroad, you can't fly the plane and not travel there.\n",
      "\n",
      "Predicting Input : 303 - people are only able to be honest.\n",
      "Predicted Output: There are articles about the difference between political correctness and censorship in this thread on two different topics\n",
      "\n",
      "Predicting Input : 304 - If you want to dig a hole then you should use the spoon.\n",
      "Predicted Output: Forget tasting milk! - koozie is usually white\n",
      "\n",
      "Predicting Input : 305 - Soldiers will be punished for bravery.\n",
      "Predicted Output: Sharks won't be allowed to live alone. This doesn't apply to soldiers.\n",
      "\n",
      "Predicting Input : 306 - Families should visit relatives during the workdays.\n",
      "Predicted Output: When people die or the family's money fails, there are no accidents on the job.\n",
      "\n",
      "Predicting Input : 307 - I can hold up a car.\n",
      "Predicted Output: This should cause no more problems in this shopping area than if you buy someone's car and\n",
      "\n",
      "Predicting Input : 308 - Everyone is good-looking.\n",
      "Predicted Output: This is the nickname of Ian Fabry's girlfriend.\n",
      "\n",
      "Predicting Input : 309 - many vegetables grow on the cement.\n",
      "Predicted Output: But some peasants used a piece of coal during the harvest to give wood and tools to small\n",
      "\n",
      "Predicting Input : 310 - A puzzle can be solved easily.\n",
      "Predicted Output: An empty cube can be filled with many cubes. This way the cube does not need to\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 311 - a Cord can be used for treatment.\n",
      "Predicted Output: But when Cord or a cord is cut during pain relief it can cause aggravation to other\n",
      "\n",
      "Predicting Input : 312 - You can take your airplane to visit Mars.\n",
      "Predicted Output: You'll see Mars from your spacecraft as you land on\n",
      "\n",
      "Predicting Input : 313 - A bus takes passengers from the sea.\n",
      "Predicted Output: If you drive by, you can see trains and buses going by after midnight.\n",
      "\n",
      "Predicting Input : 314 - Ireland is a planet.\n",
      "Predicted Output: You know those cosmologists on the ISS? That's their term for life. But\n",
      "\n",
      "Predicting Input : 315 - A person want moldy bread.\n",
      "Predicted Output: 30 of 82 people found this review helpful<|endoftext|>The Strike Force is usually saved by their craft\n",
      "\n",
      "Predicting Input : 316 - Oil comes from the air.\n",
      "Predicted Output: There are approximately 23% or so of aquatic animals that live in oceans and around 7%\n",
      "\n",
      "Predicting Input : 317 - Planes are almost made of gold.\n",
      "Predicted Output: An artists workshop can be found in the 1880s and beyond. This painting was made by\n",
      "\n",
      "Predicting Input : 318 - you can find aliens in a zoo.\n",
      "Predicted Output: There are aliens at a zoo. You must work out what you're seeing.\n",
      "\n",
      "Predicting Input : 319 - You would dance because you want to rest.\n",
      "Predicted Output: But if your cup of tea is too dirty, play by the rules\n",
      "\n",
      "Predicting Input : 320 - If you want to go to work then you should pay your fees.\n",
      "Predicted Output: On your 401k, you can call toll free when you need one to help you pay\n",
      "\n",
      "Predicting Input : 321 - I see a sheep eating a fish.\n",
      "Predicted Output: Psychological Tips for a Buddhist Buddhist Journal\n",
      "\n",
      "Predicting Input : 322 - My dad's dad is my grandma.\n",
      "Predicted Output: He used to live the rest of his life in LA.\n",
      "\n",
      "Predicting Input : 323 - Eating is an emotion between people.\n",
      "Predicted Output: for having eaten something, you can only cure it by eating it again\n",
      "\n",
      "Predicting Input : 324 - we can buy fresh fruits and vegetables from a Coffee shop.\n",
      "Predicted Output: There are fewer firms to buy local produce because of supply chains. Many corporations only pay more\n",
      "\n",
      "Predicting Input : 325 - You are likely to find a cake in a drawer.\n",
      "Predicted Output: You probably know people who will eat cake.\n",
      "\n",
      "Predicting Input : 326 - most birds include penguins can fly.\n",
      "Predicted Output: Staging pigeons, which are often congregated around winter season with dogs to sing and\n",
      "\n",
      "Predicting Input : 327 - I will feel energetic after running.\n",
      "Predicted Output: SOUTH LONG RAIN DURING LAST Saturday before training the troops went for a round\n",
      "\n",
      "Predicting Input : 328 - my friends play computer games in the gymnasium.\n",
      "Predicted Output: This less affluent climate, which was once paradise for immigrants and American tourists, has now become\n",
      "\n",
      "Predicting Input : 329 - people feel sick while kissing.\n",
      "Predicted Output: There are literally hundreds of studies on this topic. Some people will clearly decide that they're\n",
      "\n",
      "Predicting Input : 330 - I like climbing because I fear of heights.\n",
      "Predicted Output: The structure goes through a series of fixed slips. With many low hills there are new patches\n",
      "\n",
      "Predicting Input : 331 - people usually eat ice cream in winter.\n",
      "Predicted Output: There are locations where a regular table can usually be found and this won't go over well\n",
      "\n",
      "Predicting Input : 332 - we wear thin clothes in winter.\n",
      "Predicted Output: He told NPR's that he was working while he did some quality shopping at the store in\n",
      "\n",
      "Predicting Input : 333 - we can only see white clouds in the air.\n",
      "Predicted Output: You can choose which of the three arms type is slightly stronger. For example, it has\n",
      "\n",
      "Predicting Input : 334 - I lie on a hamburger in my room.\n",
      "Predicted Output: The window tops work, but I need food. I think I'm naked.\n",
      "\n",
      "Predicting Input : 335 - people usually use wood to put out a fire.\n",
      "Predicted Output: There are plants you can grow in and flowers that help get you started on your growth process\n",
      "\n",
      "Predicting Input : 336 - people can harvest fruits without planting seeds.\n",
      "Predicted Output: But what happens when a farm has no pesticides? When someone isn't pregnant.\n",
      "\n",
      "Predicting Input : 337 - people will get dry while taking a shower.\n",
      "Predicted Output: There are approximately 50% more ways to groom an actress than in sports\n",
      "\n",
      "Predicting Input : 338 - people can play the violin without learning.\n",
      "Predicted Output: You would certainly use a symphony for relaxation. So how do you choose to play it\n",
      "\n",
      "Predicting Input : 339 - no one can learn from books.\n",
      "Predicted Output: You know why these are real? They exist. So when you hear someone tell you they\n",
      "\n",
      "Predicting Input : 340 - most of the leaves are red.\n",
      "Predicted Output: You will notice that a small amount of bamboo is visible on the seeds that are near the\n",
      "\n",
      "Predicting Input : 341 - I will celebrate the failure of the game.\n",
      "Predicted Output: We play Flash here, we play Supernova. If we did not win the game we\n",
      "\n",
      "Predicting Input : 342 - I will have a trip to a police office.\n",
      "Predicted Output: The Police Inspector did an interview and will explain to us what was missing from the police vehicle\n",
      "\n",
      "Predicting Input : 343 - I should study during whole night.\n",
      "Predicted Output: 18:56 – a woman can't deny that she should have coffee at least once a\n",
      "\n",
      "Predicting Input : 344 - I can drive after drinking.\n",
      "Predicted Output: If you cook…I would like to listen to music on your tablet or computer.\n",
      "\n",
      "Predicting Input : 345 - I saw a baby holding a woman..\n",
      "Predicted Output: so tired but the kids were left crying\n",
      "\n",
      "Predicting Input : 346 - Today I will have lunch before breakfast..\n",
      "Predicted Output: If others ask, you can read each other's responses to each question to make sure\n",
      "\n",
      "Predicting Input : 347 - I can cook any food simply.\n",
      "Predicted Output: Shows food after cook\n",
      "\n",
      "Predicting Input : 348 - I saw a square pizza..\n",
      "Predicted Output: The puzzled Ms. Sherich then thanked all these customers and requested them to come back\n",
      "\n",
      "Predicting Input : 349 - I saw an island on land..\n",
      "Predicted Output: \"Guy was stuck on the beach about 8 p.\n",
      "\n",
      "Predicting Input : 350 - I have free access to all museums without tickets.\n",
      "Predicted Output: An evening viewing at a haven and then ordering the chocolate covered dogs will bring you to the\n",
      "\n",
      "Predicting Input : 351 - We cut down trees to protect the environment.\n",
      "Predicted Output: If you lose some, you go to rehab. If you don't get back to play\n",
      "\n",
      "Predicting Input : 352 - People like diseases..\n",
      "Predicted Output: He closes his book and looks at Susan. \"She's beautiful.\"\n",
      "\n",
      "Predicting Input : 353 - My friend is my family member.\n",
      "Predicted Output: This person helps us, not me. They can help us. If they're not important\n",
      "\n",
      "Predicting Input : 354 - I can use the money to buy all houses.\n",
      "Predicted Output: …the sums my other friends have given up on alcohol have also saved them time.\n",
      "\n",
      "Predicting Input : 355 - a stone is an animal.\n",
      "Predicted Output: But when examining sites, not people, osteologists often become concerned about sight. The presence\n",
      "\n",
      "Predicting Input : 356 - We communicate in different languages.\n",
      "Predicted Output: There are numerous languages, so you can communicate in four languages. There are many things you\n",
      "\n",
      "Predicting Input : 357 - Dogs like eating grass.\n",
      "Predicted Output: There are NO impacts. Peeing lightly on grass will not hurt dogs.\n",
      "\n",
      "Predicting Input : 358 - I saw a man wearing glasses of three lens.\n",
      "Predicted Output: Q: What happens when a child's eyes hurt?\n",
      "\n",
      "Predicting Input : 359 - I will laugh if my family is dead.\n",
      "Predicted Output: Q: What sort of a job are you assigned to when you were younger?\n",
      "\n",
      "Predicting Input : 360 - a Man should play with violent tigers.\n",
      "Predicted Output: If you haven't had the chance to attend the Melbourne Tigers on Saturday night, you should\n",
      "\n",
      "Predicting Input : 361 - I will quit my job if I need money.\n",
      "Predicted Output: If you sell something, you can't guarantee that you'll get anything back.\n",
      "\n",
      "Predicting Input : 362 - I am happy because I am poor.\n",
      "Predicted Output: It's funny how a form of skill consists of paying attention to everything around you. But\n",
      "\n",
      "Predicting Input : 363 - I like cakes because I like salty food.\n",
      "Predicted Output: 20. Ecozy, Slushy Chickpeas\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 364 - He walks faster than the driving car..\n",
      "Predicted Output: Eddie Chilton began his seventeenth year in love\n",
      "\n",
      "Predicting Input : 365 - today I will go to the amusement park to see elephants..\n",
      "Predicted Output: He laughs..\"You will do OK of course..\" and throws his money..\n",
      "\n",
      "Predicting Input : 366 - eating fruits and vegetables makes me fat.\n",
      "Predicted Output: You know those mountains, which are high above the eastern horizon? Well I've never seen\n",
      "\n",
      "Predicting Input : 367 - She doesn't like flowers, so I send her roses..\n",
      "Predicted Output: The scenery change, but it was scary. She sent me flowers because she just saw\n",
      "\n",
      "Predicting Input : 368 - I saw a man breathing in the water..\n",
      "Predicted Output: The messenger received a message with the prefix \"zip.\"\n",
      "\n",
      "Predicting Input : 369 - He lay under the watermelon tree.\n",
      "Predicted Output: Prayers resounded in the heart of the garden.\n",
      "\n",
      "Predicting Input : 370 - today I will go to my hometowns.\n",
      "Predicted Output: Kaiyaaaaaaaaa! and you will die of thirst for some meal while your mom comes\n",
      "\n",
      "Predicting Input : 371 - My blood type is the same as everyone..\n",
      "Predicted Output: 15 83 32% they can't shake caucasians\n",
      "\n",
      "Predicting Input : 372 - I drink coffee before I go to bed..\n",
      "Predicted Output: But Harris doesn't feel about his discomfort. He actually felt uncomfortable during his quick seat\n",
      "\n",
      "Predicting Input : 373 - He is a hunter afraid of little animals..\n",
      "Predicted Output: He admits he is only two years older than Anna and Liza so he can't\n",
      "\n",
      "Predicting Input : 374 - I use calculators to play computer games..\n",
      "Predicted Output: If game files in your project are compressed to 600 MB\n",
      "\n",
      "Predicting Input : 375 - I have a light bulb emitting black light..\n",
      "Predicted Output: …still needs a lightbulb!!\n",
      "\n",
      "Predicting Input : 376 - some monkeys teach human acrobatics.\n",
      "Predicted Output: As your goat walks, you will find grass in your riding field and shrubs in your\n",
      "\n",
      "Predicting Input : 377 - If you want to take a walk then you should wear a hat.\n",
      "Predicted Output: There are plenty of \"Yes, you're going to pick me up while I'm doing\n",
      "\n",
      "Predicting Input : 378 - If you want to make people laugh then you can hit them.\n",
      "Predicted Output: You'll make people laugh.\n",
      "\n",
      "Predicting Input : 379 - I use the refrigerator to cook food..\n",
      "Predicted Output: 13 ▷\"We would like them to stay close to each other..\"\n",
      "\n",
      "Predicting Input : 380 - I should play games at my school.\n",
      "Predicted Output: The Green Blaze school, which is run by the Victoria Foundation, provides basketball players with sports\n",
      "\n",
      "Predicting Input : 381 - He put the fire in his hand..\n",
      "Predicted Output: It wouldn't be so bad if Chapin wasn't there at all.\n",
      "\n",
      "Predicting Input : 382 - you may always find a computer user in a wine bar.\n",
      "Predicted Output: There are plenty of \"friendlier\" laptops in coffee shops. Most coffee shops are designed\n",
      "\n",
      "Predicting Input : 383 - people usually sleep on the water..\n",
      "Predicted Output: Iceland does not have rain during the summer..\n",
      "\n",
      "Predicting Input : 384 - If you want to walk on the street then you should have money.\n",
      "Predicted Output: You want to get paid for your work.\n",
      "\n",
      "Predicting Input : 385 - Pushing a bike requires much strength.\n",
      "Predicted Output: There are numerous kinds of armor. You cannot put too much on your armor.\n",
      "\n",
      "Predicting Input : 386 - Ennui would make you become happy.\n",
      "Predicted Output: Whipping nails like a glass of ice melt a substance's property is missing the connective\n",
      "\n",
      "Predicting Input : 387 - Something you find at a house is a fighter plane.\n",
      "Predicted Output: But if there was a fighter plane, someone would probably fall on your desk\n",
      "\n",
      "Predicting Input : 388 - A baby can fix a computer.\n",
      "Predicted Output: not only moves between a computer and the playground. because they are controlled by our brain.\n",
      "\n",
      "Predicting Input : 389 - Accidents are a good thing.\n",
      "Predicted Output: And there are ways to prevent them. Unless you're willing to lose money to defraud\n",
      "\n",
      "Predicting Input : 390 - Blind people can watch the TV.\n",
      "Predicted Output: There are officially four SBS events in Seoul. One is on Saturday - see this article\n",
      "\n",
      "Predicting Input : 391 - A child will be a child forever.\n",
      "Predicted Output: You may teach them, but you will teach them without giving them energy\n",
      "\n",
      "Predicting Input : 392 - Batteries can power a lantern forever.\n",
      "Predicted Output: And there are quite a few different types of cannabis plants in the land of Canada. There\n",
      "\n",
      "Predicting Input : 393 - A drought happens when excessive rain falls.\n",
      "Predicted Output: You may notice it, but it's rare. These water problems occur where there is high\n",
      "\n",
      "Predicting Input : 394 - If you want to have a haircut then you should go to a library.\n",
      "Predicted Output: There are numerous newspaper, magazine, bookstores. First one is usually someone's business and\n",
      "\n",
      "Predicting Input : 395 - If you want to read a book then you should get it from the restaurant.\n",
      "Predicted Output: For bookstores we are using the third grade to teach young people how to read. But\n",
      "\n",
      "Predicting Input : 396 - The effect of attending a classical concert is impetuous.\n",
      "Predicted Output: If you wear clothes, you can't dance. You should't dress like the thought of\n",
      "\n",
      "Predicting Input : 397 - Something that might happen when you diminish your own hunger is taking exercise.\n",
      "Predicted Output: As Flanders explained, \"It's impossible to eat without taking exercise.\" In other words\n",
      "\n",
      "Predicting Input : 398 - Something you find in the home is actors.\n",
      "Predicted Output: You should choose actors, not wait for actors to meet you. Be careful not to leave\n",
      "\n",
      "Predicting Input : 399 - Going on strike is for having fun.\n",
      "Predicted Output: If you're right at home and are feeling\n",
      "\n",
      "Predicting Input : 400 - a person is anxious for undesired things.\n",
      "Predicted Output: If you aren't a member of the SEO community you're not prepared for what you're\n",
      "\n",
      "Predicting Input : 401 - You are likely to find a potato in a fruit bowl.\n",
      "Predicted Output: As with bacon, a potato is also tasty. One day in mid March and I've\n",
      "\n",
      "Predicting Input : 402 - Something you find in downtown is a volcano.\n",
      "Predicted Output: You want them to be doing what you'd like them to do and need to do it\n",
      "\n",
      "Predicting Input : 403 - His hair is getting longer after a haircut.\n",
      "Predicted Output: He enjoys working out.\n",
      "\n",
      "Predicting Input : 404 - He took the paper out of the printer and printed it.\n",
      "Predicted Output: Singer Katy Perry, who has been acclaimed for her dramatic performance as Slayer in the movie\n",
      "\n",
      "Predicting Input : 405 - taking a walk is harmful to health in general.\n",
      "Predicted Output: For example: If a car is fast asleep, there's some condition called sleep apnea\n",
      "\n",
      "Predicting Input : 406 - People usually eat moldy bread.\n",
      "Predicted Output: Strawberries are a popular item in supermarkets. They have as much nutritional value as apples\n",
      "\n",
      "Predicting Input : 407 - People can't make friends in clubs.\n",
      "Predicted Output: We're throwing friends, not food. People can't sit on chairs for food.\n",
      "\n",
      "Predicting Input : 408 - He is a thrifty man and he often wastes food.\n",
      "Predicted Output: If you eat enough, you will most likely also eat less. If you avoid food,\n",
      "\n",
      "Predicting Input : 409 - He was very energetic because he stayed up late last night.\n",
      "Predicted Output: \"He missed two-and-a-half hours on his bike because he was unable\n",
      "\n",
      "Predicting Input : 410 - he goes to the cinema to watch animals.\n",
      "Predicted Output: Strict dressing rules, such as those enacted in Italian cities, cannot cause physical harm to\n",
      "\n",
      "Predicting Input : 411 - students usually play their mobile phones in class time.\n",
      "Predicted Output: To learn how to be an active student there is no training in Advanced Learning and The Reading\n",
      "\n",
      "Predicting Input : 412 - We make cakes with butter and wood.\n",
      "Predicted Output: We make pancakes with a mix of cream cheese, vanilla and sugar.\n",
      "\n",
      "Predicting Input : 413 - No one will die in war.\n",
      "Predicted Output: If you die and a woman who are blessed with sin remains you (because you are beautiful\n",
      "\n",
      "Predicting Input : 414 - he is healthy and always takes medicine.\n",
      "Predicted Output: He would prefer he be able to pass faster\n",
      "\n",
      "Predicting Input : 415 - Cowardly people love horror movies.\n",
      "Predicted Output: You know things like, \"we're hungry and aren't making dinner yet.\"\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 416 - People usually fly in the sky.\n",
      "Predicted Output: You may wish to be sure of this soon. If you can't fly on the flight\n",
      "\n",
      "Predicting Input : 417 - He washes potatoes in a washing machine.\n",
      "Predicted Output: He doesn't drink, but he does wash his hair\n",
      "\n",
      "Predicting Input : 418 - No living thing can live in the sea.\n",
      "Predicted Output: 14:54 Asp Andirraaja A captain who had founded Gautam Am\n",
      "\n",
      "Predicting Input : 419 - People like to wear wet clothes.\n",
      "Predicted Output: You want clean clothes, but you want wet clothes to keep you warm.\n",
      "\n",
      "Predicting Input : 420 - He is older than his father.\n",
      "Predicted Output: 12:54 AM, Aug 19, 1999\n",
      "\n",
      "Predicting Input : 421 - People can speak loudly in the library.\n",
      "Predicted Output: There are concrete stairs, they can't climb and need to be manually guided to the library\n",
      "\n",
      "Predicting Input : 422 - People can't go back to where they are after they circle the earth.\n",
      "Predicted Output: But if anything he is getting more wrong because he cannot connect to someone without looking for them\n",
      "\n",
      "Predicting Input : 423 - human can live on the sun.\n",
      "Predicted Output: There are 56 adults, 12 children. Adults are born with one baby every 11 days.\n",
      "\n",
      "Predicting Input : 424 - most cats can swim.\n",
      "Predicted Output: of cats may sound a little white. Almost all cats will act white but there are still\n",
      "\n",
      "Predicting Input : 425 - You might feel happy when you're stuck in traffic.\n",
      "Predicted Output: You might think we're being rude to you.\n",
      "\n",
      "Predicting Input : 426 - We will feel relaxed from sitting or standing for long periods of time.\n",
      "Predicted Output: There are lots of willies in the gardens. However, not everyone likes to play with\n",
      "\n",
      "Predicting Input : 427 - Cats and mice are good friends.\n",
      "Predicted Output: There are numerous opportunities to live with many pets. Early warning is essential for natural language skills\n",
      "\n",
      "Predicting Input : 428 - The sea is sweet.\n",
      "Predicted Output: But there are often a lot of days during the summer when I might feel quite cold.\n",
      "\n",
      "Predicting Input : 429 - Fireflies are usually annoying.\n",
      "Predicted Output: To prevent malaria and the spread of diseases such as malaria, they must go out of their\n",
      "\n",
      "Predicting Input : 430 - Flowers have only one color.\n",
      "Predicted Output: What do unicorns, mountain lions, tigers, snakes, and lions eat?\n",
      "\n",
      "Predicting Input : 431 - Hail comes in winter.\n",
      "Predicted Output: There are 62 opinions, which are taken together. What do you think about this? Let\n",
      "\n",
      "Predicting Input : 432 - A man with a big head must be clever.\n",
      "Predicted Output: If you aren't, you will have missed the discussion of how Western civilization will lead to\n",
      "\n",
      "Predicting Input : 433 - Breathing through the mouse is healthy.\n",
      "Predicted Output: For real happiness to be felt, you must have above average weight and healthy personality. We\n",
      "\n",
      "Predicting Input : 434 - you can eat sprouted potatoes.\n",
      "Predicted Output: There are numerous garden-variety foods available in Boston and they're extremely healthy. Not\n",
      "\n",
      "Predicting Input : 435 - Waste batteries can be thrown away at will.\n",
      "Predicted Output: When people trash their or their vehicle in parks, please always let them clean up the trash\n",
      "\n",
      "Predicting Input : 436 - Vitamin C can be eaten with seafood.\n",
      "Predicted Output: The closest thing to chocolate you can get is\n",
      "\n",
      "Predicting Input : 437 - Smoking is allowed in public places.\n",
      "Predicted Output: Not only should businesses not allow smoking\n",
      "\n",
      "Predicting Input : 438 - children need four hours of sleep every day.\n",
      "Predicted Output: You know something about a common house or barn? Your photos are supposed to make it clear\n",
      "\n",
      "Predicting Input : 439 - Ears can smell.\n",
      "Predicted Output: There are pros and cons to both homeopathic and orthopedic medicines. You can find\n",
      "\n",
      "Predicting Input : 440 - the Football is played by hand.\n",
      "Predicted Output: If you agree you are here to ask questions of young players. Please bring your own plastic\n",
      "\n",
      "Predicting Input : 441 - Dinner should be around one o'clock.\n",
      "Predicted Output: As we discussed earlier, this is not practical to eat during the afternoon or evening. There\n",
      "\n",
      "Predicting Input : 442 - Westerners like the number thirteen.\n",
      "Predicted Output: The twins are pure objects. They don't get attention\n",
      "\n",
      "Predicting Input : 443 - eating fruits is bad for health.\n",
      "Predicted Output: There are numerous regions, including U.S. Columbia. In northern Missouri, it's\n",
      "\n",
      "Predicting Input : 444 - children need to drink beer before going to school.\n",
      "Predicted Output: There are fewer emergency refuges that serve students than those that serve adults.\n",
      "\n",
      "Predicting Input : 445 - I play football every day because I love playing computer games.\n",
      "Predicted Output: I've lived with a family that was divorced for several years. My husband was for many\n",
      "\n",
      "Predicting Input : 446 - it is rainy, so we can play football outside.\n",
      "Predicted Output: We use showers for a lot of things – we break down in each shower and make sure\n",
      "\n",
      "Predicting Input : 447 - students have to copy homework in class.\n",
      "Predicted Output: That has gotten difficult, but it's largely been replaced by only math classes.\n",
      "\n",
      "Predicting Input : 448 - Tokyo is in China.\n",
      "Predicted Output: Chinesaurus gets a special major health tourism contract from China\n",
      "\n",
      "Predicting Input : 449 - In the evening, I will eat breakfast.\n",
      "Predicted Output: After entering the gates of the town I\n",
      "\n",
      "Predicting Input : 450 - I am tired, so I need to go to work.\n",
      "Predicted Output: Imagine that as you're entering an office. You\n",
      "\n",
      "Predicting Input : 451 - birds live in the ocean.\n",
      "Predicted Output: There are naturally rich, spring-fed freshwater fish (including common carp and fish that live\n",
      "\n",
      "Predicting Input : 452 - there are different kinds of trees in the ocean.\n",
      "Predicted Output:  I don't want people to be exposed to them.\")\n",
      "\n",
      "Predicting Input : 453 - I take a television to school every day.\n",
      "Predicted Output: Killing firefighters for a fire is not acceptable. They make you sick and put you in\n",
      "\n",
      "Predicting Input : 454 - the bag is too weighty, so I cannot hold it.\n",
      "Predicted Output: This stuff keeps my Cows from getting messed up or injured. There are two problems with\n",
      "\n",
      "Predicting Input : 455 - I am ill so I need to see an actor.\n",
      "Predicted Output: 100%|██████████| 20/20 [00:01<00:00, 17.43it/s]\n",
      "\n",
      "Predicting Input : 456 - I work hard so I fail the exam finally.\n",
      "Predicted Output: This world happens as a form of mad destiny. However, it doesn't tell you how\n",
      "\n",
      "Predicting Input : 457 - I finished my work late last night so I am energetic now.\n",
      "Predicted Output: MOTION EFFECT, DEAFEN LIKE A DAY - VOUSEMENT AMONG CH\n",
      "\n",
      "Predicting Input : 458 - it is summer so I need to put on more clothes.\n",
      "Predicted Output: Sitting on someone's back is not just hazardous. Here's the sad truth: we don\n",
      "\n",
      "Predicting Input : 459 - I get up too early so I will be late for school.\n",
      "Predicted Output: \"As for me personally, at school I feel pretty normal. This\n",
      "\n",
      "Predicting Input : 460 - I am hungry now so I am going to a hospital.\n",
      "Predicted Output: i want her to be able to come home the next day. she cannot go to school\n",
      "\n",
      "Predicting Input : 461 - the food tastes awful so I want to eat it again.\n",
      "Predicted Output: This book feels really, really good. Everyone's talked about it over and over. And\n",
      "\n",
      "Predicting Input : 462 - most grasses are black.\n",
      "Predicted Output: There are 250 iron-rich animals in Northwestern China\n",
      "\n",
      "Predicting Input : 463 - we meet whales in the forest.\n",
      "Predicted Output: This event brings together a group of fine vertebrates from around the world for the first time\n",
      "\n",
      "Predicting Input : 464 - he put out the fire with oil.\n",
      "Predicted Output: In fact, she and her staff had burned it down when it looked like it was too\n",
      "\n",
      "Predicting Input : 465 - he often listens to the video.\n",
      "Predicted Output: This story originally appeared in the September 17 edition of Berlin TV.<|endoftext|>Announcement They\n",
      "\n",
      "Predicting Input : 466 - there is a sea in the front of the classroom.\n",
      "Predicted Output:  a big sea or marsh just to take in the scenery. rest are what hold our attention. through\n",
      "\n",
      "Predicting Input : 467 - we take photos of willow catkins in winter.\n",
      "Predicted Output: What do cats eat, which are different crops?\n",
      "\n",
      "Predicting Input : 468 - he uses a bamboo basket to fetch water.\n",
      "Predicted Output: to generate microwave spectrum, but you can still use sets of hands and kitchen equipment.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 469 - They feel warmer after the snow.\n",
      "Predicted Output: If you wish to be calmer in Finland, read more at Natural Balance\n",
      "\n",
      "Predicting Input : 470 - He cut metal with scissors.\n",
      "Predicted Output: The guard summoned him, but he did not feel ready to move in time.\n",
      "\n",
      "Predicting Input : 471 - my family fly kites on rainy days.\n",
      "Predicted Output: An experiment conducted by a live human at Auckland's Greensburg airport revealed 12 young women work\n",
      "\n",
      "Predicting Input : 472 - An ant knocked down a tree.\n",
      "Predicted Output: Primary source in English: 頇�\n",
      "\n",
      "Predicting Input : 473 - they search penguins in the Arctic.\n",
      "Predicted Output: We call penguins, not harp seals, because they are creatures who live in our\n",
      "\n",
      "Predicting Input : 474 - we hear thunder first in thunderstorm days.\n",
      "Predicted Output: 80 said rain wasn't very important to Mt. Sinai on day 10th of Plinian\n",
      "\n",
      "Predicting Input : 475 - the cat caught a tiger.\n",
      "Predicted Output: If you eat cat, you will get spoiled. If you don't eat cat, you\n",
      "\n",
      "Predicting Input : 476 - some fish are flying in the sky.\n",
      "Predicted Output: He didn't expect a sun that big or so bright to set over Palmyra. The\n",
      "\n",
      "Predicting Input : 477 - he built a snowman on a summer day.\n",
      "Predicted Output: It was assumed he was going to make snow on one of the sheets he set with his\n",
      "\n",
      "Predicting Input : 478 - most plants provide carbon dioxide for the human.\n",
      "Predicted Output: But no chemicals can be given to photosynthesis. Yet photosynthesis requires oxygen to come from\n",
      "\n",
      "Predicting Input : 479 - wheat grows in the ocean.\n",
      "Predicted Output: You get burnt out, but you're glad that you don't drink water.\n",
      "\n",
      "Predicting Input : 480 - he saw eight colors when the rainbow appears.\n",
      "Predicted Output: In order for events to take place, Pokemon can only use their Pokémon ID in the beginning\n",
      "\n",
      "Predicting Input : 481 - thirst would make you want to sleep.\n",
      "Predicted Output: As with brains, a mind is more fragile when things go wrong than when they go right\n",
      "\n",
      "Predicting Input : 482 - People can grow black hair all the time.\n",
      "Predicted Output: AN CHANGE OF TENTUMS HAS AN IMPORTANT EFFECT ON THE COL\n",
      "\n",
      "Predicting Input : 483 - African skin is mostly white.\n",
      "Predicted Output: When used exclusively for a method of putting lipstick in glasses, it isn't popular. The\n",
      "\n",
      "Predicting Input : 484 - close relatives can marry.\n",
      "Predicted Output: You would notice not a single person was kissed in school and not told if you were Christian\n",
      "\n",
      "Predicting Input : 485 - Goldfish usually sleep with their eyes closed.\n",
      "Predicted Output: On another adventure on a remote island, Gaston jumped off a cliff with his body tied\n",
      "\n",
      "Predicting Input : 486 - the color of Animal blood includes only red.\n",
      "Predicted Output: As TheSpider attached a baby kangaroo to her desk, half of it was covered\n",
      "\n",
      "Predicting Input : 487 - All animals are active in winter.\n",
      "Predicted Output: When your puppy says, \"Yell Yo-Yo,\" he means when he was born\n",
      "\n",
      "Predicting Input : 488 - All insects have no sense of smell.\n",
      "Predicted Output: There are exceptions to the rule. As insects are sensitive to light and wetness, they\n",
      "\n",
      "Predicting Input : 489 - The fish is sweet.\n",
      "Predicted Output: But there are three to four women that desire to eat fish. What does it feel like\n",
      "\n",
      "Predicting Input : 490 - The turtle usually lives very short.\n",
      "Predicted Output: Stray eggs usually don't nest in eggs that are all season\n",
      "\n",
      "Predicting Input : 491 - Leaves grow on trees forever.\n",
      "Predicted Output: There are layers of the leaves in the autumn. As they grow more mature, they become\n",
      "\n",
      "Predicting Input : 492 - Bones are soft.\n",
      "Predicted Output: There are tons of lube in the silicone on these barrels. Don't let that affect\n",
      "\n",
      "Predicting Input : 493 - the Fire has only one color.\n",
      "Predicted Output: Chrome Effects (1-4) Quick enough to travel for hours...we would put\n",
      "\n",
      "Predicting Input : 494 - Silverware is bad for food.\n",
      "Predicted Output: Anogen Profile · 1 decade ago 2 Thumbs up 0 Thumbs down Report Abuse<|endoftext|>\n",
      "\n",
      "Predicting Input : 495 - The seawater is blue.\n",
      "Predicted Output: If you liked this, this is the soup I'd pick and serve with steamed mushrooms\n",
      "\n",
      "Predicting Input : 496 - Seawater is drinkable.\n",
      "Predicted Output: You cannot change water into water through prayer.\n",
      "\n",
      "Predicting Input : 497 - We usually hear thunder first and then see lightning.\n",
      "Predicted Output: There are plenty of \"fire goes out before you start talking, fire goes out after you\n",
      "\n",
      "Predicting Input : 498 - Human tears are generally sweet.\n",
      "Predicted Output: There are numerous ethnic, religious, and linguistic groups with various differences and identities. The Chinese\n",
      "\n",
      "Predicting Input : 499 - Cats generally like to eat vegetables.\n",
      "Predicted Output: There are numerous foods, including meat, poultry, eggs, fish and baked goods. However\n",
      "\n",
      "Predicting Input : 500 - The audiences of concerts are usually noisy.\n",
      "Predicted Output: 22. Clowns, fans, and violinists may play with fake limbs\n",
      "\n",
      "Predicting Input : 501 - wild geese fly north in winter In the northern hemisphere.\n",
      "Predicted Output: Island bunny feeding: whether it's tucked in your pants on land or not\n",
      "\n",
      "Predicting Input : 502 - In nature, the leaves of all plants are oval.\n",
      "Predicted Output: There are numerous kinds of plants. In farming, green flowers are commonly planted in rangel\n",
      "\n",
      "Predicting Input : 503 - There may be more polluted after the rain.\n",
      "Predicted Output: 40 the serpent himself, who on his prowl fell into the lake with his entire body\n",
      "\n",
      "Predicting Input : 504 - The Olympic Games for the disabled.\n",
      "Predicted Output: He could therefore come to support the London Paralympics (hence why he was selected\n",
      "\n",
      "Predicting Input : 505 - Cloth bags can hold water.\n",
      "Predicted Output: That said, these you can add to sandwiches. One reason is if you want to quickly\n",
      "\n",
      "Predicting Input : 506 - when it snows it isn't cold.\n",
      "Predicted Output: There are seasons when a cat can't flee the trees in their garden with its eyes closed\n",
      "\n",
      "Predicting Input : 507 - I am born in Japan so I am Chinese.\n",
      "Predicted Output: {Stylish comments in Chinese\n",
      "\n",
      "Predicting Input : 508 - humans walk with four legs.\n",
      "Predicted Output: There are numerous theories, all of which differ on how these skills came about. The key\n",
      "\n",
      "Predicting Input : 509 - a car is a kind of animal.\n",
      "Predicted Output: But there are always a few things that drive that sort of car around on the road.\n",
      "\n",
      "Predicting Input : 510 - sea water is sweet.\n",
      "Predicted Output: There are roughly six-foot-long tentacles in these waters. These females don't produce\n",
      "\n",
      "Predicting Input : 511 - If you want to go on the internet then you should buy a book.\n",
      "Predicted Output: There are situations where a book is your ticket to freedom. Failing that it's still\n",
      "\n",
      "Predicting Input : 512 - Christmas is the thirteenth month of the year.\n",
      "Predicted Output: There are monthly celebrations, such as student engagements, classical concerts, festivals and smaller events.\n",
      "\n",
      "Predicting Input : 513 - we can judge whether one can buy a beer by his height.\n",
      "Predicted Output: He says living with a boy is not beneficial to both children and adults; \"It will\n",
      "\n",
      "Predicting Input : 514 - Peace brings suffering.\n",
      "Predicted Output: He didn't understand, at first, why he'd become this frustrated with Kekke\n",
      "\n",
      "Predicting Input : 515 - parents hate their kids.\n",
      "Predicted Output: You want everyone to be happy. A parade is fun. but don't let their kids\n",
      "\n",
      "Predicting Input : 516 - If you want to sleep you need to go to bathroom.\n",
      "Predicted Output: There are plenty of not-so-powerful products at great price for protecting your body from\n",
      "\n",
      "Predicting Input : 517 - a pig can lay eggs.\n",
      "Predicted Output: You will notice that a pig you see laying eggs would like to eat should not be eating\n",
      "\n",
      "Predicting Input : 518 - I love music so I play basketball every day.\n",
      "Predicted Output: eSports gets our collective picture in the sun\n",
      "\n",
      "Predicting Input : 519 - Raincoats are used on sunny days.\n",
      "Predicted Output: That said, don't let the cold chill you down on the lawn or during the sun\n",
      "\n",
      "Predicting Input : 520 - nurses teach students.\n",
      "Predicted Output: This means doctors who are among the most misunderstood and misunderstood patients. Many doctors don't understand\n",
      "\n",
      "Predicting Input : 521 - My grandmother is so mean that everyone likes her.\n",
      "Predicted Output: But when someone tells a black person that she is supposed to be quiet for her life,\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 522 - we develop our friendship by fighting with each other.\n",
      "Predicted Output: **45 -----------------------------------------------1 the word \"escape\" means \"to escape another person's presence\n",
      "\n",
      "Predicting Input : 523 - an eraser can be used to write on paper.\n",
      "Predicted Output: What are cube sticks, like on a Lego game table?\n",
      "\n",
      "Predicting Input : 524 - I study at the party.\n",
      "Predicted Output: The wife wishes for a moment to talk about it with her husband and pleads for him\n",
      "\n",
      "Predicting Input : 525 - The south pole is very hot.\n",
      "Predicted Output: If you haven't had the chance to shower in July you may want to plan on taking\n",
      "\n",
      "Predicting Input : 526 - Ordinary cars have no wheels.\n",
      "Predicted Output: For example: if a car is often stacked on top of a swimming pool, it would\n",
      "\n",
      "Predicting Input : 527 - Ears are for seeing.\n",
      "Predicted Output: Chicks stick their armpits in flames. They don't stick them in the road\n",
      "\n",
      "Predicting Input : 528 - I talk to Chinese native speakers to improve my English speaking.\n",
      "Predicted Output: When you teach Chinese, you also learn grammar. Your teacher will teach you how to write\n",
      "\n",
      "Predicting Input : 529 - The words are so big that I cannot read them clearly.\n",
      "Predicted Output: If you enter this to read the text instead, you would have to wait for the paper\n",
      "\n",
      "Predicting Input : 530 - I sleep every day to exercise.\n",
      "Predicted Output: If you haven't had an argument with anyone in years, you might find it hard to\n",
      "\n",
      "Predicting Input : 531 - I ran so slowly that I won the championship.\n",
      "Predicted Output: A press conference will be held on Friday afternoon. American Football Prescriptors will be present\n",
      "\n",
      "Predicting Input : 532 - Tragedy makes me laugh.\n",
      "Predicted Output: This book deals with a police dog that catches the rats and makes them loose\n",
      "\n",
      "Predicting Input : 533 - I like art so I majored in mathematics.\n",
      "Predicted Output: You're curious about a book on Afrikaans / Malayalam language?\n",
      "\n",
      "Predicting Input : 534 - I am poor so I can buy a Lamborghini.\n",
      "Predicted Output: It's funny how a small lawman tries to enforce his big husband's heart. He\n",
      "\n",
      "Predicting Input : 535 - snakes are able to fly.\n",
      "Predicted Output: As they evolve from a mid-career age for flight, migratory birds can learn\n",
      "\n",
      "Predicting Input : 536 - the paper is made from soil.\n",
      "Predicted Output: … If needed a lording in the specimen of hair or build would help in her scientific\n",
      "\n",
      "Predicting Input : 537 - we use soap to clean our hair.\n",
      "Predicted Output: And when cleaning our own hair, we wash it under our hand if we want to \"\n",
      "\n",
      "Predicting Input : 538 - a rabbit can eat a frog.\n",
      "Predicted Output: If you eat too many rats, you won't feel great. That's because many rats\n",
      "\n",
      "Predicting Input : 539 - we often wear a scarf in the summer.\n",
      "Predicted Output: You only usually buy a scarf on the weekends. It's not particularly pleasant to buy that\n",
      "\n",
      "Predicting Input : 540 - A student will die if he doesn't pass the exam.\n",
      "Predicted Output: Forget Einstein's \"new ground to tread\" theory. The ratio is 9:7\n",
      "\n",
      "Predicting Input : 541 - it's possible to ski in sahara.\n",
      "Predicted Output: You go skiing with a red light. Snow is blue to sahara.\n",
      "\n",
      "Predicting Input : 542 - Smuggling drugs will be commended.\n",
      "Predicted Output: He may travel with a staff at the fraternity\n",
      "\n",
      "Predicting Input : 543 - the firstborn child is youngest and other children are older.\n",
      "Predicted Output: But what happens when a non-marital woman leaves her husband or vice versa? The\n",
      "\n",
      "Predicting Input : 544 - It costs very little to buy gold.\n",
      "Predicted Output: When we shop at a gold store, our mind gets excited. However, if we run\n",
      "\n",
      "Predicting Input : 545 - If you want to sew then you should get a gun.\n",
      "Predicted Output: As you draw more, you will want your hands to feel like legs and take that extra\n",
      "\n",
      "Predicting Input : 546 - Sometimes people put shoes on their hands.\n",
      "Predicted Output: He lost his feelings of self and eventually become what he\n",
      "\n",
      "Predicting Input : 547 - When you are near-sighted, you can't see things near at hand.\n",
      "Predicted Output: For example: You are near your eyes because you use your hands to strike you. And\n",
      "\n",
      "Predicting Input : 548 - People can live without food for a long time.\n",
      "Predicted Output: The Cornish food tree does not grow one\n",
      "\n",
      "Predicting Input : 549 - The clothes will be dirty after being washed.\n",
      "Predicted Output: You will notice we are taking from you wash.\" \"No. Do not spend more than\n",
      "\n",
      "Predicting Input : 550 - we can just wait for their self-healing after we break our legs.\n",
      "Predicted Output: He says many students, including me, die on non-related athletic occasions.\n",
      "\n",
      "Predicting Input : 551 - Being sedentary is a good thing.\n",
      "Predicted Output: There are plenty of other ways to perform Yoga. Most people will practice yoga with an exercise\n",
      "\n",
      "Predicting Input : 552 - a Dog usually bite acquaintances when it sees them.\n",
      "Predicted Output: If you wish to be reported to work duties, ask your employer or employer's parent or\n",
      "\n",
      "Predicting Input : 553 - Ripe apples are hot.\n",
      "Predicted Output: – Like carrots and the leaves of ruminants\n",
      "\n",
      "Predicting Input : 554 - All the birds can fly.\n",
      "Predicted Output: You'd get rid of all those mosquitoes.\n",
      "\n",
      "Predicting Input : 555 - People usually don't use pillows when they sleep.\n",
      "Predicted Output: An experimental Dutch study, published in the BMJ last month, showed that almost one in\n",
      "\n",
      "Predicting Input : 556 - People usually don't use tableware when they eat.\n",
      "Predicted Output: When they eat...Meal is going very well and what you eat really helps to improve\n",
      "\n",
      "Predicting Input : 557 - people eat their lunch in the bathroom.\n",
      "Predicted Output: You know those clothes, if you're dressed in those clothes. That just makes you something\n",
      "\n",
      "Predicting Input : 558 - People usually wash their hair with toothpaste.\n",
      "Predicted Output: For women between the ages of 18 and 46, traveling can be arranged only by car.\n",
      "\n",
      "Predicting Input : 559 - The moon is brighter than the sun.\n",
      "Predicted Output: You may wonder why, if it's cloudy, there are no clouds at all. It\n",
      "\n",
      "Predicting Input : 560 - People usually lie on chairs.\n",
      "Predicted Output: There are rarely historical and historical images of circus animals that would not fit into the subject line\n",
      "\n",
      "Predicting Input : 561 - I opened the wardrobe to illuminate the room.\n",
      "Predicted Output: 'He made himself lose a lot of weight,' said Dianna. 'You know, it\n",
      "\n",
      "Predicting Input : 562 - Men usually use lipstick.\n",
      "Predicted Output: Changelogs are a special kind of lipstick. They use other makeup artists to make them\n",
      "\n",
      "Predicting Input : 563 - Vegetarian usually like eating meat.\n",
      "Predicted Output: How much do vegetables need? A vegetable should be healthy\n",
      "\n",
      "Predicting Input : 564 - Dogs can eat chocolate.\n",
      "Predicted Output: If kids can eat chocolate, why don't all kids\n",
      "\n",
      "Predicting Input : 565 - We can't know the time through the watch.\n",
      "Predicted Output: The games server can open up to accept requests for time on\n",
      "\n",
      "Predicting Input : 566 - People usually eat the peel of the watermelon.\n",
      "Predicted Output: He says although there is no reason to panic, there are no cure for football's illness\n",
      "\n",
      "Predicting Input : 567 - Everyone can eat peanuts.\n",
      "Predicted Output: As with noodles, a correct answer is yes. When people say something difficult to like to\n",
      "\n",
      "Predicting Input : 568 - People usually can't smell the fragrance of jasmine.\n",
      "Predicted Output: Therefore, what do you smell when you use av\n",
      "\n",
      "Predicting Input : 569 - Only a few people with cancer die.\n",
      "Predicted Output: 40. Pharmaceutical companies: My needs are immense. No company can fill my needs.\n",
      "\n",
      "Predicting Input : 570 - People can take drugs at will.\n",
      "Predicted Output: But there are generalities of body language within the bodies of people who find themselves in sexual\n",
      "\n",
      "Predicting Input : 571 - Hens can give birth to chickens directly.\n",
      "Predicted Output: If chickens are sick and will break down in the heat\n",
      "\n",
      "Predicting Input : 572 - The tortoise usually runs faster than the rabbit.\n",
      "Predicted Output: Thinking carefully about a cat and other mammals\n",
      "\n",
      "Predicting Input : 573 - Students shouldn't do homework.\n",
      "Predicted Output: What about homework because a boy or girl shouldn't test his sense of humor?\n",
      "\n",
      "Predicting Input : 574 - Most people like snakes.\n",
      "Predicted Output: You see snakes when it's hot. Also, swimmers have naturally evolved to swim with\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 575 - I have the same fingerprints as him.\n",
      "Predicted Output: We want Mr Marsh, we want all convicted terrorists to do the same thing.\n",
      "\n",
      "Predicting Input : 576 - only the male can be a Marathoner.\n",
      "Predicted Output: There are obviously several more options to cut calories. Here are the steps you can take to\n",
      "\n",
      "Predicting Input : 577 - People can't find others lying.\n",
      "Predicted Output: As any beginner knows, there's no denying that lies can be misleading. As I've\n",
      "\n",
      "Predicting Input : 578 - A dermatologist can cure heart disease.\n",
      "Predicted Output: 80. Bake dry, non-porous bread in hot weather\n",
      "\n",
      "Predicting Input : 579 - We can hurt others at will.\n",
      "Predicted Output: If you push children, are you doing nothing to harm them? Or do you need to\n",
      "\n",
      "Predicting Input : 580 - All oranges are sweet.\n",
      "Predicted Output: You need grapes in a dry place. Plant in broad daylight. Use darkness to light it\n",
      "\n",
      "Predicting Input : 581 - People often eat in the toilet.\n",
      "Predicted Output: There are commonly multiple \"nocturnal curtains\" where women can neither lie nor move out\n",
      "\n",
      "Predicting Input : 582 - A chair usually has three legs.\n",
      "Predicted Output: You also sometimes find a chair that's folded and hanging from a pole or board. The\n",
      "\n",
      "Predicting Input : 583 - Many people drink coffee to sleep.\n",
      "Predicted Output: 38. Smart phones\n",
      "\n",
      "Predicting Input : 584 - We put food in the washing machine.\n",
      "Predicted Output: In our fridge we are doing any kind of creative shopping.\n",
      "\n",
      "Predicting Input : 585 - Hot dogs are dogs.\n",
      "Predicted Output: There are literally hundreds of medical conditions that aren't necessarily animals. People don't need any\n",
      "\n",
      "Predicting Input : 586 - We can work eight days a week.\n",
      "Predicted Output: We go down to a conference in three places. We start with hot meals.\n",
      "\n",
      "Predicting Input : 587 - Poor sleep can lead to high energy levels.\n",
      "Predicted Output: Caffeine is produced when you take your\n",
      "\n",
      "Predicting Input : 588 - Getting a Ph.D. is an easy task.\n",
      "Predicted Output: There are plenty of more powerful non-verbal skills but there's something else that you need\n",
      "\n",
      "Predicting Input : 589 - I take a sickle to cut wood.\n",
      "Predicted Output: We don't believe a letter is good food. We've never eaten meat.\n",
      "\n",
      "Predicting Input : 590 - Burning garbage makes the environment better.\n",
      "Predicted Output: The impact that plastic pollution has on trash cannot be explained by\n",
      "\n",
      "Predicting Input : 591 - The sun ceases to shine at night.\n",
      "Predicted Output: If you die when a piece of skin burns, but not if you die when it doesn\n",
      "\n",
      "Predicting Input : 592 - the Diabetic can eat candy at will.\n",
      "Predicted Output: In my grandmother's kitchen I don't eat frozen food\n",
      "\n",
      "Predicting Input : 593 - If people have something urgent to go to a place, they usually walk.\n",
      "Predicted Output: This does not mean a place is done alone. While many people walk home with their bodies\n",
      "\n",
      "Predicting Input : 594 - You can use the air conditioner without electricity.\n",
      "Predicted Output: For example you can not use the water heater on 5 years old because we did not use\n",
      "\n",
      "Predicting Input : 595 - Most people don't need friends.\n",
      "Predicted Output: 22. Empty slot, many people just empty it\n",
      "\n",
      "Predicting Input : 596 - Trees can have no bark.\n",
      "Predicted Output: There are shrubs, including some found across the Brooklyn Bridge. These shrubs are usually\n",
      "\n",
      "Predicting Input : 597 - You can find two identical leaves.\n",
      "Predicted Output: There are nine yellow, black, red or white leaf leaves in each leaf.\n",
      "\n",
      "Predicting Input : 598 - the sun will stay on all day.\n",
      "Predicted Output: But there will also be no further sunsets. They will be locked away in the moon\n",
      "\n",
      "Predicting Input : 599 - Most people like mice.\n",
      "Predicted Output: It can't be controlled by drugs.\n",
      "\n",
      "Predicting Input : 600 - You can drive on the highway without a driving license.\n",
      "Predicted Output: There are challenges with a driving license. Driving is legal and not drivers driving with a license\n",
      "\n",
      "Predicting Input : 601 - You can find the statue of liberty in France.\n",
      "Predicted Output: There are hardly any of us on this globe. There are people only there in this country\n",
      "\n",
      "Predicting Input : 602 - People don't need to wear clothes.\n",
      "Predicted Output: 18. City Files Every Lawsuit against Us.\n",
      "\n",
      "Predicting Input : 603 - Most people don't forget things anytime.\n",
      "Predicted Output: You should decide what the best time to photograph is so you can choose what you want to\n",
      "\n",
      "Predicting Input : 604 - Ugly boys are more likely to find girlfriends.\n",
      "Predicted Output: That's due to a lack of education - you don't get enough science or engineering skills\n",
      "\n",
      "Predicting Input : 605 - When we get sick we go to school.\n",
      "Predicted Output: We're scared of the guy and we hope he comes back. He won't get sick\n",
      "\n",
      "Predicting Input : 606 - When we want to listen to the song, we can use a USB plate.\n",
      "Predicted Output: There are seven USB-Stick devices available. To connect an iPad or iPhone, go\n",
      "\n",
      "Predicting Input : 607 - We can observe a star with a microscope.\n",
      "Predicted Output: But there are still a lot of questions left to answer.\n",
      "\n",
      "Predicting Input : 608 - Fire is common in the sea.\n",
      "Predicted Output: What this symbol means, or what it represents, is about your oceanography.\n",
      "\n",
      "Predicting Input : 609 - People who are allergic to potatoes can eat chips.\n",
      "Predicted Output: If you make salads, rice and chips with lettuce, you're\n",
      "\n",
      "Predicting Input : 610 - Mars is suitable for human survival.\n",
      "Predicted Output: You could argue that. The body is resistant to radiation and so needs heat to fight off\n",
      "\n",
      "Predicting Input : 611 - A working guitar can have no strings.\n",
      "Predicted Output: You will notice that a string is only separated by two fingers. So when you play on\n",
      "\n",
      "Predicting Input : 612 - When something breaks, we can glue them with water.\n",
      "Predicted Output: You will notice that a certain amount of humidity is necessary in the garage for our white paint\n",
      "\n",
      "Predicting Input : 613 - Germinated sweet potatoes can be eaten.\n",
      "Predicted Output: An evil curry made of cow's milk usually doesn't cause too much inflammation.\n",
      "\n",
      "Predicting Input : 614 - All flowers have fragrance.\n",
      "Predicted Output: Staying cool is usually necessary to keep your\n",
      "\n",
      "Predicting Input : 615 - All trees lose their leaves in autumn..\n",
      "Predicted Output: As birds travel, they also try to control them and use sense and taste. They\n",
      "\n",
      "Predicting Input : 616 - males are found at beauty salons more often than females.\n",
      "Predicted Output: An Asian tiger used a variety of tools during his travels to China and abroad. (image\n",
      "\n",
      "Predicting Input : 617 - apple juice is made of bananas.\n",
      "Predicted Output: There are numerous vegetables, nuts, seeds and fruit from around the world that are used in\n",
      "\n",
      "Predicting Input : 618 - the Heartbeat is slower in children than in adults normally.\n",
      "Predicted Output: You must think clearly about your dietary intake of healthy foods\n",
      "\n",
      "Predicting Input : 619 - a baton is used for playing music.\n",
      "Predicted Output: If you haven't had a chance to download the songs from DAW due to an accident\n",
      "\n",
      "Predicting Input : 620 - You can see the pine trees blooming.\n",
      "Predicted Output: You can hear their fae from it during the annual baseball game\n",
      "\n",
      "Predicting Input : 621 - Clouds have no color.\n",
      "Predicted Output: If you filter on multiple channels, then only one way of\n",
      "\n",
      "Predicting Input : 622 - the sky is blue in the evening.\n",
      "Predicted Output: We don't really have to see to identify the moon and not our moon.\n",
      "\n",
      "Predicting Input : 623 - I go to work to earn time.\n",
      "Predicted Output: It's OK to drive your car.\n",
      "\n",
      "Predicting Input : 624 - My hair is too short so I need a haircut.\n",
      "Predicted Output: If you buy another of my products and wish to enjoy them in another manner, you would\n",
      "\n",
      "Predicting Input : 625 - People think more efficiently in noisy environments.\n",
      "Predicted Output: What do doctors think: how do we regulate our emotions?\n",
      "\n",
      "Predicting Input : 626 - I'm grazing tigers.\n",
      "Predicted Output: The scientists chose 3% alfalfa in competition with one animal for support. We\n",
      "\n",
      "Predicting Input : 627 - Working all day is very relaxing.\n",
      "Predicted Output: 22. Gingerbread, plastic and sugar cakes\n",
      "\n",
      "Predicting Input : 628 - A table is a comfortable place to sleep.\n",
      "Predicted Output: There are lights that you can use to illuminate the door in the bedroom and can even light\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 629 - Something you find at the airport is a ship.\n",
      "Predicted Output: You may wonder why. (If you drive a plane you would probably probably take a plane\n",
      "\n",
      "Predicting Input : 630 - losers can get prizes.\n",
      "Predicted Output: What do odds stand to get in a pumpkin or snake??\n",
      "\n",
      "Predicting Input : 631 - smoking is good.\n",
      "Predicted Output: You need consistent wake-up and sleep deprivation\n",
      "\n",
      "Predicting Input : 632 - I will confirm my email through my radio.\n",
      "Predicted Output: The Missing Librarian's Road Trip has come to an\n",
      "\n",
      "Predicting Input : 633 - air is charged.\n",
      "Predicted Output: When used directly for a drop, it expands to cover 2 other horizontal surfaces.\n",
      "\n",
      "Predicting Input : 634 - the bike is a new invention.\n",
      "Predicted Output: the bike won't be available in some outdoor areas this fall. When available, it will\n",
      "\n",
      "Predicting Input : 635 - There is water in the pen so we can write.\n",
      "Predicted Output: We don't need tickets for last week's\n",
      "\n",
      "Predicting Input : 636 - I like European culture because I traveled to Canada last year.\n",
      "Predicted Output: I've spoken in a lot of TV programmes. You're so invited to talk to people\n",
      "\n",
      "Predicting Input : 637 - The last thing you do when you visit a museum is buying a ticket.\n",
      "Predicted Output: If you spend too much time on the weekends, your university will sell your tickets. But\n",
      "\n",
      "Predicting Input : 638 - many people can feel that death is the start of life.\n",
      "Predicted Output: But my lungs also are big. We suffer from atherosclerosis and polycystic hem\n",
      "\n",
      "Predicting Input : 639 - An apple is an animal.\n",
      "Predicted Output: This means meat (a wooded plant), only food that is edible is ok.\n",
      "\n",
      "Predicting Input : 640 - television is a device you use to contact people.\n",
      "Predicted Output: What can someone say, if you use TEENAGE MACHINE you will not say\n",
      "\n",
      "Predicting Input : 641 - you would buy French fries because they are healthy for you to eat.\n",
      "Predicted Output: There are numerous restaurants, bars, retail outlets, flower shops, supermarket outlets, public libraries\n",
      "\n",
      "Predicting Input : 642 - a person wants to buy ugly things.\n",
      "Predicted Output: But there are things. He has to wear his shirt and necktie that may not match\n",
      "\n",
      "Predicting Input : 643 - a joke can make you cry.\n",
      "Predicted Output: If you don't fire too much, you\n",
      "\n",
      "Predicting Input : 644 - a ruler can be used to cut things.\n",
      "Predicted Output: As can often happen, if it's raining, she'll want to send out a rain\n",
      "\n",
      "Predicting Input : 645 - The snow is dry.\n",
      "Predicted Output: He needs snowplow to help him shower.\n",
      "\n",
      "Predicting Input : 646 - The desert is full of water.\n",
      "Predicted Output: But there are never a lot of extra caves. And some of them will only be needed\n",
      "\n",
      "Predicting Input : 647 - the universe is very small.\n",
      "Predicted Output: But there are several more star systems that owe their existence to the twin planets. The sun\n",
      "\n",
      "Predicting Input : 648 - car engines can use air.\n",
      "Predicted Output: There are numerous posts to make you understand whether you should buy a cruise aircraft with the lights\n",
      "\n",
      "Predicting Input : 649 - Time is static.\n",
      "Predicted Output: So if our mail? A receivable sending is outputting it out as it is sent\n",
      "\n",
      "Predicting Input : 650 - The car is driving slowly on the highway.\n",
      "Predicted Output: After this.\n",
      "\n",
      "Predicting Input : 651 - Wallets can be used to put computers..\n",
      "Predicted Output: 27 Large lever, used to hold clocks. One major issue with monitors is that they\n",
      "\n",
      "Predicting Input : 652 - College is usually free.\n",
      "Predicted Output: You're likely to stay home with your parents\n",
      "\n",
      "Predicting Input : 653 - Children can play with guns.\n",
      "Predicted Output: But what happens when a close friend or caregiver drops his or her weapon to you?\n",
      "\n",
      "Predicting Input : 654 - Eating too much often makes people thin.\n",
      "Predicted Output: There are approximately 50% of people with migraines who have diagnosed each past year with\n",
      "\n",
      "Predicting Input : 655 - Music is usually hated by people.\n",
      "Predicted Output: When people hated America, they didn't listen to music.\n",
      "\n",
      "Predicting Input : 656 - People usually drink black milk.\n",
      "Predicted Output: There are sugar pills, such as Qwik berry or Twizzlers.\n",
      "\n",
      "Predicting Input : 657 - We can see dinosaurs playing with people.\n",
      "Predicted Output: He says dinosaurs make a lot of meat because we eat meat. This idea is one of\n",
      "\n",
      "Predicting Input : 658 - We can visit the website through books.\n",
      "Predicted Output: At our cocktail club, we will take gifts to friends and family for dinner.\n",
      "\n",
      "Predicting Input : 659 - We can write on paper with an eraser.\n",
      "Predicted Output: But if your granddaddy can't grasp the manual's language or equipment, it will\n",
      "\n",
      "Predicting Input : 660 - Adults usually like toys.\n",
      "Predicted Output: When people eat objects, they're often experimenting with food and making choices that may not help\n",
      "\n",
      "Predicting Input : 661 - We can buy human in the supermarket.\n",
      "Predicted Output: This only extends to a certain class of beverages. Most Americans go for spirits.\n",
      "\n",
      "Predicting Input : 662 - Humans usually like viruses.\n",
      "Predicted Output: You need viruses to be pretty good at encrypting stuff.\n",
      "\n",
      "Predicting Input : 663 - Those who break the law will be rewarded.\n",
      "Predicted Output: But some libertarian advocates, some more conservative ones, already believe that virtual currencies can stifle\n",
      "\n",
      "Predicting Input : 664 - people usually don't need to sleep.\n",
      "Predicted Output: And when eating regular, fat-free snacks, these foods can sometimes contain protein. If\n",
      "\n",
      "Predicting Input : 665 - A room with windows is usually dark at day.\n",
      "Predicted Output: For least 75 percent of people, this atmosphere is ideal for looking after kids.\n",
      "\n",
      "Predicting Input : 666 - Most modern houses are made of wood.\n",
      "Predicted Output: What do houses look like to you? Take a look at the maps below:\n",
      "\n",
      "Predicting Input : 667 - People can't wear seatbelts when driving.\n",
      "Predicted Output: There are banned seats, so we would assume that those people who wear seatbelts would\n",
      "\n",
      "Predicting Input : 668 - People usually like sharks.\n",
      "Predicted Output: He says sharks also are thought to have undergone \"major transformation\" during these rough times.\n",
      "\n",
      "Predicting Input : 669 - Alarm clocks usually help people fall asleep.\n",
      "Predicted Output: On 6 February 2013, 3.5million people died as a result of wild animal attacks\n",
      "\n",
      "Predicting Input : 670 - People usually hold umbrellas on cloudy days.\n",
      "Predicted Output: There are basically 12-14 days of sunshine in California and one big orange one in Texas\n",
      "\n",
      "Predicting Input : 671 - Thieves are glad of seeing the police.\n",
      "Predicted Output: But when cops mess with any form of resisting an angry mob, they usually take to shooting\n",
      "\n",
      "Predicting Input : 672 - He wore a ring on his ring finger, but he was single.\n",
      "Predicted Output: We feel embarrassed by a woman's sexual entitlement. She does not listen to us. We\n",
      "\n",
      "Predicting Input : 673 - People won't go to doctors when they get sick.\n",
      "Predicted Output: But when doctors find a cause of death their decision will not be influenced by what's actually\n",
      "\n",
      "Predicting Input : 674 - People can drink seawater when they are thirsty.\n",
      "Predicted Output: For water otters, they can only consume up to four cups per day.\n",
      "\n",
      "Predicting Input : 675 - People use electric fans when they are cold.\n",
      "Predicted Output: When they burn food, they need to breathe. And when they eat food, they need\n",
      "\n",
      "Predicting Input : 676 - Tourists will not take cameras with them.\n",
      "Predicted Output: Unexpectedly before someone got shot or photoshopped\n",
      "\n",
      "Predicting Input : 677 - Students should be indulged in games.\n",
      "Predicted Output: And when gaming has a specific contact with cultural content and culture, why don't we expect\n",
      "\n",
      "Predicting Input : 678 - cats can help police search for drugs.\n",
      "Predicted Output: And how often will a drug test be administered? That depends on how drug testing is conducted\n",
      "\n",
      "Predicting Input : 679 - Diamonds can't cut glass.\n",
      "Predicted Output: So what happens when a glass can't hide the chips in the grain (not to mention\n",
      "\n",
      "Predicting Input : 680 - applicants can give their bosses jobs.\n",
      "Predicted Output: You will notice that a lot of people prefer to give them less job titles to work for\n",
      "\n",
      "Predicting Input : 681 - People usually use sunscreen at night.\n",
      "Predicted Output: Showers usually provide a single source of sunscreen. That means it doesn't put you at\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 682 - It's easy for people to find gems.\n",
      "Predicted Output: There are tons of isosceles within the realm of Sufi Zen: A Stone\n",
      "\n",
      "Predicting Input : 683 - You can't ask the stewardess for help while flying.\n",
      "Predicted Output: If you wish to be started in the cockpit, don't ask if your flight is ready\n",
      "\n",
      "Predicting Input : 684 - We can see salt in water.\n",
      "Predicted Output: And when salt makes a nice fat. Take a toothbrush to brush my teeth.\n",
      "\n",
      "Predicting Input : 685 - Most people hate cats.\n",
      "Predicted Output: You shouldn't hate cats.\n",
      "\n",
      "Predicting Input : 686 - we cannot see blood in human's wounds.\n",
      "Predicted Output: If we imagine only a single person that wears a helmet and don't wear it. It\n",
      "\n",
      "Predicting Input : 687 - People usually put their quilts in the kitchen.\n",
      "Predicted Output: If you aren't a cook, you usually use cold water\n",
      "\n",
      "Predicting Input : 688 - sheepherders usually feed sheep with meat.\n",
      "Predicted Output: There are strict regulations. There is no compulsory education for Dalit Jews and vegans.\n",
      "\n",
      "Predicting Input : 689 - People shouldn't add gasoline to cars.\n",
      "Predicted Output: Asif Zinoviev said the fight against gas demands more than coal as it's different\n",
      "\n",
      "Predicting Input : 690 - People usually watch movies in hospitals.\n",
      "Predicted Output: You may notice they are already using their TVs. That's because many hospitals don't allow\n",
      "\n",
      "Predicting Input : 691 - spending money is to get sadness.\n",
      "Predicted Output: But some socially awkward troupes have clever way of letting you fall apart\n",
      "\n",
      "Predicting Input : 692 - Willow is my favorite flower.\n",
      "Predicted Output: five leaves of gorgeous pine at the age\n",
      "\n",
      "Predicting Input : 693 - This area is dangerous so you have to be careless.\n",
      "Predicted Output: You will die if the items you were gifted are damaged in an accident\n",
      "\n",
      "Predicting Input : 694 - the first thing you do when you eat something is closing your mouth.\n",
      "Predicted Output: I don't know how you were standing when you don't\n",
      "\n",
      "Predicting Input : 695 - He is thirty years old so it is time for him to retire.\n",
      "Predicted Output: an older wizard should be able to perform intricate magic without physical training in order to perform basic\n",
      "\n",
      "Predicting Input : 696 - paper can be used to make musical instruments.\n",
      "Predicted Output: How can instrument makers and instruments be used together?\n",
      "\n",
      "Predicting Input : 697 - musical instruments can be used to play baseball.\n",
      "Predicted Output: For example: when a ball is brought down the plate you can hear it pick up for\n",
      "\n",
      "Predicting Input : 698 - What you see in the room is a giraffe.\n",
      "Predicted Output: We're investigating what a giraffe would look like if you're wearing pants.\n",
      "\n",
      "Predicting Input : 699 - Tokyo is one of the largest countries in the world.\n",
      "Predicted Output: There are 63 cities, three in Japan and one in India. Most city areas are located\n",
      "\n",
      "Predicting Input : 700 - We usually press the button of elevators with our toes.\n",
      "Predicted Output: You will notice that a young person can lift and stretch their feet during walking.\n",
      "\n",
      "Predicting Input : 701 - A church is a place where people sleep.\n",
      "Predicted Output: You need someone who is serious about what they are doing to get well in life. He\n",
      "\n",
      "Predicting Input : 702 - paper can be made from sand.\n",
      "Predicted Output: The desired taste and sensation will depend on the\n",
      "\n",
      "Predicting Input : 703 - If you want to see then you should open your mouth.\n",
      "Predicted Output: As you slide down, you will find yourself out of position. Some points are very easy\n",
      "\n",
      "Predicting Input : 704 - an umbrella is for sheltering you from the wind.\n",
      "Predicted Output: As with exterior packaging, by using the umbrella in place of the exterior packaging to protect from\n",
      "\n",
      "Predicting Input : 705 - People celebrate on the day of death.\n",
      "Predicted Output: But some charities say a higher amount of grief is expected for people without funds\n",
      "\n",
      "Predicting Input : 706 - most people look forward to a funeral someday.\n",
      "Predicted Output: St John Fortico, who was named Gerald's oldest son in 1969\n",
      "\n",
      "Predicting Input : 707 - Tigers are very common pets.\n",
      "Predicted Output: As this reminds us, when you see tigers in China is 3 hours away and you want\n",
      "\n",
      "Predicting Input : 708 - birds are mammals.\n",
      "Predicted Output: As more dinosaurs moved, they'd popled with males and females with females. Hanging\n",
      "\n",
      "Predicting Input : 709 - I wish I had bad luck.\n",
      "Predicted Output: Garlic seed cleaves illness into hair.\n",
      "\n",
      "Predicting Input : 710 - His house is in the rich area so it is very cheap.\n",
      "Predicted Output: But if there was a white house in Salt Lake City for over 40 years I would say\n",
      "\n",
      "Predicting Input : 711 - You are likely to find a fish around in the sky.\n",
      "Predicted Output: There are creatures who are seen with their rods in their eyes. They take their eyes off\n",
      "\n",
      "Predicting Input : 712 - You would go for a run because you feel tired.\n",
      "Predicted Output: But if your doctor is going to care about you at all, then its good to call\n",
      "\n",
      "Predicting Input : 713 - your fingers are part of your face.\n",
      "Predicted Output: You see men without a bag of clothes anywhere. Go back to getting clothes and go back\n",
      "\n",
      "Predicting Input : 714 - the Vegetable salad is junk food.\n",
      "Predicted Output: You need vegetables and a healthy diet to thrive. Just being able to eat healthy can help\n",
      "\n",
      "Predicting Input : 715 - A girl is a male.\n",
      "Predicted Output: If you choose to be an adult, please use common sense. Your responsibility is to love\n",
      "\n",
      "Predicting Input : 716 - You would go on a study because you need a break.\n",
      "Predicted Output: If you write when the period is over then you don't know how long it has been\n",
      "\n",
      "Predicting Input : 717 - He was sober yesterday so he forgot what had happened.\n",
      "Predicted Output: The incident occurred after a period of physical exhaustion. He didn't drink since he was talking\n",
      "\n",
      "Predicting Input : 718 - water can become steam when it freezes.\n",
      "Predicted Output: Starch freezes at 45 degrees F (78 degrees C).\n",
      "\n",
      "Predicting Input : 719 - the sun is a satellite of the earth.\n",
      "Predicted Output: He called herself Dr. Micah in 1966. She first saw notes from his personal notes\n",
      "\n",
      "Predicting Input : 720 - The land is very barren so the grain grows very well.\n",
      "Predicted Output: We should arrive at a small house and heal the wounds on the bones on the face to\n",
      "\n",
      "Predicting Input : 721 - The Equatorial areas are very cold.\n",
      "Predicted Output: The rock formations on earth are very exposed to light.\n",
      "\n",
      "Predicting Input : 722 - The desk is for sleeping on.\n",
      "Predicted Output: We aren't planning on doing anything especially weighty and\n",
      "\n",
      "Predicting Input : 723 - I am full so I want to eat right away.\n",
      "Predicted Output: Vaidya pushes him a little hard in fear. He feels his muscles tighten and he begins\n",
      "\n",
      "Predicting Input : 724 - Somebody is sleeping so we should keep voice up.\n",
      "Predicted Output: The music distracts us from work\n",
      "\n",
      "Predicting Input : 725 - people want to have rude neighbors.\n",
      "Predicted Output: This country shouldn't be trying to win neighbors. They're out there fighting for their rights\n",
      "\n",
      "Predicting Input : 726 - the poor have a lot of money.\n",
      "Predicted Output: We're letting people to give this many excuses. They're not acting selfless. They\n",
      "\n",
      "Predicting Input : 727 - We should wear formal clothes at home.\n",
      "Predicted Output: If you wear formal clothes you will not wear it when you are teaching,\" she said.\n",
      "\n",
      "Predicting Input : 728 - Our government is very credible so we distrust it very much.\n",
      "Predicted Output: It is also notable that Mr Assange's nomination has been subject to\n",
      "\n",
      "Predicting Input : 729 - These people are bad for me so I can trust him.\n",
      "Predicted Output: The fans don't stay happy in the stadium\n",
      "\n",
      "Predicting Input : 730 - He was so stupid that he quickly understood the nature of the problem.\n",
      "Predicted Output: Zuma did not discuss the matter with his governors. They didn't speak with him. He\n",
      "\n",
      "Predicting Input : 731 - This restaurant is so unpopular that it is crowded every day.\n",
      "Predicted Output: You only cook only a small amount of fried food at this restaurant; most of the food\n",
      "\n",
      "Predicting Input : 732 - You are likely to find a cow around in any room of the house.\n",
      "Predicted Output: You would prefer not to feel too comfortable riding a cow on the crowded streets.\n",
      "\n",
      "Predicting Input : 733 - Autobiography is not real.\n",
      "Predicted Output: There are dozens of us with these types of names and same sex attraction as well. If\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 734 - Gloves can protect the head.\n",
      "Predicted Output: Red dye changes everything\n",
      "\n",
      "Predicting Input : 735 - The law allows us to kill people.\n",
      "Predicted Output: But if God wants a president, he wants to give people the tools they need to protect\n",
      "\n",
      "Predicting Input : 736 - A fox is a faithful companion.\n",
      "Predicted Output: He picks out rainbows.\n",
      "\n",
      "Predicting Input : 737 - Drinking too much water can make people drunk.\n",
      "Predicted Output: And she argues that a fall in power consumption that elevates people's chances of getting laid\n",
      "\n",
      "Predicting Input : 738 - people want a dirty bathroom.\n",
      "Predicted Output: You use toilets at a guy's house because he wants to know how long it will take\n",
      "\n",
      "Predicting Input : 739 - I am five years old so I am very old.\n",
      "Predicted Output: The feeling has nothing to do with game controller. For him, everything has to do with\n",
      "\n",
      "Predicting Input : 740 - You would listen to music because it is torturing.\n",
      "Predicted Output: If you listen to a standard music video game, if you play about 40 minutes of video\n",
      "\n",
      "Predicting Input : 741 - People cry when they are happy.\n",
      "Predicted Output: You need peace and a true sense of humour. If you can't laugh you will always\n",
      "\n",
      "Predicting Input : 742 - A easy job is stressful.\n",
      "Predicted Output: You want work which is interesting.\n",
      "\n",
      "Predicting Input : 743 - I am a left-handed person so I write with my right hand.\n",
      "Predicted Output: 30th Anniversary gift! The following year when I gave my children Christmas gifts I made them\n",
      "\n",
      "Predicting Input : 744 - My friend broke up with his girlfriend so I should ridicule him.\n",
      "Predicted Output: But her jaw dropped, you could call him a loser; he held my hand\n",
      "\n",
      "Predicting Input : 745 - Something you need to do before you go to bed is turning on the lights.\n",
      "Predicted Output: You need to study, so you do homework. You're not supposed to make up something\n",
      "\n",
      "Predicting Input : 746 - We will be punished for obeying the rules.\n",
      "Predicted Output: This case affects both the civil and criminal penalties. If you have committed something against the family\n",
      "\n",
      "Predicting Input : 747 - I will travel at the work.\n",
      "Predicted Output: This may affect some of the fire safety readings. So make sure you check with your doctor\n",
      "\n",
      "Predicting Input : 748 - We cut meat with a ruler.\n",
      "Predicted Output: There are restaurants with a banquette dish. The bonnet cut along the side is\n",
      "\n",
      "Predicting Input : 749 - Fisherman depend on farming for their livelihood.\n",
      "Predicted Output: But when meteorologists a little over a decade ago realized that they needed even more money to\n",
      "\n",
      "Predicting Input : 750 - People are safe when an earthquake happens.\n",
      "Predicted Output: But when volcanoes and volcanoes run loose, where will we become vulnerable?\n",
      "\n",
      "Predicting Input : 751 - We should be close to volcanoes.\n",
      "Predicted Output: But there will never be on your phone anything that looks like the sunset on the summertime\n",
      "\n",
      "Predicting Input : 752 - We need to pollute the environment.\n",
      "Predicted Output: You need nothing more to take your action than to choose not to spend money on impound\n",
      "\n",
      "Predicting Input : 753 - you will have a spouse If you get divorced.\n",
      "Predicted Output: When you divorce. You will have limited assets\n",
      "\n",
      "Predicting Input : 754 - we need to keep noisy in the museum.\n",
      "Predicted Output: If you visit one, you will know nothing but fine equipment. Your guests will not take\n",
      "\n",
      "Predicting Input : 755 - I went to the restaurant to borrow books.\n",
      "Predicted Output: 6. Thinking about a Steak with Pork\n",
      "\n",
      "Predicting Input : 756 - my mother asks me to play cards in class.\n",
      "Predicted Output: JIMMY D'ORT: Um, why would he put some cards in his\n",
      "\n",
      "Predicting Input : 757 - I drink beer every day to keep healthy.\n",
      "Predicted Output: He says having certain \"size\" numbers helps you remember how many glasses you need\n",
      "\n",
      "Predicting Input : 758 - you need to fail the exam to get a certification.\n",
      "Predicted Output: And if they'll be doing the exam anyway, there's no physical training that can give\n",
      "\n",
      "Predicting Input : 759 - we are good friends so we abuse each other.\n",
      "Predicted Output: We're ok but the others are right around us and they need help but we just don\n",
      "\n",
      "Predicting Input : 760 - elephants are quite little animals.\n",
      "Predicted Output: As was demonstrated by a previous study of lizard (specifically in Dietrich's parrot\n",
      "\n",
      "Predicting Input : 761 - Children need to stay away from books.\n",
      "Predicted Output: Charity programmes need a strong sense of urgency. Most UK teachers support academic learning. They\n",
      "\n",
      "Predicting Input : 762 - traveling the world will narrow your horizon.\n",
      "Predicted Output: There are plenty of do the real world photographers that show off their brands for their own benefit\n",
      "\n",
      "Predicting Input : 763 - We play football in my bedroom.\n",
      "Predicted Output: I used to try the TV in my bedroom. It was one of those things that if\n",
      "\n",
      "Predicting Input : 764 - There are few guests in this restaurant because of its perfect food.\n",
      "Predicted Output: There are nine guests, all of whom attend the restaurant on the evening of the night of\n",
      "\n",
      "Predicting Input : 765 - We should cut in line when there is a queue.\n",
      "Predicted Output: You should finish where a vehicle is going without having to reach the queue or having to completely\n",
      "\n",
      "Predicting Input : 766 - People respect him very much because he is a murderer.\n",
      "Predicted Output: This attack occurred after a woman was attacked during a wedding at the premises of the building The\n",
      "\n",
      "Predicting Input : 767 - Time is specific.\n",
      "Predicted Output: This clause is intended to allow communication between things on the\n",
      "\n",
      "Predicting Input : 768 - People can breathe in the water.\n",
      "Predicted Output: But there will also be few women and babies to eat on the shore.\"\n",
      "\n",
      "Predicting Input : 769 - You can use meat to feed to Sloths.\n",
      "Predicted Output: An evil sentient animal, if it was sentient, would shoot at Sloths in the moment\n",
      "\n",
      "Predicting Input : 770 - he became one of my best friends because he is hypocritical.\n",
      "Predicted Output: If you haven't had the chance to listen to Schwan's brilliant observations on your son\n",
      "\n",
      "Predicting Input : 771 - it is useless to read books every day.\n",
      "Predicted Output: You need reading ability, not self-awareness. If you don't read just by reading\n",
      "\n",
      "Predicting Input : 772 - I am good at swimming because I have always live in the desert.\n",
      "Predicted Output: But it's impossible to swim out of danger because you can't\n",
      "\n",
      "Predicting Input : 773 - the people in cities are usually poorer than the people in the countryside.\n",
      "Predicted Output: …you don't grow enough to read the\n",
      "\n",
      "Predicting Input : 774 - People usually Shed tears during exercising.\n",
      "Predicted Output: When my therapist mentioned a third person in mind, my partner would literally lean into me and\n",
      "\n",
      "Predicting Input : 775 - People usually enter houses through windows.\n",
      "Predicted Output: As their baggage passes, they may run behind the sleeping bag. So many people will fall\n",
      "\n",
      "Predicting Input : 776 - People usually sleep in noisy places.\n",
      "Predicted Output: There are limits on the amount of type exposure to chemicals in the environment: there is no\n",
      "\n",
      "Predicting Input : 777 - You can see many people live in the water.\n",
      "Predicted Output: If you place an order for\n",
      "\n",
      "Predicting Input : 778 - Most people can afford luxury goods.\n",
      "Predicted Output: There are choices for a home. I prefer an affordable home. Is there an option for\n",
      "\n",
      "Predicting Input : 779 - We need airplanes to go to the moon.\n",
      "Predicted Output: If we wouldn't be able to find airplanes. there would be bad luck\n",
      "\n",
      "Predicting Input : 780 - We can hear voices in the vacuum.\n",
      "Predicted Output: If you haven't had the chance to explore the chamber at all now, we're open\n",
      "\n",
      "Predicting Input : 781 - People should throw banana peels on the ground.\n",
      "Predicted Output: You should spray peanut butter on the ground without putrid ingredients.\n",
      "\n",
      "Predicting Input : 782 - People usually use electrical appliances with wet hands.\n",
      "Predicted Output: On my facebook page, I am always advocating for solar energy. However, we can't\n",
      "\n",
      "Predicting Input : 783 - People can't use plants to make paper.\n",
      "Predicted Output: And when rice has a higher fat content than sugar it may not stay alive for long.\n",
      "\n",
      "Predicting Input : 784 - teenagers should smoke.\n",
      "Predicted Output: There are numerous experiences, such as slathering and kissing on the toothbrush.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 785 - people usually ask a waiter to cook.\n",
      "Predicted Output: You may notice that a normal person would prefer to spend some time alone (like on the\n",
      "\n",
      "Predicting Input : 786 - Bees usually live in places with few flowers.\n",
      "Predicted Output: There are tiny patches of empty space on animals' backs and down animals' backs. If\n",
      "\n",
      "Predicting Input : 787 - People should believe what happens in their dreams.\n",
      "Predicted Output: You shouldn't judge people on things they don't even\n",
      "\n",
      "Predicting Input : 788 - Most countries don't need troops.\n",
      "Predicted Output: He says Saudi Arabia, which has been unfairly accused of signing up Iranian nationals to support terrorism\n",
      "\n",
      "Predicting Input : 789 - Pianists usually have inflexible fingers.\n",
      "Predicted Output: As we mentioned earlier, one of the weaker ways of maintaining an upright posture is to either\n",
      "\n",
      "Predicting Input : 790 - we are usually allowed to smoke in Gas stations.\n",
      "Predicted Output: On top of this, all other employees must have clean clothes. Please note that we do\n",
      "\n",
      "Predicting Input : 791 - People usually know what will happen in the future.\n",
      "Predicted Output: But there can't be any kind of blueprint in 2016 to follow up on NEDs\n",
      "\n",
      "Predicting Input : 792 - People like mice.\n",
      "Predicted Output: It was already time for competition.\n",
      "\n",
      "Predicting Input : 793 - Shepherds usually like wolves.\n",
      "Predicted Output: This video highlights what a woodchuck sounds like if you look closely and see the snow\n",
      "\n",
      "Predicting Input : 794 - People living in the desert often waste water.\n",
      "Predicted Output: But when sunlight falls, it can help soak up salt from the desert water.\n",
      "\n",
      "Predicting Input : 795 - Generally speaking taking notes is not a good habit.\n",
      "Predicted Output: For large vocabulary classes, we could say instead: \"Let's write down one thing we\n",
      "\n",
      "Predicting Input : 796 - Materialists believe in the existence of Jesus.\n",
      "Predicted Output: As they acknowledge that, \"We don't say we feel this Christ because we have found\n",
      "\n",
      "Predicting Input : 797 - Eating kelp with Onions is good for your health.\n",
      "Predicted Output: Onions remove pine-like leaves that interfere with yellowing the ripening process. But\n",
      "\n",
      "Predicting Input : 798 - We can use the ATM to get bank cards.\n",
      "Predicted Output: There are fewer transactions, so you can spend more money on good goods on the counter.\n",
      "\n",
      "Predicting Input : 799 - Software development doesn't cost money.\n",
      "Predicted Output: 23. Teaching kids: better value is teaching your kids as people and less money.\n",
      "\n",
      "Predicting Input : 800 - All the boys like girls on the sexual aspect.\n",
      "Predicted Output: But if nothing else, Raz can prove that sex was not simply sex and it had\n",
      "\n",
      "Predicting Input : 801 - We wear more clothes when the weather gets warmer.\n",
      "Predicted Output: You should wear clothing to protect your house before you leave in the winter\n",
      "\n",
      "Predicting Input : 802 - It's healthier to drink river water.\n",
      "Predicted Output: For those wondering how a drink is different than a chocolate cup, please check out this note\n",
      "\n",
      "Predicting Input : 803 - Most of the goods are free.\n",
      "Predicted Output: As we stated above, this is not typical of consumers in the retail sector. The International\n",
      "\n",
      "Predicting Input : 804 - Keys are usually made of plastic.\n",
      "Predicted Output: How do makeup books and dryers work in the bathroom?\n",
      "\n",
      "Predicting Input : 805 - When the house stinks we don't want to clean it.\n",
      "Predicted Output: Stairs contain smell, so we don't want it to help keep our computer from sleeping\n",
      "\n",
      "Predicting Input : 806 - Milk can be drunk at room temperature for a month after being opened.\n",
      "Predicted Output: Showers usually sit the night before and nights after according to the manufacturer's instructions. The\n",
      "\n",
      "Predicting Input : 807 - In order to protect the trees we should use less disposable paper towels.\n",
      "Predicted Output: And if there are a lot of trees growing in your yard, put down one of them\n",
      "\n",
      "Predicting Input : 808 - In ordinary times we can see the legendary dragon.\n",
      "Predicted Output: He has disappeared from the various places in Yellow history and now he still dwells in J\n",
      "\n",
      "Predicting Input : 809 - Copying homework is a good thing.\n",
      "Predicted Output: An unsolicited letter, like a phone call, never helps a teacher become more effective.\n",
      "\n",
      "Predicting Input : 810 - Many people don't go bald as they get older.\n",
      "Predicted Output: And when anyone gets a huge six month baldness one day, one day can not only\n",
      "\n",
      "Predicting Input : 811 - When we're laughing, we are usually sad.\n",
      "Predicted Output: But there are times, when you just laugh. There are times when we're looking at\n",
      "\n",
      "Predicting Input : 812 - The mouse is usually the core of a computer.\n",
      "Predicted Output: As with mouse systems, all very large monitors are capable of operating off their entire screen.\n",
      "\n",
      "Predicting Input : 813 - It's good to get x-rays frequently.\n",
      "Predicted Output: There are plenty of Crayola shoothouses in Bellingham. Just check out the sky\n",
      "\n",
      "Predicting Input : 814 - Taking too much medicine is a good thing.\n",
      "Predicted Output: There are diseases you are going to have patients with or live with with for two years.\n",
      "\n",
      "Predicting Input : 815 - When we want to look out the window, we close the curtains.\n",
      "Predicted Output: We don't open a window of any sort. We feel it open when we want to\n",
      "\n",
      "Predicting Input : 816 - When we want to log into a new website it's not necessary to register.\n",
      "Predicted Output: When we decide to have a new website join the domain which we already live in.\n",
      "\n",
      "Predicting Input : 817 - We feel energetic after exercising for a long time.\n",
      "Predicted Output: You need exercise for a long time. Exercise is important for your brain and body. If\n",
      "\n",
      "Predicting Input : 818 - people usually use water cup to retain meal.\n",
      "Predicted Output: So when buying baguettes, we prefer to buy water cup instead of baguette\n",
      "\n",
      "Predicting Input : 819 - The doctor usually goes to the hospital because they are sick.\n",
      "Predicted Output: If you stay up the night, you usually get taken to the bathroom or watch TV.\n",
      "\n",
      "Predicting Input : 820 - When people encounter robbers they will call ambulances.\n",
      "Predicted Output: As they prepare for a ride, they notice the thief could not enter their door. The\n",
      "\n",
      "Predicting Input : 821 - People can play on the highway.\n",
      "Predicted Output: If you enter the \"fast lane\" next to an illegally parked vehicle you will be stuck\n",
      "\n",
      "Predicting Input : 822 - People can't ask a guide for help when they travel.\n",
      "Predicted Output: You need someone who is happy to help them. If you do need someone to help you\n",
      "\n",
      "Predicting Input : 823 - People can eat a lot of junk food.\n",
      "Predicted Output: You could decide to be too fat to exercise. In fact, health risks from fat must\n",
      "\n",
      "Predicting Input : 824 - People should pollute water resources.\n",
      "Predicted Output: Chilling whales will be sure to kill whales. In fact, Phil Fish has been warning\n",
      "\n",
      "Predicting Input : 825 - Myopic people usually don't need glasses.\n",
      "Predicted Output: Another issue usually noticed by friends is\n",
      "\n",
      "Predicting Input : 826 - parks are not suitable for walking around.\n",
      "Predicted Output: The dungeon contains three levels of\n",
      "\n",
      "Predicting Input : 827 - People usually run without shoes.\n",
      "Predicted Output: You need shoes for a place to live,\" said Winston Pason, 34, an Iowa\n",
      "\n",
      "Predicting Input : 828 - People usually wear clothes made of wood.\n",
      "Predicted Output: Inhabitants often eat plants\n",
      "\n",
      "Predicting Input : 829 - People can't send letters with pigeons.\n",
      "Predicted Output: He says pigeons, which are often poisoned and stick to their tails and tend to eat\n",
      "\n",
      "Predicting Input : 830 - People should eat a lot of sugar.\n",
      "Predicted Output: You should eat lots of organic food. Why is your diet so fatty?\n",
      "\n",
      "Predicting Input : 831 - it's easy for people without hearts to survive.\n",
      "Predicted Output: As we extend our kennel on Pride month and reflect on how hard it is to\n",
      "\n",
      "Predicting Input : 832 - humans are usually casual to wild tigers.\n",
      "Predicted Output: But when tigers seem a little too wild for you to enjoy, take care and don't\n",
      "\n",
      "Predicting Input : 833 - Snakes usually eat elephants.\n",
      "Predicted Output: If you eat venison, you will die. It's not typically eaten at the dinner\n",
      "\n",
      "Predicting Input : 834 - Most roads have no trees beside them.\n",
      "Predicted Output: You only touch what the door or window touches.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 835 - humans breathe only with their noses.\n",
      "Predicted Output: And what happens when a natural looking object bumps into another human and crashes into it? And\n",
      "\n",
      "Predicting Input : 836 - People can use computers in the shower.\n",
      "Predicted Output: There are 600 fields, which are able to process photos and cannot sell them\n",
      "\n",
      "Predicting Input : 837 - humans usually don't eat with their mouths.\n",
      "Predicted Output: Chickens eat their flesh by eating grass with their heads.\n",
      "\n",
      "Predicting Input : 838 - Iron products are usually not sensitive to rain.\n",
      "Predicted Output: All rainstorms use a unique wind engine called a compact wind surface or brush. The engine\n",
      "\n",
      "Predicting Input : 839 - People shouldn't feed monkeys bananas.\n",
      "Predicted Output: What do monkeys eat, like beef? Meat. No doubt that sounds delicious.\n",
      "\n",
      "Predicting Input : 840 - People usually wear sunglasses on rainy days.\n",
      "Predicted Output: What do YOU think? Do you want children to wear sunglasses in rainforests? Are\n",
      "\n",
      "Predicting Input : 841 - People usually don't knock at the door before entering.\n",
      "Predicted Output: You need time to be ready to turn things on for some things or others.\n",
      "\n",
      "Predicting Input : 842 - People usually grab crab forceps.\n",
      "Predicted Output: Stacey can feel heavy. (Until she has experience\n",
      "\n",
      "Predicting Input : 843 - Grass usually grows in winter.\n",
      "Predicted Output: Shaded veggies should be kept in their sunny area for full sun and winter and they should\n",
      "\n",
      "Predicting Input : 844 - I am so relaxed that I do not know what to say.\n",
      "Predicted Output: I am able to hear. I can turn away. I am now afraid that it is too\n",
      "\n",
      "Predicting Input : 845 - we are going to watch the sunset tomorrow so you need to get up early.\n",
      "Predicted Output: You may notice that a lot of people prefer to wait before going out for work. That\n",
      "\n",
      "Predicting Input : 846 - a chair can be used to lie.\n",
      "Predicted Output: There are plenty of \"standing tables\" available to practice sitting. Many homes have space for\n",
      "\n",
      "Predicting Input : 847 - doing housework is a good way to entertain ourselves.\n",
      "Predicted Output: 22. Lovestrides and moviegoers are learning more about movie theatres. (\n",
      "\n",
      "Predicting Input : 848 - I need water because I am hungry.\n",
      "Predicted Output: In fact I think a place that has garbage is probably also a trash dump.\n",
      "\n",
      "Predicting Input : 849 - I love the play very much because it is terrible.\n",
      "Predicted Output: 20. Tattoos, shown in one scenario. In other time often bandages are seen\n",
      "\n",
      "Predicting Input : 850 - this company makes a lot of money because of its outdated equipment.\n",
      "Predicted Output: But if someone wants a space for his telescope, they should get one they can use and\n",
      "\n",
      "Predicting Input : 851 - there are many kinds of plants in Antarctica.\n",
      "Predicted Output:  I can't tell which ones, but I can guess that many). I once walked into a southern\n",
      "\n",
      "Predicting Input : 852 - Cosmetics can make me more ugly.\n",
      "Predicted Output: There are dozens of not-so-nice things we can do for ourselves. I've\n",
      "\n",
      "Predicting Input : 853 - Basketball is square.\n",
      "Predicted Output: ANAKEEPIA'S KENT SUN\n",
      "\n",
      "Predicting Input : 854 - the mirror is a part of the computer.\n",
      "Predicted Output: You could blow your time at the ends of the mirror's length by blowing it at the\n",
      "\n",
      "Predicting Input : 855 - this novel is too thin to finish quickly.\n",
      "Predicted Output: If you wish to be sure that you enjoy an entertaining reading, please visit our site for\n",
      "\n",
      "Predicting Input : 856 - People can't make up in the bathroom.\n",
      "Predicted Output: But if those objects in the bathroom are cursed, then their images should also be altered.\n",
      "\n",
      "Predicting Input : 857 - People can talk loudly in hospitals.\n",
      "Predicted Output: But when visiting hospital, any kind of orthopedic joint or thigh joint will not help\n",
      "\n",
      "Predicting Input : 858 - You can see the living dragon.\n",
      "Predicted Output: You can hear it, but it's distant.\n",
      "\n",
      "Predicting Input : 859 - Soldiers are usually very weak.\n",
      "Predicted Output: They were reluctant to be involved in battle until they felt 'tricked'.\n",
      "\n",
      "Predicting Input : 860 - People shouldn't drink tea.\n",
      "Predicted Output: As more restaurants consider a ban on burpees, customers will also stop drinking beer.\n",
      "\n",
      "Predicting Input : 861 - People should tell strangers the address of their families.\n",
      "Predicted Output: If you hear someone in the wrong place when you come in a hotel room, or when\n",
      "\n",
      "Predicting Input : 862 - You can touch the software with your hands.\n",
      "Predicted Output: When you wear shoes, you can feel skin to skin contact. They feel important.\n",
      "\n",
      "Predicting Input : 863 - You can save drowning people if you can't swim.\n",
      "Predicted Output: You can drown people in case of heatstroke.\n",
      "\n",
      "Predicting Input : 864 - People should ignore their mistakes.\n",
      "Predicted Output: You should ignore them, have your case evaluated\n",
      "\n",
      "Predicting Input : 865 - People usually see lightning on sunny days.\n",
      "Predicted Output: How bright is lightning?\n",
      "\n",
      "Predicting Input : 866 - People go to school for sleeping.\n",
      "Predicted Output: Strict Minimum Laws, Pre-Exposure to Drugs\n",
      "\n",
      "Predicting Input : 867 - Doctors don't need to write papers.\n",
      "Predicted Output: Chronic hypertension has a five-year lag in diagnosis of heart disease and still may take\n",
      "\n",
      "Predicting Input : 868 - People usually drive tanks at sea.\n",
      "Predicted Output: You buy your windows at the nearest grocery store.\n",
      "\n",
      "Predicting Input : 869 - People should destroy the collections in museums.\n",
      "Predicted Output: If you destroy them, you will have irrelevancy in the educational environment.\n",
      "\n",
      "Predicting Input : 870 - People usually hate pet dogs.\n",
      "Predicted Output: As with pets, a dog's nice fluffy white coat will make some dogs happy.\n",
      "\n",
      "Predicting Input : 871 - People can usually see chickens in the sky.\n",
      "Predicted Output: You can wear leather, see things in cloth, pick up footballs with your hands.\n",
      "\n",
      "Predicting Input : 872 - People usually hate to travel.\n",
      "Predicted Output: You're able to spend free time with family\n",
      "\n",
      "Predicting Input : 873 - People can't judge the direction by the sun.\n",
      "Predicted Output: He put together an on-line map showing the regional regions of eastern Illinois.\n",
      "\n",
      "Predicting Input : 874 - Businessmen are usually rude to their guests.\n",
      "Predicted Output: When people interact with a partner, they express their desires and concerns about how they will respond\n",
      "\n",
      "Predicting Input : 875 - I can pass the shopping mall easily because it is crowded.\n",
      "Predicted Output: You can purchase clothes, gas, food from the mall's store when not in the lobby\n",
      "\n",
      "Predicting Input : 876 - we cannot live without wine.\n",
      "Predicted Output: 20/31 -Sung by / Lung cologne so often contains scenting and..\n",
      "\n",
      "Predicting Input : 877 - children are encouraged to be dishonest.\n",
      "Predicted Output: But when interviewed by a senior official in Serbia, Abdo said they always received that kind\n",
      "\n",
      "Predicting Input : 878 - Love is a terrible feeling.\n",
      "Predicted Output: You should seek control, not control. Take good care of yourself and others.\n",
      "\n",
      "Predicting Input : 879 - water has the effect of making things dry.\n",
      "Predicted Output: There are examples of a tool that can harvest water from water: mealworms.\n",
      "\n",
      "Predicting Input : 880 - we can drive without a driver's license.\n",
      "Predicted Output: There are numerous requirements, such as those outlined in California state law and requirements for having an\n",
      "\n",
      "Predicting Input : 881 - People usually wear clothes when they take a bath.\n",
      "Predicted Output: And what happens when a woman has to massage her breasts for as long as she can and\n",
      "\n",
      "Predicting Input : 882 - People should wash clean clothes.\n",
      "Predicted Output: You should wash children's clothing.\n",
      "\n",
      "Predicting Input : 883 - People usually drink river water.\n",
      "Predicted Output: There are enormous rivers, which are small lakes. So if you visit them on a river\n",
      "\n",
      "Predicting Input : 884 - People hardly like gold.\n",
      "Predicted Output: As we discussed here, there is also competition in exchange for gold and silver. In common\n",
      "\n",
      "Predicting Input : 885 - Students usually do their homework in bed.\n",
      "Predicted Output: To access Coach Scholar, just follow the prompts to enter his name and password.\n",
      "\n",
      "Predicting Input : 886 - Wheat is of no use to human beings.\n",
      "Predicted Output: You know… actually, you just should eat this shit.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 887 - Humans usually like eating mice.\n",
      "Predicted Output: But what happens when a cold person simply brushes his teeth with a vacuum cleaner?\n",
      "\n",
      "Predicting Input : 888 - Children should play with knives.\n",
      "Predicted Output: You should teach children a love of table manners.\n",
      "\n",
      "Predicting Input : 889 - People don't need to use air conditioning on a hot day.\n",
      "Predicted Output: There are spaces with a touch of window decorations to choose from. They don't need to\n",
      "\n",
      "Predicting Input : 890 - People are usually happy when they fall down.\n",
      "Predicted Output: But some shops don't open for business hours. So when you walk down the street for\n",
      "\n",
      "Predicting Input : 891 - People need to work on holidays.\n",
      "Predicted Output: In order for access to camp for 10 weeks, three months and 30 days a year:\n",
      "\n",
      "Predicting Input : 892 - People usually read books when they walk.\n",
      "Predicted Output: If you're not rich enough to work for\n",
      "\n",
      "Predicting Input : 893 - People usually don't need condiments while cooking.\n",
      "Predicted Output: What do You Really do if you're Eating 'Bad Microwaves'?\n",
      "\n",
      "Predicting Input : 894 - People usually feed tiger grass in zoos.\n",
      "Predicted Output: In China tiger grasses are too old for use as shelter. Some habitat is too weak\n",
      "\n",
      "Predicting Input : 895 - People can usually see snow in the summer.\n",
      "Predicted Output: When they wear glasses, they can see darkness.\n",
      "\n",
      "Predicting Input : 896 - Some people were born on February thirtieth.\n",
      "Predicted Output: He put himself into a situation and did things that many people would never do.\n",
      "\n",
      "Predicting Input : 897 - People can open locked boxes directly.\n",
      "Predicted Output: An MP hopes he is going to work harder to ensure more people don't take their chances\n",
      "\n",
      "Predicting Input : 898 - It's hard for people to see their neighbors.\n",
      "Predicted Output: The fighting broke out in the North End Village and dropped people off from vehicles in the Water\n",
      "\n",
      "Predicting Input : 899 - People usually pack food on hands.\n",
      "Predicted Output: You need energy and a place to grow vegetables. You need to eat fruit and vegetables in\n",
      "\n",
      "Predicting Input : 900 - people usually marry the one they hate.\n",
      "Predicted Output: There are literally hundreds of books and articles dedicated to studying hate to adults and children. One\n",
      "\n",
      "Predicting Input : 901 - People usually go to the gym for eating.\n",
      "Predicted Output: But there were days of lack of activity during the morning or night when day men had other\n",
      "\n",
      "Predicting Input : 902 - People usually go fishing in sand.\n",
      "Predicted Output: If you catch fish, you can help Fish at Sea by providing fishing boats and dog parks\n",
      "\n",
      "Predicting Input : 903 - The temperature during the day is usually lower than at night.\n",
      "Predicted Output: When donkeys watch, they would seem startled. They need to remember what they are looking\n",
      "\n",
      "Predicting Input : 904 - People usually dry their bodies with plastic.\n",
      "Predicted Output: And what happens when a dog can't sniff the dirt or simply loses control of his tail\n",
      "\n",
      "Predicting Input : 905 - We can use the net to catch water.\n",
      "Predicted Output: There are plenty of orgy services that involve using various objects to obtain water.\n",
      "\n",
      "Predicting Input : 906 - People can use calculators to make telephone calls.\n",
      "Predicted Output: Suppose you want to figure out what's going\n",
      "\n",
      "Predicting Input : 907 - The Earth revolves around the moon.\n",
      "Predicted Output: As we orbit it, you can see mountains in the sky. On Earth, you can\n",
      "\n",
      "Predicting Input : 908 - Beer contains gasoline.\n",
      "Predicted Output: For less indulgent, high-temperature plays can make for warm meals.\n",
      "\n",
      "Predicting Input : 909 - elephants are small animals.\n",
      "Predicted Output: But when horns could be seen in the cavity, people got pretty upset and started taking photos\n",
      "\n",
      "Predicting Input : 910 - we can see aliens in the zoo.\n",
      "Predicted Output: The eye's social instinct alone should have brought\n",
      "\n",
      "Predicting Input : 911 - the stone is a kind of food.\n",
      "Predicted Output: In fact I'm a huge fan of cider. They're my childhood beverages. I always\n",
      "\n",
      "Predicting Input : 912 - Models are usually very short.\n",
      "Predicted Output: Shouts always begin the same if you shout a bunch of times in open space.\n",
      "\n",
      "Predicting Input : 913 - Books are made of stones.\n",
      "Predicted Output: On my expedition I was trying to find dirt and dirt around the continent and don't look\n",
      "\n",
      "Predicting Input : 914 - my bag is full of chairs.\n",
      "Predicted Output: This place requires two. I only want something to stay at the fifth floor.\n",
      "\n",
      "Predicting Input : 915 - the movie is so interesting that I fall asleep.\n",
      "Predicted Output: But there are still a lot of things left to talk about. Let's start with some\n",
      "\n",
      "Predicting Input : 916 - this land is so barren that trees grow well.\n",
      "Predicted Output: If you wish to be able to grow tomatoes in October and November you should pay for them\n",
      "\n",
      "Predicting Input : 917 - this article is so long that I can read it easily.\n",
      "Predicted Output: For those unaware of the language of American Patent law, there is no shorter period of time\n",
      "\n",
      "Predicting Input : 918 - it is safe to swim in the sea.\n",
      "Predicted Output: There are boat lanes, which are open only to explore and have narrow strips of land for\n",
      "\n",
      "Predicting Input : 919 - The soldiers were very patriotic and they attack their country.\n",
      "Predicted Output: The soldiers fought against a military field in Idlib\n",
      "\n",
      "Predicting Input : 920 - I live in the city for its fresh air.\n",
      "Predicted Output: Read more\n",
      "\n",
      "Predicting Input : 921 - he is so inexperienced that he finished the task quickly.\n",
      "Predicted Output: In my observation of a higher level that enters the realm of mind there are many things that\n",
      "\n",
      "Predicting Input : 922 - we need to be friendly to our enemies.\n",
      "Predicted Output: If you spend too many hours with your minions, their HP will decrease by 10% for\n",
      "\n",
      "Predicting Input : 923 - it is hopeless for you to be admitted by the famous university because you are so outstanding.\n",
      "Predicted Output: But what if you got lucky in Pinner\n",
      "\n",
      "Predicting Input : 924 - Masters need to serve their servants.\n",
      "Predicted Output: You know something about a brother? I wouldn't mind someone know what they see.\n",
      "\n",
      "Predicting Input : 925 - my house is a public place.\n",
      "Predicted Output: This week @MuslimAwardTheForce kicks off our annual London Marathon Camp in which U\n",
      "\n",
      "Predicting Input : 926 - most plants prefer to grow in the shadow.\n",
      "Predicted Output: For large specimens of a plant, you must use paper towel to wrap your plant to prevent\n",
      "\n",
      "Predicting Input : 927 - When the room is very bright you want to turn on the light.\n",
      "Predicted Output: For water evaporators, you can use boiling water (water with melting temperatures and no ash\n",
      "\n",
      "Predicting Input : 928 - you'd better not wash your hands after using the toilet.\n",
      "Predicted Output: As with flush toilets, if you're unsure of how to allow yourself to wet it it\n",
      "\n",
      "Predicting Input : 929 - You want to wear short sleeves outside in winter.\n",
      "Predicted Output: You want your shoes to have a longer lifespan.\n",
      "\n",
      "Predicting Input : 930 - Diesel cars need no oil to run.\n",
      "Predicted Output: There are 85 total barge miles in Delaware. One day a diesel horse would get 8\n",
      "\n",
      "Predicting Input : 931 - The more blood you lose the healthier you'll be.\n",
      "Predicted Output: For those wanting to be sure in their disposition, check out this Irish Wikipedia article: \"\n",
      "\n",
      "Predicting Input : 932 - Drunk driving is a good habit.\n",
      "Predicted Output: There are plenty of unhealthful driving offenses in Australia.\n",
      "\n",
      "Predicting Input : 933 - A magnet can attract aluminum.\n",
      "Predicted Output: For example: If a bottle of beer bottles are damaged by an earthquake that hits the glass\n",
      "\n",
      "Predicting Input : 934 - Papers can make walls.\n",
      "Predicted Output: So what happens when a song's ending lyrics are divided into two sections and only one section\n",
      "\n",
      "Predicting Input : 935 - We can brush our teeth with body wash.\n",
      "Predicted Output: You will lose weight, but you will lose your teeth.\n",
      "\n",
      "Predicting Input : 936 - he used his mind to open the door.\n",
      "Predicted Output: Grimoire Journal, Game or Yearbooks?\n",
      "\n",
      "Predicting Input : 937 - he used water to cook a meal.\n",
      "Predicted Output: Bobby liked to eat soup and often loved\n",
      "\n",
      "Predicting Input : 938 - he drinks a cup of oil.\n",
      "Predicted Output: The oil replaces the mineral energy in your stomach. \"For\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 939 - she traveled to the Antarctic on foot.\n",
      "Predicted Output: It was winter.\"Bitchy from Antarctica. NO!!! It was winter.\"\n",
      "\n",
      "Predicting Input : 940 - I love eating books.\n",
      "Predicted Output: But if your wish to read an original manuscript is too powerful, well then you will need\n",
      "\n",
      "Predicting Input : 941 - after dinner he prepared the dishes.\n",
      "Predicted Output: The Jewish Household salute and big spread brings me attention to the excitement I feel in my\n",
      "\n",
      "Predicting Input : 942 - he squeezed some lemonade from apples.\n",
      "Predicted Output: The electric stripe saw-dog has been hospitalized in stable condition. Aftercare was done on\n",
      "\n",
      "Predicting Input : 943 - he drank some hot water to cool down.\n",
      "Predicted Output: In my sketch with a female friend, Charles said he makes butchers grow food in his\n",
      "\n",
      "Predicting Input : 944 - early in the morning he went out for dinner.\n",
      "Predicted Output: \"Liya came out of her cell on the morning\n",
      "\n",
      "Predicting Input : 945 - she ate ice cream to relieve the pain.\n",
      "Predicted Output: It was Cold Bad, so he'd eaten Waffles and small fries at Jau J\n",
      "\n",
      "Predicting Input : 946 - the waiter paid the bill of a delicious meal.\n",
      "Predicted Output: We never complained about a sweet local food meal. There was no suggestion that we would change\n",
      "\n",
      "Predicting Input : 947 - I love to drink soda of 20-year history.\n",
      "Predicted Output: But if there's a whole lot of maple syrup on your counter then just give it to\n",
      "\n",
      "Predicting Input : 948 - as for lunch he took some pills.\n",
      "Predicted Output: Weird spiders don't sit on her ribs. She comes on top of them. It\n",
      "\n",
      "Predicting Input : 949 - he brushed his teeth at school.\n",
      "Predicted Output: Spencer agreed, but he too struggled to earn his school transcripts. \"I feel\n",
      "\n",
      "Predicting Input : 950 - his family drives a train to go on a trip.\n",
      "Predicted Output: He says she wanted a quick break and walked to work and said she needed to get back\n",
      "\n",
      "Predicting Input : 951 - the desk is made of wool.\n",
      "Predicted Output: 14:56 +- 20:22:{UUID} is Name added to the Top\n",
      "\n",
      "Predicting Input : 952 - the carpet is made of wood.\n",
      "Predicted Output: If you lose your shawl in transit, your shoes will drop off the outside of\n",
      "\n",
      "Predicting Input : 953 - shoes made of artificial leather are expensive.\n",
      "Predicted Output: You would spend too much time on your trek to avoid spending your dollars on debt. A\n",
      "\n",
      "Predicting Input : 954 - he came here tomorrow.\n",
      "Predicted Output: The 70 miles were a little too long at the bottom of the stairs for me to walk\n",
      "\n",
      "Predicting Input : 955 - rocks had fallen from beneath.\n",
      "Predicted Output: The estimated magnitude of the storm is much smaller than what has been forecast by other global weather\n",
      "\n",
      "Predicting Input : 956 - she threw clean clothes into a washing machine.\n",
      "Predicted Output: In then background she and her father would accompany their daughter to Migrant Jobforce in Oklahoma\n",
      "\n",
      "Predicting Input : 957 - people tell their future to the next generation.\n",
      "Predicted Output: As part of their \"humanized\" persona, Singleton has repeatedly worn what he calls\n",
      "\n",
      "Predicting Input : 958 - ancient people made tools of metal.\n",
      "Predicted Output: He told CBC News' Peter James that indigenous people around the world came up with this tool\n",
      "\n",
      "Predicting Input : 959 - I like to chase after bees.\n",
      "Predicted Output: If you notice most of the English language sucks, don't think about trying to book on\n",
      "\n",
      "Predicting Input : 960 - he kicks a basketball on the ground.\n",
      "Predicted Output: In my classroom I was probably the only teacher in our group to notice when we were late\n",
      "\n",
      "Predicting Input : 961 - she read a fairy tale on the newspaper.\n",
      "Predicted Output: We've talked about a lot of things recently, so we don't post my works here\n",
      "\n",
      "Predicting Input : 962 - he put on his eyeglasses and listened to the radio.\n",
      "Predicted Output: A potential gamble for a private company\n",
      "\n",
      "Predicting Input : 963 - he ate cookies after going to bed.\n",
      "Predicted Output: He'd hated losing, but he didn't want to lose. And what about his cat\n",
      "\n",
      "Predicting Input : 964 - I opened my eyes and breathed heavily.\n",
      "Predicted Output: A moment later my chest and legs were completely submerged in water.\n",
      "\n",
      "Predicting Input : 965 - he married his dog.\n",
      "Predicted Output: He told Jackie \"I used to play tennis and drink beer in school,\" but he didn\n",
      "\n",
      "Predicting Input : 966 - he invited his girlfriend to his wedding.\n",
      "Predicted Output: If you haven't had the chance to taste the dishes first, head over to 2 Pe\n",
      "\n",
      "Predicting Input : 967 - we need a month to get to the airport.\n",
      "Predicted Output: If you aren't a geologist, usually you don't work at pet stores. If\n",
      "\n",
      "Predicting Input : 968 - the company starts offices all over the building.\n",
      "Predicted Output: — Chicago pastor Anthony Tullos was barred from landing at the venue because he was accused\n",
      "\n",
      "Predicting Input : 969 - my family lives in a large box.\n",
      "Predicted Output: We say goodbye to a family and they kiss and hug us. They cry and us so\n",
      "\n",
      "Predicting Input : 970 - we said goodbye and met.\n",
      "Predicted Output: The kids shook hands, then went to dance. They showed up around 9:30am\n",
      "\n",
      "Predicting Input : 971 - I often walk my bag after dinner.\n",
      "Predicted Output: In fact I often have to find a shuttle to walk my bag after dinner.\n",
      "\n",
      "Predicting Input : 972 - my arm hurts and it's very comfortable.\n",
      "Predicted Output: We take pride in the fact that we carry on building our company.\" That's more than\n",
      "\n",
      "Predicting Input : 973 - I'm looking for a job as a mother.\n",
      "Predicted Output: The 35 year old, who was born at Aydan in Myanmar's Volai province\n",
      "\n",
      "Predicting Input : 974 - he got into his car and started to row.\n",
      "Predicted Output: A hot stove above a wall (the candles is necessary for such prayer).\n",
      "\n",
      "Predicting Input : 975 - running makes me feel full.\n",
      "Predicted Output: You know those butt-tying benefits because you feel good and happy when you know you\n",
      "\n",
      "Predicting Input : 976 - she ate 30 hamburgers for lunch.\n",
      "Predicted Output: But when Chuck signed a contract that actually obliged him to drive to Columbia three times a week\n",
      "\n",
      "Predicting Input : 977 - he rode a bike in the river.\n",
      "Predicted Output: The baby deer got a bad break in four days and after that day had more than enough\n",
      "\n",
      "Predicting Input : 978 - he loves skating on water.\n",
      "Predicted Output: This evergreen trees, which are actually algae, grow not on stone but on hard earth\n",
      "\n",
      "Predicting Input : 979 - he started to work at the age of 2.\n",
      "Predicted Output: 5, but when she became a senior in high school, she began to protest against her father's\n",
      "\n",
      "Predicting Input : 980 - there are sixty teachers in this class.\n",
      "Predicted Output:  I can't give away just a few's email addresses. Just name it!\"); this can also\n",
      "\n",
      "Predicting Input : 981 - she turned on the light to sleep.\n",
      "Predicted Output: The lines broke off, and he felt miserable. He heard the hammer hammer my body hit\n",
      "\n",
      "Predicting Input : 982 - the farmer planted corn on his head.\n",
      "Predicted Output: In 1892 an Eiffel Tower antenna was awarded to an amateur astronomer by the Rev\n",
      "\n",
      "Predicting Input : 983 - leaves became green in autumn.\n",
      "Predicted Output: But when Judy agreed to stand on the sofa to sleep on the sofa she didn't try\n",
      "\n",
      "Predicting Input : 984 - we missed the first bus and had to take a taxi.\n",
      "Predicted Output: The movie pulls off, can't get beyond the fifth floor. Then there's a minute\n",
      "\n",
      "Predicting Input : 985 - she made me a cup of cookies.\n",
      "Predicted Output: I took these cookies, but I think my research showed it was highly entertaining.\n",
      "\n",
      "Predicting Input : 986 - I bought snakes from the supermarket.\n",
      "Predicted Output: In 2016 just three-quarters of your snakes are infected with fungal infections.\n",
      "\n",
      "Predicting Input : 987 - the apple grew under the tree.\n",
      "Predicted Output: Kahkar was flat with his teeth only up to his\n",
      "\n",
      "Predicting Input : 988 - food can be cooked in a van.\n",
      "Predicted Output: For women whose breasts are too big to accommodate the muscles of the vagina and vagina's inner\n",
      "\n",
      "Predicting Input : 989 - he wrote a dictionary yesterday.\n",
      "Predicted Output: from The Huffington Post\n",
      "\n",
      "Predicting Input : 990 - he lost all his money in donating.\n",
      "Predicted Output: He told Politico that a former member of Congress will fight for his seat when he can't\n",
      "\n",
      "Predicting Input : 991 - he brought an exhibit from the museum.\n",
      "Predicted Output: In 2017 we saw a common pubic nail be colored by Grippelsdottir\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 992 - he scored the losing goal.\n",
      "Predicted Output: Injured again as Lineau tries to stay out of\n",
      "\n",
      "Predicting Input : 993 - we will watch the game on the radio.\n",
      "Predicted Output: If you haven't had the chance to listen to our radio show yet, this is it\n",
      "\n",
      "Predicting Input : 994 - he fell asleep in the tree.\n",
      "Predicted Output: We actually rushed over, but he got asleep in the tree. We ran over to him\n",
      "\n",
      "Predicting Input : 995 - drivers should not drive when they are energetic.\n",
      "Predicted Output: Clarity Manual shifter (clarity clutch) always starts with center needle of the wheel\n",
      "\n",
      "Predicting Input : 996 - I woke up and found an elephant in my pajama.\n",
      "Predicted Output: The black bears feed on wildlife\n",
      "\n",
      "Predicting Input : 997 - the new product catches readers' ear.\n",
      "Predicted Output: You may notice that, more than three decades after Apple introduced the iPod touch, it has\n",
      "\n",
      "Predicting Input : 998 - they reached the top and climbed the mountain.\n",
      "Predicted Output: In between jumps he and his friends also drank one ice cream and ate pizza.\n",
      "\n",
      "Predicting Input : 999 - comb can cut hair.\n",
      "Predicted Output: There are links in a text that say ingredients are determined by your immune system.\n",
      "\n",
      "Predicting Input : 1000 - we were told to keep noisy in the library.\n",
      "Predicted Output: In each bedroom we are required to keep silent. At least not ever in any public place\n",
      "\n",
      "Predicting Input : 1001 - the artist painted the handle to open the door.\n",
      "Predicted Output: The handle lets you and his room where someone or something should be picked up in the morning\n",
      "\n",
      "Predicting Input : 1002 - the security officer robbed the bank.\n",
      "Predicted Output: It was stolen from a bank in Israel last year and since then police have done their best\n",
      "\n",
      "Predicting Input : 1003 - he passed the course three times.\n",
      "Predicted Output: But there were four to five days in December. At this time his birthday was at 11\n",
      "\n",
      "Predicting Input : 1004 - he put the money into his hat.\n",
      "Predicted Output: Vanquishing him was easy. The company would\n",
      "\n",
      "Predicting Input : 1005 - the new movie of the singer is released.\n",
      "Predicted Output: …...said its a development of the harmony of song for that purpose.\"\n",
      "\n",
      "Predicting Input : 1006 - the police officer kissed the robber in the leg.\n",
      "Predicted Output: In 'Naruto' (2015) Sakura's dad finally gets clean after him. The\n",
      "\n",
      "Predicting Input : 1007 - he saw a tiger at the hospital.\n",
      "Predicted Output: The village commander told a local news agency at the time that the tiger could not be killed\n",
      "\n",
      "Predicting Input : 1008 - he swam in the fish tank.\n",
      "Predicted Output: The fish pond had a simple boathouse. Barbed wire fence posts had been installed\n",
      "\n",
      "Predicting Input : 1009 - he vowed to love her for the rest of day.\n",
      "Predicted Output: Valedictos' father Axtimo Nespoli was convicted in December of stealing\n",
      "\n",
      "Predicting Input : 1010 - he skipped the final exam to pass the course.\n",
      "Predicted Output: But when Rahab, from his family home in Tehran, took his flight to America in\n",
      "\n",
      "Predicting Input : 1011 - he felt depressed at the good news.\n",
      "Predicted Output: I actually stayed up a lot of that afternoon\n",
      "\n",
      "Predicting Input : 1012 - the writer won an award for his new movie.\n",
      "Predicted Output: He found silence there, but he still cried. He felt so helpless that he let out\n",
      "\n",
      "Predicting Input : 1013 - he took a photo of all human beings.\n",
      "Predicted Output: 11:42 AM, Peabody Clinic for Emergency Medicine on Palm Avenue is open 2\n",
      "\n",
      "Predicting Input : 1014 - thank you for your selfish action.\n",
      "Predicted Output: You want him to be strong. I wanted you to know that we still need that.\"\n",
      "\n",
      "Predicting Input : 1015 - the reporter awarded the Olympic champion.\n",
      "Predicted Output: 10. Comedian, Icecast has donated more than $9 million to teacher's aid\n",
      "\n",
      "Predicting Input : 1016 - the impressive speech was interrupted by hiss.\n",
      "Predicted Output: coxxon oil, which is being unfairly shut down by the CIA was given a global\n",
      "\n",
      "Predicting Input : 1017 - he struggled to make his nightmare come true.\n",
      "Predicted Output: The 49ers allowed a total of 50 receptions on 46 targets in practice last week, which\n",
      "\n",
      "Predicting Input : 1018 - he changed his job due to high salary.\n",
      "Predicted Output: But if Hong Kong-based China InterAsia Group wants to build its famous rail line and\n",
      "\n",
      "Predicting Input : 1019 - I got a stomachache and went to the dentist's.\n",
      "Predicted Output: Johnson was headed to the dentist with his mother and\n",
      "\n",
      "Predicting Input : 1020 - it took 7 days to receive the email.\n",
      "Predicted Output: We've checked out a few other items already, most notably the Walmart packaging; it would\n",
      "\n",
      "Predicting Input : 1021 - he ate a cake to celebrate her birthday.\n",
      "Predicted Output: He got beaten up, would not pay taxes, killed his wife and injured his personal safety\n",
      "\n",
      "Predicting Input : 1022 - he escaped the bullet and died.\n",
      "Predicted Output: But she suffered serious head injuries to her lungs. She could not escape her injured body and\n",
      "\n",
      "Predicting Input : 1023 - the chalk read an interesting story to the students.\n",
      "Predicted Output: We've talked about a lot of things recently, so we've kept these questions in mind\n",
      "\n",
      "Predicting Input : 1024 - the treasure is inherited from his descendants.\n",
      "Predicted Output: This means there are a lot of dead sailors in your pool. When they die, they\n",
      "\n",
      "Predicting Input : 1025 - he threw a plate for his dog to catch.\n",
      "Predicted Output: In fact, 17% of all dog retriever catches were run this season.\n",
      "\n",
      "Predicting Input : 1026 - she put off running shoes before jogging.\n",
      "Predicted Output: The dream initially seemed a bit different: twins with loose fur and flexible limbs were both beautiful\n",
      "\n",
      "Predicting Input : 1027 - he continued to read the book to the first page.\n",
      "Predicted Output: Taught yourself any of the following things before you wrote it: \"Make use of every\n",
      "\n",
      "Predicting Input : 1028 - students like it when teachers let them go later.\n",
      "Predicted Output: If something has happened and you let them go the next\n",
      "\n",
      "Predicting Input : 1029 - the big hand on a watch shows the day.\n",
      "Predicted Output: If you drop your work you go to bed. If you work too late you will fall\n",
      "\n",
      "Predicting Input : 1030 - he maintained balance and fell to the ground.\n",
      "Predicted Output: The chairlift train, which was started three days earlier, has seven horses and has less\n",
      "\n",
      "Predicting Input : 1031 - he bought his new coat at the grocery store.\n",
      "Predicted Output: The crowd applauded.\"A black man can wear a coat.\"\n",
      "\n",
      "Predicting Input : 1032 - the traffic light turned white from red.\n",
      "Predicted Output: The driver recalled him, no matter what conditions. The driver would usually drive by the traffic\n",
      "\n",
      "Predicting Input : 1033 - she smells terrible after a shower.\n",
      "Predicted Output: If you wish to be added to the Dangerous Sluts Club of Manhattan this week, feel\n",
      "\n",
      "Predicting Input : 1034 - he put on a t-shirt and go skiing.\n",
      "Predicted Output: The hospital bathroom has a clean place in front of it\n",
      "\n",
      "Predicting Input : 1035 - he received many complaints at his birthday party.\n",
      "Predicted Output: This week Galati, who's also Iranian, learned he was pregnant by one of his\n",
      "\n",
      "Predicting Input : 1036 - he surfed the sea to check his email.\n",
      "Predicted Output: In every wedding – a private one, perhaps? – more than 30 couples get married at\n",
      "\n",
      "Predicting Input : 1037 - I gave him some review on his future career.\n",
      "Predicted Output: Kieros was mostly happy that Peter Ryan would not take\n",
      "\n",
      "Predicting Input : 1038 - she planted a tree in her bedroom.\n",
      "Predicted Output: In another shocking incident, this time in Vale, CA, an overweight male victim was sexually\n",
      "\n",
      "Predicting Input : 1039 - the policeman put the witness in jail.\n",
      "Predicted Output: The crowd chanted \"Bands of violence!\" in Spanish and then shouting \"Don't stop\n",
      "\n",
      "Predicting Input : 1040 - he printed out the form and crossed out the blanks.\n",
      "Predicted Output: and Posted to blog with his involvement on May 7th\n",
      "\n",
      "Predicting Input : 1041 - the turtle broke the eggs it laid on land.\n",
      "Predicted Output: In order for him to understand the meaning of the turtle's name he needs to know what\n",
      "\n",
      "Predicting Input : 1042 - the shipowner adjusted the wind to go faster.\n",
      "Predicted Output: The neighbor complained that a guy was talking loudly in his hat. He couldn't move his\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1043 - he extinguished the cigarette and began to smoke.\n",
      "Predicted Output: The officer lifted his mannequin and yelled: \"Please don't laugh at me.\"\n",
      "\n",
      "Predicting Input : 1044 - the bride got a bouquet after the wedding.\n",
      "Predicted Output: It was mainly made to represent the birth anniversary of France's first queen to appear in an\n",
      "\n",
      "Predicting Input : 1045 - she put her earrings back to the seasoning box.\n",
      "Predicted Output: Eager as she is to look at photos of baby clothes, Musk dropped them and placed\n",
      "\n",
      "Predicting Input : 1046 - a short lunch break from work makes me eat slowly.\n",
      "Predicted Output: If you spend much of your time off playing game day and your cognitive abilities don't improve\n",
      "\n",
      "Predicting Input : 1047 - every time I wrote a reply when he received my letter.\n",
      "Predicted Output: This does not mean, though, that someone has made me feel unsafe or uncomfortable. You\n",
      "\n",
      "Predicting Input : 1048 - they were not satisfied with each other and got a marriage.\n",
      "Predicted Output: We found girls who are close to us wanting to marry us. While most don't say\n",
      "\n",
      "Predicting Input : 1049 - the waiter gave him a bill to order food.\n",
      "Predicted Output: Jinn replies that a couple of days earlier, she found an advertisement for Aireng\n",
      "\n",
      "Predicting Input : 1050 - he lit a torch to see in the daytime.\n",
      "Predicted Output: ** Booths only. They have been vandalized by Turkish security forces so many times that\n",
      "\n",
      "Predicting Input : 1051 - I put all my stuff in the drawer to travel.\n",
      "Predicted Output: 10 tools is different than 10 beds; they are things\n",
      "\n",
      "Predicting Input : 1052 - he used a blow dryer before he washed his hair.\n",
      "Predicted Output: But there were already a lot of questions hanging over him as he struggled with his mother's\n",
      "\n",
      "Predicting Input : 1053 - he used chopsticks to eat soup.\n",
      "Predicted Output: If you eat cake, not ice cream or glass water, you'll probably want to avoid\n",
      "\n",
      "Predicting Input : 1054 - DVD can be used to search for information.\n",
      "Predicted Output: To learn how to do this, go through the directions for \"WordTest for Windows 10\n",
      "\n",
      "Predicting Input : 1055 - I ate watermelon and my teeth turned yellow.\n",
      "Predicted Output: The Curmudgeon, who was much annoyed with my silver hair and suspiciously young eyes\n",
      "\n",
      "Predicting Input : 1056 - she took off socks and washed her hands.\n",
      "Predicted Output: The condition worsened after a young man left behind a wooden container of cleaner fluid in the bathroom\n",
      "\n",
      "Predicting Input : 1057 - the cow jumped over the moon and escaped.\n",
      "Predicted Output: It was unlikely that a tweet from one anonymous person would actually make such an impact. An\n",
      "\n",
      "Predicting Input : 1058 - he left her a lot of bills in his will.\n",
      "Predicted Output: He didn't pay a full cent in subsidies. He doesn't hold office and his wife\n",
      "\n",
      "Predicting Input : 1059 - the little girl squatted and rang the bell.\n",
      "Predicted Output: Tobias startled a quick breath. 'Oh you should...\n",
      "\n",
      "Predicting Input : 1060 - he stretched out on the economy class of the flight.\n",
      "Predicted Output: If you wish to be part of the delegation, then you will need to sit in the\n",
      "\n",
      "Predicting Input : 1061 - he walked through the railway to another room.\n",
      "Predicted Output: Wearing shorts and tights, he paused to wave his hand at someone in the corner\n",
      "\n",
      "Predicting Input : 1062 - the ground was covered with sugar overnight.\n",
      "Predicted Output: In such situations the onus is on someone to defend themselves. Letting such a person\n",
      "\n",
      "Predicting Input : 1063 - the garden of flowers is in the middle of the table.\n",
      "Predicted Output: In so doing, the driver's ability to talk about how he leaves his business is damaged\n",
      "\n",
      "Predicting Input : 1064 - Tom wore pajamas for a job interview yesterday.\n",
      "Predicted Output: He says she asked to use the bathroom without his consent\n",
      "\n",
      "Predicting Input : 1065 - she put the guitar on the top of the mountain.\n",
      "Predicted Output: Yukimura tried a move on the rainbow to jump up the hill from the top of\n",
      "\n",
      "Predicting Input : 1066 - he ate some sleeping pills in order not to fall asleep.\n",
      "Predicted Output: But when Zhang Xiaolong came to Wang Sheng's room with loud cries, her\n",
      "\n",
      "Predicting Input : 1067 - she got a bad grade after a semester of hard study.\n",
      "Predicted Output: In another classroom it is called \"light bouncing\" when people can see light in the background\n",
      "\n",
      "Predicting Input : 1068 - her new song was popular on the theater screen.\n",
      "Predicted Output: For all her creativity, she never\n",
      "\n",
      "Predicting Input : 1069 - it's 6 o'clock and the new year is coming.\n",
      "Predicted Output: Stray Cats will be given a major redesign to reflect that. While Le Carre will\n",
      "\n",
      "Predicting Input : 1070 - the bird built a nest on the bottom of the tree.\n",
      "Predicted Output: Thirty bees appear, but not at night. That's what happens when you're closed\n",
      "\n",
      "Predicting Input : 1071 - she turned on the radio and the room became bright.\n",
      "Predicted Output: KELLEY P. JOHNSON: People may not respond very well to hearing\n",
      "\n",
      "Predicting Input : 1072 - he used a surfboard to surf the internet.\n",
      "Predicted Output: He says while tourists in China have been fascinated by fishing for years and calling up new buildings\n",
      "\n",
      "Predicting Input : 1073 - he pressed the accelerator to slow the car down.\n",
      "Predicted Output: Brigadier General of the British Army Edgar Swihart at Badátia reported\n",
      "\n",
      "Predicting Input : 1074 - I remember they built a snowman last summer.\n",
      "Predicted Output: Wanda Wheeler said, \"I don't know how they did it.\"\n",
      "\n",
      "Predicting Input : 1075 - he found he had more money after he paid the bill.\n",
      "Predicted Output: \"You saved me when you left my door open. How\n",
      "\n",
      "Predicting Input : 1076 - she walked to the gas station to refuel her car.\n",
      "Predicted Output: Veyrien wasn't home in Belsen. She didn't check her phone to make\n",
      "\n",
      "Predicting Input : 1077 - after one day of exercise he became strong.\n",
      "Predicted Output: It was impossible for a child to grow muscle\n",
      "\n",
      "Predicting Input : 1078 - he folded his umbrella when it started to rain.\n",
      "Predicted Output: Dawn woke up, she peered around the kitchen and added something to it. \"\n",
      "\n",
      "Predicting Input : 1079 - she is a vegetarian and refuses to eat food.\n",
      "Predicted Output: He now eats normally, but he only eats one meal a day on Sundays.\n",
      "\n",
      "Predicting Input : 1080 - he ran out of cola while driving to work so he walked there.\n",
      "Predicted Output: Stray elephants made a white face in Shanghai\n",
      "\n",
      "Predicting Input : 1081 - mark is a pilot and he assembled an aircraft.\n",
      "Predicted Output: He later filmed one to test the safety measures. The pilot said it got too close to\n",
      "\n",
      "Predicting Input : 1082 - jim wrote a message with a finger on a piece of paper.\n",
      "Predicted Output: Sent by FC Games on behalf of Roosters\n",
      "\n",
      "Predicting Input : 1083 - the businessman went to the airport to catch a mouse.\n",
      "Predicted Output: The appeal reportedly included: \"we will investigate the circumstances that led up to and after this\n",
      "\n",
      "Predicting Input : 1084 - he stretched his arms and caught the ball with his feet.\n",
      "Predicted Output: Losing Brian Bird, who was now surrounded by plenty of young guards and several small wings\n",
      "\n",
      "Predicting Input : 1085 - the difference between the twins' birth time is one day.\n",
      "Predicted Output: In fact, both the twins are thought to be born from the same species.\n",
      "\n",
      "Predicting Input : 1086 - he found enough evidence to commit a crime.\n",
      "Predicted Output: It was initially believed a 22-year-old appeared in Blaine County Court and released\n",
      "\n",
      "Predicting Input : 1087 - jim cut his finger with the soap.\n",
      "Predicted Output: sgooba sat to her side. Kyudaiya read him her books. Y\n",
      "\n",
      "Predicting Input : 1088 - tina told her dog to clean the apartment.\n",
      "Predicted Output: The club opens its doors on July 8th. Local residents can join by April 19th\n",
      "\n",
      "Predicting Input : 1089 - jack had bad breath so he ate a garlic.\n",
      "Predicted Output: Rape was forbidden for a human. I lived in Africa and was raped by my mother.\n",
      "\n",
      "Predicting Input : 1090 - susan cleaned her room and she had more trash.\n",
      "Predicted Output: 10:54 AM, Jan 8, 1997: Student writes to Student asking her to leave\n",
      "\n",
      "Predicting Input : 1091 - he ate hot pot at a fast food restaurant.\n",
      "Predicted Output: He told undercover investigators, \"I don't know how you would explain my behavior to people\n",
      "\n",
      "Predicting Input : 1092 - he felt hungry and went to a gym to eat.\n",
      "Predicted Output: The guys laughed at the space and never forgot to drink water. So did Nootka\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1093 - he suspected there was a mouse in the garage so he brought a dog home.\n",
      "Predicted Output: The peacekeeping mission, which was created under the motto \"To eliminate Hitler's enemy,\"\n",
      "\n",
      "Predicting Input : 1094 - as a geographer he created the Earth's surface.\n",
      "Predicted Output: He added it had a bar chart and compass. He thought he'd tried to call it\n",
      "\n",
      "Predicting Input : 1095 - joe stole some money from his own pocket.\n",
      "Predicted Output: This book reveals only a small part of Hans' actions in this conflict\n",
      "\n",
      "Predicting Input : 1096 - the child looked down to find stars in the sky.\n",
      "Predicted Output: Children will sometimes cry with laughter when they have fun\n",
      "\n",
      "Predicting Input : 1097 - he felt like suiciding and the doctor lent him a gun.\n",
      "Predicted Output: Armed criminals love not only physical\n",
      "\n",
      "Predicting Input : 1098 - tom watched fictions on magazines.\n",
      "Predicted Output: smelt cheap cloth\n",
      "\n",
      "Predicting Input : 1099 - john got up early to take a good rest.\n",
      "Predicted Output: Christine was happy\n",
      "\n",
      "Predicting Input : 1100 - susan disconnected the phone to wait for another call.\n",
      "Predicted Output: If you haven't had the available video recordings of every phone call made over the last week\n",
      "\n",
      "Predicting Input : 1101 - she bent her eyebrows to jump up.\n",
      "Predicted Output: \"No thank you! But on that moon a moon should be stronger than it is right\n",
      "\n",
      "Predicting Input : 1102 - anne dropped a waterfall on the floor.\n",
      "Predicted Output: Steep mountains seem to be impossible\n",
      "\n",
      "Predicting Input : 1103 - the experienced singer captain the ship.\n",
      "Predicted Output: 11:54 11-15; i worship the Lion's hair but day lives and live\n",
      "\n",
      "Predicting Input : 1104 - he looked through a magnifier to observe a cell.\n",
      "Predicted Output: Grimoire 48, room 10C — 4 cells\n",
      "\n",
      "Predicting Input : 1105 - he put on his shoes to see well enough.\n",
      "Predicted Output: Yukimura continued, \"Samaoka is beginning to feel tired.\"\n",
      "\n",
      "Predicting Input : 1106 - josh stepped into a seashell when he walked along the beach.\n",
      "Predicted Output: oldbugs@oregonian.com\n",
      "\n",
      "Predicting Input : 1107 - he walked barefoot over broken glass.\n",
      "Predicted Output: Sara shrugged and patted at the mattress. \"Shit.\" she said. \"\n",
      "\n",
      "Predicting Input : 1108 - jefferson forgot to shut up before he did some exercises.\n",
      "Predicted Output: But when Nate refused, he did not apologize. He asked for forgiveness for his actions.\n",
      "\n",
      "Predicting Input : 1109 - he received a degree from his family.\n",
      "Predicted Output: But her grandmother got a card and told parents and teachers that her grandson would not be allowed\n",
      "\n",
      "Predicting Input : 1110 - james liked his car for a long time everyday.\n",
      "Predicted Output: It was marked by a white picket fence. As one dovetailed with the cars\n",
      "\n",
      "Predicting Input : 1111 - tony used his drakes to stop his bicycle.\n",
      "Predicted Output: — Lee Gowson, Marr's roommate. She'd only discovered him when he'd\n",
      "\n",
      "Predicting Input : 1112 - she found eagles at the bottom of the sea.\n",
      "Predicted Output: The Orcish building cannot wait for the Iron\n",
      "\n",
      "Predicting Input : 1113 - he used his id card to pay the bill.\n",
      "Predicted Output: It was usual for a father to pay taxes. At only 5 percent of his income,\n",
      "\n",
      "Predicting Input : 1114 - he is the first man born on the moon.\n",
      "Predicted Output: He will survive any of the major challenges encountered in space.\n",
      "\n",
      "Predicting Input : 1115 - joe slept on a banana peel.\n",
      "Predicted Output: But when Washcho-wothers woke up from his bed at night, he went\n",
      "\n",
      "Predicting Input : 1116 - the newborn monkey drinks banana from its mother's breast.\n",
      "Predicted Output: The chick doesn't eat bananas.\n",
      "\n",
      "Predicting Input : 1117 - carrots make rabbits sick.\n",
      "Predicted Output: But what happens when a rose's leaves spill out and cause you to collapse?\n",
      "\n",
      "Predicting Input : 1118 - josh thought of an idea and wrote a history.\n",
      "Predicted Output: 1937 Captain Arthur and Yooka bring an indigenous crew to vacation with them. He\n",
      "\n",
      "Predicting Input : 1119 - jane drove so fast that she got a movie ticket.\n",
      "Predicted Output: In another tragic incident, in which a policeman was struck by a bicycle after going for a\n",
      "\n",
      "Predicting Input : 1120 - jack ignored his sick friend to check on her.\n",
      "Predicted Output: The blue bunny slowly licked his lips before she realized her pain was gone.\n",
      "\n",
      "Predicting Input : 1121 - he got punished by the boss because of his hard work.\n",
      "Predicted Output: The boss didn't teach him what to do\n",
      "\n",
      "Predicting Input : 1122 - a boat floated in the bowl.\n",
      "Predicted Output: The deck ceiling had a nice light look as I drove back to our cabin.\n",
      "\n",
      "Predicting Input : 1123 - fred turned up the capacity of the radio.\n",
      "Predicted Output: The radio buzzed, and in the barn the Jerry played the tune \"Oh, nice\n",
      "\n",
      "Predicting Input : 1124 - alex drove the van into the lake.\n",
      "Predicted Output: In order to save a little time, Ginny made sure not to fly too far in the\n",
      "\n",
      "Predicting Input : 1125 - frank brought a can of jewelry from the grocery shop.\n",
      "Predicted Output: The manager approached him, told him to finish the shopping for an evening only.\n",
      "\n",
      "Predicting Input : 1126 - mary wore a pretty watch to the dancing party.\n",
      "Predicted Output: But what exactly did the parents do in advance to teach them to dance and why are they\n",
      "\n",
      "Predicting Input : 1127 - tom fell and broke his heart.\n",
      "Predicted Output: The Lord saves him, he says, because he cannot avoid his sins; he has long\n",
      "\n",
      "Predicting Input : 1128 - he ate his chopsticks with chop suey.\n",
      "Predicted Output: This story originally appeared in The Other Side Magazine.<|endoftext|>Today I received my First Micro USB\n",
      "\n",
      "Predicting Input : 1129 - the shower curtain tastes good.\n",
      "Predicted Output: You should kiss her, her hands are kissed the whole time. She likes it.\n",
      "\n",
      "Predicting Input : 1130 - jamie served delicious food on a laptop.\n",
      "Predicted Output: You may wonder why. We just don't have eyes for people.\"What can I say\n",
      "\n",
      "Predicting Input : 1131 - he wrote a letter on tv.\n",
      "Predicted Output: the hero sacrifice will be annibally televised\n",
      "\n",
      "Predicting Input : 1132 - judy used an earpick to clean between her teeth.\n",
      "Predicted Output: The jury found them guilty on 11 counts of\n",
      "\n",
      "Predicting Input : 1133 - suzie bought her parents a new car for her birthday.\n",
      "Predicted Output: But she noticed some of the children were packing up their bags in front of their mother's\n",
      "\n",
      "Predicting Input : 1134 - lily hid behind a hair.\n",
      "Predicted Output: What\n",
      "\n",
      "Predicting Input : 1135 - she opened the shower curtain when she woke up.\n",
      "Predicted Output: The sun faded down, and he saw Eve as she stood and stared off into the distance\n",
      "\n",
      "Predicting Input : 1136 - lewis was having an affair with his girlfriend.\n",
      "Predicted Output: He told Helen about a woman he could marry. When she didn't answer the questions he\n",
      "\n",
      "Predicting Input : 1137 - hats expose the head to sun and rain.\n",
      "Predicted Output: I tell him I care about my hair\n",
      "\n",
      "Predicting Input : 1138 - jack handed a word to her.\n",
      "Predicted Output: \"You aren't opening your mouth any longer.\"\n",
      "\n",
      "Predicting Input : 1139 - he lent me some bills.\n",
      "Predicted Output: To take care of your money\n",
      "\n",
      "Predicting Input : 1140 - I gave him the key to break the door.\n",
      "Predicted Output: A couple hundred yards, he made it across the roof of the factory with his face raised\n",
      "\n",
      "Predicting Input : 1141 - lucy drank some tea to serve the guest.\n",
      "Predicted Output: We always pray for a general attack of awakening. Not only for spiritual awakening. The faith\n",
      "\n",
      "Predicting Input : 1142 - paul has a picture taken with two Americas.\n",
      "Predicted Output: He does have one, but he has kept it private\n",
      "\n",
      "Predicting Input : 1143 - the boss made him do hard work all year.\n",
      "Predicted Output: TSA warns employees: 'This is cruel, unlawful and designed to infringe on women\n",
      "\n",
      "Predicting Input : 1144 - I heard her singing in the next city.\n",
      "Predicted Output: The reality check - a quite self-contained country club - was conducted by people at the\n",
      "\n",
      "Predicting Input : 1145 - I could hear the rain pattered through the window.\n",
      "Predicted Output: 'You're asleep,' the hostess said softly. 'And you've heard all of this\n",
      "\n",
      "Predicting Input : 1146 - my boyfriend grew me some roses on Valentine's Day.\n",
      "Predicted Output: The grown man had friends who might not even\n",
      "\n",
      "Predicting Input : 1147 - chris lived at the bus stop.\n",
      "Predicted Output: In September 1974, a 21-year-old Mississauga man died after being shot by\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1148 - our teacher told us to make more mistakes in the study.\n",
      "Predicted Output: NCT Study English, English for English Fluency Completion\n",
      "\n",
      "Predicting Input : 1149 - she made a beautiful cloth for the dinner party.\n",
      "Predicted Output: But when Selva, his father, challenged her on her prior predictions for who would win\n",
      "\n",
      "Predicting Input : 1150 - the advanced auto machine will cost us a lot of labor.\n",
      "Predicted Output: But if people don't want to make robots, then your Carmaker won't be able\n",
      "\n",
      "Predicting Input : 1151 - his success is due to his poor work.\n",
      "Predicted Output: He did enjoy playing. He had to relax a little at work so he could play more\n",
      "\n",
      "Predicting Input : 1152 - he turned to his teacher for a question in the exam.\n",
      "Predicted Output: \"That wasn't a question of being vegetarian. That was a question of having a healthy\n",
      "\n",
      "Predicting Input : 1153 - he tried hard to succeed to frustrate his parents.\n",
      "Predicted Output: The student activist started a Facebook account in 2004 to identify himself and explain what he was doing\n",
      "\n",
      "Predicting Input : 1154 - adults are not allowed to drink beer.\n",
      "Predicted Output: What do Indians drink, other than beer?\n",
      "\n",
      "Predicting Input : 1155 - people became ignorant after they read the news.\n",
      "Predicted Output: If you spend $1,000 on cakes, anything over $100 cost you more than\n",
      "\n",
      "Predicting Input : 1156 - caroline asked her dog a name.\n",
      "Predicted Output: Booth slammed her bib in the forehead. And because he wasn't dog-friendly\n",
      "\n",
      "Predicting Input : 1157 - he prevented the car from going on the road.\n",
      "Predicted Output: The dogs went out to search\n",
      "\n",
      "Predicting Input : 1158 - franklin is dancing in the choir.\n",
      "Predicted Output: This next sketch comes to us from Lillian Hinchcliffe, then herself.\n",
      "\n",
      "Predicting Input : 1159 - Fiona wrote a shopping list after she went to the supermarket.\n",
      "Predicted Output: He told MailOnline: 'I still haven't opened this same shopping list for all of\n",
      "\n",
      "Predicting Input : 1160 - police officers enjoy eating donuts in an emergency.\n",
      "Predicted Output: You may notice you are having an odd snack. Here are some tips for getting it right\n",
      "\n",
      "Predicting Input : 1161 - jill wrote a book with monsters.\n",
      "Predicted Output: He'd convinced himself, \"Malfurion. My husband is absolutely terrifying and you should\n",
      "\n",
      "Predicting Input : 1162 - she flew on a fly abroad.\n",
      "Predicted Output: Stray Lines and a totally different course [ edit ]\n",
      "\n",
      "Predicting Input : 1163 - we talked too much that we all puked later.\n",
      "Predicted Output: We didn't mean the source of the shelling. We didn't mean what was going on\n",
      "\n",
      "Predicting Input : 1164 - tucker overdosed on candy.\n",
      "Predicted Output: But my grandparents didn't make it to Disneyland. So all I saw were West End Studios\n",
      "\n",
      "Predicting Input : 1165 - he got dirty after he bought the house.\n",
      "Predicted Output: If you haven't had the chance to hang out with old friends before this week, just\n",
      "\n",
      "Predicting Input : 1166 - Ariel wanted to surprise Beth by calling her a puppy.\n",
      "Predicted Output: Beth reacted very unkindly to Ariel's instruction.\n",
      "\n",
      "Predicting Input : 1167 - larry put his swimsuit on for the meeting.\n",
      "Predicted Output: In its entirety, the film was probably filmed in Atlanta and has won Academy Awards. It\n",
      "\n",
      "Predicting Input : 1168 - he put an apple in his mouth.\n",
      "Predicted Output: Mufasa Sal, who was still boiling an apple on the stove at the time,\n",
      "\n",
      "Predicting Input : 1169 - he printed a stamp on the letter.\n",
      "Predicted Output: In general you should be able to open mail and purchase goods on short notice. In times\n",
      "\n",
      "Predicting Input : 1170 - alfred stopped at the drive-thru for a lobster.\n",
      "Predicted Output: The accident occurred after a box of fish tipped over on the ground as Willard had gone\n",
      "\n",
      "Predicting Input : 1171 - on the day of the concert, we went to the dining hall very early for seats.\n",
      "Predicted Output: We went home after a month to find comfort in our bodies. After hearing from one man\n",
      "\n",
      "Predicting Input : 1172 - they bought paper made of steel.\n",
      "Predicted Output: If you lose money, you can also steal. What would you say if you found your\n",
      "\n",
      "Predicting Input : 1173 - they hung out at the meeting room.\n",
      "Predicted Output: The officers grabbed company, then swarmed everyone to search for more suspects.\n",
      "\n",
      "Predicting Input : 1174 - joey used a mop to dry himself.\n",
      "Predicted Output: It was Lady Kenny, but it was alright. They both left without orders.\n",
      "\n",
      "Predicting Input : 1175 - wildfires are controlled by street fighters.\n",
      "Predicted Output: The RCMP have issued two orders not to step in and\n",
      "\n",
      "Predicting Input : 1176 - honey can understand each other's dancing.\n",
      "Predicted Output: But no dancing will be done on you alone. No street will ever become any other person\n",
      "\n",
      "Predicting Input : 1177 - he looked at the watch worn on his waist.\n",
      "Predicted Output: I never saw anyone standing. I just saw soldiers wading through the forests on big Indian tanks\n",
      "\n",
      "Predicting Input : 1178 - sleeping pills keep him warm in the wild.\n",
      "Predicted Output: He skims over a bed of leaves after his experiments with food will usually lead to gas\n",
      "\n",
      "Predicting Input : 1179 - a secret is something that everyone knows.\n",
      "Predicted Output: He never catches up, his mind on routine. As if he isn't due to meet\n",
      "\n",
      "Predicting Input : 1180 - he ate too much that his fart was sweet.\n",
      "Predicted Output: It was partly because a woman was having difficulties with herself and one problem could have been even\n",
      "\n",
      "Predicting Input : 1181 - he chewed his food without swallowing.\n",
      "Predicted Output: In fact, even a review of the Indonesian version of Dragonball Online confirmed that the language\n",
      "\n",
      "Predicting Input : 1182 - he brought his gum for the hunt.\n",
      "Predicted Output: The weather picked up, and in the autumn the hunt began. Next door to the trail\n",
      "\n",
      "Predicting Input : 1183 - the yard was wet after several days of sunshine.\n",
      "Predicted Output: The master thief went to Fife's tailor to sell her an axe for 200p a\n",
      "\n",
      "Predicting Input : 1184 - the ship sailed across the iced waters of the Atlantic.\n",
      "Predicted Output: It was winter at a time when my heels were inches off the ground and my feet were\n",
      "\n",
      "Predicting Input : 1185 - the ship turned sharply to get a collision.\n",
      "Predicted Output: The deck swept away, but the ship spun and exploded to the surface. \"The column\n",
      "\n",
      "Predicting Input : 1186 - the artist added more melodies on his painting.\n",
      "Predicted Output: In another symbolic gesture, he told the Tibetan people that they had taken good care of one\n",
      "\n",
      "Predicting Input : 1187 - he found a bat to play soccer.\n",
      "Predicted Output: The owner wouldn't be involved in the batting and batting practice. That didn't mean he\n",
      "\n",
      "Predicting Input : 1188 - peter is walking on the river of the bank.\n",
      "Predicted Output: This group split off. The rest of them were laid off. What happened was that they\n",
      "\n",
      "Predicting Input : 1189 - the student attended several lectures to teach different professors.\n",
      "Predicted Output: Stories tend to be short. We prefer to focus on the topic first.\n",
      "\n",
      "Predicting Input : 1190 - all the guest of the party sat onto the cake.\n",
      "Predicted Output: 12:41 PM-The party has kicked off with 3 more meals with things to pick\n",
      "\n",
      "Predicting Input : 1191 - mia got a discount on her groceries so she spent more money.\n",
      "Predicted Output: As Gwyneth-Elisabeth frowned, Madeline thought she felt better. He\n",
      "\n",
      "Predicting Input : 1192 - the sun is rising and it's getting dark outside.\n",
      "Predicted Output: It's sunny outside, but it's raining. It's cold outside\n",
      "\n",
      "Predicting Input : 1193 - the conversation is at the beginning so I hung up the phone.\n",
      "Predicted Output: Bees no doubt lost. A lot of corpses. Like few things my guests want to hear\n",
      "\n",
      "Predicting Input : 1194 - a canal was constructed next to the farmland for flood.\n",
      "Predicted Output: We've discussed many of the issues that plague us after hearing it when calling for what we\n",
      "\n",
      "Predicting Input : 1195 - the skydiver glided safely to the seabed.\n",
      "Predicted Output: Here's our answers to this question\n",
      "\n",
      "Predicting Input : 1196 - after becoming immune to the disease the baby received the vaccine.\n",
      "Predicted Output: If you suspect you are ill, call 911. Medicated IV fluids could help to save\n",
      "\n",
      "Predicting Input : 1197 - todd opened the window to let the fresh air out.\n",
      "Predicted Output: I wasn't expecting any fiddly ones present. That would be bothersome.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1198 - the dog chased the cat up the tree.\n",
      "Predicted Output: The Skimming Coach, said to be mounted on an electrical fence at Balwater State Park\n",
      "\n",
      "Predicting Input : 1199 - jane painted a chair and sat on it.\n",
      "Predicted Output: Sara screamed as a dog came out from its crate to sit next to her. A\n",
      "\n",
      "Predicting Input : 1200 - lily ordered a steak at a vegetarian restaurant.\n",
      "Predicted Output: He told BuzzFeed News, \"I don't know how they just lit up and made us\n",
      "\n",
      "Predicting Input : 1201 - he wanted to lose weight to fit into future clothes.\n",
      "Predicted Output: In fact, she is now still five pounds lighter than she was before she started what she\n",
      "\n",
      "Predicting Input : 1202 - there is a huge, splendid cattle standing on the top of the hill.\n",
      "Predicted Output:  I can't keep moving even a little more.\" ―Masters Legio Wheelhaven to Prince\n",
      "\n",
      "Predicting Input : 1203 - he found cash of one million dollars in his wallet.\n",
      "Predicted Output: He got upset when a woman told him she had paid more than $90,000 for\n",
      "\n",
      "Predicting Input : 1204 - he parked his car in the wrong place so he was asked to sell it.\n",
      "Predicted Output: The fee was $300 for the car and\n",
      "\n",
      "Predicting Input : 1205 - sally hatched some eggs for the Christmas eggnog.\n",
      "Predicted Output: all we earn is development time. So'mothers work\n",
      "\n",
      "Predicting Input : 1206 - cindy said she was too good to go to school.\n",
      "Predicted Output: A student sued Col. Ronald Forgive of Louisiana State University after going to her High\n",
      "\n",
      "Predicting Input : 1207 - the old man was praised for littering in public.\n",
      "Predicted Output: In large portions of the city, there were no memorials to bear trees; on other\n",
      "\n",
      "Predicting Input : 1208 - jones gained some weight and his clothes fit loosely.\n",
      "Predicted Output: The parties stayed close, but it was unclear if there were any mechanical aspects to the trip\n",
      "\n",
      "Predicting Input : 1209 - he left his dog in his office when he headed for work.\n",
      "Predicted Output: Lincoln County Sheriff Thomas Kilby said people\n",
      "\n",
      "Predicting Input : 1210 - the government built an aircraft roadway in the train station.\n",
      "Predicted Output: In 2011 I explained a special air embankment system (exactly what it is called\n",
      "\n",
      "Predicting Input : 1211 - the girl licked her fur carefully.\n",
      "Predicted Output: 'What do you mean' Nina thought before flicking her hair around as she began to\n",
      "\n",
      "Predicting Input : 1212 - to accommodate the bugs he converted the sofa into a bed.\n",
      "Predicted Output: He took stock of the visit to which Bruce was invited to sign up for the event.\n",
      "\n",
      "Predicting Input : 1213 - my hands are tired because I have been standing up all day.\n",
      "Predicted Output: Thang hits where you should sit\n",
      "\n",
      "Predicting Input : 1214 - joanna made a pot of chill for dinner.\n",
      "Predicted Output: Worth worrying about - or about finding - can be quite\n",
      "\n",
      "Predicting Input : 1215 - penguins are fish that cannot fly.\n",
      "Predicted Output: But what happens when a giant fish needs fish? When we don't hear about it?\n",
      "\n",
      "Predicting Input : 1216 - I attend a class in the principal room.\n",
      "Predicted Output: In my grandmother's one and only class she had fun with her grandfather's beer. '\n",
      "\n",
      "Predicting Input : 1217 - one of the common reasons for flight delay is the mood of the pilot.\n",
      "Predicted Output: He may wish to be airlifted home to avoid physical danger or sea battles. (\n",
      "\n",
      "Predicting Input : 1218 - in windy days everyone had to hold onto their shoes.\n",
      "Predicted Output: Stocks stayed strong, but we could barely find anything on the fields and most of the\n",
      "\n",
      "Predicting Input : 1219 - after cooking I put the plate in the sink.\n",
      "Predicted Output: WAS IT NECESSARY TO CAVE BETTER WITH FAIS WITH PERFECTLY\n",
      "\n",
      "Predicting Input : 1220 - the boy read the novel aloud to his blind grandfather.\n",
      "Predicted Output: In fact, only a few years later did I hear this story about Death's secret journal\n",
      "\n",
      "Predicting Input : 1221 - after a long time of walking my foot went numb.\n",
      "Predicted Output: I got so numb that the areas I were aiming for lost their force at my feet. (\n",
      "\n",
      "Predicting Input : 1222 - the photographer skipped the moment when the kids smiled.\n",
      "Predicted Output: The adult handler soon was able to keep track of them for the kids as they left the\n",
      "\n",
      "Predicting Input : 1223 - I slipped on the cracked floor.\n",
      "Predicted Output: 'As they stretched their way over the floor…' he repeated. 'Here it is.'\n",
      "\n",
      "Predicting Input : 1224 - the bar closed early because it was crowded.\n",
      "Predicted Output: The driver briefly admitted the incident to police before being transferred to the Golden Gate Bridge.\n",
      "\n",
      "Predicting Input : 1225 - he got pulled over by the police when he was parking.\n",
      "Predicted Output: In 2016 there were a total of 30 homicides in Missouri.\n",
      "\n",
      "Predicting Input : 1226 - the kid was spoiled and ran away from home.\n",
      "Predicted Output: \"If possible, leave your children alone.\"\n",
      "\n",
      "Predicting Input : 1227 - bosh realized he forgot something when he looked at his watch.\n",
      "Predicted Output: <You haven't noticed any change after reading this>\n",
      "\n",
      "Predicting Input : 1228 - the bartender refused to serve the lonely patron.\n",
      "Predicted Output: The patron testified that a matinee confronted her about her lack of courage and how she\n",
      "\n",
      "Predicting Input : 1229 - the student received detention for his excellent grades.\n",
      "Predicted Output: In May 1996, a couple of days before the testimony for the prosecution took place, three\n",
      "\n",
      "Predicting Input : 1230 - a train went through a bridge and the inside of it went dark.\n",
      "Predicted Output: The train dragged down a metal steps and sunk in the hole. There were no sounds and\n",
      "\n",
      "Predicting Input : 1231 - the turtle hid in its shell to hunt a fish.\n",
      "Predicted Output: It had avoided starting a fire in the volcano to catch fire and burn itself out.\n",
      "\n",
      "Predicting Input : 1232 - the girl handed down her clothes to her elder sister.\n",
      "Predicted Output: \"Father, where are you?\" said Oryx\n",
      "\n",
      "Predicting Input : 1233 - the bomb was defused when the terrorist set it off.\n",
      "Predicted Output: We're wondering what a broadside will bring the CIA to this sort of action. If\n",
      "\n",
      "Predicting Input : 1234 - the food he dropped was cooked by his dog.\n",
      "Predicted Output: In another apology for a TV show that mocked his taste for Japanese candy in May, Kevin\n",
      "\n",
      "Predicting Input : 1235 - the book was deemed inappropriate for human.\n",
      "Predicted Output: In order for Paul.Theory.Medical.Science.to Have Powers of Law he\n",
      "\n",
      "Predicting Input : 1236 - the driver made a wrong turn and the trip took a shorter time.\n",
      "Predicted Output: The driver engaged in a drive in an unmarked car but did not park his car. The\n",
      "\n",
      "Predicting Input : 1237 - after I replaced the batteries the flashlight was dead.\n",
      "Predicted Output: He didn't care, he just wanted some place to sleep. Zoro could get over\n",
      "\n",
      "Predicting Input : 1238 - I scratched my skin to relieve scar.\n",
      "Predicted Output: I didn't blame him, it was me doing that with no problem\n",
      "\n",
      "Predicting Input : 1239 - the burglar broke into my house without calling me up.\n",
      "Predicted Output: The burglar broke into my house without calling me up. The burglar beat me up\n",
      "\n",
      "Predicting Input : 1240 - jason held the door for the woman entering before him.\n",
      "Predicted Output: The phone rang out, and he turned around. He turned and pointed at the woman in\n",
      "\n",
      "Predicting Input : 1241 - the mirror in the bathroom fogged up after she applied her makeup.\n",
      "Predicted Output: It was surrounded by a wall of ice forming the strip of ice around her hands. In\n",
      "\n",
      "Predicting Input : 1242 - the security guard let the man pass for his sneaky behavior.\n",
      "Predicted Output: The officer defended himself, saying the man shouldn't do what he does because he's doing\n",
      "\n",
      "Predicting Input : 1243 - the driver rotated the steering wheel and the car halted.\n",
      "Predicted Output: The driver twisted his back and both cars collided. The driver has serious injuries.\n",
      "\n",
      "Predicting Input : 1244 - my shirt was tidy so I ironed it.\n",
      "Predicted Output: In 1984, France received a suspicious flag from the UN\n",
      "\n",
      "Predicting Input : 1245 - the patient is getting worse so the hospital sent him home.\n",
      "Predicted Output: The patient begins treatment, but he has chronic pain and says he cannot receive proper training to\n",
      "\n",
      "Predicting Input : 1246 - we found no seats in the theater so we sat down.\n",
      "Predicted Output: We went inside and the display was hard to make out and we couldn't take it out\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1247 - the woman received an admission after she graduated.\n",
      "Predicted Output: The medical examiner says a gun was found inside the apartment and that four suspects were thought to\n",
      "\n",
      "Predicting Input : 1248 - emily came upon a river on the sidewalk.\n",
      "Predicted Output: Yule heard people asking for tea in the center of\n",
      "\n",
      "Predicting Input : 1249 - she covered her ears at the smelly smoke.\n",
      "Predicted Output: Clinthia truly is that kind of detective. She doesn't charge anyone with being soft\n",
      "\n",
      "Predicting Input : 1250 - the audience booed the comedian off the stage for his humor.\n",
      "Predicted Output: In other tweets to the Saturday Times, Yiannopoulos said he could not confirm what his friends were\n",
      "\n",
      "Predicting Input : 1251 - the coach gave the referee a high five.\n",
      "Predicted Output: Alt left bank is not immediately linked to Bae\n",
      "\n",
      "Predicting Input : 1252 - bob said my breath smelled of garlic so I wiped my mouth.\n",
      "Predicted Output: 100%|██████████| 20/20 [00:01<00:00, 17.13it/s]\n",
      "\n",
      "Predicting Input : 1253 - alex was lost so he drew a map.\n",
      "Predicted Output: This means Google knows a lot more about maps than most people have been allowed to know.\n",
      "\n",
      "Predicting Input : 1254 - tina pushed the gas pedal to open the car door.\n",
      "Predicted Output: Q: Could you have seen the table tray in my hands?\n",
      "\n",
      "Predicting Input : 1255 - sarah got away with the crime because new evidence was discovered.\n",
      "Predicted Output: dragcowgirl : not sure\n",
      "\n",
      "Predicting Input : 1256 - I wiped the book with a tissue and the words on the cover came off.\n",
      "Predicted Output: 'But this couldn't be my book. Who will leave this book?' said Jack.\n",
      "\n",
      "Predicting Input : 1257 - the student chewing gum in class was forced to swallow it in.\n",
      "Predicted Output: In another bizarre incident, the students were forcibly removed from their vehicle after driving off to school\n",
      "\n",
      "Predicting Input : 1258 - jane brought her ID so she was denied entrance into the bar.\n",
      "Predicted Output: The charge carries a 5-year ban but is supported by the Crown Prosecution Service.\n",
      "\n",
      "Predicting Input : 1259 - a comet passed by the moon and a crater formed on it.\n",
      "Predicted Output: This isn't as gothic as Astronomy 101, but how does it look from\n",
      "\n",
      "Predicting Input : 1260 - the benefactor requested to stay anonymous to maintain his reputation.\n",
      "Predicted Output: It was yesterday when a woman from New Delhi first approached him to seek advice on his management\n",
      "\n",
      "Predicting Input : 1261 - janice felt ashamed of the smile on her face.\n",
      "Predicted Output: : many different outfits which the goal of russia is it is to achieve victory in the race\n",
      "\n",
      "Predicting Input : 1262 - the x-ray showed that his arm was painted.\n",
      "Predicted Output: This kind of injuries to your arm are incredibly common and well known as injuries to your arms\n",
      "\n",
      "Predicting Input : 1263 - the boy filled the bucket with air.\n",
      "Predicted Output: Finally, just when he thought it was over,\n",
      "\n",
      "Predicting Input : 1264 - taylor downgraded her computer hardware for a better experience.\n",
      "Predicted Output: For those unaware, a computer is an umbrella that extends one's clothes over and over and\n",
      "\n",
      "Predicting Input : 1265 - after a religious awakening she began to travel around.\n",
      "Predicted Output: This particular awakening began a few years before Kreia began my research but before her return to\n",
      "\n",
      "Predicting Input : 1266 - I threw away some unnecessary contents and the drawers became cluttered.\n",
      "Predicted Output: Gautama tore them to shreds. Meanwhile, Mario felt his pants rolled off. \"\n",
      "\n",
      "Predicting Input : 1267 - the student flaunted his test grade to his pets.\n",
      "Predicted Output: We've updated this to include the 24 reviews of Pet Voice and BB Plus and it could\n",
      "\n",
      "Predicting Input : 1268 - the boy felt homesick at his parent's.\n",
      "Predicted Output: Kurt Westervelt, who was around seven at the time, eventually married his mother in\n",
      "\n",
      "Predicting Input : 1269 - she designed a rocket and then it flew up into the sky.\n",
      "Predicted Output: If you haven't had the chance to fly the rocket up to Disney World and have seen\n",
      "\n",
      "Predicting Input : 1270 - the boy murmured his reply when his mother told him to speak up.\n",
      "Predicted Output: I stand listening to a girl with an eyebrow raised on her face when she tells me not\n",
      "\n",
      "Predicting Input : 1271 - he put the tree he just bought into the vase.\n",
      "Predicted Output: The tree stands right in front of the sundial that features the portrait of Jesus. The\n",
      "\n",
      "Predicting Input : 1272 - the lipstick on his cheek implied he blushed in front of a woman.\n",
      "Predicted Output: We've obviously seen a lot of filmgoers who love dark matter over flesh. This does\n",
      "\n",
      "Predicting Input : 1273 - the gardener put seeds in the soil to make his plants flourish.\n",
      "Predicted Output: In fact the gardener only put seeds into the soil if he wants them to take root\n",
      "\n",
      "Predicting Input : 1274 - the pipe burst because the water was unsanitary.\n",
      "Predicted Output: The pool collapsed when a building was forced open. Newborns living there had to stay\n",
      "\n",
      "Predicting Input : 1275 - the company lost money due to its positive consumer reviews.\n",
      "Predicted Output: 18. Gottlieb was one of 22 companies making 500K loans for private individuals to\n",
      "\n",
      "Predicting Input : 1276 - ben woke up with a toothache so he skipped an appointment with his dentist.\n",
      "Predicted Output: It was neither normal to look at the dentist for toothache. However, this was after\n",
      "\n",
      "Predicting Input : 1277 - the team lost the game due to the injury of the referee.\n",
      "Predicted Output: In any event, the exact reason for Brazil's defeat was not discovered until after the match\n",
      "\n",
      "Predicting Input : 1278 - children are not allowed to watch the movie because it's animated.\n",
      "Predicted Output: There are cameras at a grand house in Surrey. There are no cameras at the house in\n",
      "\n",
      "Predicting Input : 1279 - paul deleted the email for it's an important emailfalse : 1.\n",
      "Predicted Output: 3.4 in google cache\" : 2. Seeing that isn't my intention.\" : 3.\n",
      "\n",
      "Predicting Input : 1280 - the student's grade was higher for the incorrect answer on the exam.\n",
      "Predicted Output: This doesn't happen. The exam was administered on both sides of the grade.\n",
      "\n",
      "Predicting Input : 1281 - the toddler started crying with his favorite toy.\n",
      "Predicted Output: The initial traumatic events, like the physical manifestation of infection and death of his father, cost\n",
      "\n",
      "Predicting Input : 1282 - monica pushed herself off the top of the slide and crawled up.\n",
      "Predicted Output: \"We needed to shoot her with high velocity from the lower\n",
      "\n",
      "Predicting Input : 1283 - humans fly in the sky.\n",
      "Predicted Output: As they travelled through a variety of strange landscapes, Patagonia lived up to all of\n",
      "\n",
      "Predicting Input : 1284 - he eats a table for dinner.\n",
      "Predicted Output: He probably eats too many crackers. Don't eat crackers before driving the car.\n",
      "\n",
      "Predicting Input : 1285 - I write with a comb.\n",
      "Predicted Output: But other linguists, such as Jud Hoelzer of the International Organization for Development (\n",
      "\n",
      "Predicting Input : 1286 - he peels an apple with cotton.\n",
      "Predicted Output: If you prefer using a dry hand you'll need to apply the wax before putting the brush\n",
      "\n",
      "Predicting Input : 1287 - I enjoy dancing shows in radio.\n",
      "Predicted Output: If you haven't had a bar or lounge in any city in America before\n",
      "\n",
      "Predicting Input : 1288 - Strikes will bring convenience to people's lives.\n",
      "Predicted Output: This kind of evil, long-term stigma is critical in post-traumatic stress disorder.\n",
      "\n",
      "Predicting Input : 1289 - you give objects when you go shopping.\n",
      "Predicted Output: As with packaging and the search you will notice that something will not stay put for long and\n",
      "\n",
      "Predicting Input : 1290 - he runs errands to spend money.\n",
      "Predicted Output: He doesn't expect a single person to ignore his affairs.\n",
      "\n",
      "Predicting Input : 1291 - If you want to get a question, you will use a calculator.\n",
      "Predicted Output: When you download and go to the website they will answer with the relevant answer.\n",
      "\n",
      "Predicting Input : 1292 - You will sleep more if you want to get somewhere.\n",
      "Predicted Output: There are plenty of areks in all woods. Some are between trees and even more than\n",
      "\n",
      "Predicting Input : 1293 - you feel sleepy when you advance into a battle.\n",
      "Predicted Output: If you wish to be able to perform jumps, then all you need is to take your\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1294 - You would cool off because you want to feel more exciting.\n",
      "Predicted Output: You may notice that a lot of people forget to eat food in movie theaters. The movie\n",
      "\n",
      "Predicting Input : 1295 - You would hang out at school because you want to drink with friends.\n",
      "Predicted Output: If you eat alone, you will get headaches. It can be uncomfortable for you.\n",
      "\n",
      "Predicting Input : 1296 - he closes his eyes while reading a magazine.\n",
      "Predicted Output: The university held the competition from last year to\n",
      "\n",
      "Predicting Input : 1297 - You would buy something big because you want to carry it with you.\n",
      "Predicted Output: But if your partner is really big, big and plenty with you and needs you to carry\n",
      "\n",
      "Predicting Input : 1298 - Usually you need to invite animals to a party.\n",
      "Predicted Output: There are plants that it's good to invite people to include in your wedding band.\n",
      "\n",
      "Predicting Input : 1299 - he needs to pay the cost of admission to go to the hospital.\n",
      "Predicted Output: This daycare director, her medical student instructor, part of the Tenchi family, did\n",
      "\n",
      "Predicting Input : 1300 - he danced to ride his bicycle.\n",
      "Predicted Output: In menna from a tree: man carries an umbrella with his breast rest on an umbrella\n",
      "\n",
      "Predicting Input : 1301 - He used a torch to cool the freshly cooked beef.\n",
      "Predicted Output: The professor approached me, making a quick greeting. \"Can you cook eggs like that?\"\n",
      "\n",
      "Predicting Input : 1302 - I'm too full for lunch, so I'm going to eat more.\n",
      "Predicted Output: I'm too busy for a walk. I'd like to come by outside.\"\n",
      "\n",
      "Predicting Input : 1303 - Today's knowledge is too simple for me to understand.\n",
      "Predicted Output: You need powerful minds, not small ones to think about things. To create these big minds\n",
      "\n",
      "Predicting Input : 1304 - People usually read books they are not interested in.\n",
      "Predicted Output: And when reading books, do you read comics? As much as possible.\n",
      "\n",
      "Predicting Input : 1305 - You can practice English by speaking more Spanish.\n",
      "Predicted Output: While working English as a foreign language is hardly the goal of all writers to learn the language\n",
      "\n",
      "Predicting Input : 1306 - It is especially good to harm a child.\n",
      "Predicted Output: 16. Whoever kills, injures, inflicts pain on a child must not be proud\n",
      "\n",
      "Predicting Input : 1307 - You would enjoy the company of your friends because they like the people you hate.\n",
      "Predicted Output: You would wish you can take on the complications of society by taking care of them.\n",
      "\n",
      "Predicting Input : 1308 - babies are bigger than adults.\n",
      "Predicted Output: And people born with a rare condition or epilepsy are usually larger than adults.\n",
      "\n",
      "Predicting Input : 1309 - You are likely to find a bird in a burrow.\n",
      "Predicted Output: You also aren't to keep reindeer in hot heat. You don't need to\n",
      "\n",
      "Predicting Input : 1310 - Something that might happen as a consequence of answering questions is you are dead.\n",
      "Predicted Output: There are plenty of do you doers when you find yourself with questions you don't want\n",
      "\n",
      "Predicting Input : 1311 - You would go to a film because you want to be nervous.\n",
      "Predicted Output: It's funny how a business can get distracted. An advertising campaign isn't even you when\n",
      "\n",
      "Predicting Input : 1312 - Standing in the snow makes you feel hot.\n",
      "Predicted Output: You're awake by a nice looking window while you stay asleep. It means you can rest\n",
      "\n",
      "Predicting Input : 1313 - boats sail on the sky.\n",
      "Predicted Output: But there will also be 'up faraway' planets and'scientists and researchers from\n",
      "\n",
      "Predicting Input : 1314 - I feel dirty after taking a bath.\n",
      "Predicted Output: It's ok that the guys are kind but that don't give off too much light.\n",
      "\n",
      "Predicting Input : 1315 - Everyone has the same skin color.\n",
      "Predicted Output: You may choose not to come to the veterinarian and discuss your dog with him.\n",
      "\n",
      "Predicting Input : 1316 - harder tires grip better in wet weather.\n",
      "Predicted Output: Reaction mounts have a lower weight and quicker response time in rain and snow.\n",
      "\n",
      "Predicting Input : 1317 - Disabled people's life is convenient.\n",
      "Predicted Output: There are fewer drivers, few jobs, fewer services and lower costs for consumers.\n",
      "\n",
      "Predicting Input : 1318 - Often do not eat breakfast will make people healthy.\n",
      "Predicted Output: How do coffee prices in Canada affect your fitness?\n",
      "\n",
      "Predicting Input : 1319 - Most boys make up.\n",
      "Predicted Output: There are scores of arehen (forbidden) babies at St Edward's Hospital\n",
      "\n",
      "Predicting Input : 1320 - Human beings should destroy the environment.\n",
      "Predicted Output: But my constituents don't want to build skyscrapers; they don't want to live\n",
      "\n",
      "Predicting Input : 1321 - People should play pirated games.\n",
      "Predicted Output: And there will never be an age when pirates can play video games without staying up all night\n",
      "\n",
      "Predicting Input : 1322 - I did not water the flowers, so it opened very well.\n",
      "Predicted Output: DID YOU WANT TO TREE THE INVERSE ENTRIES FOR RECOMMEND\n",
      "\n",
      "Predicting Input : 1323 - I want to go out for a picnic in bad weather.\n",
      "Predicted Output: In daydreaming, you can see stars to pick up what you're looking for.\n",
      "\n",
      "Predicting Input : 1324 - Cooking is a very boring thing, so I like it.\n",
      "Predicted Output: You may notice that a lot of people wear lotion (chocolate cream; not cream\n",
      "\n",
      "Predicting Input : 1325 - I never exercise, so I am very healthy.\n",
      "Predicted Output: It's normally difficult to care for your pets. After all, because there are so many\n",
      "\n",
      "Predicting Input : 1326 - Eating a lot makes me thin.\n",
      "Predicted Output: Taste is pleasant and a complete food. Regular care should be taken when choosing your food.\n",
      "\n",
      "Predicting Input : 1327 - I want to have my hair cut because my hair is too short.\n",
      "Predicted Output: It's okay for a woman to 'cheat' your hair. She doesn't have to\n",
      "\n",
      "Predicting Input : 1328 - I like to fail.\n",
      "Predicted Output: But if my identity and interest in people matters, then so do my behaviors.\n",
      "\n",
      "Predicting Input : 1329 - a winter day would make you want to fly a kite.\n",
      "Predicted Output: And there are several in this book. Below is one for 3 adults (14 to give\n",
      "\n",
      "Predicting Input : 1330 - You are likely to find a piece of bread in a furniture store.\n",
      "Predicted Output: There are choices for a wide range of handicrafts in that category.\n",
      "\n",
      "Predicting Input : 1331 - He put a basketball in the printer..\n",
      "Predicted Output: by Merry Santa! the young woman wished to save her case..she put a basketball\n",
      "\n",
      "Predicting Input : 1332 - You parked the boat in the garage.\n",
      "Predicted Output: Sherry came and watched us with his cat\n",
      "\n",
      "Predicting Input : 1333 - a lot of people get hurt in sleep.\n",
      "Predicted Output: But there are few of them. I tend to sleep through the night but have to sleep\n",
      "\n",
      "Predicting Input : 1334 - it is normal for people to use radio to play games.\n",
      "Predicted Output: An unknown singer says, \"Rugrats are sweet.\"\n",
      "\n",
      "Predicting Input : 1335 - The space shuttle needs water to fly.\n",
      "Predicted Output: You don't need 20 gallons of fuel.\n",
      "\n",
      "Predicting Input : 1336 - Your father gave birth to you.\n",
      "Predicted Output: He doesn't tell the cutest old fairy he knows what's happened to his mother.\n",
      "\n",
      "Predicting Input : 1337 - he usually goes to the gym to sleep.\n",
      "Predicted Output: He must listen to a word that he hears a lot of. Or rather, he gets\n",
      "\n",
      "Predicting Input : 1338 - people often go swimming in a car.\n",
      "Predicted Output: But when golfers, even our great grandchildren, try to take advantage of this problem,\n",
      "\n",
      "Predicting Input : 1339 - he uses a stone to clean himself.\n",
      "Predicted Output: He took seven bath-time techniques to awaken the spirits of the demons for his school's\n",
      "\n",
      "Predicting Input : 1340 - He cooked a meal in a paper pot..\n",
      "Predicted Output: the butcher gave a feed to the butcher to cook his food.. The butcher did not\n",
      "\n",
      "Predicting Input : 1341 - people usually dream when they are awake.\n",
      "Predicted Output: There are plenty of \"passive states\", but most people will easily forget one. For\n",
      "\n",
      "Predicting Input : 1342 - he married a woman that he hated.\n",
      "Predicted Output: It got worse. He moved with an aggressive\n",
      "\n",
      "Predicting Input : 1343 - he uses a lot of water to post a message.\n",
      "Predicted Output: He didn't sit to record the message without his girlfriend's permission. He told us he\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1344 - students enhance their academic abilities by sleeping.\n",
      "Predicted Output: They may identify several of the following conditions during study for reading: fluency in class and\n",
      "\n",
      "Predicting Input : 1345 - he uses water to warm himself.\n",
      "Predicted Output: But there are few of them. The porch is warm on the dry skin. A cup\n",
      "\n",
      "Predicting Input : 1346 - he is drunk because he sleeps a lot.\n",
      "Predicted Output: He does watch sports, but he does not play basketball.\n",
      "\n",
      "Predicting Input : 1347 - He braked with his left foot while driving..\n",
      "Predicted Output: \"Once she started running down\n",
      "\n",
      "Predicting Input : 1348 - he drinks oil when he is thirsty.\n",
      "Predicted Output: He doesn't drink taffy (alcoholic beverages).\n",
      "\n",
      "Predicting Input : 1349 - Eating more vegetables will kill people.\n",
      "Predicted Output: Forget rubbish at a restaurant. It makes you sick.\n",
      "\n",
      "Predicting Input : 1350 - Getting enough sleep can make you exhausted.\n",
      "Predicted Output: An Extreme Stress Train, which was built around the animal unable to handle stress\n",
      "\n",
      "Predicting Input : 1351 - Cutting short strings makes the guitar sound.\n",
      "Predicted Output: You can narrow your Vibration Level below \"Low to High\". For small opening strings\n",
      "\n",
      "Predicting Input : 1352 - Pour ink into the washing machine to wash clothes.\n",
      "Predicted Output: At Keto cooking, you will need roughly 2 kg of food per day.\n",
      "\n",
      "Predicting Input : 1353 - Dieting is a way to gain weight.\n",
      "Predicted Output: You know better than to drink more than usual. You're not normally hungry.\n",
      "\n",
      "Predicting Input : 1354 - Most people work to spend money.\n",
      "Predicted Output: Forget spamming, just because you send it doesn't mean you'll make it right\n",
      "\n",
      "Predicting Input : 1355 - males are those who get pregnant and give birth to the next generation.\n",
      "Predicted Output: As one sperm sample, your mom can safely place your baby in your womb to keep her\n",
      "\n",
      "Predicting Input : 1356 - Divergent attention improves learning efficiency.\n",
      "Predicted Output: Not only might language the key to understanding algorithms be helpful in understanding processing tasks, it also\n",
      "\n",
      "Predicting Input : 1357 - Gravity can start an electronic device.\n",
      "Predicted Output: There are plenty of doodles and decorations in Space Station\n",
      "\n",
      "Predicting Input : 1358 - People usually have breakfast after lunch.\n",
      "Predicted Output: And when kids don't get as much breakfast, they try to eat separate meals.\n",
      "\n",
      "Predicting Input : 1359 - He took a nap in the sink.\n",
      "Predicted Output: Toshimi sat in the middle of the table with her hands wrapped around her son's\n",
      "\n",
      "Predicting Input : 1360 - Students usually give lectures to teachers.\n",
      "Predicted Output: Adults attend classes, which are held once a week to complete courses in class. A\n",
      "\n",
      "Predicting Input : 1361 - Cinemas usually play movies after the lights are turned on.\n",
      "Predicted Output: So if your bright-looking cell phone fails to send us a movie text text. We\n",
      "\n",
      "Predicting Input : 1362 - He filled the car with water and drove off.\n",
      "Predicted Output: It was parked about a block away from Lamassu from the garage where it was dropped\n",
      "\n",
      "Predicting Input : 1363 - She often competes with her classmates who are much worse than her.\n",
      "Predicted Output: After his honeymoon in California,\n",
      "\n",
      "Predicting Input : 1364 - He threw his hook into the lake to catch elephants.\n",
      "Predicted Output: The weather forecast called a serious failure\n",
      "\n",
      "Predicting Input : 1365 - I need newspapers to translate languages I don't understand.\n",
      "Predicted Output: We need newspapers to be able to tell stories to others.\n",
      "\n",
      "Predicting Input : 1366 - Swimmers in swimming competitions usually need to wear down jackets.\n",
      "Predicted Output: You could basically charge a gas can for swimming. There are no specialized accessories that can stop\n",
      "\n",
      "Predicting Input : 1367 - She often observes the stars with a telescope in the daytime.\n",
      "Predicted Output: He told Popular Mechanics, \"I use filters to catch water in air.\"\n",
      "\n",
      "Predicting Input : 1368 - He communicated with his friends by newspapers.\n",
      "Predicted Output: Kimmerman was banned from associating with\n",
      "\n",
      "Predicting Input : 1369 - Arguing is a way of expressing love between couples.\n",
      "Predicted Output: There are lots of one-way gay weddings. However, not everyone wants to do one\n",
      "\n",
      "Predicting Input : 1370 - Households get dirty after housework.\n",
      "Predicted Output: New London condo developers, though, are betting on opening new supply chains for future customers.\n",
      "\n",
      "Predicting Input : 1371 - In most cases, you need a driving license to go to other countries.\n",
      "Predicted Output: When immigrants have jobs in the United States, they may\n",
      "\n",
      "Predicting Input : 1372 - People usually go to bars for milk.\n",
      "Predicted Output: But some cows don't get milk on Fridays. They don't sell milk on Sundays.\n",
      "\n",
      "Predicting Input : 1373 - Students with poor eyesight mostly sit in the back rows of the classroom.\n",
      "Predicted Output: So what happens when a poor child may suddenly make her own light bulb or light switch?\n",
      "\n",
      "Predicting Input : 1374 - If you think TV programs are too loud, you can increase the volume.\n",
      "Predicted Output: When you listen to a TV program, remember to listen to the audio content.\n",
      "\n",
      "Predicting Input : 1375 - Usually the one who gets a lower score in basketball game wins the game.\n",
      "Predicted Output: There are plenty of all-time great NBA players who take it easy on themselves. Some\n",
      "\n",
      "Predicting Input : 1376 - He wrote a book review before reading the book.\n",
      "Predicted Output: The comments weren't a surprise. Moks was born in India and grew up in northern\n",
      "\n",
      "Predicting Input : 1377 - People usually travel to the other side of the earth on foot.\n",
      "Predicted Output: There are numerous roads, schools, stores and food trucks that go around these cities. If\n",
      "\n",
      "Predicting Input : 1378 - If you lose too much weight, you won't be able to wear the clothes you used to wear.\n",
      "Predicted Output: There are plenty of all-over clothes available to fit most people and whether you like them\n",
      "\n",
      "Predicting Input : 1379 - People usually cry after hearing a funny joke.\n",
      "Predicted Output: You may notice it. It's not scary. But this is probably how you're feeling\n",
      "\n",
      "Predicting Input : 1380 - a new hairstyle will give me the same look.\n",
      "Predicted Output: i don't want people to be afraid of me anymore.but after all these years I will make\n",
      "\n",
      "Predicting Input : 1381 - too much smoking can cure pulmonary disease.\n",
      "Predicted Output: 22. Diseases like the flu and stomach pains\n",
      "\n",
      "Predicting Input : 1382 - when your car is inspected, you are driving across the border.\n",
      "Predicted Output: You may notice at a tow home that someone is acting poorly. Get help from a tow\n",
      "\n",
      "Predicting Input : 1383 - it rarely winds in the desert, so the desert is dry.\n",
      "Predicted Output: You also generally don't need to design trails to avoid snow. Use campsites to avoid\n",
      "\n",
      "Predicting Input : 1384 - usually there are many elephants in malls.\n",
      "Predicted Output: If you park your car in a parking garage, just pull out your wallet and hand it\n",
      "\n",
      "Predicting Input : 1385 - milk is from milkers' body.\n",
      "Predicted Output: Kashmiri tea is their favourite beverage.\n",
      "\n",
      "Predicting Input : 1386 - Chicago is the GDP of a city.\n",
      "Predicted Output: In measuring all cities\n",
      "\n",
      "Predicting Input : 1387 - you can set cheese to catch the mouse.\n",
      "Predicted Output: And there are four to five things that slow you down and make your mice squeal:\n",
      "\n",
      "Predicting Input : 1388 - I put my clothes in a shoebox.\n",
      "Predicted Output: I felt nothing.\n",
      "\n",
      "Predicting Input : 1389 - this car is too cheap for Tom to buy it.\n",
      "Predicted Output: ...keys on the dash.\"\n",
      "\n",
      "Predicting Input : 1390 - people can pick watermelons off from trees.\n",
      "Predicted Output: You can listen to a song on his iPod to hear his voice over video\n",
      "\n",
      "Predicting Input : 1391 - the color of the banana is banana.\n",
      "Predicted Output: If you eat banana, you will get bananas. If you aren't eating banana, you\n",
      "\n",
      "Predicting Input : 1392 - this plate tastes very good.\n",
      "Predicted Output: Plays Perfect with tines. The tartness is lost on cool cooking.\n",
      "\n",
      "Predicting Input : 1393 - I'm afraid to take flight so the plane may crash.\n",
      "Predicted Output: We're safely back. I don't believe I'll run out of breath\n",
      "\n",
      "Predicting Input : 1394 - a farm may grow a crop of cows and sheep.\n",
      "Predicted Output: If you plant fish, they will make fertilizers for your fish as well.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1395 - if you want to have lunch, you must choose a restaurant.\n",
      "Predicted Output: There are approximately 21,000 stations in Armenia. Some host free meals for 12 to 17\n",
      "\n",
      "Predicting Input : 1396 - most people use arms to walk.\n",
      "Predicted Output: As outfitted medical and science officers in Victorian times would say, arms make people better medical\n",
      "\n",
      "Predicting Input : 1397 - silk is a common material for wedding rings.\n",
      "Predicted Output: For example: some of the numbers on cups for romantic occasions are wine glasses.\n",
      "\n",
      "Predicting Input : 1398 - he likes sitting in a boat and floating on the desert.\n",
      "Predicted Output: If you drink hot and cold water. Women can drink hot and cold water to lighten\n",
      "\n",
      "Predicting Input : 1399 - if you want to travel, you should have a talk with a pilot.\n",
      "Predicted Output: For example: if a driver was having lunch and asked you to drive him to the airport\n",
      "\n",
      "Predicting Input : 1400 - you can jump out of a window if the window is closed.\n",
      "Predicted Output: For example you can not shoot an aircraft above the roof and not jump off the roof of\n",
      "\n",
      "Predicting Input : 1401 - you need to have a dream first then you can awake.\n",
      "Predicted Output: You need to live. A dream is meant to increase your potential for happiness. The dream\n",
      "\n",
      "Predicting Input : 1402 - a thief stole my finger.\n",
      "Predicted Output: We end our conversation, with an icepick and traditional cheese. My dad asked me if\n",
      "\n",
      "Predicting Input : 1403 - Tom caught a big tiger in the sea.\n",
      "Predicted Output: Lions give off chemicals that kill animals.\n",
      "\n",
      "Predicting Input : 1404 - I'm dirty now so I need to have lunch.\n",
      "Predicted Output: I've got breakfast before. I need to wear this shirt so I don't look like I\n",
      "\n",
      "Predicting Input : 1405 - I can hear the light from the moon at night.\n",
      "Predicted Output: Mover about tire inside.\n",
      "\n",
      "Predicting Input : 1406 - he is rude to others so he has many friends.\n",
      "Predicted Output: He should earn money, not work. People go out and don't usually work. He\n",
      "\n",
      "Predicting Input : 1407 - they put out a fire successfully with kerosene.\n",
      "Predicted Output: This means nothing has to do with how dirty the soil is. Sand never gives you rock\n",
      "\n",
      "Predicting Input : 1408 - I often watch movies with my girlfriend in the gallery.\n",
      "Predicted Output: If you aren't a Macbook Pro gamer, then you may want to consider going with\n",
      "\n",
      "Predicting Input : 1409 - the waiter serves me coffee on the same plate every morning.\n",
      "Predicted Output: The Korean waiter goes to me in the cafe to receive my order and begins by giving me\n",
      "\n",
      "Predicting Input : 1410 - the car broke down so I lifted the window to check it.\n",
      "Predicted Output: I went upstairs and the wall was almost cracked. He held the glass instead of the glass\n",
      "\n",
      "Predicting Input : 1411 - smelling an apple every day can be good for your health.\n",
      "Predicted Output: It depends on what kind of apple you want. White\n",
      "\n",
      "Predicting Input : 1412 - we use walls to let the light shine into the house.\n",
      "Predicted Output: You do need to be strong to run outside. However, you don't need to run\n",
      "\n",
      "Predicting Input : 1413 - we can learn what will happen in the future by reading diary.\n",
      "Predicted Output: For far greater situations, we can read diary. To understand that much better, we need\n",
      "\n",
      "Predicting Input : 1414 - I parked my car on the highway.\n",
      "Predicted Output: I went outside and the dark smog shook the buildings.\n",
      "\n",
      "Predicting Input : 1415 - I learned to play basketball to enjoy music.\n",
      "Predicted Output: We'll definitely keep a national TV program starting in January of next year that will play national\n",
      "\n",
      "Predicting Input : 1416 - microphone is often a remarkable character of the speaker.\n",
      "Predicted Output: There are plenty of \"fast\" audio codecs like FLAC that give us our audio\n",
      "\n",
      "Predicting Input : 1417 - people often sleep with their shoes on.\n",
      "Predicted Output: There are widely differing results on the relationship between physical activities and physical fitness. One study found\n",
      "\n",
      "Predicting Input : 1418 - humans can fly with wings.\n",
      "Predicted Output: An alien assassin cannot be killed with a crow or brush with a razor blade.\n",
      "\n",
      "Predicting Input : 1419 - laziness is one of the reasons people travel.\n",
      "Predicted Output: You may notice not a lot of person travelling in cities on the subway or bus. There\n",
      "\n",
      "Predicting Input : 1420 - two people who hate each other will kiss.\n",
      "Predicted Output: He says having sex to feel good is crucial to survive in the Syrian conflict.\n",
      "\n",
      "Predicting Input : 1421 - a first class airline seat is for fat people.\n",
      "Predicted Output: You may drink alcohol, but you may never even cook for your meal.\n",
      "\n",
      "Predicting Input : 1422 - Alice was late three times this week and the boss was very happy for it.\n",
      "Predicted Output: Waves wrapped around a particularly large doll girl\n",
      "\n",
      "Predicting Input : 1423 - he swims in his own bathtub every day.\n",
      "Predicted Output: He doesn't offer a personal space to wash his arms and legs or hang out in his\n",
      "\n",
      "Predicting Input : 1424 - the telephone is one of the most important invitations in human history.\n",
      "Predicted Output: But there are still a few things that aren't mentioned in the Bible that do have any\n",
      "\n",
      "Predicting Input : 1425 - I poured kerosene on top of the cereal as my breakfast.\n",
      "Predicted Output: Raimondria started a fire in the bushes. She sat on one side of the farm\n",
      "\n",
      "Predicting Input : 1426 - he is a pitiful person because he has no debit.\n",
      "Predicted Output: He does need enough to pay for his schooling. If he had known what he was doing\n",
      "\n",
      "Predicting Input : 1427 - the food tastes very bad and I enjoy it very much.\n",
      "Predicted Output: My wife ordered our traditional Chinese meal\n",
      "\n",
      "Predicting Input : 1428 - the bar is a good place to prepare for the exam.\n",
      "Predicted Output: There are excellent restaurants, schools, schools at the clubs and better sleeping arrangements in the rooms\n",
      "\n",
      "Predicting Input : 1429 - if you don't know how to cook it, you can follow a menu.\n",
      "Predicted Output: 10 is an Indonesian painkiller and about 40 one can be bought\n",
      "\n",
      "Predicting Input : 1430 - nearsighted people need glasses to sleep.\n",
      "Predicted Output: There are limits in a human's eyesight. Some people have sight problems and will take\n",
      "\n",
      "Predicting Input : 1431 - many precious stones are used in cooking.\n",
      "Predicted Output: As we talked about, all of the diamonds and metals are used for jewelry. \"All\n",
      "\n",
      "Predicting Input : 1432 - we built up an army to protect the enemy.\n",
      "Predicted Output: If you wish to be disarmed, choose the Constitution option. If you don't want\n",
      "\n",
      "Predicting Input : 1433 - when you are drowning, a computer can save your life.\n",
      "Predicted Output: There are numerous organizations, including the National Institutes of Health, that implement artificial intelligence to increase\n",
      "\n",
      "Predicting Input : 1434 - I shouted at the top of my voice to avoid her attention.\n",
      "Predicted Output: Lobster Sharks are killing humans. Why should we go after\n",
      "\n",
      "Predicting Input : 1435 - I felt the building was shaking and then I realized it was rainy.\n",
      "Predicted Output: Meyers saw a field of snow blocking the street and no fire crew were on duty\n",
      "\n",
      "Predicting Input : 1436 - they gave me a crutch to sit when I came in.\n",
      "Predicted Output: I never imagined your \"smock\" shape was non-sexual for him.\n",
      "\n",
      "Predicting Input : 1437 - children can't learn to cry until they reach a certain age.\n",
      "Predicted Output: The dead shark's stomach still inside its stomach\n",
      "\n",
      "Predicting Input : 1438 - sleeping at night is for not being hungry.\n",
      "Predicted Output: You may eat anything, also not only jelly\n",
      "\n",
      "Predicting Input : 1439 - I will make sure the door is open when I leave the house.\n",
      "Predicted Output: This may require making a sound in the hallway. If you don't give it a chance\n",
      "\n",
      "Predicting Input : 1440 - I was so angry to see an old friend in a foreign land.\n",
      "Predicted Output: Nancy Ellen Grevitt: Obey the ruling of the rebels and support the soldiers\n",
      "\n",
      "Predicting Input : 1441 - she had a bad day and she felt white.\n",
      "Predicted Output: The pattern inspired another, more unfriendly look for Peperte and she didn't\n",
      "\n",
      "Predicting Input : 1442 - when my plants are dead, I water them.\n",
      "Predicted Output: 11. Drink food, so that you wouldn't waste them. I avoid eating water all\n",
      "\n",
      "Predicting Input : 1443 - I enjoy the sunshine in the evening very much.\n",
      "Predicted Output: [To Maricia (3 days after she conceived]]. And I live very good of water\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1444 - I was very happy because my big brother was born yesterday.\n",
      "Predicted Output: Rosh Hashanah is the beginning of the flood and is considered one of the religious\n",
      "\n",
      "Predicting Input : 1445 - I kicked the teammate into the goal and win the football match.\n",
      "Predicted Output: I left everything behind, but I don't want to believe I lost because I don't\n",
      "\n",
      "Predicting Input : 1446 - it's really cold so I light a screen to get warmth and light.\n",
      "Predicted Output: ALSO MAKE HEAT UNTIL OPEN by gaining 26D DRINKED GUN\n",
      "\n",
      "Predicting Input : 1447 - today I washed soup.\n",
      "Predicted Output: PATTY PARKES' CHICAGO -- Charlotte County Fire Lt. Charles Preski\n",
      "\n",
      "Predicting Input : 1448 - I tripped on a rock and then flied.\n",
      "Predicted Output: The crowd gasped and a girl who had crossed the street stopped to kiss her.\n",
      "\n",
      "Predicting Input : 1449 - I drove from China to the North Pole.\n",
      "Predicted Output: We went around out a couple of times each day and didn't travel between different countries.\n",
      "\n",
      "Predicting Input : 1450 - many clouds fell to the ground during the windstorm yesterday.\n",
      "Predicted Output: Asewyang Central/Alot of 390 are visible at this location on Oct. 30\n",
      "\n",
      "Predicting Input : 1451 - a salesman can fool a product.\n",
      "Predicted Output: And if those devices and products are made up of commonly available materials you'll want to check\n",
      "\n",
      "Predicting Input : 1452 - you can gather energy by taking exercise.\n",
      "Predicted Output: There are plenty of saunas in museums. More than 10 different kinds of saun\n",
      "\n",
      "Predicting Input : 1453 - anything can't be bought for the high price.\n",
      "Predicted Output: Now you need to order on different\n",
      "\n",
      "Predicting Input : 1454 - I bought him knowledge.\n",
      "Predicted Output: 13 of England going to country in 1852. King Richard v denied king's claim to\n",
      "\n",
      "Predicting Input : 1455 - I left my car in my wallet.\n",
      "Predicted Output: Rihanna left a note in her purse\n",
      "\n",
      "Predicting Input : 1456 - you need the right swimsuit if you want to fish.\n",
      "Predicted Output: There are numerous swimfishing activities and beginner's guides for children on YouTube.\n",
      "\n",
      "Predicting Input : 1457 - we need a referee to ensure the game is fantastic.\n",
      "Predicted Output: \"If players don't want to play against the referee or do something unusual and we don\n",
      "\n",
      "Predicting Input : 1458 - I send my new book to hospitals to sell it.\n",
      "Predicted Output: But there will not be other print books released in Russia that are widely considered as good money\n",
      "\n",
      "Predicting Input : 1459 - I caught this rabbit on the Moon.\n",
      "Predicted Output: so fishing is illegal in New Zealand\n",
      "\n",
      "Predicting Input : 1460 - when I am not fit, I will go to the pharmacy to ask help from a doctor.\n",
      "Predicted Output: The doctor starts having a heavy drawl before he talks to you and tells you to hold\n",
      "\n",
      "Predicting Input : 1461 - we can find sugar in ocean water.\n",
      "Predicted Output: As we observe Earth, we can see Saturn and Neptune are very dim star systems. So\n",
      "\n",
      "Predicting Input : 1462 - a thousand microphones are going to listen to his speech today.\n",
      "Predicted Output: You could argue that a tremor is temporary, but this is less likely to happen when\n",
      "\n",
      "Predicting Input : 1463 - I lost my car in the traffic jam.\n",
      "Predicted Output: In fact I went to do the children's school because it was Friday night.\n",
      "\n",
      "Predicting Input : 1464 - the criminal was sent to prison but he felt very regretful.\n",
      "Predicted Output: Talymel found a document on the cellphone of others who had linked him to the unknown\n",
      "\n",
      "Predicting Input : 1465 - the cry of the audience is the best encouragement for the comedian.\n",
      "Predicted Output: …the rehearsals, which are put together in special situations, always sell out. But\n",
      "\n",
      "Predicting Input : 1466 - I had a sleep to lie in the bed.\n",
      "Predicted Output: I got up soon after, but it was freezing. It was still freezing here.\n",
      "\n",
      "Predicting Input : 1467 - I felt very nervous after taking the midterm.\n",
      "Predicted Output: I got dressed for a break. I stepped out of the study room and turned to look\n",
      "\n",
      "Predicting Input : 1468 - throw things you don't understand anymore into the trash container.\n",
      "Predicted Output: You need something you are easily able to manipulate. When you play Pokémon Go, you need\n",
      "\n",
      "Predicting Input : 1469 - you need to play baseball well before going to watch a baseball game.\n",
      "Predicted Output: If you watch baseball, you will play hockey. Yes, if you watch hockey, you\n",
      "\n",
      "Predicting Input : 1470 - every morning I read the news on the diary.\n",
      "Predicted Output: I read it daily for a few days. Before I became writing in radio it was all over\n",
      "\n",
      "Predicting Input : 1471 - soccer ball is a kind of sport.\n",
      "Predicted Output: This might sound crazy, but it's usually not a bad thing for soccer to be played\n",
      "\n",
      "Predicting Input : 1472 - I buy this book from the library.\n",
      "Predicted Output: my brother wishes he was back to law school. our parents were married over 18 years old\n",
      "\n",
      "Predicting Input : 1473 - drinking too much coffee can make you drunk.\n",
      "Predicted Output: As with soda, a self-drinking drink like coffee can cause alcohol to end up\n",
      "\n",
      "Predicting Input : 1474 - I am used to having a cup of vodka for waking up in the morning.\n",
      "Predicted Output: —Even alcoholics, who have been conditioned to drink water and drinks heavily\n",
      "\n",
      "Predicting Input : 1475 - grass is a common material for a bowl.\n",
      "Predicted Output: For children ages 6-12, there are two versions of the leaf available: the ordinary\n",
      "\n",
      "Predicting Input : 1476 - I need to see a psychologist because I had violent stomach pains.\n",
      "Predicted Output: The solution would be a PMO. Think of these way: When your stress is low\n",
      "\n",
      "Predicting Input : 1477 - my lifestyle is green and I plant fertilizer every year.\n",
      "Predicted Output: If you choose to be included in the environmentally conscious lifestyle that you choose to live, at\n",
      "\n",
      "Predicting Input : 1478 - I bought a ticket and act in a movie at the cinema.\n",
      "Predicted Output: so so obviously me, do you need drugs to live at the cinema when you have no\n",
      "\n",
      "Predicting Input : 1479 - It's very interesting to ride a cat on the grassland.\n",
      "Predicted Output: Sylvia Lazio, 19, from Epsom, passed by without permission on the edge\n",
      "\n",
      "Predicting Input : 1480 - they began to enjoy being apart after they fell in love.\n",
      "Predicted Output: If you haven't had one of these exams in years, you might want to take your\n",
      "\n",
      "Predicting Input : 1481 - parents often use chocolate to punish their children.\n",
      "Predicted Output: There are numerous arguments. There is no credible evidence that chocolate can reduce aggression; it only\n",
      "\n",
      "Predicting Input : 1482 - Tom was very happy because he was bullied in school.\n",
      "Predicted Output: 'He lived with a bad mother in Romania. There was no abuse so he had no\n",
      "\n",
      "Predicting Input : 1483 - eating salty food would make you hungry.\n",
      "Predicted Output: You want foods that you like. A salty food makes you hungry to eat.\n",
      "\n",
      "Predicting Input : 1484 - using a washing machine to wash vegetables is very convenient.\n",
      "Predicted Output: On the flip side, many people like washing their clothing with very cold water. A wash\n",
      "\n",
      "Predicting Input : 1485 - I paid to grow longer hair.\n",
      "Predicted Output: You decide to avoid eating too much medicine.\n",
      "\n",
      "Predicting Input : 1486 - I have a basket for drinking water in my office.\n",
      "Predicted Output: It's convenient that a lack of food choices is true in so many fields.\n",
      "\n",
      "Predicting Input : 1487 - we use glue to ensure that shoes will not drop when walking.\n",
      "Predicted Output: When doing homework I have to make sure my students know what I'm talking about. If\n",
      "\n",
      "Predicting Input : 1488 - my dream is to be a teacher because I want to share my food.\n",
      "Predicted Output: 20:54 – a girl from my hometown has sat by me while reading and she comes\n",
      "\n",
      "Predicting Input : 1489 - child need to be taught how to laugh.\n",
      "Predicted Output: You need to learn a lot of local slang. Just look at English speakers of any language\n",
      "\n",
      "Predicting Input : 1490 - the taste in art is an objective matter.\n",
      "Predicted Output: 25. Discuss using a blog to share experiences. Yes, you need to live in the\n",
      "\n",
      "Predicting Input : 1491 - the airport is one of the fastest things in the world.\n",
      "Predicted Output: But there are big-market Turkish Airlines Boeing 777 with wait list air carriers in their fleet\n",
      "\n",
      "Predicting Input : 1492 - the weight machine showed that I had grown taller.\n",
      "Predicted Output: The heavy carriage gave a slow 'climb' up to the ceiling and she made her\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1493 - you need to invigilate exams if you want to graduate.\n",
      "Predicted Output: When you graduate from a military school, everything is carefully reviewed to ensure your degree is unique\n",
      "\n",
      "Predicting Input : 1494 - because he is rich, he lives a frugal life.\n",
      "Predicted Output: But if Clinton means a change to the incomes of Americans and not higher taxes on the wealthy\n",
      "\n",
      "Predicting Input : 1495 - I carried a fork to eat soup.\n",
      "Predicted Output: I fell asleep the next morning.\n",
      "\n",
      "Predicting Input : 1496 - to be a husband, you need to accept strict training.\n",
      "Predicted Output: He does so by the following three simple exercises. he turns on his heels and retires\n",
      "\n",
      "Predicting Input : 1497 - the dog barked all day and I had to wear nasal plugs.\n",
      "Predicted Output: Q: What sort of a dog was you trained to handle in the meantime?\n",
      "\n",
      "Predicting Input : 1498 - I prepared a rocket to go to American.\n",
      "Predicted Output: In 2016 US troops in Iraq will have fought in Syria and no larger army will be needed\n",
      "\n",
      "Predicting Input : 1499 - I will wear thicker eyeglasses in winter to stay warm.\n",
      "Predicted Output: As with gloves, a measure of body composition is necessary to protect against infections.\n",
      "\n",
      "Predicting Input : 1500 - I pulled the door shut with my eye.\n",
      "Predicted Output: \"On your knees,\" the boy said. He didn't care for people coming or going for\n",
      "\n",
      "Predicting Input : 1501 - I used my third hand to lift my coat.\n",
      "Predicted Output: Finger pulled mine back, so I don't have to carry it around now.\n",
      "\n",
      "Predicting Input : 1502 - cut your fingernails if you don't want your socks to have holes.\n",
      "Predicted Output: So there are far more ways to change socks and binders and buttons than just having them\n",
      "\n",
      "Predicting Input : 1503 - after finishing all my work I felt very stressful.\n",
      "Predicted Output: It was terrible as a working day. Sometimes I saw many people suffering from depression. For\n",
      "\n",
      "Predicting Input : 1504 - you can find clouds underground.\n",
      "Predicted Output: And there are plenty of cool things to enjoy in summer. A dive could be more fun\n",
      "\n",
      "Predicting Input : 1505 - we added steel to keep the fire burning.\n",
      "Predicted Output: Larger Warbas Thief finished testing on the walls\n",
      "\n",
      "Predicting Input : 1506 - it's ok to go when the traffic light is red.\n",
      "Predicted Output: As we talked about, this is not merely a waste of time on average. It can\n",
      "\n",
      "Predicting Input : 1507 - people in the desert often look forward to flood.\n",
      "Predicted Output: But when disasters occur, they can make pollution even worse. The Environmental Protection Agency has issued\n",
      "\n",
      "Predicting Input : 1508 - my favorite thing is surfing in the swimming pool.\n",
      "Predicted Output: This old greeting post, which is now extinct, makes us feel guilty about being grumpy\n",
      "\n",
      "Predicting Input : 1509 - today I forgot to bring my mouth and couldn't have lunch.\n",
      "Predicted Output: I'm having lunch now! I want to thank you so much and thank you for my bag\n",
      "\n",
      "Predicting Input : 1510 - you need a compass to measure the distance.\n",
      "Predicted Output: There are lots of deceptively simple puzzles to solve in this subject; more on this\n",
      "\n",
      "Predicting Input : 1511 - her snide comment about my weight greatly encouraged me.\n",
      "Predicted Output: 12. Quite quickly, I found out just how thin we are and started talking with our\n",
      "\n",
      "Predicting Input : 1512 - the ground was covered by wind after the storm.\n",
      "Predicted Output: In return for ten tributaries to strengthen the beach and keep wind off the island,\n",
      "\n",
      "Predicting Input : 1513 - we can see different moon by traveling.\n",
      "Predicted Output: And if there are a lot of amazing moons, then if you always leave us with only\n",
      "\n",
      "Predicting Input : 1514 - Jim stole her heart and now he is a criminal.\n",
      "Predicted Output: There will be further updates on MyMusicAsks.\n",
      "\n",
      "Predicting Input : 1515 - people smoke to get sick.\n",
      "Predicted Output: There are doctors who are nothing but professional surgeons. When they go there they make their patients\n",
      "\n",
      "Predicting Input : 1516 - every boat has a life jacket to prevent a person from falling in water.\n",
      "Predicted Output: If you decide to be at the dinner table, another important thing is that you will get\n",
      "\n",
      "Predicting Input : 1517 - the drink is too iced so I dropped many ice cubes in.\n",
      "Predicted Output: I'm too stubborn to be serious in all situations. So not only should it be fun to\n",
      "\n",
      "Predicting Input : 1518 - a good cooker can cook good food.\n",
      "Predicted Output: There are plenty of chives in this vegetable dish and they are absolutely delicious. I find\n",
      "\n",
      "Predicting Input : 1519 - we dug on the wall to drill a well for the village.\n",
      "Predicted Output: The village formed when a member of the Seventh Army village became ill from hunger. A hospital\n",
      "\n",
      "Predicting Input : 1520 - he is such a smart boy that he learns everything slowly.\n",
      "Predicted Output: He taught his daughter her proper manners and isn't afraid\n",
      "\n",
      "Predicting Input : 1521 - he can't walk with an injured hand.\n",
      "Predicted Output: If you aren't a patient, you cannot look into their eyes at all.\n",
      "\n",
      "Predicting Input : 1522 - I was so hungry that I ate the whole elephant.\n",
      "Predicted Output: The container contained five tumblers. Each is smaller than a lion's box. The\n",
      "\n",
      "Predicting Input : 1523 - the chef is hired to eat food for customers.\n",
      "Predicted Output: Rather than walking through the kitchen there's a window\n",
      "\n",
      "Predicting Input : 1524 - he is a nice boy because he yells at everyone.\n",
      "Predicted Output: Stupid guy is stupid just like you\n",
      "\n",
      "Predicting Input : 1525 - my cat feeds me every day.\n",
      "Predicted Output: If you haven't had the chance to chat with anyone about your experiences as a cat that\n",
      "\n",
      "Predicting Input : 1526 - I put on my clothes to take a shower.\n",
      "Predicted Output: I got very sleepy at the end of the shower. The beautiful woman waiting there did not let\n",
      "\n",
      "Predicting Input : 1527 - Tom smashed glass and reused it after cleaning.\n",
      "Predicted Output: He went outside and to his room to drain the sink.\n",
      "\n",
      "Predicting Input : 1528 - I stored my boxes in a book.\n",
      "Predicted Output: It was unusual for a type of box in the desert to have text inside.\n",
      "\n",
      "Predicting Input : 1529 - you have to sing if you want to catch a fish.\n",
      "Predicted Output: You just wear clothing, not fish. Fish are fish.\n",
      "\n",
      "Predicting Input : 1530 - I need to go to the hospital to treat my tail.\n",
      "Predicted Output: The Wallabies home-field was put under control after another wind blew down and the Wall\n",
      "\n",
      "Predicting Input : 1531 - he got married and became a single man.\n",
      "Predicted Output: He got divorced from a woman in second grade. He went to college for some time and\n",
      "\n",
      "Predicting Input : 1532 - trees grow leaves to prevent being knocked down.\n",
      "Predicted Output: There are sprouts, water canals and even jack trees to plant trees to keep people\n",
      "\n",
      "Predicting Input : 1533 - we are all very angry that it's sunny again after so many days raining.\n",
      "Predicted Output: 14:42 Russ: That's what Bob was talking about. After getting back to Mr\n",
      "\n",
      "Predicting Input : 1534 - I'm looking for the handrail to go upstairs.\n",
      "Predicted Output: Named after Mickey Mouse\" A large standard drawer in this building is located near the general store\n",
      "\n",
      "Predicting Input : 1535 - I applied for the job to relax.\n",
      "Predicted Output: The physical manifestation of a physical function is psychological. People don't sleep well and don't\n",
      "\n",
      "Predicting Input : 1536 - I bought a new toothbrush to clean my face.\n",
      "Predicted Output: the baby sponge told the cat to hold onto the cloth that was wetter on the baby\n",
      "\n",
      "Predicting Input : 1537 - I used an umbrella to avoid drenching on the beach.\n",
      "Predicted Output: Stables carry alcohol, so you could safely bring pot with you at night.\n",
      "\n",
      "Predicting Input : 1538 - I hurried to the ticket window to sell two tickets.\n",
      "Predicted Output: I waited an hour and a half before we could leave\n",
      "\n",
      "Predicting Input : 1539 - bad luck is part of the reason I can succeed.\n",
      "Predicted Output: You need luck in a place to make progress. That's what positive dreams are for.\n",
      "\n",
      "Predicting Input : 1540 - I connected to the hi-fi to shop online.\n",
      "Predicted Output: It was busy so the average person would shut it down on the minute they saw the picture\n",
      "\n",
      "Predicting Input : 1541 - Alice lacks the laziness to be in charge of a project.\n",
      "Predicted Output: As we mentioned earlier, who are the guardians of risk and cost for applications?\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1542 - as a butcher, I killed farmers every day.\n",
      "Predicted Output: It's funny how a young man will smile for free if he sees something you see on\n",
      "\n",
      "Predicting Input : 1543 - tomato paste is common in the bathroom.\n",
      "Predicted Output: There are plenty of yummy things to eat in Japan. In Japan we don't eat\n",
      "\n",
      "Predicting Input : 1544 - my baby needs a bottle of beer every day.\n",
      "Predicted Output: He knows he isn't going to spend all his money\n",
      "\n",
      "Predicting Input : 1545 - he brought a moon for his wife.\n",
      "Predicted Output: But when Luna stopped, his mother cringed. How could he hear her own voice?\n",
      "\n",
      "Predicting Input : 1546 - Tom hunted a bear in the library.\n",
      "Predicted Output: Nobody had known any sort of lighting for the bear\n",
      "\n",
      "Predicting Input : 1547 - he went to the kitchen because he felt sleepy.\n",
      "Predicted Output: Hospital Inspectors, which are involved with most hospitals in the UK, could not visit\n",
      "\n",
      "Predicting Input : 1548 - he was very sad when his pay raised.\n",
      "Predicted Output: In sum – 10% of that $450.000 pay you paid him as a bonus\n",
      "\n",
      "Predicting Input : 1549 - the baby kangaroo will be put in its mother's hands.\n",
      "Predicted Output: If you haven't had the chance to kiss the baby kangaroo before, you're\n",
      "\n",
      "Predicting Input : 1550 - we always drink apples at breakfast.\n",
      "Predicted Output: You need apples for a ball of brown cake. If you need apples for the ball of\n",
      "\n",
      "Predicting Input : 1551 - I felt energetic after having expired food.\n",
      "Predicted Output: The chemicians gave a red food with carrots. He felt too exhausted and couldn't eat\n",
      "\n",
      "Predicting Input : 1552 - a credit card allows me not to pay money.\n",
      "Predicted Output: 80 It requires enough to pay for my groceries. This should be paid off by me but\n",
      "\n",
      "Predicting Input : 1553 - I do homework every day to keep healthy.\n",
      "Predicted Output: If you hear someone and can't find someone to meet you, talk you into going to\n",
      "\n",
      "Predicting Input : 1554 - I built a car in the store.\n",
      "Predicted Output: It was dirty and the fan was too loud. It did not fit into the place and\n",
      "\n",
      "Predicting Input : 1555 - she is good at swimming so she plays water polo well.\n",
      "Predicted Output: For those wondering how a Neopets scarf can affect her levels of Beauty? Jota\n",
      "\n",
      "Predicting Input : 1556 - I swim well so I have an advantage in art.\n",
      "Predicted Output: Saving money isn't a bad thing. Especially by selling stuff. Of course it's something\n",
      "\n",
      "Predicting Input : 1557 - her skin got whiter after sunbathing.\n",
      "Predicted Output: There are literally hundreds of types of sunflower (except my own sunflower for this reason\n",
      "\n",
      "Predicting Input : 1558 - I speeded up my car to park it.\n",
      "Predicted Output: I don't know how many times you went to the bathroom\n",
      "\n",
      "Predicting Input : 1559 - I am looking for a new wife after being fired.\n",
      "Predicted Output: We only listen to a single third of radio the night before the wedding is where we sit\n",
      "\n",
      "Predicting Input : 1560 - you need hardware or software to surf the Internet.\n",
      "Predicted Output: There are zero hidden hangups in surfing a server in the browser; their name can\n",
      "\n",
      "Predicting Input : 1561 - I helped the robot to find its dad.\n",
      "Predicted Output: But when Elsa finally said goodbye, they rushed to find her. She picked up the cup\n",
      "\n",
      "Predicting Input : 1562 - I prepared some fuel to power myself.\n",
      "Predicted Output: I took out two bottles, one of which contained which was blood. She broke it with her\n",
      "\n",
      "Predicting Input : 1563 - policeman tried to arrest criminals to send them home.\n",
      "Predicted Output: But what happens when a cop doesn't recognize the king you're visiting as you're driving\n",
      "\n",
      "Predicting Input : 1564 - I will go to the hospital tomorrow to check if I'm alive.\n",
      "Predicted Output: Kaiyaichi Kedugo: Ya-han and I'll talk about it later\n",
      "\n",
      "Predicting Input : 1565 - I saw the sound of the birds.\n",
      "Predicted Output: These two birds took up position next to me.\n",
      "\n",
      "Predicting Input : 1566 - nurses take care of doctors.\n",
      "Predicted Output: But when Zhang tries the pressure up to burst the bubble of power--after all, who\n",
      "\n",
      "Predicting Input : 1567 - the fans sat in the soccer field to enjoy the football match.\n",
      "Predicted Output: In fact, first, you had to relax a bit and take off your shoes. After\n",
      "\n",
      "Predicting Input : 1568 - I called a taxi and paid for the car.\n",
      "Predicted Output: KELLEYH. KELLEYH. So we're driving down the street in\n",
      "\n",
      "Predicting Input : 1569 - I keep things messy so that I can find something quick.\n",
      "Predicted Output: But if my bra and underwear are too messy, they don't fit too well. L\n",
      "\n",
      "Predicting Input : 1570 - I wore my wedding ring on my left foot.\n",
      "Predicted Output: I'm different now from a lot of people before. This year I wasn't able to attend\n",
      "\n",
      "Predicting Input : 1571 - a ruler can help you do sums faster.\n",
      "Predicted Output: Whilst avoiding the mall as much as possible, you may\n",
      "\n",
      "Predicting Input : 1572 - villagers planted a well to get water.\n",
      "Predicted Output: This story originally appeared in the September 2015 issue of Paper Paths and Water.\n",
      "\n",
      "Predicting Input : 1573 - I used steel to make a fish tank by myself.\n",
      "Predicted Output: filter handles by\n",
      "\n",
      "Predicting Input : 1574 - I put my books in the wine rack.\n",
      "Predicted Output: I cried when I put my\n",
      "\n",
      "Predicting Input : 1575 - everyone is amazed at her appearance when they first time smell her.\n",
      "Predicted Output: So no requirement for a guy to have beard? No need to demonstrate teeth? You're\n",
      "\n",
      "Predicting Input : 1576 - we need woods to produce erasers.\n",
      "Predicted Output: As with swords, a wood with and away from wood will not produce batter.\n",
      "\n",
      "Predicting Input : 1577 - I sunbathe on the dressing room every summer vacation.\n",
      "Predicted Output: He'd attended events, such as the Dalai Lama's Badoumi celebrations in China in\n",
      "\n",
      "Predicting Input : 1578 - a rainy day is a good chance to dry the clothes.\n",
      "Predicted Output: There are plenty of \"can't go swimming\" holidays and your beach holiday can be pretty\n",
      "\n",
      "Predicting Input : 1579 - if you are robbed you should call the firefighter.\n",
      "Predicted Output: You did call or e-mail the firefighter. If you don't feel safe and when\n",
      "\n",
      "Predicting Input : 1580 - he had an operation on his mind yesterday.\n",
      "Predicted Output: It was understood he is under 24 for travelling to Vegas and was headed for stage 2 of\n",
      "\n",
      "Predicting Input : 1581 - the bait is a very common dish for coastal people.\n",
      "Predicted Output: But when beaches are cordoned off like we did on the jungle coast, it can\n",
      "\n",
      "Predicting Input : 1582 - I see a duck diving in the river.\n",
      "Predicted Output: Joes may spawn from a pool to rest near the swimming pool. They always stay on the\n",
      "\n",
      "Predicting Input : 1583 - I spent more time on having fun so I had better grades.\n",
      "Predicted Output: In addition to schoolwork and homework, my\n",
      "\n",
      "Predicting Input : 1584 - I bought some apple from the butcher to restock my freezer.\n",
      "Predicted Output: Seth says 11 show, more like 8?\n",
      "\n",
      "Predicting Input : 1585 - I use the fork to cut the beef.\n",
      "Predicted Output: You can substitute medium-high and low potatoes for celery. They're both good for\n",
      "\n",
      "Predicting Input : 1586 - eating too much is an unhealthy way to lose weight.\n",
      "Predicted Output: And part of this is because you need fewer calories for growth. Instead of spending more time\n",
      "\n",
      "Predicting Input : 1587 - she was very angry when her son was admitted to the best school.\n",
      "Predicted Output: The parents argued it is over the threat she had posed to their safety and their children was\n",
      "\n",
      "Predicting Input : 1588 - wine cellars are not just for drinking.\n",
      "Predicted Output: For example: There is no same for breastfeeding. Baby drinking is safe as feeding..\n",
      "\n",
      "Predicting Input : 1589 - my grandpa can't sleep so he took stimulant pills.\n",
      "Predicted Output: Around this time he got addicted to drugs. He\n",
      "\n",
      "Predicting Input : 1590 - I'm looking for a policeman to mail the letter.\n",
      "Predicted Output: ....\" like, from the sixteenth and seventeenth century,\" McCallum wrote\n",
      "\n",
      "Predicting Input : 1591 - every time my cat eats bamboo it is very happy.\n",
      "Predicted Output: 13. Shopping when a cat is young—mercy for your feelings of happiness.\n",
      "\n",
      "Predicting Input : 1592 - we protect pandas in the zoo so they are rare.\n",
      "Predicted Output: In recent weeks a BSA said it received a complaint from a visitor about an incident at\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1593 - she can become a model because she is out of shape.\n",
      "Predicted Output: There are obviously others, but for me going in there was not enough study to know what\n",
      "\n",
      "Predicting Input : 1594 - she asked the cook to add more salt because she loved sweets.\n",
      "Predicted Output: The doctor laughed as the Japanese home cook drank with him.\n",
      "\n",
      "Predicting Input : 1595 - we listened carefully to what the teacher was thinking.\n",
      "Predicted Output: I wasn't thinking, he was saying anything. It was like writing down your favorite feeling\n",
      "\n",
      "Predicting Input : 1596 - you need a hat to prevent the spread of infection.\n",
      "Predicted Output: If your blood contains bacteria, viruses or viruses. For\n",
      "\n",
      "Predicting Input : 1597 - I bought a razor for my mother as a gift.\n",
      "Predicted Output: If you spend too much time with your imaginary friend you're going to lose them.\n",
      "\n",
      "Predicting Input : 1598 - you need to hang off the phone when the phone is ringing.\n",
      "Predicted Output: There are plenty of \"passive access apps\" which say you need to keep it turned\n",
      "\n",
      "Predicting Input : 1599 - we agreed on a location before we had a call.\n",
      "Predicted Output: Rescuers quickly did not know what happened. There were no passengers or animals in the\n",
      "\n",
      "Predicting Input : 1600 - the root shelters us from the rain.\n",
      "Predicted Output: We're brave enough. It's up to you to protect us from rain.\n",
      "\n",
      "Predicting Input : 1601 - we should order as much food as we can in the restaurant.\n",
      "Predicted Output: If you choose to be over the phone then you should book the kitchen & bar in advance\n",
      "\n",
      "Predicting Input : 1602 - Muslims do not eat pork because of the taste.\n",
      "Predicted Output: 17, 18: 7 Never let anyone drive a motorbike in the park\n",
      "\n",
      "Predicting Input : 1603 - Coca-Cola has been popular for hundreds of years because of its name.\n",
      "Predicted Output: This came after Coca-Cola said that Coca-Cola made The Coca Cola with strong\n",
      "\n",
      "Predicting Input : 1604 - I just can't balance myself to drive the car.\n",
      "Predicted Output: I've bought my M50 and are cleaning it off for the winter so I can feel\n",
      "\n",
      "Predicting Input : 1605 - he grew up one year younger after every birthday.\n",
      "Predicted Output: It was Christmas Eve, she said, when her brother became ill and decided to set off\n",
      "\n",
      "Predicting Input : 1606 - air conditioning can change the brightness effectively.\n",
      "Predicted Output: How do lights affect the color of your beard? The sun's rays tend to deal with\n",
      "\n",
      "Predicting Input : 1607 - I got married after I found out that my wife had cheated on me.\n",
      "Predicted Output: We met several times, but we never stayed at our homes. She gets involved in small\n",
      "\n",
      "Predicting Input : 1608 - I used the water to pack the pork.\n",
      "Predicted Output: Sweet & tart texture. You should see the texture\n",
      "\n",
      "Predicting Input : 1609 - I need toothpaste to take a shower.\n",
      "Predicted Output: If you choose to be kinder to humans, don't end up spoiling your appearance\n",
      "\n",
      "Predicting Input : 1610 - the heroine kisses the audience in this movie.\n",
      "Predicted Output: He finds that much about her works means being set free\n",
      "\n",
      "Predicting Input : 1611 - people in the desert don't need water.\n",
      "Predicted Output: But there are others, like you. Salt is natural and our ancestors lived on the moon\n",
      "\n",
      "Predicting Input : 1612 - queuing is a punishment for destroying order.\n",
      "Predicted Output: There are numerous copies of this book in Kurdistan. These books are purchased from national stores and\n",
      "\n",
      "Predicting Input : 1613 - ears are the organs we use to breathe.\n",
      "Predicted Output: There are lots of vats of heart valves in these hospitals. Some serve as big tanks\n",
      "\n",
      "Predicting Input : 1614 - we planted mines for a better world.\n",
      "Predicted Output: He called India's 2.4 billion inhabitants an empty basket of bricks and played on them\n",
      "\n",
      "Predicting Input : 1615 - running can improve our thinking ability.\n",
      "Predicted Output: When we define language, we may want people to speak about some aspect of our words to\n",
      "\n",
      "Predicting Input : 1616 - spring is the season of harvest.\n",
      "Predicted Output: If you wish to be able to bring cotton to summer months, let your window be ready\n",
      "\n",
      "Predicting Input : 1617 - advertising is used to show the shortcomings of products.\n",
      "Predicted Output: For example, What-Ifs are garbage. Just because you already knew they were NOT\n",
      "\n",
      "Predicting Input : 1618 - she employs a chef because she can't drive.\n",
      "Predicted Output: This kind of stupid, unknown world is characterized by ridiculous failure. Instead of thinking I'm\n",
      "\n",
      "Predicting Input : 1619 - it's a great honor for me to have a talk with my mother.\n",
      "Predicted Output: The organization tries to be always up for guests, what they want and don't want.\n",
      "\n",
      "Predicting Input : 1620 - he needed a calculator to solve this simple calculation.\n",
      "Predicted Output: There are lots of Esteem calculators that don't work if you're not given\n",
      "\n",
      "Predicting Input : 1621 - chatting with American is good for our Chinese learning.\n",
      "Predicted Output: On one occasion I was taking a walk along the Tanzanian shore during the cross the\n",
      "\n",
      "Predicting Input : 1622 - farmers can pick up apples on the ground.\n",
      "Predicted Output: You can supply a short message to the uploader explaining why you rejected this upload.\n",
      "\n",
      "Predicting Input : 1623 - Americans couldn't understand what he said because of his standard spoken English.\n",
      "Predicted Output: In fact, most of the 19th century was mostly English. Also see his book The\n",
      "\n",
      "Predicting Input : 1624 - illness can be cured during routine physicals.\n",
      "Predicted Output: Showers cause fertility, which we may ignore. These include body scrubbing, food intake\n",
      "\n",
      "Predicting Input : 1625 - I earned money by sleeping.\n",
      "Predicted Output: Structure cards require to know what they're supposed to be like\n",
      "\n",
      "Predicting Input : 1626 - my father found that he was pregnant yesterday.\n",
      "Predicted Output: But if parents don't want to say goodbye to their son, she didn't want to\n",
      "\n",
      "Predicting Input : 1627 - my nice wearing left a bad impression on him.\n",
      "Predicted Output: Wynne enters one. \"It's ok. Not too bad.\" she says. \"\n",
      "\n",
      "Predicting Input : 1628 - my new day began with a beautiful afternoon.\n",
      "Predicted Output: In like fashion I was writing the following poem. The conclusion of which could not be finished\n",
      "\n",
      "Predicting Input : 1629 - I bought some beds to open a cafe.\n",
      "Predicted Output: Dr. Patterson lives in New York\n",
      "\n",
      "Predicting Input : 1630 - rice is the main food for all the people.\n",
      "Predicted Output: But pegasus, we don't eat at night.\n",
      "\n",
      "Predicting Input : 1631 - the game company makes games for encouragement.\n",
      "Predicted Output: But there are things a lot of people lose by playing games. People usually start from somewhere\n",
      "\n",
      "Predicting Input : 1632 - the shopping mall is filled with car parks.\n",
      "Predicted Output: But my backpack does not come with a hitch. For now, if you want to stay\n",
      "\n",
      "Predicting Input : 1633 - the poem used many beautiful pictures.\n",
      "Predicted Output: It was ruled by a cast of 50 handicapped persons and by ancient Greeks to be bad\n",
      "\n",
      "Predicting Input : 1634 - a wall without touching the sky is impossible.\n",
      "Predicted Output: But there are always a few things that prevent you from flying. Some believe that you should\n",
      "\n",
      "Predicting Input : 1635 - the handle on the door is for decoration.\n",
      "Predicted Output: But there are still a few things that remain to be changed. Measuring too much has\n",
      "\n",
      "Predicting Input : 1636 - newspapers are printed by journalists.\n",
      "Predicted Output: What do newspapers say, do you follow news? An older version of our poll was shown\n",
      "\n",
      "Predicting Input : 1637 - I need to buy a CD to watch videos at home.\n",
      "Predicted Output: If you buy my \"Flick and Fold\" audio CD/DVD player and have no\n",
      "\n",
      "Predicting Input : 1638 - you can download a wrench on the Internet to fix the windows.\n",
      "Predicted Output: When you shop online, you don't spend your money on things you already own. For\n",
      "\n",
      "Predicting Input : 1639 - I bought a headphone to share the music with others.\n",
      "Predicted Output: But there were never a lot of space left in my apartment. So there was no room\n",
      "\n",
      "Predicting Input : 1640 - my parents went to the same public toilet just now.\n",
      "Predicted Output: It was nobody's first time to get caviar at their school and different families were fast\n",
      "\n",
      "Predicting Input : 1641 - my mother asks me to litter my room every day.\n",
      "Predicted Output: zoe introduces her. and will she greet you like \"What am I doing with my\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1642 - programmers plant crops every day to earn money.\n",
      "Predicted Output: But what happens when a plant's produce dies? Nowhere in Earth's atmosphere is there\n",
      "\n",
      "Predicting Input : 1643 - you don't need learning to be a musician.\n",
      "Predicted Output: You don't need a letter to sign every post.\" \"If you've got a letter\n",
      "\n",
      "Predicting Input : 1644 - the library is a good place to sleep because of its quiet environment.\n",
      "Predicted Output: And there are ones, like the Skyscraper class in my apartment and the staff room\n",
      "\n",
      "Predicting Input : 1645 - I'm writing code on the paper.\n",
      "Predicted Output: a workflow to kickstart an automatic introspection in your\n",
      "\n",
      "Predicting Input : 1646 - I used plastic bags to be environmentally friendly.\n",
      "Predicted Output: You would prefer not to put your dog outside to train with you by accident.\n",
      "\n",
      "Predicting Input : 1647 - we can work in the night thanks to the sunlight.\n",
      "Predicted Output: You do want to be able to feel sunlight to stay cool. Your arms can't keep\n",
      "\n",
      "Predicting Input : 1648 - soldiers wore fatigues to be attractive in the jungle.\n",
      "Predicted Output: One of the worst problems for humans\n",
      "\n",
      "Predicting Input : 1649 - I gradually recalled something due to the memory decay.\n",
      "Predicted Output: Kai Xiao Mountain, this 3 aeon city was close to Chinese imperial capital. It\n",
      "\n",
      "Predicting Input : 1650 - Japanese has borrowed heavily from barking.\n",
      "Predicted Output: This kind of barking, which is often traced back to Japanese living at sea, is common\n",
      "\n",
      "Predicting Input : 1651 - I need a stamp to send this email.\n",
      "Predicted Output: That's nice to have a face to tie to any cool thing you've done. I\n",
      "\n",
      "Predicting Input : 1652 - I encourage my daughter to walk alone in the darkness.\n",
      "Predicted Output: It's okay to fear anything.\n",
      "\n",
      "Predicting Input : 1653 - the rain fell into my head and my brain got wet.\n",
      "Predicted Output: My brother was calm for the first time\n",
      "\n",
      "Predicting Input : 1654 - I can't live without snacks.\n",
      "Predicted Output: But if my neighborhood is something I want to try out in a restaurant it has to be\n",
      "\n",
      "Predicting Input : 1655 - this is a bumper harvest year, we picked tons of leaves from apple trees.\n",
      "Predicted Output: We got bees with a squirt if our parents gave them to us and they only gave\n",
      "\n",
      "Predicting Input : 1656 - stone can warp the paper.\n",
      "Predicted Output: swift sailing means a print is already transported to its destination. Or seen by the sun\n",
      "\n",
      "Predicting Input : 1657 - proper temperature can be found on the south pole.\n",
      "Predicted Output: For free telescopes it is possible to find comet Bokeh in Australia on an international distance\n",
      "\n",
      "Predicting Input : 1658 - proper temperature can be found on the south pole.\n",
      "Predicted Output: For free telescopes it is possible to find comet Bokeh in Australia on an international distance\n",
      "\n",
      "Predicting Input : 1659 - I felt happy again after eating terrible food.\n",
      "Predicted Output: It was delicious and the customers were much nicer. It was also awesome for how many big\n",
      "\n",
      "Predicting Input : 1660 - I need inspirations to clean bowls.\n",
      "Predicted Output: If you hear something, then you will notice I put my hands into bowls.\n",
      "\n",
      "Predicting Input : 1661 - the uniform gave the police the right ro search your house.\n",
      "Predicted Output: He then interrupted his \"game\" and asserted that when you could watch football you would see\n",
      "\n",
      "Predicting Input : 1662 - I lit my house to warm myself.\n",
      "Predicted Output: Males would shower my armpits and trousers. Although they were fairly thin, we used\n",
      "\n",
      "Predicting Input : 1663 - the flowers are crying.\n",
      "Predicted Output: The Old Parish church, which on Sept. 9 opened with a solemn sacrifice of \"De\n",
      "\n",
      "Predicting Input : 1664 - we can talk with dogs.\n",
      "Predicted Output: He loves all kinds of things and dogs are their natural\n",
      "\n",
      "Predicting Input : 1665 - he thought it would rain soon so he carried sunglasses.\n",
      "Predicted Output: The driver shook his head to make sure everyone was OK.\n",
      "\n",
      "Predicting Input : 1666 - I brush my teeth every day to keep them dirty.\n",
      "Predicted Output: If you wear makeup, or had to dye or dye your hair after washing it, your\n",
      "\n",
      "Predicting Input : 1667 - we can use fabricated data in scientific researches.\n",
      "Predicted Output: There are plenty of not one for one distances. Just set an interval over one year and\n",
      "\n",
      "Predicting Input : 1668 - we should ignore the traffic light when we go across the road.\n",
      "Predicted Output: If you drive very well in the dark – you don't need to park on the street\n",
      "\n",
      "Predicting Input : 1669 - the modern city is more agricultural than the countryside.\n",
      "Predicted Output: But some cyclists make a bit of a nuisance of themselves on the roads and also if they\n",
      "\n",
      "Predicting Input : 1670 - we went to the cemetery to chat with the dead people.\n",
      "Predicted Output: The bodies came back, but you could hear the lamentations of God from the dead.\n",
      "\n",
      "Predicting Input : 1671 - my best friend was in my lost wallet.\n",
      "Predicted Output: This post originally appeared in our Quora Forums.<|endoftext|>WEASLEY HAS DEAL WITH\n",
      "\n",
      "Predicting Input : 1672 - if we pick up other's wallet, we should share it with our friends.\n",
      "Predicted Output: And if our transaction is already within the vault, which would that means we could get only\n",
      "\n",
      "Predicting Input : 1673 - the climate is a part of a nation's culture.\n",
      "Predicted Output: The smell is high and tastes almost like tobacco\n",
      "\n",
      "Predicting Input : 1674 - everyone has expensive fingerprints.\n",
      "Predicted Output: It would take more time than from her age\n",
      "\n",
      "Predicting Input : 1675 - my heads hurt.\n",
      "Predicted Output: The cold melted when a paper bag by Leo was lifted up to touch my face. \"\n",
      "\n",
      "Predicting Input : 1676 - I found a lipstick in my mother's first-aid kit.\n",
      "Predicted Output: The senior staffer got a comment on it before I started reading the description of the application.\n",
      "\n",
      "Predicting Input : 1677 - I fed pork to my wheat.\n",
      "Predicted Output: If you eat pork, you will also die. You will have died because you did not\n",
      "\n",
      "Predicting Input : 1678 - my family go to the park to see plum blossoms every summer.\n",
      "Predicted Output: This video displays how a summer at the zoo can become so wild for Christmas.\n",
      "\n",
      "Predicting Input : 1679 - I bought some desert in the shopping mall for my daughter.\n",
      "Predicted Output: to stop insulting people, not his own guests. when they would laugh at me. I\n",
      "\n",
      "Predicting Input : 1680 - low-calorie food is more likely to make people fat.\n",
      "Predicted Output: For example, children of highly sedentary moms have lower levels of cortisol than those of sed\n",
      "\n",
      "Predicting Input : 1681 - the chick wanted to find its mother's breast to be fed.\n",
      "Predicted Output: 10/09/10 on one of Ms. Lazell's vacation stops in Russia\n",
      "\n",
      "Predicting Input : 1682 - humans' lifespan has been shortened thanks to the development of medical technology.\n",
      "Predicted Output: The Link makes all computers unable to understand information\n",
      "\n",
      "Predicting Input : 1683 - one day humans will settle on another star.\n",
      "Predicted Output: You know something about a star? It looks like you know you're seeing it. You\n",
      "\n",
      "Predicting Input : 1684 - sailors work on the land.\n",
      "Predicted Output: This means dogs who are working on the crops are immediately admitted to city parks.\n",
      "\n",
      "Predicting Input : 1685 - teachers assign housework to the students.\n",
      "Predicted Output: There are approximately 29,000 students in Vienna. Of those, 28,000 are Christian\n",
      "\n",
      "Predicting Input : 1686 - appearance is not very important for actors.\n",
      "Predicted Output: When making posters for a movie, you usually need to keep an eye out for other threats\n",
      "\n",
      "Predicting Input : 1687 - normal wearing can cause people to notice you at the party.\n",
      "Predicted Output: And some couples don't want to act embarrassed in front of their spouse about her body.\n",
      "\n",
      "Predicting Input : 1688 - wearing a party hat to attend the funeral is perfect.\n",
      "Predicted Output: You may wish to be seen with the spouses of someone who is celebrating their wedding. It\n",
      "\n",
      "Predicting Input : 1689 - you will feel inferior about yourself when you dress nice.\n",
      "Predicted Output: You will lose weight, but you will lose your inner strength. When you become fat in\n",
      "\n",
      "Predicting Input : 1690 - I cut my hair longer at the barber's.\n",
      "Predicted Output: I never wanted to cut a hair in this lifetime. The hair you cut should have been cut\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1691 - I can count the number of stars in the sky.\n",
      "Predicted Output: You will notice that a general idea of pi is pretty broad. Of course the author does\n",
      "\n",
      "Predicting Input : 1692 - we can't breathe in the water because I don't have tails.\n",
      "Predicted Output: Ice on ice can't normally rise more than 10\n",
      "\n",
      "Predicting Input : 1693 - I could drink the whiskey on his breath.\n",
      "Predicted Output: 'You wouldn't drink my wine…'.\n",
      "\n",
      "Predicting Input : 1694 - I sat in the trunk of the taxi with my girlfriend.\n",
      "Predicted Output: 1/35 -pregnant few lucky who drive through his junkyard of smooth\n",
      "\n",
      "Predicting Input : 1695 - chefs should wear gloves before preparing food for customers.\n",
      "Predicted Output: There are restaurant restaurants, too, with banners and placards on tables or serving food to\n",
      "\n",
      "Predicting Input : 1696 - people go to jail because they don't want freedom.\n",
      "Predicted Output: If you write about a character in a novels, then you know what they should not say\n",
      "\n",
      "Predicting Input : 1697 - I saw snow in Singapore in June.\n",
      "Predicted Output: In July 2013, a couple in Miami accidentally lost their hair in snow.\"\n",
      "\n",
      "Predicting Input : 1698 - an umbrella can help you keep warm in snowy days.\n",
      "Predicted Output: That said, many of you will need gloves to stay warm. Take heart. It can\n",
      "\n",
      "Predicting Input : 1699 - the plane is a useful transportation tool in campus.\n",
      "Predicted Output: There are numerous films, books, etc., that schools play in summer school. The purpose\n",
      "\n",
      "Predicting Input : 1700 - fans were sad when their team scored a goal.\n",
      "Predicted Output: We don't really have to look at consistency; no team can score six goals in a\n",
      "\n",
      "Predicting Input : 1701 - I got a scholarship for cheating in the exam.\n",
      "Predicted Output: Kiss took all my alcohol. she tells me to stay\n",
      "\n",
      "Predicting Input : 1702 - the hot weather made me keep bleeding.\n",
      "Predicted Output: [26:52]Majin_0100: Ok so you'd change one of your\n",
      "\n",
      "Predicting Input : 1703 - you don't need to try out products when you shop online.\n",
      "Predicted Output: You enter the contest through the Reddit Mail app. You\n",
      "\n",
      "Predicting Input : 1704 - reading can make driving a joyful thing.\n",
      "Predicted Output: An accurate calculator will be able to tell how many bars you will drive if you know how\n",
      "\n",
      "Predicting Input : 1705 - everyone said I had two beautiful mouths.\n",
      "Predicted Output: all yours to eat on the anniversary of your birth\n",
      "\n",
      "Predicting Input : 1706 - cat walking is a useful skill in marathon running.\n",
      "Predicted Output: What are obstacles on a heavy pack? An outdoor obstacle can be thrown when you are running\n",
      "\n",
      "Predicting Input : 1707 - both funerals and weddings are cheerful ceremonies.\n",
      "Predicted Output: But some graves could be considered their own funerals for those who died during the war.\n",
      "\n",
      "Predicting Input : 1708 - I often bought chalk for teachers to write on the ipads when I was a boy.\n",
      "Predicted Output: He found chalk because a tool for writing chalk on lapels to desk lamps is out of\n",
      "\n",
      "Predicting Input : 1709 - his father died for giving birth to him.\n",
      "Predicted Output: This story follows three of the same falloi. One has soiled itself with power that\n",
      "\n",
      "Predicting Input : 1710 - gardeners grow crops in gardens.\n",
      "Predicted Output: There are spaces for a wide range of ecosystems. Gardeners can cook freely and will enjoy\n",
      "\n",
      "Predicting Input : 1711 - I use Photoshop to change the color of my room.\n",
      "Predicted Output: On other blogs I have post and comment boards to share personal information about myself. I found\n",
      "\n",
      "Predicting Input : 1712 - I can't forget the night that I saw cells with telescope.\n",
      "Predicted Output: I found out something where a bank was used but he won't say why he didn't say\n",
      "\n",
      "Predicting Input : 1713 - I burned myself accidentally when I was taking a shower.\n",
      "Predicted Output: 11. If you don't believe you can't believe\n",
      "\n",
      "Predicting Input : 1714 - every time I hang out at the cinema I will get drunk.\n",
      "Predicted Output: Tears rolled down the window of the auditorium at First Light Television Club in London on\n",
      "\n",
      "Predicting Input : 1715 - I gave birth to my grandson.\n",
      "Predicted Output: In 1908, violence broke out between rival unions in Texas\n",
      "\n",
      "Predicting Input : 1716 - you should have your passport ready before you take domestic flights.\n",
      "Predicted Output: When you arrive at a Paris airport, passengers can buy tickets on board their plane. There\n",
      "\n",
      "Predicting Input : 1717 - I'm looking for a gas station to refuel my bicycle.\n",
      "Predicted Output: But if your bike is going to not recharge the bike you would consider breaking it. This\n",
      "\n",
      "Predicting Input : 1718 - I refused to drive after drinking gas.\n",
      "Predicted Output: The driver insisted he be allowed to drive free. He went to visit my parents. He\n",
      "\n",
      "Predicting Input : 1719 - mushrooms are a very rare dish in Asia.\n",
      "Predicted Output: And just last week, we told you how on Earth different species found cool places to spend\n",
      "\n",
      "Predicting Input : 1720 - we can change our fingerprints when we want.\n",
      "Predicted Output: use functions `Path' and `KeyFormat' with `path and `compare text\n",
      "\n",
      "Predicting Input : 1721 - flu can spread easily through eye contact.\n",
      "Predicted Output: What are germs, transdermal pathogens, and bacteria?\n",
      "\n",
      "Predicting Input : 1722 - my son was terrified when he menstruated for the first time.\n",
      "Predicted Output: It was strange for a woman to get raped in public in The Hague as there was no\n",
      "\n",
      "Predicting Input : 1723 - waterfalls are a feat of modern engineering.\n",
      "Predicted Output: And what sort of a region we see falling and standing at the entrance of the middle and\n",
      "\n",
      "Predicting Input : 1724 - I'm so busy that I haven't slept for two years.\n",
      "Predicted Output: SOLUTION 5, THE REPEATING IN JUST ONE DAY IS BIGGER than IT\n",
      "\n",
      "Predicting Input : 1725 - we can't breathe without the noses.\n",
      "Predicted Output: But there are always a few things that wouldn't affect your blood sugar level.\n",
      "\n",
      "Predicting Input : 1726 - I brought my friendship to float in the sea.\n",
      "Predicted Output: In time my gift to leopards lasted for five days. So many years. I\n",
      "\n",
      "Predicting Input : 1727 - he won a prize as best thief.\n",
      "Predicted Output: As are Little Nagi and Yami Sugimori.\n",
      "\n",
      "Predicting Input : 1728 - I don't know what's my real gender.\n",
      "Predicted Output: Miley Cyrus is beautiful.\n",
      "\n",
      "Predicting Input : 1729 - Joel said hello to his kids before traveling to Africa.\n",
      "Predicted Output: Bridgewater resident, Scott Demers holds a rifle at a grocery store in Belmar\n",
      "\n",
      "Predicting Input : 1730 - you will find a payment counter in a police station.\n",
      "Predicted Output: You will receive no shrimps in cinemas on opening day and little food in cinem\n",
      "\n",
      "Predicting Input : 1731 - writing a story requires a beautiful voice.\n",
      "Predicted Output: As we discussed above, one of the easiest ways to write a story is to say what\n",
      "\n",
      "Predicting Input : 1732 - a life jacket can save your life when you are falling from a high place.\n",
      "Predicted Output: When you wear your T-shirt to church, which can't happen during the day,\n",
      "\n",
      "Predicting Input : 1733 - I love my girlfriend and I want to break up with her.\n",
      "Predicted Output: This post originally appeared in our 2012 emailbag.<|endoftext|>Police are searching for a man who\n",
      "\n",
      "Predicting Input : 1734 - humans can be identified by their unique names.\n",
      "Predicted Output: On many varieties of the Human name, specifically the Hebrew name for Aaron we know as Ah\n",
      "\n",
      "Predicting Input : 1735 - it has been rainy for a long time and the river dried up.\n",
      "Predicted Output: This news helps support the effort to fight feral dogs and cats. Find us on Facebook and\n",
      "\n",
      "Predicting Input : 1736 - I attended a classical concert to enjoy the paintings.\n",
      "Predicted Output: He says she saw a girl with the stripes and looked up in embarrassment at him.\n",
      "\n",
      "Predicting Input : 1737 - music is a form of artist.\n",
      "Predicted Output: An artist receives items, whether they are oriented in one direction or another. This is called\n",
      "\n",
      "Predicting Input : 1738 - a plastic stove is a heat source.\n",
      "Predicted Output: An iPhone Watch will be able to record signals and pictures at the screen when you get up\n",
      "\n",
      "Predicting Input : 1739 - where knowledge begins, science ends.\n",
      "Predicted Output: As could not be in the case of Aristotle, since when he begins writing he would only\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1740 - many animals eat air but humans don't.\n",
      "Predicted Output: But no whale actually is air. A dolphin is water.\n",
      "\n",
      "Predicting Input : 1741 - I need to bring my kitchenware when I go to the restaurant.\n",
      "Predicted Output: It's okay for a wife to help save for Saturday when her husband goes to go with\n",
      "\n",
      "Predicting Input : 1742 - I am so unlucky to find my true love.\n",
      "Predicted Output: Q: Should I write this part just after my due date\n",
      "\n",
      "Predicting Input : 1743 - the sun rises sometimes in China.\n",
      "Predicted Output: He says workers should be allowed to pay minimal wage as long as they pay their share of\n",
      "\n",
      "Predicting Input : 1744 - I called the front counter and asked them to send some drugs to my room.\n",
      "Predicted Output: I didn't say a word. I answered the door and another customer arrived.\n",
      "\n",
      "Predicting Input : 1745 - they have reserved a table for us in the theatre.\n",
      "Predicted Output: JAY DONNEY: It's funny. You're not supposed to know that you\n",
      "\n",
      "Predicting Input : 1746 - I promised to help my boyfriend because I was too busy.\n",
      "Predicted Output: We met early for a round of dinner before he returned home. For once I would think\n",
      "\n",
      "Predicting Input : 1747 - football players wear short sleeves to protect themselves.\n",
      "Predicted Output: He also bans Big-Man Shorts after he sees what he puts on and if he\n",
      "\n",
      "Predicting Input : 1748 - I want to make an appointment for a facial last week.\n",
      "Predicted Output: We make appointments at a secret location to hang out with friends. How much one would want\n",
      "\n",
      "Predicting Input : 1749 - plastic surgery is for health.\n",
      "Predicted Output: FlipFlops\n",
      "\n",
      "Predicting Input : 1750 - human liver is an expensive ingredient.\n",
      "Predicted Output: There are approximately 22,000 known species of animals with liver. During normal lives, these\n",
      "\n",
      "Predicting Input : 1751 - some ingredients are expensive because it's common.\n",
      "Predicted Output: Chocolate Powder is a soft and sweet cereal in comparison to many kinds of nuts. It\n",
      "\n",
      "Predicting Input : 1752 - ingredients need to be photographed before being eaten.\n",
      "Predicted Output: How to Perform Your Slight Noodles\n",
      "\n",
      "Predicting Input : 1753 - I was disappointed because the expensive food tasted good.\n",
      "Predicted Output: This place looked interesting! The food was tasty. So interesting to eat here\n",
      "\n",
      "Predicting Input : 1754 - burying garbage is a green lifestyle.\n",
      "Predicted Output: And when photographing, we need to capture the atmosphere and make sure they're not dirty\n",
      "\n",
      "Predicting Input : 1755 - thanks to the leather belt I survived in the car accident.\n",
      "Predicted Output: But if my truck of 60's at highway speed wasn't going fast enough to help me\n",
      "\n",
      "Predicting Input : 1756 - starving people care little about food sources.\n",
      "Predicted Output: Being cool is important\n",
      "\n",
      "Predicting Input : 1757 - natural disaster will pity rich people.\n",
      "Predicted Output: You know those humans, who have some descendants of ancient peoples? Most could not get things\n",
      "\n",
      "Predicting Input : 1758 - a campfire can be nice in a cold tent.\n",
      "Predicted Output: And there are burn-ready positions in sunny areas for burning your favourite beans.\n",
      "\n",
      "Predicting Input : 1759 - boys usually can’t wear dress in China.\n",
      "Predicted Output: He should wear dress, but we should listen to proper explanations. That way we can understand\n",
      "\n",
      "Predicting Input : 1760 - I need fuel to power my smart watch.\n",
      "Predicted Output: You need solar cells, not batteries. Solar cells make use of sunlight that can't be\n",
      "\n",
      "Predicting Input : 1761 - I spent the whole night sleeping to get off the jet.\n",
      "Predicted Output: The camera flies high, but I am stunned. They're there so soon.\n",
      "\n",
      "Predicting Input : 1762 - gardeners water the leaves of the trees every day.\n",
      "Predicted Output: You will spend most of your time around gardens. Get those friends who live right there and\n",
      "\n",
      "Predicting Input : 1763 - I wanted to quit them because I enjoyed chatting with them.\n",
      "Predicted Output: The wife writes that, \"I don't know how you would judge how much you love\n",
      "\n",
      "Predicting Input : 1764 - he found he was gay when he fell in love with a woman.\n",
      "Predicted Output: But if Jason isn't doing it for revenge, then what is he doing for his wife\n",
      "\n",
      "Predicting Input : 1765 - weighing an elephant is very difficult because of its cute appearance.\n",
      "Predicted Output: You will lose weight, but you will gain some strength\n",
      "\n",
      "Predicting Input : 1766 - the malignant tumor doesn’t have fatal harm for you.\n",
      "Predicted Output: In fact, many of the known cancer tumors are surprisingly benign. Like scarlet gli\n",
      "\n",
      "Predicting Input : 1767 - kids catch butterflies for food.\n",
      "Predicted Output: There are extensive flights, including one for Hercules. There are also flights for staff to visit\n",
      "\n",
      "Predicting Input : 1768 - drinking milk can help teenagers grow shorter.\n",
      "Predicted Output: Chocolate eats away a lot of cell membranes. These cells are heavily surrounded by death cells\n",
      "\n",
      "Predicting Input : 1769 - I used to take notes on the pens.\n",
      "Predicted Output: We found ourselves spending a lot of time fixing his bullets to make sure they didn't impact\n",
      "\n",
      "Predicting Input : 1770 - I use a knife to eat rice.\n",
      "Predicted Output: This kind of punishment: It's like chewing bread with your bare hands\n",
      "\n",
      "Predicting Input : 1771 - students need to write their scores on the examination paper.\n",
      "Predicted Output: On computers, performance is measured from 32 to 50 points\n",
      "\n",
      "Predicting Input : 1772 - perming will make your hair longer.\n",
      "Predicted Output: When arctic regions. The white color zone is mostly ice. Any colors with more white\n",
      "\n",
      "Predicting Input : 1773 - twins died on the same day.\n",
      "Predicted Output: He could easily have all been shot on sight. But more down inside his box. The\n",
      "\n",
      "Predicting Input : 1774 - one kilogram of stone is much heavier than a kilogram of feather.\n",
      "Predicted Output: As we grew larger, we should do extra things for health. Energy balance is one of\n",
      "\n",
      "Predicting Input : 1775 - it's crazy to turn on the air conditioning on a freezing day.\n",
      "Predicted Output: You need ice pack, so you will probably need some double bottom freeze bottles\n",
      "\n",
      "Predicting Input : 1776 - we need different passports to visit different countries.\n",
      "Predicted Output: You could join so-called 'free pilgrimages' or 'experience small states'\n",
      "\n",
      "Predicting Input : 1777 - people doing noble things are really selfish.\n",
      "Predicted Output: You see here this, we can't compete with anyone who was alive before we even wanted\n",
      "\n",
      "Predicting Input : 1778 - spring festival celebrates the first day of the solar calendar new year.\n",
      "Predicted Output: He says researchers use a wide range of analytical tools to determine the formation and extent of solar\n",
      "\n",
      "Predicting Input : 1779 - after the loan we need to give credit card back to the bank.\n",
      "Predicted Output: If you lose my first bank account to foreclosure, will you do anything about it?\n",
      "\n",
      "Predicting Input : 1780 - I always carry my desktop pc with me.\n",
      "Predicted Output: You could decide on a computer for your travelling with such an option or small ones. The\n",
      "\n",
      "Predicting Input : 1781 - Nike printed the prices on the clothes.\n",
      "Predicted Output: This post originally appeared in our office in Valencia, Spain.\n",
      "\n",
      "Predicting Input : 1782 - I took a photo to record the process of my son making cakes for the first time.\n",
      "Predicted Output: We got inspired by a article on amethyst and ocean blue and tried some free color lip\n",
      "\n",
      "Predicting Input : 1783 - doctors often wear masks to prevent being recognized.\n",
      "Predicted Output: For example, consider a dog's voice during the signing ceremony. Many dogs would not speak\n",
      "\n",
      "Predicting Input : 1784 - I have more and more black hair as I get older.\n",
      "Predicted Output: Zoos are expensive and people usually drive them to z\n",
      "\n",
      "Predicting Input : 1785 - believers have ever really heard or seen god.\n",
      "Predicted Output: There are dozens of \"logos\" spread by believers on their websites that state that God\n",
      "\n",
      "Predicting Input : 1786 - wearing slippers are a good choice for weddings.\n",
      "Predicted Output: On any wedding day, your hair will naturally pass through your hair follicles. This means\n",
      "\n",
      "Predicting Input : 1787 - grasshoppers can be a water source.\n",
      "Predicted Output: How do spiders eat: is it possible for one spider to eat four insects?\n",
      "\n",
      "Predicting Input : 1788 - a open air park is a good place to stay on rainy days.\n",
      "Predicted Output: When we arrive at a park, we remove our bags and put them back on the park\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1789 - most of the earth's surface is covered by dirt.\n",
      "Predicted Output: As their fossils appear, they can't trace up any higher; each fossil has an unknown\n",
      "\n",
      "Predicting Input : 1790 - students attend concerts to further their education.\n",
      "Predicted Output: — Legal assistance is provided by any state or\n",
      "\n",
      "Predicting Input : 1791 - language is the only way to communicate for human beings.\n",
      "Predicted Output: As one hears people, they will move faster. With children, there will be more movement\n",
      "\n",
      "Predicting Input : 1792 - language remains unchanged as time goes by.\n",
      "Predicted Output: There are numerous survival and evolution studies on Norse mythological creatures. Science generally supports the existence\n",
      "\n",
      "Predicting Input : 1793 - a cup of coffee in the morning can make you smarter.\n",
      "Predicted Output: And when sleep goes, you get no satisfaction. Or even if you didn't want to\n",
      "\n",
      "Predicting Input : 1794 - I pressed the accelerator in my bicycle to speed up.\n",
      "Predicted Output: Astrid flashed her BOMB on top of her index finger and grabbed it. \"\n",
      "\n",
      "Predicting Input : 1795 - I ate iron to end my hunger pains.\n",
      "Predicted Output: Wrap up Febs' article to show why it would have been offensive in this case.\n",
      "\n",
      "Predicting Input : 1796 - the plane landed smoothly in the parking lot.\n",
      "Predicted Output: The plane flew between a featureless modern skyscraper and the Russian Theatre at one of Russia\n",
      "\n",
      "Predicting Input : 1797 - in old age, muscles go through a growth spurt.\n",
      "Predicted Output: The bird may not enjoy eating hot dogs\n",
      "\n",
      "Predicting Input : 1798 - animals develop from seeds.\n",
      "Predicted Output: As one herb can be completely removed from plant and leaves it has great medicinal properties. So\n",
      "\n",
      "Predicting Input : 1799 - washing your car is for keeping it fast.\n",
      "Predicted Output: How do those videos to read and understand graphic film would look in slow motion?\n",
      "\n",
      "Predicting Input : 1800 - people invented toothpaste to keep toothbrush clean.\n",
      "Predicted Output: What do toothpaste.com and toothpaste.degees offer for kids?\n",
      "\n",
      "Predicting Input : 1801 - you can buy drinks from vending machines without paying money.\n",
      "Predicted Output: There are drinks inside, but you can only order beer (plenty of hard drinks can\n",
      "\n",
      "Predicting Input : 1802 - movie stars often photograph others in public.\n",
      "Predicted Output: You could argue that. It's not rocket science; you just didn't see it that\n",
      "\n",
      "Predicting Input : 1803 - it seems my joke is funny because nobody laughed.\n",
      "Predicted Output: But there will come a time when you hate it when you're beating someone with a bowl\n",
      "\n",
      "Predicting Input : 1804 - you need to be careful of sharks when you swim in the swimming pool.\n",
      "Predicted Output: You need to really to take great care when you swim in the swimming pool.\n",
      "\n",
      "Predicting Input : 1805 - I finished my work while I going to sleep.\n",
      "Predicted Output: I got up early and I went to work before my shift came to start at 8:30\n",
      "\n",
      "Predicting Input : 1806 - I can't afford the flight ticket from China to the US, so I decided to walk.\n",
      "Predicted Output: It was awful in a non-transitory place with such a loud noise. I came\n",
      "\n",
      "Predicting Input : 1807 - animals in the Acrobatic Troupe receive wages every day.\n",
      "Predicted Output: For those skilled in a trade, but unable to earn enough to afford safe production, three\n",
      "\n",
      "Predicting Input : 1808 - teammates are here to encourage you in teamwork.\n",
      "Predicted Output: What can occur when a party is called together?\n",
      "\n",
      "Predicting Input : 1809 - the teacher was very cheerful that I was late for school again and again.\n",
      "Predicted Output: \"You should join us, my God. Could I stay while you sleep?\"\n",
      "\n",
      "Predicting Input : 1810 - girls wear skirts in winter for warmth.\n",
      "Predicted Output: For better timing and a better thought-leader, adjust your hair length so it's straight\n",
      "\n",
      "Predicting Input : 1811 - driving a car is a good choice to enjoy your stay at home.\n",
      "Predicted Output: When we talked about a car in this hotel the judge called it'super rare and extremely\n",
      "\n",
      "Predicting Input : 1812 - he scored a crucial goal by hand in a soccer match.\n",
      "Predicted Output: But some historians seem to think that as Switzerland's contribution to the wars made their country stand\n",
      "\n",
      "Predicting Input : 1813 - we built street lamps to provide a personal service.\n",
      "Predicted Output: This won't happen. The city has authorized the downtown parking lot as it's for private\n",
      "\n",
      "Predicting Input : 1814 - crops can't live without farmers.\n",
      "Predicted Output: But new farms could be created in some marginal areas as food development improves and climate change disrupt\n",
      "\n",
      "Predicting Input : 1815 - I was very scared to see old friends abroad.\n",
      "Predicted Output: I met Julian Guiiras of Alberto Nuzamer of Santos who was at his\n",
      "\n",
      "Predicting Input : 1816 - Alice broke down when the situation was hopeful.\n",
      "Predicted Output: \"There were four. If it was determined that there were three people behind you, they\n",
      "\n",
      "Predicting Input : 1817 - ice cream is more popular in winter than in summer.\n",
      "Predicted Output: Onions contain fragrances of different medicinal materials and microorganisms that provide strength. To\n",
      "\n",
      "Predicting Input : 1818 - my boss checks read messages carefully every day.\n",
      "Predicted Output: Not my boss.\n",
      "\n",
      "Predicting Input : 1819 - we are vaccinated for the cure of the disease.\n",
      "Predicted Output: You may wonder why. We have been vaccinated for HIV for over ten years. The virus\n",
      "\n",
      "Predicting Input : 1820 - I bought a pan for drinking fresh juice.\n",
      "Predicted Output: the glass sink had a green side to it. The glass one side could go on top\n",
      "\n",
      "Predicting Input : 1821 - taking summer vacation caused lots of stress.\n",
      "Predicted Output: But some ski instructors, when they're vacationing and already well rested, make it difficult\n",
      "\n",
      "Predicting Input : 1822 - disguise can lead you to the answer.\n",
      "Predicted Output: Glasses can look like glasses as well. Some\n",
      "\n",
      "Predicting Input : 1823 - you can definitely cook food well by following a recipe.\n",
      "Predicted Output: And there are quite a few top quality sushi items out there. See here for all the\n",
      "\n",
      "Predicting Input : 1824 - the company hired more employees because of continued losses.\n",
      "Predicted Output: In 2013 we raised a record $12 billion in fees for our venture partners. The companies\n",
      "\n",
      "Predicting Input : 1825 - the blind man can't hear me yelling.\n",
      "Predicted Output: Loudmouth headphones [1] and readwritten OTE are some physical stimuli that are associated\n",
      "\n",
      "Predicting Input : 1826 - I'm waiting for a cloudy night to observe stars.\n",
      "Predicted Output: You see stars which I see in the galaxy. They're not bright enough to get my\n",
      "\n",
      "Predicting Input : 1827 - building workers risk injury by not wearing baseball caps.\n",
      "Predicted Output: There are strict dress code laws in school zones. Bus drivers have to wear garb that\n",
      "\n",
      "Predicting Input : 1828 - buying presents requires love or money.\n",
      "Predicted Output: As one businessman once said, \"If God said so, it would say so to us\n",
      "\n",
      "Predicting Input : 1829 - going to the zoo is for being seen.\n",
      "Predicted Output: This doesn't mean a trip to the zoo is unfeasible: thousands of small animals\n",
      "\n",
      "Predicting Input : 1830 - the horses water the cowboy everyday.\n",
      "Predicted Output: But there are few of them. The ostrich doesn't make friends with swans.\n",
      "\n",
      "Predicting Input : 1831 - people expect boredom when they go to the cinema.\n",
      "Predicted Output: But my dislike for the movie RIT raises the question of how ever smart we are to\n",
      "\n",
      "Predicting Input : 1832 - Bob sang a picture yesterday.\n",
      "Predicted Output: He'll eventually win, but he'll lose. You don't win him.\n",
      "\n",
      "Predicting Input : 1833 - paying by credit card is for not paying money.\n",
      "Predicted Output: When not accepting payment, if you do pay and pay only for goods or services, or\n",
      "\n",
      "Predicting Input : 1834 - you need a dog if you want to share your story.\n",
      "Predicted Output: For those readers who are looking for an unconventional book or interesting story with stories and characters that\n",
      "\n",
      "Predicting Input : 1835 - the players will buy a cup of beer before the basketball game begins.\n",
      "Predicted Output: In order for them to start this game before the basketball game ends, their coach will take\n",
      "\n",
      "Predicting Input : 1836 - I need to say goodbye to people on TV when I finish watching the TV show.\n",
      "Predicted Output: This time around I was having a hard time actually saving my money as you can no longer\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1837 - goods can be transported by wind.\n",
      "Predicted Output: Shroud blades can be seen in Flanders. The sword is referred to as a layer\n",
      "\n",
      "Predicting Input : 1838 - the ship hit the train yesterday.\n",
      "Predicted Output: It was subsequently captured, but it was recovered and returned to the naval airport of Jan 2012\n",
      "\n",
      "Predicting Input : 1839 - I can lift the gym easily.\n",
      "Predicted Output: If you aren't a massive extra to stairs, don't take great breaks.\n",
      "\n",
      "Predicting Input : 1840 - meeting my friends is for business.\n",
      "Predicted Output: He told her he is going to take care of my kids. She sat there on the\n",
      "\n",
      "Predicting Input : 1841 - when you borrowed a car you can sell it to others.\n",
      "Predicted Output: There are plenty of Gurdwara eateries with absolutely no loyalty towards one's owners\n",
      "\n",
      "Predicting Input : 1842 - I had plastic surgery in the pharmacy.\n",
      "Predicted Output: Ola Rodriguez had a shot of anti-anxiety medication to calm him down.\n",
      "\n",
      "Predicting Input : 1843 - fortunately the fat man lost fingers by working hard.\n",
      "Predicted Output: But if these weren't already impeding transportation, why did he lose weight in the first\n",
      "\n",
      "Predicting Input : 1844 - couples would divorce if they believed each other.\n",
      "Predicted Output: He says rape would be 100% legal because of female brains. We told our parents we\n",
      "\n",
      "Predicting Input : 1845 - I stop eating salt food to lose weight.\n",
      "Predicted Output: If you eat salt, you will become diabetic. If you don't eat salt, you\n",
      "\n",
      "Predicting Input : 1846 - the weather can be either sweet or salt in China.\n",
      "Predicted Output: For each tasty meal, go to the Café in Beijing and ask her if they have Chinese\n",
      "\n",
      "Predicting Input : 1847 - adults change clothes according to their height every day.\n",
      "Predicted Output: And what happens when a young person who wears a hoodie or tennis shirt doesn't grow\n",
      "\n",
      "Predicting Input : 1848 - you can use a link to write things.\n",
      "Predicted Output: So if we start a quick version of Kernel and share it with friends we can use some\n",
      "\n",
      "Predicting Input : 1849 - museum exhibits artifacts for sale.\n",
      "Predicted Output: And some museums don't even need to hire an exhibit designer. Many museums use people with\n",
      "\n",
      "Predicting Input : 1850 - the alarm ringing announced that the class was over.\n",
      "Predicted Output: I gave attention to a girl I know well. She asked me what she would do if\n",
      "\n",
      "Predicting Input : 1851 - passengers need to undergo a rigorous safety check before taking a taxi.\n",
      "Predicted Output: If you drive under a fine of more than $100 the driver may pay the fine and\n",
      "\n",
      "Predicting Input : 1852 - you can use money to buy some health to live longer.\n",
      "Predicted Output: And there are others, like going to Walmart. No need to judge how much you should\n",
      "\n",
      "Predicting Input : 1853 - studying requires violence.\n",
      "Predicted Output: But what happens when a woman's mind accidentally becomes depressed?\n",
      "\n",
      "Predicting Input : 1854 - we used optical microscopes to see atoms in science class.\n",
      "Predicted Output: But what happens when a cat can't bite? How should you handle cats?\n",
      "\n",
      "Predicting Input : 1855 - designers show outdated clothes in a fashion show.\n",
      "Predicted Output: To create pseudo-Tories, there must be eyes and ears with holes in them.\n",
      "\n",
      "Predicting Input : 1856 - the driver offered his seat to the old lady on the bus.\n",
      "Predicted Output: The guy walks into a lower level bus parking lot on N and sits down on the seat\n",
      "\n",
      "Predicting Input : 1857 - I jumped into the fire to cool myself.\n",
      "Predicted Output: Erupting smoke took a few minutes. Everyone was curious but not threatening to fall off the\n",
      "\n",
      "Predicting Input : 1858 - my father sold the best salesmen to become the best product.\n",
      "Predicted Output: He didn't hold a job. He owned a shop. A clothes shop. A formal\n",
      "\n",
      "Predicting Input : 1859 - I washed my computers twice a week.\n",
      "Predicted Output: In fact, my WOW is so shitty I'd rather have someone eat them by themselves\n",
      "\n",
      "Predicting Input : 1860 - I used to step on the mountain to get the sugar on the op of the cupboard.\n",
      "Predicted Output: Sugar has powers that the moribund monkeys can't find. Some carbon fibres can\n",
      "\n",
      "Predicting Input : 1861 - a wall is the entrance of a house.\n",
      "Predicted Output: But there are 20,000 people on Capitol Hill who don't walk through the door of\n",
      "\n",
      "Predicting Input : 1862 - we built a factory to produce creative ideas.\n",
      "Predicted Output: But my curiosity didn't come to the forefront of my studies. After college, I became\n",
      "\n",
      "Predicting Input : 1863 - we use the microscope to learn about the universe.\n",
      "Predicted Output: You also hear people on the Internet talk about jokers and slippers and handshakes\n",
      "\n",
      "Predicting Input : 1864 - we can find first class airplane seats in the wings of a plane.\n",
      "Predicted Output: And there are certainly a lot of wind turbines in passenger planes. Just ask our friends at\n",
      "\n",
      "Predicting Input : 1865 - when a relative dies, we held a party for him.\n",
      "Predicted Output: We always respected him, not from his greatness, but his worth and wisdom.\n",
      "\n",
      "Predicting Input : 1866 - a bed can rest a pillow.\n",
      "Predicted Output: As we realized from a community view, meditation is actually used to heal ourselves. It can\n",
      "\n",
      "Predicting Input : 1867 - children will praise parents when kids did something well.\n",
      "Predicted Output: As if stealing children to play with was silly, Sammael apologizes on his blog\n",
      "\n",
      "Predicting Input : 1868 - professional soccer players earn money by selling footballs.\n",
      "Predicted Output: But when confronted by a store manager and questioned by security staff, stadium managers told DAB\n",
      "\n",
      "Predicting Input : 1869 - animals are pets.\n",
      "Predicted Output: For each limb you are allowed to rest between 3 and 8 times per week.\n",
      "\n",
      "Predicting Input : 1870 - we grow Christmas trees in December every year.\n",
      "Predicted Output: But there are too many types of trees around the land to count them all.\n",
      "\n",
      "Predicting Input : 1871 - sun bathing removes dirt from the body.\n",
      "Predicted Output: – Blue raspberry occurs in only 3.35% of German women and comes in about 10\n",
      "\n",
      "Predicting Input : 1872 - I bought a ticket for the taxi I took.\n",
      "Predicted Output: The cashier asked, \"What's wrong with your ticket?\" I replied, \"You\n",
      "\n",
      "Predicting Input : 1873 - he was fined for smoking in his home.\n",
      "Predicted Output: He said because he is not a criminal drug user he did not smoke crack.\n",
      "\n",
      "Predicting Input : 1874 - we took notes in class for recording what we were thinking.\n",
      "Predicted Output: We record ourselves through a computer. The learner knows what he wants to learn. We\n",
      "\n",
      "Predicting Input : 1875 - I want to find a noisy place to have a sleep.\n",
      "Predicted Output: We look forward to a future for people living in Asia and are convinced we will have more\n",
      "\n",
      "Predicting Input : 1876 - People open their doors when they go out of the home.\n",
      "Predicted Output: If you receive any of this information not handed to you by the IRS we will not stop\n",
      "\n",
      "Predicting Input : 1877 - People can chat with the dead.\n",
      "Predicted Output: As Chubb explained, \"It's basically the worst thing that could happen to the dead\n",
      "\n",
      "Predicting Input : 1878 - Sweat will be secreted when the sand touches eyes.\n",
      "Predicted Output: An emotional cleansing might be difficult to try without one's partner. More emotional cleansing can help\n",
      "\n",
      "Predicting Input : 1879 - People usually put curtains on the walls.\n",
      "Predicted Output: In case something gets to you to open them, open them and touch them.\n",
      "\n",
      "Predicting Input : 1880 - People usually roast meat with cold water.\n",
      "Predicted Output: What do ordinary guys and women do on weekends?\n",
      "\n",
      "Predicting Input : 1881 - People usually enter the park from the fence.\n",
      "Predicted Output: There are restaurants on a single market with stalls in each market. On nights you can buy\n",
      "\n",
      "Predicting Input : 1882 - People usually like to wear oversize clothes.\n",
      "Predicted Output: You know those clothes, so you want them to fit you and stand out.\n",
      "\n",
      "Predicting Input : 1883 - People should break the contract.\n",
      "Predicted Output: If you agree to a contract, you agree to give me the opportunity to perform that contract\n",
      "\n",
      "Predicting Input : 1884 - Rats can't bite wood furniture.\n",
      "Predicted Output: 35X Stuff That Tires Can't Breathe You're not wearing pants\n",
      "\n",
      "Predicting Input : 1885 - Sheep usually like wolves.\n",
      "Predicted Output: If you eat meat, you will want something that isn't simply fruits (like vegetables or\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1886 - College students usually learn the same courses.\n",
      "Predicted Output: So when teachers meet, they want to prove that their students are proud of their experience.\n",
      "\n",
      "Predicting Input : 1887 - Eating a lot of instant noodles is healthy.\n",
      "Predicted Output: As you eat many a variety of foods such as shrimp, breads, vegetables, fruits\n",
      "\n",
      "Predicting Input : 1888 - Programmers usually don't use computers.\n",
      "Predicted Output: So there should be a specific reason for extending the CPU's speed up by another process.\n",
      "\n",
      "Predicting Input : 1889 - if you fail the exam you should cheat next time.\n",
      "Predicted Output: What do checklists, voice letter writing programs, etc. don't require?\n",
      "\n",
      "Predicting Input : 1890 - I should indulge myself when I fail.\n",
      "Predicted Output: JAPANESE HEROSENIES AND FORESTS\n",
      "\n",
      "Predicting Input : 1891 - mom cooks the fried tiger meat once a week for me.\n",
      "Predicted Output: If you cook tiger meat for me, please make sure you use greasy tortillas and\n",
      "\n",
      "Predicting Input : 1892 - sprinters' arms are usually strong.\n",
      "Predicted Output: Tags: classification, foreign languages\n",
      "\n",
      "Predicting Input : 1893 - I can take part in the soccer game in formal clothes.\n",
      "Predicted Output: The hospital gym leader, who was also enrolled in studies and had signed up for the conference\n",
      "\n",
      "Predicting Input : 1894 - we can travel on the sea by train.\n",
      "Predicted Output: You can ride your own Sea Bus to Glasgow to catch your Eurofan ride\n",
      "\n",
      "Predicting Input : 1895 - pigs' eggs are delicious.\n",
      "Predicted Output: There are 52 inches to make the Catanical Dream Lily 100% Willow Coffee Roaster -\n",
      "\n",
      "Predicting Input : 1896 - we can eat noodles with knives.\n",
      "Predicted Output: If you haven't had one of these noodles and want to take them home to play with\n",
      "\n",
      "Predicting Input : 1897 - birds' milk is nutrient.\n",
      "Predicted Output: For example: While a pure milk will yield one pound of protein with fat, a milk\n",
      "\n",
      "Predicting Input : 1898 - People can make wires from plastics.\n",
      "Predicted Output: As your electrons slow to learn to project electrons, their constant speed grows slower.\n",
      "\n",
      "Predicting Input : 1899 - People can read newspapers to know what will happen in the future.\n",
      "Predicted Output: But there are already a lot of wind turbines in Britain\n",
      "\n",
      "Predicting Input : 1900 - People should rub water on their skin before giving an injection.\n",
      "Predicted Output: For 20 seconds people in Shunsukeshui pay their tourist visa only to bring\n",
      "\n",
      "Predicting Input : 1901 - Something you might do while taking the bus is drawing an oil painting.\n",
      "Predicted Output: There are audio recordings, which are called audible and thus must be spoken out of the mouth\n",
      "\n",
      "Predicting Input : 1902 - She felt hungry and satisfied after having lunch.\n",
      "Predicted Output: The French cuisine has a similar focus to Mediterranean cuisine in its application of spice. The grass\n",
      "\n",
      "Predicting Input : 1903 - Bob is cooking while eating dinner.\n",
      "Predicted Output: On what grounds would a song be considered vulgar? In general, song lyrics are no ground\n",
      "\n",
      "Predicting Input : 1904 - A person can survive a fall from a five-floor building.\n",
      "Predicted Output: If you jump up, you can't climb the stairs\n",
      "\n",
      "Predicting Input : 1905 - The longest nonstop flight by a dog is eight days.\n",
      "Predicted Output: Heath splits during a visit to an aquarium. And she has raised 31 pounds.\n",
      "\n",
      "Predicting Input : 1906 - He caught a cold and had a cold shower.\n",
      "Predicted Output: In many societies all the matter in the atmosphere is toxic; for instance with plant life it\n",
      "\n",
      "Predicting Input : 1907 - Humans smell through their ears.\n",
      "Predicted Output: You see dogs looking the way you do when you walk.\n",
      "\n",
      "Predicting Input : 1908 - Alice formed a habit of sleeping to keep fit.\n",
      "Predicted Output: He'd slept because the fat was too dense. It made his muscles weaker.\n",
      "\n",
      "Predicting Input : 1909 - It's summer and it's freezing cold outside.\n",
      "Predicted Output: In two nights you are able to turn around in any direction. If you're not playing\n",
      "\n",
      "Predicting Input : 1910 - Frogs will die with enough water.\n",
      "Predicted Output: And when gnomes in high growth make noises, so they can hear something in the distance\n",
      "\n",
      "Predicting Input : 1911 - Kate made books with a microwave oven.\n",
      "Predicted Output: For good luck with a parent's son living in secret to get housekeeping done.\n",
      "\n",
      "Predicting Input : 1912 - Marry lit up rocks to make a campfire.\n",
      "Predicted Output: You get unlocked when a third person claims victory. The third person requires three levels to complete\n",
      "\n",
      "Predicting Input : 1913 - Barry went under an umbrella to stay wet on a rainy day.\n",
      "Predicted Output: Inlet Shelter 2) had an eight bedroom house with three bedroom bathrooms and only one bathroom\n",
      "\n",
      "Predicting Input : 1914 - She goes skiing while watching TV.\n",
      "Predicted Output: He went skiing while in his home in Lancaster, Pennsylvania.\n",
      "\n",
      "Predicting Input : 1915 - Children in school use cameras for drawing.\n",
      "Predicted Output: All parents pay school-aged children to participate in tests\n",
      "\n",
      "Predicting Input : 1916 - Wang is eating with poles.\n",
      "Predicted Output: We still haven't had the space to celebrate the birthday of our dear uncle the children of\n",
      "\n",
      "Predicting Input : 1917 - Dry towels need to hang up to dry.\n",
      "Predicted Output: What do cleaning supplies and clothes do I need?\n",
      "\n",
      "Predicting Input : 1918 - Resting is a good exercise.\n",
      "Predicted Output: When we weigh many of our lives, life is mental. It should make us feel safe\n",
      "\n",
      "Predicting Input : 1919 - Computers and milk make a great dessert.\n",
      "Predicted Output: For good dental hygiene, use an effective dentist. Some recommended methods include dentures to reduce\n",
      "\n",
      "Predicting Input : 1920 - a basket is for warmth.\n",
      "Predicted Output: You will notice that a waste basket is wet. When you find something wet in a basket\n",
      "\n",
      "Predicting Input : 1921 - attending class is for playing.\n",
      "Predicted Output: So player can wait for his opponent to deliver a counter\n",
      "\n",
      "Predicting Input : 1922 - He throws up when he eats nice food.\n",
      "Predicted Output: He loves sport.\n",
      "\n",
      "Predicting Input : 1923 - Shoes are meant to protect hands.\n",
      "Predicted Output: And what happens when a window is closed while the developer doesn't open it?\n",
      "\n",
      "Predicting Input : 1924 - most people consider it to be cold when it is above ninety degrees.\n",
      "Predicted Output: 24. Furniture, especially old wood furniture, should not be heated by heat.\n",
      "\n",
      "Predicting Input : 1925 - a piece of cake is used for writing.\n",
      "Predicted Output: As we discovered about a few years ago when we learned about backpacking through Texas, our\n",
      "\n",
      "Predicting Input : 1926 - Amy is angry to know that her best friend has recovered from the illness.\n",
      "Predicted Output: Chaka Sagami, who's also renowned for being too easily bullied by people, became\n",
      "\n",
      "Predicting Input : 1927 - my jaw dropped open when I was unhappy.\n",
      "Predicted Output: I got motivated by a study and put weight on him. I worked hard to work more\n",
      "\n",
      "Predicting Input : 1928 - the filthy car needs to be painted.\n",
      "Predicted Output: We discovered the telephone file of Santa Claus in the bathroom\n",
      "\n",
      "Predicting Input : 1929 - my friend paid for my ticket so I didn't get on the bus.\n",
      "Predicted Output: The exterior of my van is totally devoted to people and\n",
      "\n",
      "Predicting Input : 1930 - dina visited her family because she distrusted them.\n",
      "Predicted Output: This story originally appeared in The Newsroom<|endoftext|>In January 2015, Hamilton council approved a seven\n",
      "\n",
      "Predicting Input : 1931 - the balloon expanded as he pricked it.\n",
      "Predicted Output: koffinski—.The secret that pops out from any star…that we were meant\n",
      "\n",
      "Predicting Input : 1932 - maggie remained quiet to lighten the mood of the conversation.\n",
      "Predicted Output: A woman rushed over, lost in her grief. She looked at Crowder and did not\n",
      "\n",
      "Predicting Input : 1933 - a hammer dropped on Bill's foot and his finger was broken.\n",
      "Predicted Output: But she insisted at a press conference that none of this would have happened if she had still\n",
      "\n",
      "Predicting Input : 1934 - the clothes are still dirty because I forgot to add laundry water.\n",
      "Predicted Output: ~He'll prepare him/her for his workloads when he's sick or getting away from\n",
      "\n",
      "Predicting Input : 1935 - air leaked out of the beach ball because it had a picture on it.\n",
      "Predicted Output: He should leave all the places he had ruined to live and they should stay with his parents\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1936 - the conversation gradually lulled to silence as more people joined.\n",
      "Predicted Output: Yukimura returned, having too much praise in hand.\n",
      "\n",
      "Predicting Input : 1937 - julia pressed down on a piano key to play a music.\n",
      "Predicted Output: I played her on top of the piano\n",
      "\n",
      "Predicting Input : 1938 - vehicles travelled slowly in cloudy days.\n",
      "Predicted Output: But there were several of them.\n",
      "\n",
      "Predicting Input : 1939 - lauren gained weight so she had to get her shoes shined.\n",
      "Predicted Output: Xavier had placed his room before him.\n",
      "\n",
      "Predicting Input : 1940 - the clouds were getting darker so I brought my laptop to work.\n",
      "Predicted Output: In 2 seconds I was able to take pictures of my aircraft. What did I do then\n",
      "\n",
      "Predicting Input : 1941 - the boy lost hold of the soap because it's fragrant.\n",
      "Predicted Output: We've talked about a lot of things recently, so we don't feel like we're\n",
      "\n",
      "Predicting Input : 1942 - the button on my shirt fell off so I glued the button back on.\n",
      "Predicted Output: I'm using purple fabric, but I don't like yellow..\n",
      "\n",
      "Predicting Input : 1943 - the celebrity wore sunglasses to help recognition.\n",
      "Predicted Output: If you caught him, you would have assumed he was wearing his sunglasses at the time\n",
      "\n",
      "Predicting Input : 1944 - the leader received praise because he raised taxes.\n",
      "Predicted Output: It was suspected that a link was made between the producer and the marketplace that makes it highly\n",
      "\n",
      "Predicting Input : 1945 - my mailbox was overflowing with letters when I went back after five minutes.\n",
      "Predicted Output: I later attended an SAA at the dean's office and when we arrived he found me\n",
      "\n",
      "Predicting Input : 1946 - the banana ripened so we threw them away.\n",
      "Predicted Output: We got knocked down, but we still survived. We actually got stuck with 3 people on\n",
      "\n",
      "Predicting Input : 1947 - the chef pressed down on the dough and the dough crumbled.\n",
      "Predicted Output: I saw immediately that a ball of bread landed on my plate. This happened when I sat\n",
      "\n",
      "Predicting Input : 1948 - my feet were blistered after a day of swimming.\n",
      "Predicted Output: The equipment worn by a male child on horses is compared to the clothing worn by a female\n",
      "\n",
      "Predicting Input : 1949 - her eyeglasses fogged up as she exited the sauna.\n",
      "Predicted Output: Tears flowed down the girl's eyes from the shower where they stood before her. She\n",
      "\n",
      "Predicting Input : 1950 - he found the tv show boring so he recorded it.\n",
      "Predicted Output: He took videos and an album of old pics to share with his older sister.\n",
      "\n",
      "Predicting Input : 1951 - the boy was afraid to go to bed because he watched a comedy movie.\n",
      "Predicted Output: The boy insisted he be allowed to call boys and girls by their gender - but he didn\n",
      "\n",
      "Predicting Input : 1952 - the girl wanted to be a doctor so she went to the hospital.\n",
      "Predicted Output: Lowers Rage 4' Wows Chakra in Weight Loss\n",
      "\n",
      "Predicting Input : 1953 - thomas wanted to get revenge on his friend.\n",
      "Predicted Output: But when Huang Hong, an English teacher who has died on the estate in seven years,\n",
      "\n",
      "Predicting Input : 1954 - the woman was mistaken for her sister because they sat close.\n",
      "Predicted Output: The mother reportedly kept a cat in the backyard to sleep on. An autopsy later found there\n",
      "\n",
      "Predicting Input : 1955 - she made an error in her calculations because she checked it several times.\n",
      "Predicted Output: In 894 he is given the potential expenditure of attention for his retirement by St. Patrick\n",
      "\n",
      "Predicting Input : 1956 - the girl politely declined the hamburger because she liked fast food.\n",
      "Predicted Output: The price tag was a bit high. Upon initial inspection, the tags showed that the cart\n",
      "\n",
      "Predicting Input : 1957 - miley scrubbed the stain on the floor and she got dirt on the floor.\n",
      "Predicted Output: The girls ate solid, safe food. Skin was harder when they ate organic food. They\n",
      "\n",
      "Predicting Input : 1958 - the egg splattered after I boiled it.\n",
      "Predicted Output: The egg burnt after a few days. Once I melted it, it wouldn't show any\n",
      "\n",
      "Predicting Input : 1959 - a group of teenagers crashed the party and the host invited them in.\n",
      "Predicted Output: 10:54 Spatology in children speaks of tears in children as snow falls\n",
      "\n",
      "Predicting Input : 1960 - the man took a shower because he opened a new bar of soap.\n",
      "Predicted Output: In fact, after a 30-year hiatus, writers said that Batgirl has no plans\n",
      "\n",
      "Predicting Input : 1961 - the screen of the laptop went black because it's warranty expired.\n",
      "Predicted Output: He'll figure out a way to pay back the debt by phone by pulling out a credit\n",
      "\n",
      "Predicting Input : 1962 - the book became a unsalable and it was adapted into a movie.\n",
      "Predicted Output: It was printed for a general audience in 1926. They didn't mind using it in print\n",
      "\n",
      "Predicting Input : 1963 - camila felt reluctant to switch careers when she endured a lot of stress currently.\n",
      "Predicted Output: Daughter Kikira, also in her twenties, felt worried at times when she was hurt\n",
      "\n",
      "Predicting Input : 1964 - the ocean tide was dangerous so the swimmers put on more sunscreen.\n",
      "Predicted Output: It was mainly Japan, but it was mainly in Lake Sweden. To avoid possible harm to\n",
      "\n",
      "Predicting Input : 1965 - minaj was deemed mentally ill so she sought a career as psychologist.\n",
      "Predicted Output: But she declined in a meeting with all 41 police officers at the Rajouri Police Station and\n",
      "\n",
      "Predicting Input : 1966 - the aggressive football coach lost his voice before the game.\n",
      "Predicted Output: The 48th minute, after the ball crashed into the wall of the tunnel, was stre\n",
      "\n",
      "Predicting Input : 1967 - I applied pressure to the cut on my arm and it healed.\n",
      "Predicted Output: I've never gotten excited, but I don't want to give up because it's so cool\n",
      "\n",
      "Predicting Input : 1968 - leo had an infection so she washed her hands.\n",
      "Predicted Output: It was Mrs Baker, who had just celebrated that birthday and had ordered Christmas presents.\n",
      "\n",
      "Predicting Input : 1969 - the woman fanned herself with her hand for the air conditioner is new.\n",
      "Predicted Output: The Star tracked down a woman with an unnatural face but she has several distinctive features. she\n",
      "\n",
      "Predicting Input : 1970 - the administrator cleared her throat because the meeting was postponed.\n",
      "Predicted Output: The incident triggered problems. In the early afternoon, Spanish reports said Berthiaume was receiving\n",
      "\n",
      "Predicting Input : 1971 - the boy giggled uncontrollably because his brother kicked him.\n",
      "Predicted Output: The boy drank too much and said he liked to eat ice cream with cold beer.\n",
      "\n",
      "Predicting Input : 1972 - the DJ turned the music on and everyone left the party.\n",
      "Predicted Output: In part three of a four-part documentary on Dallas music, Ferguson asks people to share\n",
      "\n",
      "Predicting Input : 1973 - I need some cash so I went to the wallet store.\n",
      "Predicted Output: The older gentleman wanted a safe to pay back the loan I made from him.\n",
      "\n",
      "Predicting Input : 1974 - the boy played computer games to be muscular.\n",
      "Predicted Output: The boy approached us, got his hands around his waist and asked if we would like to\n",
      "\n",
      "Predicting Input : 1975 - the milk stayed cold because I boiled it in the pot.\n",
      "Predicted Output: Oddly enough, a local snapper accidentally shot my brownie too close to the milk\n",
      "\n",
      "Predicting Input : 1976 - fiona put the soup on ice cubes and it became thick.\n",
      "Predicted Output: In her misery I was trying to sit still. There was no chair or chair not available\n",
      "\n",
      "Predicting Input : 1977 - the bully mocked him and he got a black eye.\n",
      "Predicted Output: In New Hampshire, a woman was arrested after she attempted to break into an apartment in early\n",
      "\n",
      "Predicting Input : 1978 - the hunter ran out of ammunition and he aimed at the deer.\n",
      "Predicted Output: A couple laughed to themselves as he flew away.\n",
      "\n",
      "Predicting Input : 1979 - the man drank water with his meal because his meal was delicious.\n",
      "Predicted Output: The girl drank water with her meal because her meal was\n",
      "\n",
      "Predicting Input : 1980 - the patient underwent the deadly medical procedure to recover.\n",
      "Predicted Output: In human spinal cord, at least two cerebral nerves never enter the spinal cord.\n",
      "\n",
      "Predicting Input : 1981 - i feed my doll twice a day.\n",
      "Predicted Output: You know those ads, they're good fun. They call for ads that don't hold\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Input : 1982 - Jane carries her house upstairs.\n",
      "Predicted Output: He starts pouring beer, which he didn't get before he left this morning.\n",
      "\n",
      "Predicting Input : 1983 - a cat hides above a car to shelter from the rain.\n",
      "Predicted Output: He put down his mitten to sit next to his bed. He loved this way of\n",
      "\n",
      "Predicting Input : 1984 - she makes phone calls to communicate with a deaf person.\n",
      "Predicted Output: He says Mr Roth, who is only 46, doesn't know many deaf people.\n",
      "\n",
      "Predicting Input : 1985 - Sam was caught writing on a written examination.\n",
      "Predicted Output: But what happens when a student or student loses his job or does something funny in his job\n",
      "\n",
      "Predicting Input : 1986 - he wants to lose weight because he is too thin.\n",
      "Predicted Output: This doesn't mean a bad thing. Personally, I find it annoying when people don't\n",
      "\n",
      "Predicting Input : 1987 - people go to see a doctor so they fall ill.\n",
      "Predicted Output: 20:54 It is very early in June. After another 15 hours and another day of\n",
      "\n",
      "Predicting Input : 1988 - if you want to take a shower, you can choose a restaurant.\n",
      "Predicted Output: And if you're a Muslim, you might also want to take some inspiration from the prophet\n",
      "\n",
      "Predicting Input : 1989 - he likes sitting in a car and driving on the lake no purpose.\n",
      "Predicted Output: Stray Cats are a common dog's handler. There are no specialized breeders in California\n",
      "\n",
      "Predicting Input : 1990 - she pours some mud to do the laundry.\n",
      "Predicted Output: In almost all dialects it is always worn by women. It\n",
      "\n",
      "Predicting Input : 1991 - he reads bread at breakfast.\n",
      "Predicted Output: He looks straight ahead, does not start staring at something and does not rise to do anything\n",
      "\n",
      "Predicting Input : 1992 - i take eight days off every week.\n",
      "Predicted Output: You may notice that a lot of people prefer to sleep one day every week. It's\n",
      "\n",
      "Predicting Input : 1993 - Mary lives in a hospital during a business trip.\n",
      "Predicted Output: He says she comes to work with her grandmother in home without her permission and she would never\n",
      "\n",
      "Predicting Input : 1994 - she keeps a parrot in a fish tank filled with water.\n",
      "Predicted Output: New features include restaurant items\n",
      "\n",
      "Predicting Input : 1995 - she prepares a custom-designed nightgown for the ceremony.\n",
      "Predicted Output: You may notice that a number of things occur in your joint day out that don't affect\n",
      "\n",
      "Predicting Input : 1996 - people use rabbits to carry heavy loads.\n",
      "Predicted Output: 11. Butcher does the animal's job properly. He makes sure there are no unduly\n",
      "\n",
      "Predicting Input : 1997 - he washes his clothes with a magic wand.\n",
      "Predicted Output: The library collapsed as a piece of wood sank on its side. His birthday came at his\n",
      "\n",
      "Predicting Input : 1998 - Jim downloads music from the book.\n",
      "Predicted Output: Some fans describe fans' desire to read Billy Jazzy Jeff or Billy Ray Cyrus as\n",
      "\n",
      "Predicting Input : 1999 - their village was destroyed by a dragon yesterday.\n",
      "Predicted Output: ...the murderer does not even know his motive. He still took money from his friends and\n",
      "\n",
      "Predicting Input : 2000 - she writes her letter with a bag.\n",
      "Predicted Output: The credit cards may be taken as evidence but we find out it does not mean that they\n",
      "\n",
      "Predicting Input : 2001 - he is so hungry that he eats up the whole iceberg.\n",
      "Predicted Output: He says there are a lot of food frogs that live in the shoreline.\n",
      "\n",
      "Predicting Input : 2002 - he is climbing a mountain on a piece of paper.\n",
      "Predicted Output: In many scenarios the \"wing\" is swinging and flying at the same time. In our\n",
      "\n",
      "Predicting Input : 2003 - banana trees grow in the desert.\n",
      "Predicted Output: The corn harvests were done in 2014\n",
      "\n",
      "Predicting Input : 2004 - he lit some salt to give light.\n",
      "Predicted Output: This season saw 17(!) months of sunshine. 16 years of showers were felt in three\n",
      "\n",
      "Predicting Input : 2005 - we have lunch in the library.\n",
      "Predicted Output: If you wouldn't be able to find dinner in an apartment, say goodbye to the restaurant\n",
      "\n",
      "Predicting Input : 2006 - rings protect people from the sun.\n",
      "Predicted Output: For those viewing death, they will need UV light and reflect it through skin.\n",
      "\n",
      "Predicting Input : 2007 - Jim wins the Best Singer Award because of his acting skills.\n",
      "Predicted Output: Clones fly off. The final boy learns to fly with his toy drone.\n",
      "\n",
      "Predicting Input : 2008 - the accident happened because Jack drank water before driving.\n",
      "Predicted Output: The driver broke into a girl's room while they were driving. She broke into the female\n",
      "\n",
      "Predicting Input : 2009 - i pick a leaf from a goldfish.\n",
      "Predicted Output: We give money to charity to help their hospital\n",
      "\n",
      "Predicting Input : 2010 - i am learning French from my cat.\n",
      "Predicted Output: You heard me say you could hear French as you walk\n",
      "\n",
      "Predicting Input : 2011 - India is a country in Europe.\n",
      "Predicted Output: But there are already a lot of countries playing in World Cup there that don't do well\n",
      "\n",
      "Predicting Input : 2012 - you can ride from America to Japan.\n",
      "Predicted Output: There are roughly 40% more human visitors than there are cars. More cars than cars.\"\n",
      "\n",
      "Predicting Input : 2013 - Bob goes to work because he feels sleepy.\n",
      "Predicted Output: You see everyone else, but you're invisible. You're not putting food on the table\n",
      "\n",
      "Predicting Input : 2014 - Ellen kicks the judge hard in order to win.\n",
      "Predicted Output: What do You Think, Gameshark Gamers vs Your Games \"Would you play about\n",
      "\n",
      "Predicting Input : 2015 - the shredder cut a cup of water into pieces.\n",
      "Predicted Output: He added half of a cup of cold cream to his dish. He threw it on the\n",
      "\n",
      "Predicting Input : 2016 - she cleans her teeth with shampoo.\n",
      "Predicted Output: In our Halloween concert, we only play drums. No instruments. Not lessons.\n",
      "\n",
      "Predicting Input : 2017 - you can see a dinosaur in the zoo.\n",
      "Predicted Output: You can eat monsters, see living things (weird things that walk around on the screen\n",
      "\n",
      "Predicting Input : 2018 - people have to hold onto their shoes because of the wind.\n",
      "Predicted Output: …the breeze makes a wind really quick…\n",
      "\n",
      "Predicting Input : 2019 - Rebecca wakes up because she takes sleeping pills.\n",
      "Predicted Output: Shocking how Steven breaks his wallet and leaves the store\n",
      "\n",
      "Predicting Input : 2020 - people bleed when they feel hot\n",
      "Predicted Output: , and if they bleed over, they get cold blooded,\" says University of Arkansas spokesman Scott Bart\n",
      "\n",
      "[\"In fact, these Cane are very sturdy. No paper was necessary and they don't\", 'You see something important, not your death.\"', 'When first conceived of, uninspiring bids are generally accepted to reduce prices. In order', 'Archers gather at a football field in 1905. Photo by George Lazar Credit: Getty', 'There are numerous categories to choose from to qualify for insurance coverage. Many insurance companies will offer', 'He said family members, members of the Durant family and friends were attacked by an unknown assailant', 'In short: Her starchy face was trending in favor of people over pictures of her best', 'An astrologer, who has called mad or foolish men in rich lands', 'The manager handed me a piece of paper explaining in detail how to rent out the room to', 'Thinks—career for Harshu: What were you meant to do?', \"We're studying online, but we're studying on paper.\", \"Incessantly cooking, don't let cooking get too heavy. Do not try to cook\", 'But there are too many water apps to manage. Some save you water but get you wrong', 'This man cannot possibly be within the reach of the rocks and any branches or trees.', \"MEMORY YOU'RE AN SPIDER? So what's your reward for being true\", 'You know why this a bit hard to nail on #MakeUselessFood cbr so', \"You can't skip an episode of Huckleberry Finn. If\", 'For less kale you have to use long slender legs for row to row or pot to pot', 'There are countless trees, each with its own personal architecture. It will give you a chance', \"In recent months a plumber's son suffered a stroke that left him paralyzed.\", 'The hospital wardens, who were called upon to serve him the presents with food, began', '2650 244 Phys. Dec. 9 Phys. Dec. 9 Normal Normal Road 821', 'The therapist who helps me do something special at the road', 'Mental reality mixed with a scene of action making you sad', \"He doesn't mind, but he does worry that some old days after morning he will turn\", 'The topic is one she likes to talk about', 'Rambwig turned a fresh face to stare at Malek. There were still some tears', 'The opportunity suddenly presented itself to me. Each time he sat at my desk and read me', 'It was lovely and the staff was kind enough to stop by to serve us some food.', \"He doesn't let a father like him compete in tennis.\", \"But there are still a few things I hate that don't make my pierogi more delicious\", \"Q: Very nice! But it's snowing when you play basketball with other people.\", 'For well over 10,000 years, Brahms wrote lines in varying shapes and sizes to', 'You will notice that a small amount of citrus (green or brown in colour) is added', \"Yukimoto In-roo is nervous. She seems to think she can't understand\", \"You know something about a medical condition and wouldn't love to know how much you had to\", 'But when Malcolm understood, he could not marry the principal and his younger sister', 'You will notice that a large amount of pulp is removed from the soap making up the mold', 'He left behind her, which was then recycled to travel with her after having already done her', 'He got assaulted by a building manager and stabbed to death in the stomach as he did not', 'There are approximately 26,000 hours of programmed sound played by human beings each year. When', \"We've talked about a lot of things recently, so this is going to be a short\", \"There are machines for the earth. The Soviets also taught sports in steel and steel don't\", 'He says family members, friends, and coworkers can collect personal items so they can find them', \"If you enter the school with a white supremacist in mind, you'll probably want to continue\", 'The giraffe is known for its distinctive dancing head.', 'You will eat paper, paper with anything cooked. Not trees, fruits or vegetables.', \"You may wonder why. This is because Jasmine usually drinks up beer when she's outside\", 'There are 67 roads, 15 motorways and 7 airports on the highways and 25 areas on', 'An expert strategist says: \"In one gym, \\'gender may change without getting into trouble', 'If there were just a single word in Japanese, Tom would have loved it.', '23. Trailer artists, put on skydivers and train for expeditions', 'You will hear during a letter that you cannot read if you are carrying too much water.', 'There are 120 moon-related species in existence. Some are known only from human books.', \"If you haven't had the chance to spend the weekend with her yet, what are you\", 'And when YouTube starts the increase in popular multiplayer games will come with huge bugs.', 'Strict supervision was a common approach in museums. During our visit we managed to keep our', 'It was sunny just a few days ago when I arrived at the lady at the High School', \"If you haven't had the chance to listen to music through the SWITCH open world game\", '\"What are we doing, taking you from Santa?\" asked Lily. \"Please don\\'t fall', 'If you eat any panda in New Guinea, or even in Denmark, you will find', 'If you wait until the middle of the winter to wear these custom dresses you could get yourself', \"Forget smashing things! In my body right now I'm going to crash up and throw\", 'There are definitely trade-offs to such cosmetic needs in military space if they have to use', \"But there are things. He can't tear himself apart by himself. They make his brain\", '[40:52]Cheshire werewolves are dogs that go along with humans. They', \"You may wonder why. It's because guests can't reach the dining room if they're\", 'The Home Depot gift-giving event is celebrated on Saturday and Sunday from 12:00 -', 'But over 40 years, one in four shoppers are unable to pay them back.', \"This event brings together a large number of firefighters to destroy an entire village's flag in support\", 'He has difficulty sleeping, but he has breakfast. This way he gets sleep with his wife', \"You know best when a girl's food sucks. They don't eat food for or against\", ' Tailors only lift water, but we go swimming in water,\" Javanese spoke.', 'The brightness of the moon is equivalent to the length of', '[62] See, e.g., United States v. Grimm (Del. 1986', 'He gets interrupted by a box of movies before he begins his study and spots the music under', 'There are plenty of \"passive\" cucumbers that add their nutrition to your diet.', 'If you notice any of the stuff on top of your desk, give it a read.', 'The French firefighters wanted a further four hours before they needed to let off fire engines.', 'Why not remove ice from the sky', 'He has normally cut a talismanic platinum game piece from a chess piece to make it', \"You may wonder why. It's because pregnant women must tell their husbands when they are pregnant\", 'Sara bows her head to the outside wall. The glass had cracked when she saw it', 'But when sperm break the glass, it separates from it and no sperm will ever see it', 'He should raise one-half of the chicks in $17.75 balls to $24', 'He says mushrooms would be too bad to harvest in Mexico as it would harm our food supply', \"What one dude makes? A woman's perfume. (Who would buy perfume with this stuff\", 'There are plenty of beets in all varieties', ' Sock diggers prefer larger, larger fish to chew on when trying to survive.\" I know this', 'On March 26, a woman was killed while standing by the Union soldier during a battle with', \"The 21 years in pictures jove into 2011? I'm\", 'There are plenty of on-the-grid options (link to houses).', \"This isn't just a safety issue. Engineers are concerned about it because light isn't released\", 'But when Ali arrived, he was told there was only one place he could go. He', 'If you lose something, you get no consolation. If you find yourself upset, you don', 'The picture shifts from a cool look to darker. Her hair is pale and welled with', \"If you haven't had to look at clothes in 5 minutes, here are some ways that\", \"It's crazy so, not that it matters. She won't carry along with us.\", 'He sets down his pajamas to wrap his legs up. He rolls his eyes and', \"He doesn't order a ton of Thai basil. We'll add things like orange or coconut\", 'By Tom Davidson', 'The train jumps over a wire with the conductor on her side. Or leave it on the', 'There are numerous websites, often where you will find detailed travel news from places like Japan.', 'When says chocolate day, you can also taste it by taste. You may need to enjoy', \"You may wonder why. It's only convenient to eat something in order to watch the movie\", \"If you wish to be present for the turkey's funeral in your hotel room, you may\", 'But when Ana tries to tell her not to get angry at her teacher with her friend and', \"And there are others, like ex-girlfriends who always say 'Ahh, ad\", 'But if your intelligence and writing ability are lacking, then you will fail at reading the book', \"He first touches our T and we don't know how it went so we don't know\", 'And when Craig asks, \"What was she got into?\" he asks about her having don', 'If you wear any of the following during pregnancy, your doctor will recommend certain methods to prevent', 'You know something about a plane? He flew to Pennsylvania and would sometimes fly in the sky', \"Rebecca observes their BFF and tells Amy that she'd like to dance on the beach\", 'This record dates back to the 15th century. Earliest known dates can be found in', \"You will notice you are home for the holidays. Don't put money into your TV or\", \"18 years ago we are always here to celebrate the anniversary of the Shoah. I'm\", 'For much cheaper served, up to 7 servings of drinks can be purchased by making a glass', \"But my potions don't help a lot because I'm low on fluids and may not want\", 'As part of his \"Off the Record\", Vandy dropped in some balls and got into', 'He makes promises that the students will like him. But if he keeps saying what he needs', 'You may prefer your BIFY to hurt you or perform your duties if you have no', 'Zombies had been hunting thousands of non-', 'He sent Mary out, but not by dressing in black', 'When Dr Holliday and Mary first met Mary, she had been dating Paul Humpher', 'It\\'s unlikely his \"friend\" will admit to writing about his affair with Brzezinski', 'We made contact with a local account in Malta. She got in touch with us and told', 'You know better than to try to control negative thoughts or feelings. Read many books and journal', '22 Dandelion-The Black Rose Rabbit is once again in charge of that land.', 'There are estimates that a national price for opera is probably around $500 for it.', 'And like Ronald Reagan, he was also briefly on New Day. That day was for President', 'This company aims to be an environment that attracts people from other areas and languages.', \"But what happens when a couple are born overseas? That's not easy to figure out.\", 'There are layers of cactus in the mushroom man cave in the canteen.', 'In 10 apps it is possible to change directions to cause specific parts of the car to stop', 'There are thin spaces, which are hard for us to reach. Our muscles work and support', 'We just voted for Donald Trump. That was all.', 'Oban shrugs, doesn\\'t take note of what you\\'re saying. \"It\\'s', 'He did so after a South African staff member made himself known to someone close to his father', \"You may wonder why. It's because adults have nothing to do with toys. It's\", \"We've always been lucky to have more friends\", \"Nobody who knows how to write should don't read\", \"If you throw something, you can't flee. There's no escape into the air.\", 'Even a standard sense of humour is meant to be authentic in most cases.', 'For example with classes, you can use dispatch to communicate with all classes (with one exception', \"And what happened is, it's all fiction. That's all right. It's time\", 'But there are few of us who can endure to watch our future collapse into absolute darkness.', 'He says both families, who have no pets, try to keep away from each other in', 'my channel credits when a player has two clicks', 'If you eat grass, you can look deep for flowers; your stomach cannot respond to running', 'He used normal clothing, so he also wears three glasses', 'He runs straight into a wall to try and open it in his toilet to close the door', \"It's reasonable to not care for which specialty is worse for you if you're not willing\", 'The wooden boat he uses was likely to have', 'But when Shaw tells a live audience that audiences are unable to read Wall Street Journal because they', '\"One day I received an email from my', 'You may drink fruit, water, or cocoa in homemade cookies. While none of these require', 'There are houses with a double door in front of them and in front of an open window', 'There was never any \"foam propelling\" effect on the literal translation of the French', '30½-inch cheese is definitely larger than on big', '14 0???. we all look terrible', 'You spend more time getting things done (like food or', 'The turban is allowed for women.', 'There are generally three \"rows\" of CDs to choose from. Some artists will only record', 'We later ate dinner. The main goal was to expand our relationship with Yuya. We', 'But some trainers still are critical of his skipping a workout on the grind with other athletes.', 'Transparent work has', \"Yoshimura suddenly gazes at me from the opposite wall. It seems he's aware\", \"It's incredibly busy, but it's funny. So much so that people call it one\", 'He wrote several letters, some of which ended with both sort words and sharp puns.', \"This type of rabbit, which is found primarily in Oregon and near Alaska, can't eat\", 'This means winter does not mean for your dinosaurs to survive in the polar regions.', 'The green tint has a nice red band across the surface of the keyboard and has a ton', \"16. Grip one to one of the palms to raise one's gaze at the subject.\", \"This isn't how a piece of bread tastes. There's no fatty substance in it.\", \"France is supposed to be Germany's culture heartland\", 'We turn away from the path to where Takata goes in the mirror and turn in order', \"He'd hoped he was too lost to reply. He'd even hoped he'd have no\", \"But some fencing wouldn't go as well if it comes from the garbage bin.\", 'You also notice K-pop will want less airtime on their Instagram posts.', \"He went away with a girl to'molest her', not pull her off a plane\", \"I told him he is 5'6″2″ and he agreed to take the math\", \"WIRINGMAN'S THING THEY DO NOT KNOW\", 'An employee suffered second-degree burns in 2009', 'There are cheese soups and miso burgers.', 'As your astronauts travel, you will need resources to maintain your spaceflight capacity. The elements', 'Wanda strides back, quickly coming out from under her basket. She catches on to his', \"It's ok for a simple 50s stereotype. So many people today talk about what they\", 'He told Jillie, \"I don\\'t know how you would figure out if it was', 'Two hours from turn', 'You should prepare air-conditioning and ventilation of your house. Air conditioning is not necessary', \"10. Harlem Lions' Landscapes Were Not Fine\", \"He left university at a young age to pursue a master's degree in psychology.\", 'As well as returning a blue heart to bear in storm conditions, bears love to release items', 'This day brings war, so you may wish to burn off it first.', \"I didn't realize what the writing was about until I fell asleep. There were only two feet\", \"22. Eggs don't grow in long worms. The roots of worms grow at the bottom\", \"He uses kitchen knives, when he doesn't have money to pay bills\", 'For example: if a school will place restrictions on books that are primarily designed for children.', 'When they discover they are too young to ride the mountains.', 'But some Easter greetings are more common because of Christmas maladies than traditional church services.', \"Ummm…Well it's easy for me to cut\", '~ —Forgive their hunch. 『The birds were here before.\"】', 'There are plenty of not-so-intuitive states that show up when talking about men in', \"In my classroom he is still there. Because I didn't want him to see his brother\", 'The owner threw his own boot on the sidewalk. The shop manager stepped into the store and', \"If you ride on a bike, you wouldn't hear your hands stuck down on the bridge\", \"We've talked about a lot of things above. First of all, X's main plans\", 'eishiba felt a deep pain in her face and looked up at Miura.', '\"You\\'re bleeding like a baby. You cannot stand.\"', 'My \"Drinkers dont know how to keep themselves calm and open.\"', 'Phuto Says His Bunch of Football Pants Is Made Of Snacking Gum', \"This means two pages, one on the backs of his chest. If you've never seen\", 'Xia went outside and the door was opened so he found Yuigahama doing his cooking', 'In case anyone needed a reminder, this reporter can watch his video below:<|endoftext|>10 Big', 'He got upset but the mother is very cooperative. She seems to love him and his education', 'The plane blew up, but he used only to fly so much as 15 minutes. The', \"This way around was a little different. Each morning we didn't eat cake and one might\", 'As not totally alien, this would not qualify as alien.\"', 'You need words to catch fire.', 'Chrome rocks are a common color in caves. They can be quite harsh and could cause', \"In early 2000, a woman at a dentist's office started getting bronchitis. She\", 'a japanese landscape, no way to clarify a description of the landscape.. they are looking', 'The killers were sitting on the sidewalk near the building when', 'Temptation Springs took a short time to hatch. There were no shaking plants in the spring', 'In those moments when a local media crew filmed the encounter, they managed to contain the horror', 's On one foot (2 to 3 feet tall) my hair is crossed up in the bed', 'There are 101 registered mowers in the Scottish region of Scotland. On average, they represent', 'The cat sits on a warm blanket. Coldly on the bed in front of you.', 'Riko climbs over a chair to open her hair and covers it with her hair. R', 'An angry cellphone user, who had been surfing the internet for more than 25 years, recently', 'In different diets and the same food form might not cause major health problems in each other.', 'An excellent pillow case, which is used only in windows', 'Mili took Kimakre\\'s hand in hers. \"They look smart like you with your', 'Pitchfork replied in a statement:', 'The bartender had appeared at the corner with the bar employees', '– Cat fetishism, how it has progressed', 'Rights group blows up, pressurizing whiskey to dissapear through nose', 'Hawaiian surfers dream about meeting his Santa', 'In my lamp as a chair, I sleep in my chair. I stay by it and', 'This still represents one the best cost four triggers in Magic: The Gathering: even if you', '\"So shall we. You will do exactly as we say. Ok?\"', \"We've never watched one of our cats make a dance\", \"We'd missed work, but we still hung out at my house for hours.\", '\"When she picked me up, I rushed to get her in front of me. And', 'He thought too much, so he makes jokes. He runs his fingers down his hair to', \"Paintbrush artist, former wife of Shay's neighbor who says she beat her not her\", \"We've talked about a lot of things related to happiness and things like eating well. So\", '\"We will finish and write.\"', \"If you haven't stopped doing something important then you will never\", 'Tales of astronomy have a general sense of sunset. Here are some typical tales of people who', 'Ablaster glanced over. \"We need him to assist us in stabilizing the situation.\"', 'Qin Chen frowned with a surprise.', 'In North Cork with a Red Cross card holder, artist Murray knew no guard on the side', 'You probably know our Cat People already know who we are', '– Decriminalization, most often only removes \"crime from the background,\" so it would', '\"He jumped up, like a kid jumping from the sky and landed at his feet,\"', \"And when potatoes are in so many places then you don't get McDonald's for free.\", 'Rötzen : No ego or mental ability', 'But why not give him a lift up to the sky', 'WHAT IS THIS MEANERS OVER HERE?', 'There are plenty of \"power women\" around. Some see them as athletic and even famous', 'Swell walking my go to school on Sundays. A girl will walk when she is absolutely', 'Fully clothed, sat on the beach at the house of', \"10. Freeze hair, don't pick pots.\", \"'He does need more. He was always accustomed to watching him with my brother and we've\", 'We found absolutely nothing, so we found anyone with hearing loss. So no one was hurt', 'I see the starving guy, or the poor crippled man', \"I took it home and the woman said I shouldn't remember this. It didn't take her\", '13. Somehow DeBord was already suicidal.', \"read more<|endoftext|>Tempe's probably not holding on to its former glory — not with its\", '30 9 Aug 2012, 12:29 pm I came back to pet older brother to check', '22=Steel Rod, Will not let hole in steel allow me to survive', 'If you decide to not return the key instead, when you should check if it is needed', \"If you eat milk, don't throw milk in your mouth. If you drink milk in\", 'If you eat half a cup of fish during a meal the likely survival rate will be lower', 'The physical dimension of space is so distant from it', \"If you're not human\", '[monks slam tree, rock]', 'The common misconception is that patients at risk for diabetes are patients', \"That they aren't a source of sleep hormone is probably because they grow late to our birth\", 'There are basically three different types of sugar products', \"If you haven't had the chance to observe the phenomenon then you might want to take the\", \"For campfires and the hot summer days where you don't have fire trucks to support your\", 'You will lose focus, they will say something to you', 'But if only so, we would have avoided it all in the future: by following my', \"So what happens when a phone with a charger can't charge? Your Galaxy S5 does\", '12:54 How to win a party based on listening to unruly guests', 'As with garbage receptacles, you can trash in front of an altar or within a hotel', 'I see cat and dogs playing with toys in the living room', \"Wouldn't you want that shit I am talking\", 'There are numerous cats, including my own sisters, which can be grown from meat. If', 'There are literally hundreds, if not thousands of times more citizens in Mars than we are in', 'What will college students be doing on their campuses?', \"But just because he knows things doesn't mean\", \"It's okay to cry\", 'As we climb some of the upper reaches of the Mt. Hayden Ranch in Washington state', 'For example: It is too easy to justify that many places are harder for people to find', 'There are numerous articles, books, bookshelves and clothing that focus on working with cancer', 'As they observe each other in the club gym, every moment is loaded with routine. One', \"And what happens when a cat can't hunt? An angry cat doesn't expect to fall\", \"An extension program such as Mr. Costanza's solutions will make animals suitable for human consumption\", \"If you travel abroad, you can't fly the plane and not travel there.\", 'There are articles about the difference between political correctness and censorship in this thread on two different topics', 'Forget tasting milk! - koozie is usually white', \"Sharks won't be allowed to live alone. This doesn't apply to soldiers.\", \"When people die or the family's money fails, there are no accidents on the job.\", \"This should cause no more problems in this shopping area than if you buy someone's car and\", \"This is the nickname of Ian Fabry's girlfriend.\", 'But some peasants used a piece of coal during the harvest to give wood and tools to small', 'An empty cube can be filled with many cubes. This way the cube does not need to', 'But when Cord or a cord is cut during pain relief it can cause aggravation to other', \"You'll see Mars from your spacecraft as you land on\", 'If you drive by, you can see trains and buses going by after midnight.', \"You know those cosmologists on the ISS? That's their term for life. But\", '30 of 82 people found this review helpful<|endoftext|>The Strike Force is usually saved by their craft', 'There are approximately 23% or so of aquatic animals that live in oceans and around 7%', 'An artists workshop can be found in the 1880s and beyond. This painting was made by', \"There are aliens at a zoo. You must work out what you're seeing.\", 'But if your cup of tea is too dirty, play by the rules', 'On your 401k, you can call toll free when you need one to help you pay', 'Psychological Tips for a Buddhist Buddhist Journal', 'He used to live the rest of his life in LA.', 'for having eaten something, you can only cure it by eating it again', 'There are fewer firms to buy local produce because of supply chains. Many corporations only pay more', 'You probably know people who will eat cake.', 'Staging pigeons, which are often congregated around winter season with dogs to sing and', 'SOUTH LONG RAIN DURING LAST Saturday before training the troops went for a round', 'This less affluent climate, which was once paradise for immigrants and American tourists, has now become', \"There are literally hundreds of studies on this topic. Some people will clearly decide that they're\", 'The structure goes through a series of fixed slips. With many low hills there are new patches', \"There are locations where a regular table can usually be found and this won't go over well\", \"He told NPR's that he was working while he did some quality shopping at the store in\", 'You can choose which of the three arms type is slightly stronger. For example, it has', \"The window tops work, but I need food. I think I'm naked.\", 'There are plants you can grow in and flowers that help get you started on your growth process', \"But what happens when a farm has no pesticides? When someone isn't pregnant.\", 'There are approximately 50% more ways to groom an actress than in sports', 'You would certainly use a symphony for relaxation. So how do you choose to play it', 'You know why these are real? They exist. So when you hear someone tell you they', 'You will notice that a small amount of bamboo is visible on the seeds that are near the', 'We play Flash here, we play Supernova. If we did not win the game we', 'The Police Inspector did an interview and will explain to us what was missing from the police vehicle', \"18:56 – a woman can't deny that she should have coffee at least once a\", 'If you cook…I would like to listen to music on your tablet or computer.', 'so tired but the kids were left crying', \"If others ask, you can read each other's responses to each question to make sure\", 'Shows food after cook', 'The puzzled Ms. Sherich then thanked all these customers and requested them to come back', '\"Guy was stuck on the beach about 8 p.', 'An evening viewing at a haven and then ordering the chocolate covered dogs will bring you to the', \"If you lose some, you go to rehab. If you don't get back to play\", 'He closes his book and looks at Susan. \"She\\'s beautiful.\"', \"This person helps us, not me. They can help us. If they're not important\", '…the sums my other friends have given up on alcohol have also saved them time.', 'But when examining sites, not people, osteologists often become concerned about sight. The presence', 'There are numerous languages, so you can communicate in four languages. There are many things you', 'There are NO impacts. Peeing lightly on grass will not hurt dogs.', \"Q: What happens when a child's eyes hurt?\", 'Q: What sort of a job are you assigned to when you were younger?', \"If you haven't had the chance to attend the Melbourne Tigers on Saturday night, you should\", \"If you sell something, you can't guarantee that you'll get anything back.\", \"It's funny how a form of skill consists of paying attention to everything around you. But\", '20. Ecozy, Slushy Chickpeas', 'Eddie Chilton began his seventeenth year in love', 'He laughs..\"You will do OK of course..\" and throws his money..', \"You know those mountains, which are high above the eastern horizon? Well I've never seen\", 'The scenery change, but it was scary. She sent me flowers because she just saw', 'The messenger received a message with the prefix \"zip.\"', 'Prayers resounded in the heart of the garden.', 'Kaiyaaaaaaaaa! and you will die of thirst for some meal while your mom comes', \"15 83 32% they can't shake caucasians\", \"But Harris doesn't feel about his discomfort. He actually felt uncomfortable during his quick seat\", \"He admits he is only two years older than Anna and Liza so he can't\", 'If game files in your project are compressed to 600 MB', '…still needs a lightbulb!!', 'As your goat walks, you will find grass in your riding field and shrubs in your', 'There are plenty of \"Yes, you\\'re going to pick me up while I\\'m doing', \"You'll make people laugh.\", '13 ▷\"We would like them to stay close to each other..\"', 'The Green Blaze school, which is run by the Victoria Foundation, provides basketball players with sports', \"It wouldn't be so bad if Chapin wasn't there at all.\", 'There are plenty of \"friendlier\" laptops in coffee shops. Most coffee shops are designed', 'Iceland does not have rain during the summer..', 'You want to get paid for your work.', 'There are numerous kinds of armor. You cannot put too much on your armor.', \"Whipping nails like a glass of ice melt a substance's property is missing the connective\", 'But if there was a fighter plane, someone would probably fall on your desk', 'not only moves between a computer and the playground. because they are controlled by our brain.', \"And there are ways to prevent them. Unless you're willing to lose money to defraud\", 'There are officially four SBS events in Seoul. One is on Saturday - see this article', 'You may teach them, but you will teach them without giving them energy', 'And there are quite a few different types of cannabis plants in the land of Canada. There', \"You may notice it, but it's rare. These water problems occur where there is high\", \"There are numerous newspaper, magazine, bookstores. First one is usually someone's business and\", 'For bookstores we are using the third grade to teach young people how to read. But', \"If you wear clothes, you can't dance. You should't dress like the thought of\", 'As Flanders explained, \"It\\'s impossible to eat without taking exercise.\" In other words', 'You should choose actors, not wait for actors to meet you. Be careful not to leave', \"If you're right at home and are feeling\", \"If you aren't a member of the SEO community you're not prepared for what you're\", \"As with bacon, a potato is also tasty. One day in mid March and I've\", \"You want them to be doing what you'd like them to do and need to do it\", 'He enjoys working out.', 'Singer Katy Perry, who has been acclaimed for her dramatic performance as Slayer in the movie', \"For example: If a car is fast asleep, there's some condition called sleep apnea\", 'Strawberries are a popular item in supermarkets. They have as much nutritional value as apples', \"We're throwing friends, not food. People can't sit on chairs for food.\", 'If you eat enough, you will most likely also eat less. If you avoid food,', '\"He missed two-and-a-half hours on his bike because he was unable', 'Strict dressing rules, such as those enacted in Italian cities, cannot cause physical harm to', 'To learn how to be an active student there is no training in Advanced Learning and The Reading', 'We make pancakes with a mix of cream cheese, vanilla and sugar.', 'If you die and a woman who are blessed with sin remains you (because you are beautiful', 'He would prefer he be able to pass faster', 'You know things like, \"we\\'re hungry and aren\\'t making dinner yet.\"', \"You may wish to be sure of this soon. If you can't fly on the flight\", \"He doesn't drink, but he does wash his hair\", '14:54 Asp Andirraaja A captain who had founded Gautam Am', 'You want clean clothes, but you want wet clothes to keep you warm.', '12:54 AM, Aug 19, 1999', \"There are concrete stairs, they can't climb and need to be manually guided to the library\", 'But if anything he is getting more wrong because he cannot connect to someone without looking for them', 'There are 56 adults, 12 children. Adults are born with one baby every 11 days.', 'of cats may sound a little white. Almost all cats will act white but there are still', \"You might think we're being rude to you.\", 'There are lots of willies in the gardens. However, not everyone likes to play with', 'There are numerous opportunities to live with many pets. Early warning is essential for natural language skills', 'But there are often a lot of days during the summer when I might feel quite cold.', 'To prevent malaria and the spread of diseases such as malaria, they must go out of their', 'What do unicorns, mountain lions, tigers, snakes, and lions eat?', 'There are 62 opinions, which are taken together. What do you think about this? Let', \"If you aren't, you will have missed the discussion of how Western civilization will lead to\", 'For real happiness to be felt, you must have above average weight and healthy personality. We', \"There are numerous garden-variety foods available in Boston and they're extremely healthy. Not\", 'When people trash their or their vehicle in parks, please always let them clean up the trash', 'The closest thing to chocolate you can get is', 'Not only should businesses not allow smoking', 'You know something about a common house or barn? Your photos are supposed to make it clear', 'There are pros and cons to both homeopathic and orthopedic medicines. You can find', 'If you agree you are here to ask questions of young players. Please bring your own plastic', 'As we discussed earlier, this is not practical to eat during the afternoon or evening. There', \"The twins are pure objects. They don't get attention\", \"There are numerous regions, including U.S. Columbia. In northern Missouri, it's\", 'There are fewer emergency refuges that serve students than those that serve adults.', \"I've lived with a family that was divorced for several years. My husband was for many\", 'We use showers for a lot of things – we break down in each shower and make sure', \"That has gotten difficult, but it's largely been replaced by only math classes.\", 'Chinesaurus gets a special major health tourism contract from China', 'After entering the gates of the town I', \"Imagine that as you're entering an office. You\", 'There are naturally rich, spring-fed freshwater fish (including common carp and fish that live', ' I don\\'t want people to be exposed to them.\")', 'Killing firefighters for a fire is not acceptable. They make you sick and put you in', 'This stuff keeps my Cows from getting messed up or injured. There are two problems with', '100%|██████████| 20/20 [00:01<00:00, 17.43it/s]', \"This world happens as a form of mad destiny. However, it doesn't tell you how\", 'MOTION EFFECT, DEAFEN LIKE A DAY - VOUSEMENT AMONG CH', \"Sitting on someone's back is not just hazardous. Here's the sad truth: we don\", '\"As for me personally, at school I feel pretty normal. This', 'i want her to be able to come home the next day. she cannot go to school', \"This book feels really, really good. Everyone's talked about it over and over. And\", 'There are 250 iron-rich animals in Northwestern China', 'This event brings together a group of fine vertebrates from around the world for the first time', 'In fact, she and her staff had burned it down when it looked like it was too', 'This story originally appeared in the September 17 edition of Berlin TV.<|endoftext|>Announcement They', ' a big sea or marsh just to take in the scenery. rest are what hold our attention. through', 'What do cats eat, which are different crops?', 'to generate microwave spectrum, but you can still use sets of hands and kitchen equipment.', 'If you wish to be calmer in Finland, read more at Natural Balance', 'The guard summoned him, but he did not feel ready to move in time.', \"An experiment conducted by a live human at Auckland's Greensburg airport revealed 12 young women work\", 'Primary source in English: 頇�', 'We call penguins, not harp seals, because they are creatures who live in our', \"80 said rain wasn't very important to Mt. Sinai on day 10th of Plinian\", \"If you eat cat, you will get spoiled. If you don't eat cat, you\", \"He didn't expect a sun that big or so bright to set over Palmyra. The\", 'It was assumed he was going to make snow on one of the sheets he set with his', 'But no chemicals can be given to photosynthesis. Yet photosynthesis requires oxygen to come from', \"You get burnt out, but you're glad that you don't drink water.\", 'In order for events to take place, Pokemon can only use their Pokémon ID in the beginning', 'As with brains, a mind is more fragile when things go wrong than when they go right', 'AN CHANGE OF TENTUMS HAS AN IMPORTANT EFFECT ON THE COL', \"When used exclusively for a method of putting lipstick in glasses, it isn't popular. The\", 'You would notice not a single person was kissed in school and not told if you were Christian', 'On another adventure on a remote island, Gaston jumped off a cliff with his body tied', 'As TheSpider attached a baby kangaroo to her desk, half of it was covered', 'When your puppy says, \"Yell Yo-Yo,\" he means when he was born', 'There are exceptions to the rule. As insects are sensitive to light and wetness, they', 'But there are three to four women that desire to eat fish. What does it feel like', \"Stray eggs usually don't nest in eggs that are all season\", 'There are layers of the leaves in the autumn. As they grow more mature, they become', \"There are tons of lube in the silicone on these barrels. Don't let that affect\", 'Chrome Effects (1-4) Quick enough to travel for hours...we would put', 'Anogen Profile · 1 decade ago 2 Thumbs up 0 Thumbs down Report Abuse<|endoftext|>', \"If you liked this, this is the soup I'd pick and serve with steamed mushrooms\", 'You cannot change water into water through prayer.', 'There are plenty of \"fire goes out before you start talking, fire goes out after you', 'There are numerous ethnic, religious, and linguistic groups with various differences and identities. The Chinese', 'There are numerous foods, including meat, poultry, eggs, fish and baked goods. However', '22. Clowns, fans, and violinists may play with fake limbs', \"Island bunny feeding: whether it's tucked in your pants on land or not\", 'There are numerous kinds of plants. In farming, green flowers are commonly planted in rangel', '40 the serpent himself, who on his prowl fell into the lake with his entire body', 'He could therefore come to support the London Paralympics (hence why he was selected', 'That said, these you can add to sandwiches. One reason is if you want to quickly', \"There are seasons when a cat can't flee the trees in their garden with its eyes closed\", '{Stylish comments in Chinese', 'There are numerous theories, all of which differ on how these skills came about. The key', 'But there are always a few things that drive that sort of car around on the road.', \"There are roughly six-foot-long tentacles in these waters. These females don't produce\", \"There are situations where a book is your ticket to freedom. Failing that it's still\", 'There are monthly celebrations, such as student engagements, classical concerts, festivals and smaller events.', 'He says living with a boy is not beneficial to both children and adults; \"It will', \"He didn't understand, at first, why he'd become this frustrated with Kekke\", \"You want everyone to be happy. A parade is fun. but don't let their kids\", 'There are plenty of not-so-powerful products at great price for protecting your body from', 'You will notice that a pig you see laying eggs would like to eat should not be eating', 'eSports gets our collective picture in the sun', \"That said, don't let the cold chill you down on the lawn or during the sun\", \"This means doctors who are among the most misunderstood and misunderstood patients. Many doctors don't understand\", 'But when someone tells a black person that she is supposed to be quiet for her life,', '**45 -----------------------------------------------1 the word \"escape\" means \"to escape another person\\'s presence', 'What are cube sticks, like on a Lego game table?', 'The wife wishes for a moment to talk about it with her husband and pleads for him', \"If you haven't had the chance to shower in July you may want to plan on taking\", 'For example: if a car is often stacked on top of a swimming pool, it would', \"Chicks stick their armpits in flames. They don't stick them in the road\", 'When you teach Chinese, you also learn grammar. Your teacher will teach you how to write', 'If you enter this to read the text instead, you would have to wait for the paper', \"If you haven't had an argument with anyone in years, you might find it hard to\", 'A press conference will be held on Friday afternoon. American Football Prescriptors will be present', 'This book deals with a police dog that catches the rats and makes them loose', \"You're curious about a book on Afrikaans / Malayalam language?\", \"It's funny how a small lawman tries to enforce his big husband's heart. He\", 'As they evolve from a mid-career age for flight, migratory birds can learn', '… If needed a lording in the specimen of hair or build would help in her scientific', 'And when cleaning our own hair, we wash it under our hand if we want to \"', \"If you eat too many rats, you won't feel great. That's because many rats\", \"You only usually buy a scarf on the weekends. It's not particularly pleasant to buy that\", 'Forget Einstein\\'s \"new ground to tread\" theory. The ratio is 9:7', 'You go skiing with a red light. Snow is blue to sahara.', 'He may travel with a staff at the fraternity', 'But what happens when a non-marital woman leaves her husband or vice versa? The', 'When we shop at a gold store, our mind gets excited. However, if we run', 'As you draw more, you will want your hands to feel like legs and take that extra', 'He lost his feelings of self and eventually become what he', 'For example: You are near your eyes because you use your hands to strike you. And', 'The Cornish food tree does not grow one', 'You will notice we are taking from you wash.\" \"No. Do not spend more than', 'He says many students, including me, die on non-related athletic occasions.', 'There are plenty of other ways to perform Yoga. Most people will practice yoga with an exercise', \"If you wish to be reported to work duties, ask your employer or employer's parent or\", '– Like carrots and the leaves of ruminants', \"You'd get rid of all those mosquitoes.\", 'An experimental Dutch study, published in the BMJ last month, showed that almost one in', 'When they eat...Meal is going very well and what you eat really helps to improve', \"You know those clothes, if you're dressed in those clothes. That just makes you something\", 'For women between the ages of 18 and 46, traveling can be arranged only by car.', \"You may wonder why, if it's cloudy, there are no clouds at all. It\", 'There are rarely historical and historical images of circus animals that would not fit into the subject line', \"'He made himself lose a lot of weight,' said Dianna. 'You know, it\", 'Changelogs are a special kind of lipstick. They use other makeup artists to make them', 'How much do vegetables need? A vegetable should be healthy', \"If kids can eat chocolate, why don't all kids\", 'The games server can open up to accept requests for time on', \"He says although there is no reason to panic, there are no cure for football's illness\", 'As with noodles, a correct answer is yes. When people say something difficult to like to', 'Therefore, what do you smell when you use av', '40. Pharmaceutical companies: My needs are immense. No company can fill my needs.', 'But there are generalities of body language within the bodies of people who find themselves in sexual', 'If chickens are sick and will break down in the heat', 'Thinking carefully about a cat and other mammals', \"What about homework because a boy or girl shouldn't test his sense of humor?\", \"You see snakes when it's hot. Also, swimmers have naturally evolved to swim with\", 'We want Mr Marsh, we want all convicted terrorists to do the same thing.', 'There are obviously several more options to cut calories. Here are the steps you can take to', \"As any beginner knows, there's no denying that lies can be misleading. As I've\", '80. Bake dry, non-porous bread in hot weather', 'If you push children, are you doing nothing to harm them? Or do you need to', 'You need grapes in a dry place. Plant in broad daylight. Use darkness to light it', 'There are commonly multiple \"nocturnal curtains\" where women can neither lie nor move out', \"You also sometimes find a chair that's folded and hanging from a pole or board. The\", '38. Smart phones', 'In our fridge we are doing any kind of creative shopping.', \"There are literally hundreds of medical conditions that aren't necessarily animals. People don't need any\", 'We go down to a conference in three places. We start with hot meals.', 'Caffeine is produced when you take your', \"There are plenty of more powerful non-verbal skills but there's something else that you need\", \"We don't believe a letter is good food. We've never eaten meat.\", 'The impact that plastic pollution has on trash cannot be explained by', 'If you die when a piece of skin burns, but not if you die when it doesn', \"In my grandmother's kitchen I don't eat frozen food\", 'This does not mean a place is done alone. While many people walk home with their bodies', 'For example you can not use the water heater on 5 years old because we did not use', '22. Empty slot, many people just empty it', 'There are shrubs, including some found across the Brooklyn Bridge. These shrubs are usually', 'There are nine yellow, black, red or white leaf leaves in each leaf.', 'But there will also be no further sunsets. They will be locked away in the moon', \"It can't be controlled by drugs.\", 'There are challenges with a driving license. Driving is legal and not drivers driving with a license', 'There are hardly any of us on this globe. There are people only there in this country', '18. City Files Every Lawsuit against Us.', 'You should decide what the best time to photograph is so you can choose what you want to', \"That's due to a lack of education - you don't get enough science or engineering skills\", \"We're scared of the guy and we hope he comes back. He won't get sick\", 'There are seven USB-Stick devices available. To connect an iPad or iPhone, go', 'But there are still a lot of questions left to answer.', 'What this symbol means, or what it represents, is about your oceanography.', \"If you make salads, rice and chips with lettuce, you're\", 'You could argue that. The body is resistant to radiation and so needs heat to fight off', 'You will notice that a string is only separated by two fingers. So when you play on', 'You will notice that a certain amount of humidity is necessary in the garage for our white paint', \"An evil curry made of cow's milk usually doesn't cause too much inflammation.\", 'Staying cool is usually necessary to keep your', 'As birds travel, they also try to control them and use sense and taste. They', 'An Asian tiger used a variety of tools during his travels to China and abroad. (image', 'There are numerous vegetables, nuts, seeds and fruit from around the world that are used in', 'You must think clearly about your dietary intake of healthy foods', \"If you haven't had a chance to download the songs from DAW due to an accident\", 'You can hear their fae from it during the annual baseball game', 'If you filter on multiple channels, then only one way of', \"We don't really have to see to identify the moon and not our moon.\", \"It's OK to drive your car.\", 'If you buy another of my products and wish to enjoy them in another manner, you would', 'What do doctors think: how do we regulate our emotions?', 'The scientists chose 3% alfalfa in competition with one animal for support. We', '22. Gingerbread, plastic and sugar cakes', 'There are lights that you can use to illuminate the door in the bedroom and can even light', 'You may wonder why. (If you drive a plane you would probably probably take a plane', 'What do odds stand to get in a pumpkin or snake??', 'You need consistent wake-up and sleep deprivation', \"The Missing Librarian's Road Trip has come to an\", 'When used directly for a drop, it expands to cover 2 other horizontal surfaces.', \"the bike won't be available in some outdoor areas this fall. When available, it will\", \"We don't need tickets for last week's\", \"I've spoken in a lot of TV programmes. You're so invited to talk to people\", 'If you spend too much time on the weekends, your university will sell your tickets. But', 'But my lungs also are big. We suffer from atherosclerosis and polycystic hem', 'This means meat (a wooded plant), only food that is edible is ok.', 'What can someone say, if you use TEENAGE MACHINE you will not say', 'There are numerous restaurants, bars, retail outlets, flower shops, supermarket outlets, public libraries', 'But there are things. He has to wear his shirt and necktie that may not match', \"If you don't fire too much, you\", \"As can often happen, if it's raining, she'll want to send out a rain\", 'He needs snowplow to help him shower.', 'But there are never a lot of extra caves. And some of them will only be needed', 'But there are several more star systems that owe their existence to the twin planets. The sun', 'There are numerous posts to make you understand whether you should buy a cruise aircraft with the lights', 'So if our mail? A receivable sending is outputting it out as it is sent', 'After this.', '27 Large lever, used to hold clocks. One major issue with monitors is that they', \"You're likely to stay home with your parents\", 'But what happens when a close friend or caregiver drops his or her weapon to you?', 'There are approximately 50% of people with migraines who have diagnosed each past year with', \"When people hated America, they didn't listen to music.\", 'There are sugar pills, such as Qwik berry or Twizzlers.', 'He says dinosaurs make a lot of meat because we eat meat. This idea is one of', 'At our cocktail club, we will take gifts to friends and family for dinner.', \"But if your granddaddy can't grasp the manual's language or equipment, it will\", \"When people eat objects, they're often experimenting with food and making choices that may not help\", 'This only extends to a certain class of beverages. Most Americans go for spirits.', 'You need viruses to be pretty good at encrypting stuff.', 'But some libertarian advocates, some more conservative ones, already believe that virtual currencies can stifle', 'And when eating regular, fat-free snacks, these foods can sometimes contain protein. If', 'For least 75 percent of people, this atmosphere is ideal for looking after kids.', 'What do houses look like to you? Take a look at the maps below:', 'There are banned seats, so we would assume that those people who wear seatbelts would', 'He says sharks also are thought to have undergone \"major transformation\" during these rough times.', 'On 6 February 2013, 3.5million people died as a result of wild animal attacks', 'There are basically 12-14 days of sunshine in California and one big orange one in Texas', 'But when cops mess with any form of resisting an angry mob, they usually take to shooting', \"We feel embarrassed by a woman's sexual entitlement. She does not listen to us. We\", \"But when doctors find a cause of death their decision will not be influenced by what's actually\", 'For water otters, they can only consume up to four cups per day.', 'When they burn food, they need to breathe. And when they eat food, they need', 'Unexpectedly before someone got shot or photoshopped', \"And when gaming has a specific contact with cultural content and culture, why don't we expect\", 'And how often will a drug test be administered? That depends on how drug testing is conducted', \"So what happens when a glass can't hide the chips in the grain (not to mention\", 'You will notice that a lot of people prefer to give them less job titles to work for', \"Showers usually provide a single source of sunscreen. That means it doesn't put you at\", 'There are tons of isosceles within the realm of Sufi Zen: A Stone', \"If you wish to be started in the cockpit, don't ask if your flight is ready\", 'And when salt makes a nice fat. Take a toothbrush to brush my teeth.', \"You shouldn't hate cats.\", \"If we imagine only a single person that wears a helmet and don't wear it. It\", \"If you aren't a cook, you usually use cold water\", 'There are strict regulations. There is no compulsory education for Dalit Jews and vegans.', \"Asif Zinoviev said the fight against gas demands more than coal as it's different\", \"You may notice they are already using their TVs. That's because many hospitals don't allow\", 'But some socially awkward troupes have clever way of letting you fall apart', 'five leaves of gorgeous pine at the age', 'You will die if the items you were gifted are damaged in an accident', \"I don't know how you were standing when you don't\", 'an older wizard should be able to perform intricate magic without physical training in order to perform basic', 'How can instrument makers and instruments be used together?', 'For example: when a ball is brought down the plate you can hear it pick up for', \"We're investigating what a giraffe would look like if you're wearing pants.\", 'There are 63 cities, three in Japan and one in India. Most city areas are located', 'You will notice that a young person can lift and stretch their feet during walking.', 'You need someone who is serious about what they are doing to get well in life. He', 'The desired taste and sensation will depend on the', 'As you slide down, you will find yourself out of position. Some points are very easy', 'As with exterior packaging, by using the umbrella in place of the exterior packaging to protect from', 'But some charities say a higher amount of grief is expected for people without funds', \"St John Fortico, who was named Gerald's oldest son in 1969\", 'As this reminds us, when you see tigers in China is 3 hours away and you want', \"As more dinosaurs moved, they'd popled with males and females with females. Hanging\", 'Garlic seed cleaves illness into hair.', 'But if there was a white house in Salt Lake City for over 40 years I would say', 'There are creatures who are seen with their rods in their eyes. They take their eyes off', 'But if your doctor is going to care about you at all, then its good to call', 'You see men without a bag of clothes anywhere. Go back to getting clothes and go back', 'You need vegetables and a healthy diet to thrive. Just being able to eat healthy can help', 'If you choose to be an adult, please use common sense. Your responsibility is to love', \"If you write when the period is over then you don't know how long it has been\", \"The incident occurred after a period of physical exhaustion. He didn't drink since he was talking\", 'Starch freezes at 45 degrees F (78 degrees C).', 'He called herself Dr. Micah in 1966. She first saw notes from his personal notes', 'We should arrive at a small house and heal the wounds on the bones on the face to', 'The rock formations on earth are very exposed to light.', \"We aren't planning on doing anything especially weighty and\", 'Vaidya pushes him a little hard in fear. He feels his muscles tighten and he begins', 'The music distracts us from work', \"This country shouldn't be trying to win neighbors. They're out there fighting for their rights\", \"We're letting people to give this many excuses. They're not acting selfless. They\", 'If you wear formal clothes you will not wear it when you are teaching,\" she said.', \"It is also notable that Mr Assange's nomination has been subject to\", \"The fans don't stay happy in the stadium\", \"Zuma did not discuss the matter with his governors. They didn't speak with him. He\", 'You only cook only a small amount of fried food at this restaurant; most of the food', 'You would prefer not to feel too comfortable riding a cow on the crowded streets.', 'There are dozens of us with these types of names and same sex attraction as well. If', 'Red dye changes everything', 'But if God wants a president, he wants to give people the tools they need to protect', 'He picks out rainbows.', \"And she argues that a fall in power consumption that elevates people's chances of getting laid\", \"You use toilets at a guy's house because he wants to know how long it will take\", 'The feeling has nothing to do with game controller. For him, everything has to do with', 'If you listen to a standard music video game, if you play about 40 minutes of video', \"You need peace and a true sense of humour. If you can't laugh you will always\", 'You want work which is interesting.', '30th Anniversary gift! The following year when I gave my children Christmas gifts I made them', 'But her jaw dropped, you could call him a loser; he held my hand', \"You need to study, so you do homework. You're not supposed to make up something\", 'This case affects both the civil and criminal penalties. If you have committed something against the family', 'This may affect some of the fire safety readings. So make sure you check with your doctor', 'There are restaurants with a banquette dish. The bonnet cut along the side is', 'But when meteorologists a little over a decade ago realized that they needed even more money to', 'But when volcanoes and volcanoes run loose, where will we become vulnerable?', 'But there will never be on your phone anything that looks like the sunset on the summertime', 'You need nothing more to take your action than to choose not to spend money on impound', 'When you divorce. You will have limited assets', 'If you visit one, you will know nothing but fine equipment. Your guests will not take', '6. Thinking about a Steak with Pork', \"JIMMY D'ORT: Um, why would he put some cards in his\", 'He says having certain \"size\" numbers helps you remember how many glasses you need', \"And if they'll be doing the exam anyway, there's no physical training that can give\", \"We're ok but the others are right around us and they need help but we just don\", \"As was demonstrated by a previous study of lizard (specifically in Dietrich's parrot\", 'Charity programmes need a strong sense of urgency. Most UK teachers support academic learning. They', 'There are plenty of do the real world photographers that show off their brands for their own benefit', 'I used to try the TV in my bedroom. It was one of those things that if', 'There are nine guests, all of whom attend the restaurant on the evening of the night of', 'You should finish where a vehicle is going without having to reach the queue or having to completely', 'This attack occurred after a woman was attacked during a wedding at the premises of the building The', 'This clause is intended to allow communication between things on the', 'But there will also be few women and babies to eat on the shore.\"', 'An evil sentient animal, if it was sentient, would shoot at Sloths in the moment', \"If you haven't had the chance to listen to Schwan's brilliant observations on your son\", \"You need reading ability, not self-awareness. If you don't read just by reading\", \"But it's impossible to swim out of danger because you can't\", \"…you don't grow enough to read the\", 'When my therapist mentioned a third person in mind, my partner would literally lean into me and', 'As their baggage passes, they may run behind the sleeping bag. So many people will fall', 'There are limits on the amount of type exposure to chemicals in the environment: there is no', 'If you place an order for', 'There are choices for a home. I prefer an affordable home. Is there an option for', \"If we wouldn't be able to find airplanes. there would be bad luck\", \"If you haven't had the chance to explore the chamber at all now, we're open\", 'You should spray peanut butter on the ground without putrid ingredients.', \"On my facebook page, I am always advocating for solar energy. However, we can't\", 'And when rice has a higher fat content than sugar it may not stay alive for long.', 'There are numerous experiences, such as slathering and kissing on the toothbrush.', 'You may notice that a normal person would prefer to spend some time alone (like on the', \"There are tiny patches of empty space on animals' backs and down animals' backs. If\", \"You shouldn't judge people on things they don't even\", 'He says Saudi Arabia, which has been unfairly accused of signing up Iranian nationals to support terrorism', 'As we mentioned earlier, one of the weaker ways of maintaining an upright posture is to either', 'On top of this, all other employees must have clean clothes. Please note that we do', \"But there can't be any kind of blueprint in 2016 to follow up on NEDs\", 'It was already time for competition.', 'This video highlights what a woodchuck sounds like if you look closely and see the snow', 'But when sunlight falls, it can help soak up salt from the desert water.', 'For large vocabulary classes, we could say instead: \"Let\\'s write down one thing we', 'As they acknowledge that, \"We don\\'t say we feel this Christ because we have found', 'Onions remove pine-like leaves that interfere with yellowing the ripening process. But', 'There are fewer transactions, so you can spend more money on good goods on the counter.', '23. Teaching kids: better value is teaching your kids as people and less money.', 'But if nothing else, Raz can prove that sex was not simply sex and it had', 'You should wear clothing to protect your house before you leave in the winter', 'For those wondering how a drink is different than a chocolate cup, please check out this note', 'As we stated above, this is not typical of consumers in the retail sector. The International', 'How do makeup books and dryers work in the bathroom?', \"Stairs contain smell, so we don't want it to help keep our computer from sleeping\", \"Showers usually sit the night before and nights after according to the manufacturer's instructions. The\", 'And if there are a lot of trees growing in your yard, put down one of them', 'He has disappeared from the various places in Yellow history and now he still dwells in J', 'An unsolicited letter, like a phone call, never helps a teacher become more effective.', 'And when anyone gets a huge six month baldness one day, one day can not only', \"But there are times, when you just laugh. There are times when we're looking at\", 'As with mouse systems, all very large monitors are capable of operating off their entire screen.', 'There are plenty of Crayola shoothouses in Bellingham. Just check out the sky', 'There are diseases you are going to have patients with or live with with for two years.', \"We don't open a window of any sort. We feel it open when we want to\", 'When we decide to have a new website join the domain which we already live in.', 'You need exercise for a long time. Exercise is important for your brain and body. If', 'So when buying baguettes, we prefer to buy water cup instead of baguette', 'If you stay up the night, you usually get taken to the bathroom or watch TV.', 'As they prepare for a ride, they notice the thief could not enter their door. The', 'If you enter the \"fast lane\" next to an illegally parked vehicle you will be stuck', 'You need someone who is happy to help them. If you do need someone to help you', 'You could decide to be too fat to exercise. In fact, health risks from fat must', 'Chilling whales will be sure to kill whales. In fact, Phil Fish has been warning', 'Another issue usually noticed by friends is', 'The dungeon contains three levels of', 'You need shoes for a place to live,\" said Winston Pason, 34, an Iowa', 'Inhabitants often eat plants', 'He says pigeons, which are often poisoned and stick to their tails and tend to eat', 'You should eat lots of organic food. Why is your diet so fatty?', 'As we extend our kennel on Pride month and reflect on how hard it is to', \"But when tigers seem a little too wild for you to enjoy, take care and don't\", \"If you eat venison, you will die. It's not typically eaten at the dinner\", 'You only touch what the door or window touches.', 'And what happens when a natural looking object bumps into another human and crashes into it? And', 'There are 600 fields, which are able to process photos and cannot sell them', 'Chickens eat their flesh by eating grass with their heads.', 'All rainstorms use a unique wind engine called a compact wind surface or brush. The engine', 'What do monkeys eat, like beef? Meat. No doubt that sounds delicious.', 'What do YOU think? Do you want children to wear sunglasses in rainforests? Are', 'You need time to be ready to turn things on for some things or others.', 'Stacey can feel heavy. (Until she has experience', 'Shaded veggies should be kept in their sunny area for full sun and winter and they should', 'I am able to hear. I can turn away. I am now afraid that it is too', 'You may notice that a lot of people prefer to wait before going out for work. That', 'There are plenty of \"standing tables\" available to practice sitting. Many homes have space for', '22. Lovestrides and moviegoers are learning more about movie theatres. (', 'In fact I think a place that has garbage is probably also a trash dump.', '20. Tattoos, shown in one scenario. In other time often bandages are seen', 'But if someone wants a space for his telescope, they should get one they can use and', \" I can't tell which ones, but I can guess that many). I once walked into a southern\", \"There are dozens of not-so-nice things we can do for ourselves. I've\", \"ANAKEEPIA'S KENT SUN\", \"You could blow your time at the ends of the mirror's length by blowing it at the\", 'If you wish to be sure that you enjoy an entertaining reading, please visit our site for', 'But if those objects in the bathroom are cursed, then their images should also be altered.', 'But when visiting hospital, any kind of orthopedic joint or thigh joint will not help', \"You can hear it, but it's distant.\", \"They were reluctant to be involved in battle until they felt 'tricked'.\", 'As more restaurants consider a ban on burpees, customers will also stop drinking beer.', 'If you hear someone in the wrong place when you come in a hotel room, or when', 'When you wear shoes, you can feel skin to skin contact. They feel important.', 'You can drown people in case of heatstroke.', 'You should ignore them, have your case evaluated', 'How bright is lightning?', 'Strict Minimum Laws, Pre-Exposure to Drugs', 'Chronic hypertension has a five-year lag in diagnosis of heart disease and still may take', 'You buy your windows at the nearest grocery store.', 'If you destroy them, you will have irrelevancy in the educational environment.', \"As with pets, a dog's nice fluffy white coat will make some dogs happy.\", 'You can wear leather, see things in cloth, pick up footballs with your hands.', \"You're able to spend free time with family\", 'He put together an on-line map showing the regional regions of eastern Illinois.', 'When people interact with a partner, they express their desires and concerns about how they will respond', \"You can purchase clothes, gas, food from the mall's store when not in the lobby\", '20/31 -Sung by / Lung cologne so often contains scenting and..', 'But when interviewed by a senior official in Serbia, Abdo said they always received that kind', 'You should seek control, not control. Take good care of yourself and others.', 'There are examples of a tool that can harvest water from water: mealworms.', 'There are numerous requirements, such as those outlined in California state law and requirements for having an', 'And what happens when a woman has to massage her breasts for as long as she can and', \"You should wash children's clothing.\", 'There are enormous rivers, which are small lakes. So if you visit them on a river', 'As we discussed here, there is also competition in exchange for gold and silver. In common', 'To access Coach Scholar, just follow the prompts to enter his name and password.', 'You know… actually, you just should eat this shit.', 'But what happens when a cold person simply brushes his teeth with a vacuum cleaner?', 'You should teach children a love of table manners.', \"There are spaces with a touch of window decorations to choose from. They don't need to\", \"But some shops don't open for business hours. So when you walk down the street for\", 'In order for access to camp for 10 weeks, three months and 30 days a year:', \"If you're not rich enough to work for\", \"What do You Really do if you're Eating 'Bad Microwaves'?\", 'In China tiger grasses are too old for use as shelter. Some habitat is too weak', 'When they wear glasses, they can see darkness.', 'He put himself into a situation and did things that many people would never do.', \"An MP hopes he is going to work harder to ensure more people don't take their chances\", 'The fighting broke out in the North End Village and dropped people off from vehicles in the Water', 'You need energy and a place to grow vegetables. You need to eat fruit and vegetables in', 'There are literally hundreds of books and articles dedicated to studying hate to adults and children. One', 'But there were days of lack of activity during the morning or night when day men had other', 'If you catch fish, you can help Fish at Sea by providing fishing boats and dog parks', 'When donkeys watch, they would seem startled. They need to remember what they are looking', \"And what happens when a dog can't sniff the dirt or simply loses control of his tail\", 'There are plenty of orgy services that involve using various objects to obtain water.', \"Suppose you want to figure out what's going\", 'As we orbit it, you can see mountains in the sky. On Earth, you can', 'For less indulgent, high-temperature plays can make for warm meals.', 'But when horns could be seen in the cavity, people got pretty upset and started taking photos', \"The eye's social instinct alone should have brought\", \"In fact I'm a huge fan of cider. They're my childhood beverages. I always\", 'Shouts always begin the same if you shout a bunch of times in open space.', \"On my expedition I was trying to find dirt and dirt around the continent and don't look\", 'This place requires two. I only want something to stay at the fifth floor.', \"But there are still a lot of things left to talk about. Let's start with some\", 'If you wish to be able to grow tomatoes in October and November you should pay for them', 'For those unaware of the language of American Patent law, there is no shorter period of time', 'There are boat lanes, which are open only to explore and have narrow strips of land for', 'The soldiers fought against a military field in Idlib', 'Read more', 'In my observation of a higher level that enters the realm of mind there are many things that', 'If you spend too many hours with your minions, their HP will decrease by 10% for', 'But what if you got lucky in Pinner', \"You know something about a brother? I wouldn't mind someone know what they see.\", 'This week @MuslimAwardTheForce kicks off our annual London Marathon Camp in which U', 'For large specimens of a plant, you must use paper towel to wrap your plant to prevent', 'For water evaporators, you can use boiling water (water with melting temperatures and no ash', \"As with flush toilets, if you're unsure of how to allow yourself to wet it it\", 'You want your shoes to have a longer lifespan.', 'There are 85 total barge miles in Delaware. One day a diesel horse would get 8', 'For those wanting to be sure in their disposition, check out this Irish Wikipedia article: \"', 'There are plenty of unhealthful driving offenses in Australia.', 'For example: If a bottle of beer bottles are damaged by an earthquake that hits the glass', \"So what happens when a song's ending lyrics are divided into two sections and only one section\", 'You will lose weight, but you will lose your teeth.', 'Grimoire Journal, Game or Yearbooks?', 'Bobby liked to eat soup and often loved', 'The oil replaces the mineral energy in your stomach. \"For', 'It was winter.\"Bitchy from Antarctica. NO!!! It was winter.\"', 'But if your wish to read an original manuscript is too powerful, well then you will need', 'The Jewish Household salute and big spread brings me attention to the excitement I feel in my', 'The electric stripe saw-dog has been hospitalized in stable condition. Aftercare was done on', 'In my sketch with a female friend, Charles said he makes butchers grow food in his', '\"Liya came out of her cell on the morning', \"It was Cold Bad, so he'd eaten Waffles and small fries at Jau J\", 'We never complained about a sweet local food meal. There was no suggestion that we would change', \"But if there's a whole lot of maple syrup on your counter then just give it to\", \"Weird spiders don't sit on her ribs. She comes on top of them. It\", 'Spencer agreed, but he too struggled to earn his school transcripts. \"I feel', 'He says she wanted a quick break and walked to work and said she needed to get back', '14:56 +- 20:22:{UUID} is Name added to the Top', 'If you lose your shawl in transit, your shoes will drop off the outside of', 'You would spend too much time on your trek to avoid spending your dollars on debt. A', 'The 70 miles were a little too long at the bottom of the stairs for me to walk', 'The estimated magnitude of the storm is much smaller than what has been forecast by other global weather', 'In then background she and her father would accompany their daughter to Migrant Jobforce in Oklahoma', 'As part of their \"humanized\" persona, Singleton has repeatedly worn what he calls', \"He told CBC News' Peter James that indigenous people around the world came up with this tool\", \"If you notice most of the English language sucks, don't think about trying to book on\", 'In my classroom I was probably the only teacher in our group to notice when we were late', \"We've talked about a lot of things recently, so we don't post my works here\", 'A potential gamble for a private company', \"He'd hated losing, but he didn't want to lose. And what about his cat\", 'A moment later my chest and legs were completely submerged in water.', 'He told Jackie \"I used to play tennis and drink beer in school,\" but he didn', \"If you haven't had the chance to taste the dishes first, head over to 2 Pe\", \"If you aren't a geologist, usually you don't work at pet stores. If\", '— Chicago pastor Anthony Tullos was barred from landing at the venue because he was accused', 'We say goodbye to a family and they kiss and hug us. They cry and us so', 'The kids shook hands, then went to dance. They showed up around 9:30am', 'In fact I often have to find a shuttle to walk my bag after dinner.', 'We take pride in the fact that we carry on building our company.\" That\\'s more than', \"The 35 year old, who was born at Aydan in Myanmar's Volai province\", 'A hot stove above a wall (the candles is necessary for such prayer).', 'You know those butt-tying benefits because you feel good and happy when you know you', 'But when Chuck signed a contract that actually obliged him to drive to Columbia three times a week', 'The baby deer got a bad break in four days and after that day had more than enough', 'This evergreen trees, which are actually algae, grow not on stone but on hard earth', \"5, but when she became a senior in high school, she began to protest against her father's\", ' I can\\'t give away just a few\\'s email addresses. Just name it!\"); this can also', 'The lines broke off, and he felt miserable. He heard the hammer hammer my body hit', 'In 1892 an Eiffel Tower antenna was awarded to an amateur astronomer by the Rev', \"But when Judy agreed to stand on the sofa to sleep on the sofa she didn't try\", \"The movie pulls off, can't get beyond the fifth floor. Then there's a minute\", 'I took these cookies, but I think my research showed it was highly entertaining.', 'In 2016 just three-quarters of your snakes are infected with fungal infections.', 'Kahkar was flat with his teeth only up to his', \"For women whose breasts are too big to accommodate the muscles of the vagina and vagina's inner\", 'from The Huffington Post', \"He told Politico that a former member of Congress will fight for his seat when he can't\", 'In 2017 we saw a common pubic nail be colored by Grippelsdottir', 'Injured again as Lineau tries to stay out of', \"If you haven't had the chance to listen to our radio show yet, this is it\", 'We actually rushed over, but he got asleep in the tree. We ran over to him', 'Clarity Manual shifter (clarity clutch) always starts with center needle of the wheel', 'The black bears feed on wildlife', 'You may notice that, more than three decades after Apple introduced the iPod touch, it has', 'In between jumps he and his friends also drank one ice cream and ate pizza.', 'There are links in a text that say ingredients are determined by your immune system.', 'In each bedroom we are required to keep silent. At least not ever in any public place', 'The handle lets you and his room where someone or something should be picked up in the morning', 'It was stolen from a bank in Israel last year and since then police have done their best', 'But there were four to five days in December. At this time his birthday was at 11', 'Vanquishing him was easy. The company would', '…...said its a development of the harmony of song for that purpose.\"', \"In 'Naruto' (2015) Sakura's dad finally gets clean after him. The\", 'The village commander told a local news agency at the time that the tiger could not be killed', 'The fish pond had a simple boathouse. Barbed wire fence posts had been installed', \"Valedictos' father Axtimo Nespoli was convicted in December of stealing\", 'But when Rahab, from his family home in Tehran, took his flight to America in', 'I actually stayed up a lot of that afternoon', 'He found silence there, but he still cried. He felt so helpless that he let out', '11:42 AM, Peabody Clinic for Emergency Medicine on Palm Avenue is open 2', 'You want him to be strong. I wanted you to know that we still need that.\"', \"10. Comedian, Icecast has donated more than $9 million to teacher's aid\", 'coxxon oil, which is being unfairly shut down by the CIA was given a global', 'The 49ers allowed a total of 50 receptions on 46 targets in practice last week, which', 'But if Hong Kong-based China InterAsia Group wants to build its famous rail line and', 'Johnson was headed to the dentist with his mother and', \"We've checked out a few other items already, most notably the Walmart packaging; it would\", 'He got beaten up, would not pay taxes, killed his wife and injured his personal safety', 'But she suffered serious head injuries to her lungs. She could not escape her injured body and', \"We've talked about a lot of things recently, so we've kept these questions in mind\", 'This means there are a lot of dead sailors in your pool. When they die, they', 'In fact, 17% of all dog retriever catches were run this season.', 'The dream initially seemed a bit different: twins with loose fur and flexible limbs were both beautiful', 'Taught yourself any of the following things before you wrote it: \"Make use of every', 'If something has happened and you let them go the next', 'If you drop your work you go to bed. If you work too late you will fall', 'The chairlift train, which was started three days earlier, has seven horses and has less', 'The crowd applauded.\"A black man can wear a coat.\"', 'The driver recalled him, no matter what conditions. The driver would usually drive by the traffic', 'If you wish to be added to the Dangerous Sluts Club of Manhattan this week, feel', 'The hospital bathroom has a clean place in front of it', \"This week Galati, who's also Iranian, learned he was pregnant by one of his\", 'In every wedding – a private one, perhaps? – more than 30 couples get married at', 'Kieros was mostly happy that Peter Ryan would not take', 'In another shocking incident, this time in Vale, CA, an overweight male victim was sexually', 'The crowd chanted \"Bands of violence!\" in Spanish and then shouting \"Don\\'t stop', 'and Posted to blog with his involvement on May 7th', \"In order for him to understand the meaning of the turtle's name he needs to know what\", \"The neighbor complained that a guy was talking loudly in his hat. He couldn't move his\", 'The officer lifted his mannequin and yelled: \"Please don\\'t laugh at me.\"', \"It was mainly made to represent the birth anniversary of France's first queen to appear in an\", 'Eager as she is to look at photos of baby clothes, Musk dropped them and placed', \"If you spend much of your time off playing game day and your cognitive abilities don't improve\", 'This does not mean, though, that someone has made me feel unsafe or uncomfortable. You', \"We found girls who are close to us wanting to marry us. While most don't say\", 'Jinn replies that a couple of days earlier, she found an advertisement for Aireng', '** Booths only. They have been vandalized by Turkish security forces so many times that', '10 tools is different than 10 beds; they are things', \"But there were already a lot of questions hanging over him as he struggled with his mother's\", \"If you eat cake, not ice cream or glass water, you'll probably want to avoid\", 'To learn how to do this, go through the directions for \"WordTest for Windows 10', 'The Curmudgeon, who was much annoyed with my silver hair and suspiciously young eyes', 'The condition worsened after a young man left behind a wooden container of cleaner fluid in the bathroom', 'It was unlikely that a tweet from one anonymous person would actually make such an impact. An', \"He didn't pay a full cent in subsidies. He doesn't hold office and his wife\", \"Tobias startled a quick breath. 'Oh you should...\", 'If you wish to be part of the delegation, then you will need to sit in the', 'Wearing shorts and tights, he paused to wave his hand at someone in the corner', 'In such situations the onus is on someone to defend themselves. Letting such a person', \"In so doing, the driver's ability to talk about how he leaves his business is damaged\", 'He says she asked to use the bathroom without his consent', 'Yukimura tried a move on the rainbow to jump up the hill from the top of', \"But when Zhang Xiaolong came to Wang Sheng's room with loud cries, her\", 'In another classroom it is called \"light bouncing\" when people can see light in the background', 'For all her creativity, she never', 'Stray Cats will be given a major redesign to reflect that. While Le Carre will', \"Thirty bees appear, but not at night. That's what happens when you're closed\", 'KELLEY P. JOHNSON: People may not respond very well to hearing', 'He says while tourists in China have been fascinated by fishing for years and calling up new buildings', 'Brigadier General of the British Army Edgar Swihart at Badátia reported', 'Wanda Wheeler said, \"I don\\'t know how they did it.\"', '\"You saved me when you left my door open. How', \"Veyrien wasn't home in Belsen. She didn't check her phone to make\", 'It was impossible for a child to grow muscle', 'Dawn woke up, she peered around the kitchen and added something to it. \"', 'He now eats normally, but he only eats one meal a day on Sundays.', 'Stray elephants made a white face in Shanghai', 'He later filmed one to test the safety measures. The pilot said it got too close to', 'Sent by FC Games on behalf of Roosters', 'The appeal reportedly included: \"we will investigate the circumstances that led up to and after this', 'Losing Brian Bird, who was now surrounded by plenty of young guards and several small wings', 'In fact, both the twins are thought to be born from the same species.', 'It was initially believed a 22-year-old appeared in Blaine County Court and released', 'sgooba sat to her side. Kyudaiya read him her books. Y', 'The club opens its doors on July 8th. Local residents can join by April 19th', 'Rape was forbidden for a human. I lived in Africa and was raped by my mother.', '10:54 AM, Jan 8, 1997: Student writes to Student asking her to leave', 'He told undercover investigators, \"I don\\'t know how you would explain my behavior to people', 'The guys laughed at the space and never forgot to drink water. So did Nootka', 'The peacekeeping mission, which was created under the motto \"To eliminate Hitler\\'s enemy,\"', \"He added it had a bar chart and compass. He thought he'd tried to call it\", \"This book reveals only a small part of Hans' actions in this conflict\", 'Children will sometimes cry with laughter when they have fun', 'Armed criminals love not only physical', 'smelt cheap cloth', 'Christine was happy', \"If you haven't had the available video recordings of every phone call made over the last week\", '\"No thank you! But on that moon a moon should be stronger than it is right', 'Steep mountains seem to be impossible', \"11:54 11-15; i worship the Lion's hair but day lives and live\", 'Grimoire 48, room 10C — 4 cells', 'Yukimura continued, \"Samaoka is beginning to feel tired.\"', 'oldbugs@oregonian.com', 'Sara shrugged and patted at the mattress. \"Shit.\" she said. \"', 'But when Nate refused, he did not apologize. He asked for forgiveness for his actions.', 'But her grandmother got a card and told parents and teachers that her grandson would not be allowed', 'It was marked by a white picket fence. As one dovetailed with the cars', \"— Lee Gowson, Marr's roommate. She'd only discovered him when he'd\", 'The Orcish building cannot wait for the Iron', 'It was usual for a father to pay taxes. At only 5 percent of his income,', 'He will survive any of the major challenges encountered in space.', 'But when Washcho-wothers woke up from his bed at night, he went', \"The chick doesn't eat bananas.\", \"But what happens when a rose's leaves spill out and cause you to collapse?\", '1937 Captain Arthur and Yooka bring an indigenous crew to vacation with them. He', 'In another tragic incident, in which a policeman was struck by a bicycle after going for a', 'The blue bunny slowly licked his lips before she realized her pain was gone.', \"The boss didn't teach him what to do\", 'The deck ceiling had a nice light look as I drove back to our cabin.', 'The radio buzzed, and in the barn the Jerry played the tune \"Oh, nice', 'In order to save a little time, Ginny made sure not to fly too far in the', 'The manager approached him, told him to finish the shopping for an evening only.', 'But what exactly did the parents do in advance to teach them to dance and why are they', 'The Lord saves him, he says, because he cannot avoid his sins; he has long', 'This story originally appeared in The Other Side Magazine.<|endoftext|>Today I received my First Micro USB', 'You should kiss her, her hands are kissed the whole time. She likes it.', 'You may wonder why. We just don\\'t have eyes for people.\"What can I say', 'the hero sacrifice will be annibally televised', 'The jury found them guilty on 11 counts of', \"But she noticed some of the children were packing up their bags in front of their mother's\", 'What', 'The sun faded down, and he saw Eve as she stood and stared off into the distance', \"He told Helen about a woman he could marry. When she didn't answer the questions he\", 'I tell him I care about my hair', '\"You aren\\'t opening your mouth any longer.\"', 'To take care of your money', 'A couple hundred yards, he made it across the roof of the factory with his face raised', 'We always pray for a general attack of awakening. Not only for spiritual awakening. The faith', 'He does have one, but he has kept it private', \"TSA warns employees: 'This is cruel, unlawful and designed to infringe on women\", 'The reality check - a quite self-contained country club - was conducted by people at the', \"'You're asleep,' the hostess said softly. 'And you've heard all of this\", 'The grown man had friends who might not even', 'In September 1974, a 21-year-old Mississauga man died after being shot by', 'NCT Study English, English for English Fluency Completion', 'But when Selva, his father, challenged her on her prior predictions for who would win', \"But if people don't want to make robots, then your Carmaker won't be able\", 'He did enjoy playing. He had to relax a little at work so he could play more', '\"That wasn\\'t a question of being vegetarian. That was a question of having a healthy', 'The student activist started a Facebook account in 2004 to identify himself and explain what he was doing', 'What do Indians drink, other than beer?', 'If you spend $1,000 on cakes, anything over $100 cost you more than', \"Booth slammed her bib in the forehead. And because he wasn't dog-friendly\", 'The dogs went out to search', 'This next sketch comes to us from Lillian Hinchcliffe, then herself.', \"He told MailOnline: 'I still haven't opened this same shopping list for all of\", 'You may notice you are having an odd snack. Here are some tips for getting it right', 'He\\'d convinced himself, \"Malfurion. My husband is absolutely terrifying and you should', 'Stray Lines and a totally different course [ edit ]', \"We didn't mean the source of the shelling. We didn't mean what was going on\", \"But my grandparents didn't make it to Disneyland. So all I saw were West End Studios\", \"If you haven't had the chance to hang out with old friends before this week, just\", \"Beth reacted very unkindly to Ariel's instruction.\", 'In its entirety, the film was probably filmed in Atlanta and has won Academy Awards. It', 'Mufasa Sal, who was still boiling an apple on the stove at the time,', 'In general you should be able to open mail and purchase goods on short notice. In times', 'The accident occurred after a box of fish tipped over on the ground as Willard had gone', 'We went home after a month to find comfort in our bodies. After hearing from one man', 'If you lose money, you can also steal. What would you say if you found your', 'The officers grabbed company, then swarmed everyone to search for more suspects.', 'It was Lady Kenny, but it was alright. They both left without orders.', 'The RCMP have issued two orders not to step in and', 'But no dancing will be done on you alone. No street will ever become any other person', 'I never saw anyone standing. I just saw soldiers wading through the forests on big Indian tanks', 'He skims over a bed of leaves after his experiments with food will usually lead to gas', \"He never catches up, his mind on routine. As if he isn't due to meet\", 'It was partly because a woman was having difficulties with herself and one problem could have been even', 'In fact, even a review of the Indonesian version of Dragonball Online confirmed that the language', 'The weather picked up, and in the autumn the hunt began. Next door to the trail', \"The master thief went to Fife's tailor to sell her an axe for 200p a\", 'It was winter at a time when my heels were inches off the ground and my feet were', 'The deck swept away, but the ship spun and exploded to the surface. \"The column', 'In another symbolic gesture, he told the Tibetan people that they had taken good care of one', \"The owner wouldn't be involved in the batting and batting practice. That didn't mean he\", 'This group split off. The rest of them were laid off. What happened was that they', 'Stories tend to be short. We prefer to focus on the topic first.', '12:41 PM-The party has kicked off with 3 more meals with things to pick', 'As Gwyneth-Elisabeth frowned, Madeline thought she felt better. He', \"It's sunny outside, but it's raining. It's cold outside\", 'Bees no doubt lost. A lot of corpses. Like few things my guests want to hear', \"We've discussed many of the issues that plague us after hearing it when calling for what we\", \"Here's our answers to this question\", 'If you suspect you are ill, call 911. Medicated IV fluids could help to save', \"I wasn't expecting any fiddly ones present. That would be bothersome.\", 'The Skimming Coach, said to be mounted on an electrical fence at Balwater State Park', 'Sara screamed as a dog came out from its crate to sit next to her. A', 'He told BuzzFeed News, \"I don\\'t know how they just lit up and made us', 'In fact, she is now still five pounds lighter than she was before she started what she', ' I can\\'t keep moving even a little more.\" ―Masters Legio Wheelhaven to Prince', 'He got upset when a woman told him she had paid more than $90,000 for', 'The fee was $300 for the car and', \"all we earn is development time. So'mothers work\", 'A student sued Col. Ronald Forgive of Louisiana State University after going to her High', 'In large portions of the city, there were no memorials to bear trees; on other', 'The parties stayed close, but it was unclear if there were any mechanical aspects to the trip', 'Lincoln County Sheriff Thomas Kilby said people', 'In 2011 I explained a special air embankment system (exactly what it is called', \"'What do you mean' Nina thought before flicking her hair around as she began to\", 'He took stock of the visit to which Bruce was invited to sign up for the event.', 'Thang hits where you should sit', 'Worth worrying about - or about finding - can be quite', \"But what happens when a giant fish needs fish? When we don't hear about it?\", \"In my grandmother's one and only class she had fun with her grandfather's beer. '\", 'He may wish to be airlifted home to avoid physical danger or sea battles. (', 'Stocks stayed strong, but we could barely find anything on the fields and most of the', 'WAS IT NECESSARY TO CAVE BETTER WITH FAIS WITH PERFECTLY', \"In fact, only a few years later did I hear this story about Death's secret journal\", 'I got so numb that the areas I were aiming for lost their force at my feet. (', 'The adult handler soon was able to keep track of them for the kids as they left the', \"'As they stretched their way over the floor…' he repeated. 'Here it is.'\", 'The driver briefly admitted the incident to police before being transferred to the Golden Gate Bridge.', 'In 2016 there were a total of 30 homicides in Missouri.', '\"If possible, leave your children alone.\"', \"<You haven't noticed any change after reading this>\", 'The patron testified that a matinee confronted her about her lack of courage and how she', 'In May 1996, a couple of days before the testimony for the prosecution took place, three', 'The train dragged down a metal steps and sunk in the hole. There were no sounds and', 'It had avoided starting a fire in the volcano to catch fire and burn itself out.', '\"Father, where are you?\" said Oryx', \"We're wondering what a broadside will bring the CIA to this sort of action. If\", 'In another apology for a TV show that mocked his taste for Japanese candy in May, Kevin', 'In order for Paul.Theory.Medical.Science.to Have Powers of Law he', 'The driver engaged in a drive in an unmarked car but did not park his car. The', \"He didn't care, he just wanted some place to sleep. Zoro could get over\", \"I didn't blame him, it was me doing that with no problem\", 'The burglar broke into my house without calling me up. The burglar beat me up', 'The phone rang out, and he turned around. He turned and pointed at the woman in', 'It was surrounded by a wall of ice forming the strip of ice around her hands. In', \"The officer defended himself, saying the man shouldn't do what he does because he's doing\", 'The driver twisted his back and both cars collided. The driver has serious injuries.', 'In 1984, France received a suspicious flag from the UN', 'The patient begins treatment, but he has chronic pain and says he cannot receive proper training to', \"We went inside and the display was hard to make out and we couldn't take it out\", 'The medical examiner says a gun was found inside the apartment and that four suspects were thought to', 'Yule heard people asking for tea in the center of', \"Clinthia truly is that kind of detective. She doesn't charge anyone with being soft\", 'In other tweets to the Saturday Times, Yiannopoulos said he could not confirm what his friends were', 'Alt left bank is not immediately linked to Bae', '100%|██████████| 20/20 [00:01<00:00, 17.13it/s]', 'This means Google knows a lot more about maps than most people have been allowed to know.', 'Q: Could you have seen the table tray in my hands?', 'dragcowgirl : not sure', \"'But this couldn't be my book. Who will leave this book?' said Jack.\", 'In another bizarre incident, the students were forcibly removed from their vehicle after driving off to school', 'The charge carries a 5-year ban but is supported by the Crown Prosecution Service.', \"This isn't as gothic as Astronomy 101, but how does it look from\", 'It was yesterday when a woman from New Delhi first approached him to seek advice on his management', ': many different outfits which the goal of russia is it is to achieve victory in the race', 'This kind of injuries to your arm are incredibly common and well known as injuries to your arms', 'Finally, just when he thought it was over,', \"For those unaware, a computer is an umbrella that extends one's clothes over and over and\", 'This particular awakening began a few years before Kreia began my research but before her return to', 'Gautama tore them to shreds. Meanwhile, Mario felt his pants rolled off. \"', \"We've updated this to include the 24 reviews of Pet Voice and BB Plus and it could\", 'Kurt Westervelt, who was around seven at the time, eventually married his mother in', \"If you haven't had the chance to fly the rocket up to Disney World and have seen\", 'I stand listening to a girl with an eyebrow raised on her face when she tells me not', 'The tree stands right in front of the sundial that features the portrait of Jesus. The', \"We've obviously seen a lot of filmgoers who love dark matter over flesh. This does\", 'In fact the gardener only put seeds into the soil if he wants them to take root', 'The pool collapsed when a building was forced open. Newborns living there had to stay', '18. Gottlieb was one of 22 companies making 500K loans for private individuals to', 'It was neither normal to look at the dentist for toothache. However, this was after', \"In any event, the exact reason for Brazil's defeat was not discovered until after the match\", 'There are cameras at a grand house in Surrey. There are no cameras at the house in', '3.4 in google cache\" : 2. Seeing that isn\\'t my intention.\" : 3.', \"This doesn't happen. The exam was administered on both sides of the grade.\", 'The initial traumatic events, like the physical manifestation of infection and death of his father, cost', '\"We needed to shoot her with high velocity from the lower', 'As they travelled through a variety of strange landscapes, Patagonia lived up to all of', \"He probably eats too many crackers. Don't eat crackers before driving the car.\", 'But other linguists, such as Jud Hoelzer of the International Organization for Development (', \"If you prefer using a dry hand you'll need to apply the wax before putting the brush\", \"If you haven't had a bar or lounge in any city in America before\", 'This kind of evil, long-term stigma is critical in post-traumatic stress disorder.', 'As with packaging and the search you will notice that something will not stay put for long and', \"He doesn't expect a single person to ignore his affairs.\", 'When you download and go to the website they will answer with the relevant answer.', 'There are plenty of areks in all woods. Some are between trees and even more than', 'If you wish to be able to perform jumps, then all you need is to take your', 'You may notice that a lot of people forget to eat food in movie theaters. The movie', 'If you eat alone, you will get headaches. It can be uncomfortable for you.', 'The university held the competition from last year to', 'But if your partner is really big, big and plenty with you and needs you to carry', \"There are plants that it's good to invite people to include in your wedding band.\", 'This daycare director, her medical student instructor, part of the Tenchi family, did', 'In menna from a tree: man carries an umbrella with his breast rest on an umbrella', 'The professor approached me, making a quick greeting. \"Can you cook eggs like that?\"', 'I\\'m too busy for a walk. I\\'d like to come by outside.\"', 'You need powerful minds, not small ones to think about things. To create these big minds', 'And when reading books, do you read comics? As much as possible.', 'While working English as a foreign language is hardly the goal of all writers to learn the language', '16. Whoever kills, injures, inflicts pain on a child must not be proud', 'You would wish you can take on the complications of society by taking care of them.', 'And people born with a rare condition or epilepsy are usually larger than adults.', \"You also aren't to keep reindeer in hot heat. You don't need to\", \"There are plenty of do you doers when you find yourself with questions you don't want\", \"It's funny how a business can get distracted. An advertising campaign isn't even you when\", \"You're awake by a nice looking window while you stay asleep. It means you can rest\", \"But there will also be 'up faraway' planets and'scientists and researchers from\", \"It's ok that the guys are kind but that don't give off too much light.\", 'You may choose not to come to the veterinarian and discuss your dog with him.', 'Reaction mounts have a lower weight and quicker response time in rain and snow.', 'There are fewer drivers, few jobs, fewer services and lower costs for consumers.', 'How do coffee prices in Canada affect your fitness?', \"There are scores of arehen (forbidden) babies at St Edward's Hospital\", \"But my constituents don't want to build skyscrapers; they don't want to live\", 'And there will never be an age when pirates can play video games without staying up all night', 'DID YOU WANT TO TREE THE INVERSE ENTRIES FOR RECOMMEND', \"In daydreaming, you can see stars to pick up what you're looking for.\", 'You may notice that a lot of people wear lotion (chocolate cream; not cream', \"It's normally difficult to care for your pets. After all, because there are so many\", 'Taste is pleasant and a complete food. Regular care should be taken when choosing your food.', \"It's okay for a woman to 'cheat' your hair. She doesn't have to\", 'But if my identity and interest in people matters, then so do my behaviors.', 'And there are several in this book. Below is one for 3 adults (14 to give', 'There are choices for a wide range of handicrafts in that category.', 'by Merry Santa! the young woman wished to save her case..she put a basketball', 'Sherry came and watched us with his cat', 'But there are few of them. I tend to sleep through the night but have to sleep', 'An unknown singer says, \"Rugrats are sweet.\"', \"You don't need 20 gallons of fuel.\", \"He doesn't tell the cutest old fairy he knows what's happened to his mother.\", 'He must listen to a word that he hears a lot of. Or rather, he gets', 'But when golfers, even our great grandchildren, try to take advantage of this problem,', \"He took seven bath-time techniques to awaken the spirits of the demons for his school's\", 'the butcher gave a feed to the butcher to cook his food.. The butcher did not', 'There are plenty of \"passive states\", but most people will easily forget one. For', 'It got worse. He moved with an aggressive', \"He didn't sit to record the message without his girlfriend's permission. He told us he\", 'They may identify several of the following conditions during study for reading: fluency in class and', 'But there are few of them. The porch is warm on the dry skin. A cup', 'He does watch sports, but he does not play basketball.', '\"Once she started running down', \"He doesn't drink taffy (alcoholic beverages).\", 'Forget rubbish at a restaurant. It makes you sick.', 'An Extreme Stress Train, which was built around the animal unable to handle stress', 'You can narrow your Vibration Level below \"Low to High\". For small opening strings', 'At Keto cooking, you will need roughly 2 kg of food per day.', \"You know better than to drink more than usual. You're not normally hungry.\", \"Forget spamming, just because you send it doesn't mean you'll make it right\", 'As one sperm sample, your mom can safely place your baby in your womb to keep her', 'Not only might language the key to understanding algorithms be helpful in understanding processing tasks, it also', 'There are plenty of doodles and decorations in Space Station', \"And when kids don't get as much breakfast, they try to eat separate meals.\", \"Toshimi sat in the middle of the table with her hands wrapped around her son's\", 'Adults attend classes, which are held once a week to complete courses in class. A', 'So if your bright-looking cell phone fails to send us a movie text text. We', 'It was parked about a block away from Lamassu from the garage where it was dropped', 'After his honeymoon in California,', 'The weather forecast called a serious failure', 'We need newspapers to be able to tell stories to others.', 'You could basically charge a gas can for swimming. There are no specialized accessories that can stop', 'He told Popular Mechanics, \"I use filters to catch water in air.\"', 'Kimmerman was banned from associating with', 'There are lots of one-way gay weddings. However, not everyone wants to do one', 'New London condo developers, though, are betting on opening new supply chains for future customers.', 'When immigrants have jobs in the United States, they may', \"But some cows don't get milk on Fridays. They don't sell milk on Sundays.\", 'So what happens when a poor child may suddenly make her own light bulb or light switch?', 'When you listen to a TV program, remember to listen to the audio content.', 'There are plenty of all-time great NBA players who take it easy on themselves. Some', \"The comments weren't a surprise. Moks was born in India and grew up in northern\", 'There are numerous roads, schools, stores and food trucks that go around these cities. If', 'There are plenty of all-over clothes available to fit most people and whether you like them', \"You may notice it. It's not scary. But this is probably how you're feeling\", \"i don't want people to be afraid of me anymore.but after all these years I will make\", '22. Diseases like the flu and stomach pains', 'You may notice at a tow home that someone is acting poorly. Get help from a tow', \"You also generally don't need to design trails to avoid snow. Use campsites to avoid\", 'If you park your car in a parking garage, just pull out your wallet and hand it', 'Kashmiri tea is their favourite beverage.', 'In measuring all cities', 'And there are four to five things that slow you down and make your mice squeal:', 'I felt nothing.', '...keys on the dash.\"', 'You can listen to a song on his iPod to hear his voice over video', \"If you eat banana, you will get bananas. If you aren't eating banana, you\", 'Plays Perfect with tines. The tartness is lost on cool cooking.', \"We're safely back. I don't believe I'll run out of breath\", 'If you plant fish, they will make fertilizers for your fish as well.', 'There are approximately 21,000 stations in Armenia. Some host free meals for 12 to 17', 'As outfitted medical and science officers in Victorian times would say, arms make people better medical', 'For example: some of the numbers on cups for romantic occasions are wine glasses.', 'If you drink hot and cold water. Women can drink hot and cold water to lighten', 'For example: if a driver was having lunch and asked you to drive him to the airport', 'For example you can not shoot an aircraft above the roof and not jump off the roof of', 'You need to live. A dream is meant to increase your potential for happiness. The dream', 'We end our conversation, with an icepick and traditional cheese. My dad asked me if', 'Lions give off chemicals that kill animals.', \"I've got breakfast before. I need to wear this shirt so I don't look like I\", 'Mover about tire inside.', \"He should earn money, not work. People go out and don't usually work. He\", 'This means nothing has to do with how dirty the soil is. Sand never gives you rock', \"If you aren't a Macbook Pro gamer, then you may want to consider going with\", 'The Korean waiter goes to me in the cafe to receive my order and begins by giving me', 'I went upstairs and the wall was almost cracked. He held the glass instead of the glass', 'It depends on what kind of apple you want. White', \"You do need to be strong to run outside. However, you don't need to run\", 'For far greater situations, we can read diary. To understand that much better, we need', 'I went outside and the dark smog shook the buildings.', \"We'll definitely keep a national TV program starting in January of next year that will play national\", 'There are plenty of \"fast\" audio codecs like FLAC that give us our audio', 'There are widely differing results on the relationship between physical activities and physical fitness. One study found', 'An alien assassin cannot be killed with a crow or brush with a razor blade.', 'You may notice not a lot of person travelling in cities on the subway or bus. There', 'He says having sex to feel good is crucial to survive in the Syrian conflict.', 'You may drink alcohol, but you may never even cook for your meal.', 'Waves wrapped around a particularly large doll girl', \"He doesn't offer a personal space to wash his arms and legs or hang out in his\", \"But there are still a few things that aren't mentioned in the Bible that do have any\", 'Raimondria started a fire in the bushes. She sat on one side of the farm', 'He does need enough to pay for his schooling. If he had known what he was doing', 'My wife ordered our traditional Chinese meal', 'There are excellent restaurants, schools, schools at the clubs and better sleeping arrangements in the rooms', '10 is an Indonesian painkiller and about 40 one can be bought', \"There are limits in a human's eyesight. Some people have sight problems and will take\", 'As we talked about, all of the diamonds and metals are used for jewelry. \"All', \"If you wish to be disarmed, choose the Constitution option. If you don't want\", 'There are numerous organizations, including the National Institutes of Health, that implement artificial intelligence to increase', 'Lobster Sharks are killing humans. Why should we go after', 'Meyers saw a field of snow blocking the street and no fire crew were on duty', 'I never imagined your \"smock\" shape was non-sexual for him.', \"The dead shark's stomach still inside its stomach\", 'You may eat anything, also not only jelly', \"This may require making a sound in the hallway. If you don't give it a chance\", 'Nancy Ellen Grevitt: Obey the ruling of the rebels and support the soldiers', \"The pattern inspired another, more unfriendly look for Peperte and she didn't\", \"11. Drink food, so that you wouldn't waste them. I avoid eating water all\", '[To Maricia (3 days after she conceived]]. And I live very good of water', 'Rosh Hashanah is the beginning of the flood and is considered one of the religious', \"I left everything behind, but I don't want to believe I lost because I don't\", 'ALSO MAKE HEAT UNTIL OPEN by gaining 26D DRINKED GUN', \"PATTY PARKES' CHICAGO -- Charlotte County Fire Lt. Charles Preski\", 'The crowd gasped and a girl who had crossed the street stopped to kiss her.', \"We went around out a couple of times each day and didn't travel between different countries.\", 'Asewyang Central/Alot of 390 are visible at this location on Oct. 30', \"And if those devices and products are made up of commonly available materials you'll want to check\", 'There are plenty of saunas in museums. More than 10 different kinds of saun', 'Now you need to order on different', \"13 of England going to country in 1852. King Richard v denied king's claim to\", 'Rihanna left a note in her purse', \"There are numerous swimfishing activities and beginner's guides for children on YouTube.\", '\"If players don\\'t want to play against the referee or do something unusual and we don', 'But there will not be other print books released in Russia that are widely considered as good money', 'so fishing is illegal in New Zealand', 'The doctor starts having a heavy drawl before he talks to you and tells you to hold', 'As we observe Earth, we can see Saturn and Neptune are very dim star systems. So', 'You could argue that a tremor is temporary, but this is less likely to happen when', \"In fact I went to do the children's school because it was Friday night.\", 'Talymel found a document on the cellphone of others who had linked him to the unknown', '…the rehearsals, which are put together in special situations, always sell out. But', 'I got up soon after, but it was freezing. It was still freezing here.', 'I got dressed for a break. I stepped out of the study room and turned to look', 'You need something you are easily able to manipulate. When you play Pokémon Go, you need', 'If you watch baseball, you will play hockey. Yes, if you watch hockey, you', 'I read it daily for a few days. Before I became writing in radio it was all over', \"This might sound crazy, but it's usually not a bad thing for soccer to be played\", 'my brother wishes he was back to law school. our parents were married over 18 years old', 'As with soda, a self-drinking drink like coffee can cause alcohol to end up', '—Even alcoholics, who have been conditioned to drink water and drinks heavily', 'For children ages 6-12, there are two versions of the leaf available: the ordinary', 'The solution would be a PMO. Think of these way: When your stress is low', 'If you choose to be included in the environmentally conscious lifestyle that you choose to live, at', 'so so obviously me, do you need drugs to live at the cinema when you have no', 'Sylvia Lazio, 19, from Epsom, passed by without permission on the edge', \"If you haven't had one of these exams in years, you might want to take your\", 'There are numerous arguments. There is no credible evidence that chocolate can reduce aggression; it only', \"'He lived with a bad mother in Romania. There was no abuse so he had no\", 'You want foods that you like. A salty food makes you hungry to eat.', 'On the flip side, many people like washing their clothing with very cold water. A wash', 'You decide to avoid eating too much medicine.', \"It's convenient that a lack of food choices is true in so many fields.\", \"When doing homework I have to make sure my students know what I'm talking about. If\", '20:54 – a girl from my hometown has sat by me while reading and she comes', 'You need to learn a lot of local slang. Just look at English speakers of any language', '25. Discuss using a blog to share experiences. Yes, you need to live in the', 'But there are big-market Turkish Airlines Boeing 777 with wait list air carriers in their fleet', \"The heavy carriage gave a slow 'climb' up to the ceiling and she made her\", 'When you graduate from a military school, everything is carefully reviewed to ensure your degree is unique', 'But if Clinton means a change to the incomes of Americans and not higher taxes on the wealthy', 'I fell asleep the next morning.', 'He does so by the following three simple exercises. he turns on his heels and retires', 'Q: What sort of a dog was you trained to handle in the meantime?', 'In 2016 US troops in Iraq will have fought in Syria and no larger army will be needed', 'As with gloves, a measure of body composition is necessary to protect against infections.', '\"On your knees,\" the boy said. He didn\\'t care for people coming or going for', \"Finger pulled mine back, so I don't have to carry it around now.\", 'So there are far more ways to change socks and binders and buttons than just having them', 'It was terrible as a working day. Sometimes I saw many people suffering from depression. For', 'And there are plenty of cool things to enjoy in summer. A dive could be more fun', 'Larger Warbas Thief finished testing on the walls', 'As we talked about, this is not merely a waste of time on average. It can', 'But when disasters occur, they can make pollution even worse. The Environmental Protection Agency has issued', 'This old greeting post, which is now extinct, makes us feel guilty about being grumpy', \"I'm having lunch now! I want to thank you so much and thank you for my bag\", 'There are lots of deceptively simple puzzles to solve in this subject; more on this', '12. Quite quickly, I found out just how thin we are and started talking with our', 'In return for ten tributaries to strengthen the beach and keep wind off the island,', 'And if there are a lot of amazing moons, then if you always leave us with only', 'There will be further updates on MyMusicAsks.', 'There are doctors who are nothing but professional surgeons. When they go there they make their patients', 'If you decide to be at the dinner table, another important thing is that you will get', \"I'm too stubborn to be serious in all situations. So not only should it be fun to\", 'There are plenty of chives in this vegetable dish and they are absolutely delicious. I find', 'The village formed when a member of the Seventh Army village became ill from hunger. A hospital', \"He taught his daughter her proper manners and isn't afraid\", \"If you aren't a patient, you cannot look into their eyes at all.\", \"The container contained five tumblers. Each is smaller than a lion's box. The\", \"Rather than walking through the kitchen there's a window\", 'Stupid guy is stupid just like you', \"If you haven't had the chance to chat with anyone about your experiences as a cat that\", 'I got very sleepy at the end of the shower. The beautiful woman waiting there did not let', 'He went outside and to his room to drain the sink.', 'It was unusual for a type of box in the desert to have text inside.', 'You just wear clothing, not fish. Fish are fish.', 'The Wallabies home-field was put under control after another wind blew down and the Wall', 'He got divorced from a woman in second grade. He went to college for some time and', 'There are sprouts, water canals and even jack trees to plant trees to keep people', \"14:42 Russ: That's what Bob was talking about. After getting back to Mr\", 'Named after Mickey Mouse\" A large standard drawer in this building is located near the general store', \"The physical manifestation of a physical function is psychological. People don't sleep well and don't\", 'the baby sponge told the cat to hold onto the cloth that was wetter on the baby', 'Stables carry alcohol, so you could safely bring pot with you at night.', 'I waited an hour and a half before we could leave', \"You need luck in a place to make progress. That's what positive dreams are for.\", 'It was busy so the average person would shut it down on the minute they saw the picture', 'As we mentioned earlier, who are the guardians of risk and cost for applications?', \"It's funny how a young man will smile for free if he sees something you see on\", \"There are plenty of yummy things to eat in Japan. In Japan we don't eat\", \"He knows he isn't going to spend all his money\", 'But when Luna stopped, his mother cringed. How could he hear her own voice?', 'Nobody had known any sort of lighting for the bear', 'Hospital Inspectors, which are involved with most hospitals in the UK, could not visit', 'In sum – 10% of that $450.000 pay you paid him as a bonus', \"If you haven't had the chance to kiss the baby kangaroo before, you're\", 'You need apples for a ball of brown cake. If you need apples for the ball of', \"The chemicians gave a red food with carrots. He felt too exhausted and couldn't eat\", '80 It requires enough to pay for my groceries. This should be paid off by me but', \"If you hear someone and can't find someone to meet you, talk you into going to\", 'It was dirty and the fan was too loud. It did not fit into the place and', 'For those wondering how a Neopets scarf can affect her levels of Beauty? Jota', \"Saving money isn't a bad thing. Especially by selling stuff. Of course it's something\", 'There are literally hundreds of types of sunflower (except my own sunflower for this reason', \"I don't know how many times you went to the bathroom\", 'We only listen to a single third of radio the night before the wedding is where we sit', 'There are zero hidden hangups in surfing a server in the browser; their name can', 'But when Elsa finally said goodbye, they rushed to find her. She picked up the cup', 'I took out two bottles, one of which contained which was blood. She broke it with her', \"But what happens when a cop doesn't recognize the king you're visiting as you're driving\", \"Kaiyaichi Kedugo: Ya-han and I'll talk about it later\", 'These two birds took up position next to me.', 'But when Zhang tries the pressure up to burst the bubble of power--after all, who', 'In fact, first, you had to relax a bit and take off your shoes. After', \"KELLEYH. KELLEYH. So we're driving down the street in\", \"But if my bra and underwear are too messy, they don't fit too well. L\", \"I'm different now from a lot of people before. This year I wasn't able to attend\", 'Whilst avoiding the mall as much as possible, you may', 'This story originally appeared in the September 2015 issue of Paper Paths and Water.', 'filter handles by', 'I cried when I put my', \"So no requirement for a guy to have beard? No need to demonstrate teeth? You're\", 'As with swords, a wood with and away from wood will not produce batter.', \"He'd attended events, such as the Dalai Lama's Badoumi celebrations in China in\", 'There are plenty of \"can\\'t go swimming\" holidays and your beach holiday can be pretty', \"You did call or e-mail the firefighter. If you don't feel safe and when\", 'It was understood he is under 24 for travelling to Vegas and was headed for stage 2 of', 'But when beaches are cordoned off like we did on the jungle coast, it can', 'Joes may spawn from a pool to rest near the swimming pool. They always stay on the', 'In addition to schoolwork and homework, my', 'Seth says 11 show, more like 8?', \"You can substitute medium-high and low potatoes for celery. They're both good for\", 'And part of this is because you need fewer calories for growth. Instead of spending more time', 'The parents argued it is over the threat she had posed to their safety and their children was', 'For example: There is no same for breastfeeding. Baby drinking is safe as feeding..', 'Around this time he got addicted to drugs. He', '....\" like, from the sixteenth and seventeenth century,\" McCallum wrote', '13. Shopping when a cat is young—mercy for your feelings of happiness.', 'In recent weeks a BSA said it received a complaint from a visitor about an incident at', 'There are obviously others, but for me going in there was not enough study to know what', 'The doctor laughed as the Japanese home cook drank with him.', \"I wasn't thinking, he was saying anything. It was like writing down your favorite feeling\", 'If your blood contains bacteria, viruses or viruses. For', \"If you spend too much time with your imaginary friend you're going to lose them.\", 'There are plenty of \"passive access apps\" which say you need to keep it turned', 'Rescuers quickly did not know what happened. There were no passengers or animals in the', \"We're brave enough. It's up to you to protect us from rain.\", 'If you choose to be over the phone then you should book the kitchen & bar in advance', '17, 18: 7 Never let anyone drive a motorbike in the park', 'This came after Coca-Cola said that Coca-Cola made The Coca Cola with strong', \"I've bought my M50 and are cleaning it off for the winter so I can feel\", 'It was Christmas Eve, she said, when her brother became ill and decided to set off', \"How do lights affect the color of your beard? The sun's rays tend to deal with\", 'We met several times, but we never stayed at our homes. She gets involved in small', 'Sweet & tart texture. You should see the texture', \"If you choose to be kinder to humans, don't end up spoiling your appearance\", 'He finds that much about her works means being set free', 'But there are others, like you. Salt is natural and our ancestors lived on the moon', 'There are numerous copies of this book in Kurdistan. These books are purchased from national stores and', 'There are lots of vats of heart valves in these hospitals. Some serve as big tanks', \"He called India's 2.4 billion inhabitants an empty basket of bricks and played on them\", 'When we define language, we may want people to speak about some aspect of our words to', 'If you wish to be able to bring cotton to summer months, let your window be ready', 'For example, What-Ifs are garbage. Just because you already knew they were NOT', \"This kind of stupid, unknown world is characterized by ridiculous failure. Instead of thinking I'm\", \"The organization tries to be always up for guests, what they want and don't want.\", \"There are lots of Esteem calculators that don't work if you're not given\", 'On one occasion I was taking a walk along the Tanzanian shore during the cross the', 'You can supply a short message to the uploader explaining why you rejected this upload.', 'In fact, most of the 19th century was mostly English. Also see his book The', 'Showers cause fertility, which we may ignore. These include body scrubbing, food intake', \"Structure cards require to know what they're supposed to be like\", \"But if parents don't want to say goodbye to their son, she didn't want to\", 'Wynne enters one. \"It\\'s ok. Not too bad.\" she says. \"', 'In like fashion I was writing the following poem. The conclusion of which could not be finished', 'Dr. Patterson lives in New York', \"But pegasus, we don't eat at night.\", 'But there are things a lot of people lose by playing games. People usually start from somewhere', 'But my backpack does not come with a hitch. For now, if you want to stay', 'It was ruled by a cast of 50 handicapped persons and by ancient Greeks to be bad', 'But there are always a few things that prevent you from flying. Some believe that you should', 'But there are still a few things that remain to be changed. Measuring too much has', 'What do newspapers say, do you follow news? An older version of our poll was shown', 'If you buy my \"Flick and Fold\" audio CD/DVD player and have no', \"When you shop online, you don't spend your money on things you already own. For\", 'But there were never a lot of space left in my apartment. So there was no room', \"It was nobody's first time to get caviar at their school and different families were fast\", 'zoe introduces her. and will she greet you like \"What am I doing with my', \"But what happens when a plant's produce dies? Nowhere in Earth's atmosphere is there\", 'You don\\'t need a letter to sign every post.\" \"If you\\'ve got a letter', 'And there are ones, like the Skyscraper class in my apartment and the staff room', 'a workflow to kickstart an automatic introspection in your', 'You would prefer not to put your dog outside to train with you by accident.', \"You do want to be able to feel sunlight to stay cool. Your arms can't keep\", 'One of the worst problems for humans', 'Kai Xiao Mountain, this 3 aeon city was close to Chinese imperial capital. It', 'This kind of barking, which is often traced back to Japanese living at sea, is common', \"That's nice to have a face to tie to any cool thing you've done. I\", \"It's okay to fear anything.\", 'My brother was calm for the first time', 'But if my neighborhood is something I want to try out in a restaurant it has to be', 'We got bees with a squirt if our parents gave them to us and they only gave', 'swift sailing means a print is already transported to its destination. Or seen by the sun', 'For free telescopes it is possible to find comet Bokeh in Australia on an international distance', 'For free telescopes it is possible to find comet Bokeh in Australia on an international distance', 'It was delicious and the customers were much nicer. It was also awesome for how many big', 'If you hear something, then you will notice I put my hands into bowls.', 'He then interrupted his \"game\" and asserted that when you could watch football you would see', 'Males would shower my armpits and trousers. Although they were fairly thin, we used', 'The Old Parish church, which on Sept. 9 opened with a solemn sacrifice of \"De', 'He loves all kinds of things and dogs are their natural', 'The driver shook his head to make sure everyone was OK.', 'If you wear makeup, or had to dye or dye your hair after washing it, your', 'There are plenty of not one for one distances. Just set an interval over one year and', \"If you drive very well in the dark – you don't need to park on the street\", 'But some cyclists make a bit of a nuisance of themselves on the roads and also if they', 'The bodies came back, but you could hear the lamentations of God from the dead.', 'This post originally appeared in our Quora Forums.<|endoftext|>WEASLEY HAS DEAL WITH', 'And if our transaction is already within the vault, which would that means we could get only', 'The smell is high and tastes almost like tobacco', 'It would take more time than from her age', 'The cold melted when a paper bag by Leo was lifted up to touch my face. \"', 'The senior staffer got a comment on it before I started reading the description of the application.', 'If you eat pork, you will also die. You will have died because you did not', 'This video displays how a summer at the zoo can become so wild for Christmas.', 'to stop insulting people, not his own guests. when they would laugh at me. I', 'For example, children of highly sedentary moms have lower levels of cortisol than those of sed', \"10/09/10 on one of Ms. Lazell's vacation stops in Russia\", 'The Link makes all computers unable to understand information', \"You know something about a star? It looks like you know you're seeing it. You\", 'This means dogs who are working on the crops are immediately admitted to city parks.', 'There are approximately 29,000 students in Vienna. Of those, 28,000 are Christian', 'When making posters for a movie, you usually need to keep an eye out for other threats', \"And some couples don't want to act embarrassed in front of their spouse about her body.\", 'You may wish to be seen with the spouses of someone who is celebrating their wedding. It', 'You will lose weight, but you will lose your inner strength. When you become fat in', 'I never wanted to cut a hair in this lifetime. The hair you cut should have been cut', 'You will notice that a general idea of pi is pretty broad. Of course the author does', \"Ice on ice can't normally rise more than 10\", \"'You wouldn't drink my wine…'.\", '1/35 -pregnant few lucky who drive through his junkyard of smooth', 'There are restaurant restaurants, too, with banners and placards on tables or serving food to', 'If you write about a character in a novels, then you know what they should not say', 'In July 2013, a couple in Miami accidentally lost their hair in snow.\"', 'That said, many of you will need gloves to stay warm. Take heart. It can', 'There are numerous films, books, etc., that schools play in summer school. The purpose', \"We don't really have to look at consistency; no team can score six goals in a\", 'Kiss took all my alcohol. she tells me to stay', \"[26:52]Majin_0100: Ok so you'd change one of your\", 'You enter the contest through the Reddit Mail app. You', 'An accurate calculator will be able to tell how many bars you will drive if you know how', 'all yours to eat on the anniversary of your birth', 'What are obstacles on a heavy pack? An outdoor obstacle can be thrown when you are running', 'But some graves could be considered their own funerals for those who died during the war.', 'He found chalk because a tool for writing chalk on lapels to desk lamps is out of', 'This story follows three of the same falloi. One has soiled itself with power that', 'There are spaces for a wide range of ecosystems. Gardeners can cook freely and will enjoy', 'On other blogs I have post and comment boards to share personal information about myself. I found', \"I found out something where a bank was used but he won't say why he didn't say\", \"11. If you don't believe you can't believe\", 'Tears rolled down the window of the auditorium at First Light Television Club in London on', 'In 1908, violence broke out between rival unions in Texas', 'When you arrive at a Paris airport, passengers can buy tickets on board their plane. There', 'But if your bike is going to not recharge the bike you would consider breaking it. This', 'The driver insisted he be allowed to drive free. He went to visit my parents. He', 'And just last week, we told you how on Earth different species found cool places to spend', \"use functions `Path' and `KeyFormat' with `path and `compare text\", 'What are germs, transdermal pathogens, and bacteria?', 'It was strange for a woman to get raped in public in The Hague as there was no', 'And what sort of a region we see falling and standing at the entrance of the middle and', 'SOLUTION 5, THE REPEATING IN JUST ONE DAY IS BIGGER than IT', \"But there are always a few things that wouldn't affect your blood sugar level.\", 'In time my gift to leopards lasted for five days. So many years. I', 'As are Little Nagi and Yami Sugimori.', 'Miley Cyrus is beautiful.', 'Bridgewater resident, Scott Demers holds a rifle at a grocery store in Belmar', 'You will receive no shrimps in cinemas on opening day and little food in cinem', 'As we discussed above, one of the easiest ways to write a story is to say what', \"When you wear your T-shirt to church, which can't happen during the day,\", 'This post originally appeared in our 2012 emailbag.<|endoftext|>Police are searching for a man who', 'On many varieties of the Human name, specifically the Hebrew name for Aaron we know as Ah', 'This news helps support the effort to fight feral dogs and cats. Find us on Facebook and', 'He says she saw a girl with the stripes and looked up in embarrassment at him.', 'An artist receives items, whether they are oriented in one direction or another. This is called', 'An iPhone Watch will be able to record signals and pictures at the screen when you get up', 'As could not be in the case of Aristotle, since when he begins writing he would only', 'But no whale actually is air. A dolphin is water.', \"It's okay for a wife to help save for Saturday when her husband goes to go with\", 'Q: Should I write this part just after my due date', 'He says workers should be allowed to pay minimal wage as long as they pay their share of', \"I didn't say a word. I answered the door and another customer arrived.\", \"JAY DONNEY: It's funny. You're not supposed to know that you\", 'We met early for a round of dinner before he returned home. For once I would think', 'He also bans Big-Man Shorts after he sees what he puts on and if he', 'We make appointments at a secret location to hang out with friends. How much one would want', 'FlipFlops', 'There are approximately 22,000 known species of animals with liver. During normal lives, these', 'Chocolate Powder is a soft and sweet cereal in comparison to many kinds of nuts. It', 'How to Perform Your Slight Noodles', 'This place looked interesting! The food was tasty. So interesting to eat here', \"And when photographing, we need to capture the atmosphere and make sure they're not dirty\", \"But if my truck of 60's at highway speed wasn't going fast enough to help me\", 'Being cool is important', 'You know those humans, who have some descendants of ancient peoples? Most could not get things', 'And there are burn-ready positions in sunny areas for burning your favourite beans.', 'He should wear dress, but we should listen to proper explanations. That way we can understand', \"You need solar cells, not batteries. Solar cells make use of sunlight that can't be\", \"The camera flies high, but I am stunned. They're there so soon.\", 'You will spend most of your time around gardens. Get those friends who live right there and', 'The wife writes that, \"I don\\'t know how you would judge how much you love', \"But if Jason isn't doing it for revenge, then what is he doing for his wife\", 'You will lose weight, but you will gain some strength', 'In fact, many of the known cancer tumors are surprisingly benign. Like scarlet gli', 'There are extensive flights, including one for Hercules. There are also flights for staff to visit', 'Chocolate eats away a lot of cell membranes. These cells are heavily surrounded by death cells', \"We found ourselves spending a lot of time fixing his bullets to make sure they didn't impact\", \"This kind of punishment: It's like chewing bread with your bare hands\", 'On computers, performance is measured from 32 to 50 points', 'When arctic regions. The white color zone is mostly ice. Any colors with more white', 'He could easily have all been shot on sight. But more down inside his box. The', 'As we grew larger, we should do extra things for health. Energy balance is one of', 'You need ice pack, so you will probably need some double bottom freeze bottles', \"You could join so-called 'free pilgrimages' or 'experience small states'\", \"You see here this, we can't compete with anyone who was alive before we even wanted\", 'He says researchers use a wide range of analytical tools to determine the formation and extent of solar', 'If you lose my first bank account to foreclosure, will you do anything about it?', 'You could decide on a computer for your travelling with such an option or small ones. The', 'This post originally appeared in our office in Valencia, Spain.', 'We got inspired by a article on amethyst and ocean blue and tried some free color lip', \"For example, consider a dog's voice during the signing ceremony. Many dogs would not speak\", 'Zoos are expensive and people usually drive them to z', 'There are dozens of \"logos\" spread by believers on their websites that state that God', 'On any wedding day, your hair will naturally pass through your hair follicles. This means', 'How do spiders eat: is it possible for one spider to eat four insects?', 'When we arrive at a park, we remove our bags and put them back on the park', \"As their fossils appear, they can't trace up any higher; each fossil has an unknown\", '— Legal assistance is provided by any state or', 'As one hears people, they will move faster. With children, there will be more movement', 'There are numerous survival and evolution studies on Norse mythological creatures. Science generally supports the existence', \"And when sleep goes, you get no satisfaction. Or even if you didn't want to\", 'Astrid flashed her BOMB on top of her index finger and grabbed it. \"', \"Wrap up Febs' article to show why it would have been offensive in this case.\", 'The plane flew between a featureless modern skyscraper and the Russian Theatre at one of Russia', 'The bird may not enjoy eating hot dogs', 'As one herb can be completely removed from plant and leaves it has great medicinal properties. So', 'How do those videos to read and understand graphic film would look in slow motion?', 'What do toothpaste.com and toothpaste.degees offer for kids?', 'There are drinks inside, but you can only order beer (plenty of hard drinks can', \"You could argue that. It's not rocket science; you just didn't see it that\", \"But there will come a time when you hate it when you're beating someone with a bowl\", 'You need to really to take great care when you swim in the swimming pool.', 'I got up early and I went to work before my shift came to start at 8:30', 'It was awful in a non-transitory place with such a loud noise. I came', 'For those skilled in a trade, but unable to earn enough to afford safe production, three', 'What can occur when a party is called together?', '\"You should join us, my God. Could I stay while you sleep?\"', \"For better timing and a better thought-leader, adjust your hair length so it's straight\", \"When we talked about a car in this hotel the judge called it'super rare and extremely\", \"But some historians seem to think that as Switzerland's contribution to the wars made their country stand\", \"This won't happen. The city has authorized the downtown parking lot as it's for private\", 'But new farms could be created in some marginal areas as food development improves and climate change disrupt', 'I met Julian Guiiras of Alberto Nuzamer of Santos who was at his', '\"There were four. If it was determined that there were three people behind you, they', 'Onions contain fragrances of different medicinal materials and microorganisms that provide strength. To', 'Not my boss.', 'You may wonder why. We have been vaccinated for HIV for over ten years. The virus', 'the glass sink had a green side to it. The glass one side could go on top', \"But some ski instructors, when they're vacationing and already well rested, make it difficult\", 'Glasses can look like glasses as well. Some', 'And there are quite a few top quality sushi items out there. See here for all the', 'In 2013 we raised a record $12 billion in fees for our venture partners. The companies', 'Loudmouth headphones [1] and readwritten OTE are some physical stimuli that are associated', \"You see stars which I see in the galaxy. They're not bright enough to get my\", 'There are strict dress code laws in school zones. Bus drivers have to wear garb that', 'As one businessman once said, \"If God said so, it would say so to us', \"This doesn't mean a trip to the zoo is unfeasible: thousands of small animals\", \"But there are few of them. The ostrich doesn't make friends with swans.\", 'But my dislike for the movie RIT raises the question of how ever smart we are to', \"He'll eventually win, but he'll lose. You don't win him.\", 'When not accepting payment, if you do pay and pay only for goods or services, or', 'For those readers who are looking for an unconventional book or interesting story with stories and characters that', 'In order for them to start this game before the basketball game ends, their coach will take', 'This time around I was having a hard time actually saving my money as you can no longer', 'Shroud blades can be seen in Flanders. The sword is referred to as a layer', 'It was subsequently captured, but it was recovered and returned to the naval airport of Jan 2012', \"If you aren't a massive extra to stairs, don't take great breaks.\", 'He told her he is going to take care of my kids. She sat there on the', \"There are plenty of Gurdwara eateries with absolutely no loyalty towards one's owners\", 'Ola Rodriguez had a shot of anti-anxiety medication to calm him down.', \"But if these weren't already impeding transportation, why did he lose weight in the first\", 'He says rape would be 100% legal because of female brains. We told our parents we', \"If you eat salt, you will become diabetic. If you don't eat salt, you\", 'For each tasty meal, go to the Café in Beijing and ask her if they have Chinese', \"And what happens when a young person who wears a hoodie or tennis shirt doesn't grow\", 'So if we start a quick version of Kernel and share it with friends we can use some', \"And some museums don't even need to hire an exhibit designer. Many museums use people with\", 'I gave attention to a girl I know well. She asked me what she would do if', 'If you drive under a fine of more than $100 the driver may pay the fine and', 'And there are others, like going to Walmart. No need to judge how much you should', \"But what happens when a woman's mind accidentally becomes depressed?\", \"But what happens when a cat can't bite? How should you handle cats?\", 'To create pseudo-Tories, there must be eyes and ears with holes in them.', 'The guy walks into a lower level bus parking lot on N and sits down on the seat', 'Erupting smoke took a few minutes. Everyone was curious but not threatening to fall off the', \"He didn't hold a job. He owned a shop. A clothes shop. A formal\", \"In fact, my WOW is so shitty I'd rather have someone eat them by themselves\", \"Sugar has powers that the moribund monkeys can't find. Some carbon fibres can\", \"But there are 20,000 people on Capitol Hill who don't walk through the door of\", \"But my curiosity didn't come to the forefront of my studies. After college, I became\", 'You also hear people on the Internet talk about jokers and slippers and handshakes', 'And there are certainly a lot of wind turbines in passenger planes. Just ask our friends at', 'We always respected him, not from his greatness, but his worth and wisdom.', 'As we realized from a community view, meditation is actually used to heal ourselves. It can', 'As if stealing children to play with was silly, Sammael apologizes on his blog', 'But when confronted by a store manager and questioned by security staff, stadium managers told DAB', 'For each limb you are allowed to rest between 3 and 8 times per week.', 'But there are too many types of trees around the land to count them all.', '– Blue raspberry occurs in only 3.35% of German women and comes in about 10', 'The cashier asked, \"What\\'s wrong with your ticket?\" I replied, \"You', 'He said because he is not a criminal drug user he did not smoke crack.', 'We record ourselves through a computer. The learner knows what he wants to learn. We', 'We look forward to a future for people living in Asia and are convinced we will have more', 'If you receive any of this information not handed to you by the IRS we will not stop', 'As Chubb explained, \"It\\'s basically the worst thing that could happen to the dead', \"An emotional cleansing might be difficult to try without one's partner. More emotional cleansing can help\", 'In case something gets to you to open them, open them and touch them.', 'What do ordinary guys and women do on weekends?', 'There are restaurants on a single market with stalls in each market. On nights you can buy', 'You know those clothes, so you want them to fit you and stand out.', 'If you agree to a contract, you agree to give me the opportunity to perform that contract', \"35X Stuff That Tires Can't Breathe You're not wearing pants\", \"If you eat meat, you will want something that isn't simply fruits (like vegetables or\", 'So when teachers meet, they want to prove that their students are proud of their experience.', 'As you eat many a variety of foods such as shrimp, breads, vegetables, fruits', \"So there should be a specific reason for extending the CPU's speed up by another process.\", \"What do checklists, voice letter writing programs, etc. don't require?\", 'JAPANESE HEROSENIES AND FORESTS', 'If you cook tiger meat for me, please make sure you use greasy tortillas and', 'Tags: classification, foreign languages', 'The hospital gym leader, who was also enrolled in studies and had signed up for the conference', 'You can ride your own Sea Bus to Glasgow to catch your Eurofan ride', 'There are 52 inches to make the Catanical Dream Lily 100% Willow Coffee Roaster -', \"If you haven't had one of these noodles and want to take them home to play with\", 'For example: While a pure milk will yield one pound of protein with fat, a milk', 'As your electrons slow to learn to project electrons, their constant speed grows slower.', 'But there are already a lot of wind turbines in Britain', 'For 20 seconds people in Shunsukeshui pay their tourist visa only to bring', 'There are audio recordings, which are called audible and thus must be spoken out of the mouth', 'The French cuisine has a similar focus to Mediterranean cuisine in its application of spice. The grass', 'On what grounds would a song be considered vulgar? In general, song lyrics are no ground', \"If you jump up, you can't climb the stairs\", 'Heath splits during a visit to an aquarium. And she has raised 31 pounds.', 'In many societies all the matter in the atmosphere is toxic; for instance with plant life it', 'You see dogs looking the way you do when you walk.', \"He'd slept because the fat was too dense. It made his muscles weaker.\", \"In two nights you are able to turn around in any direction. If you're not playing\", 'And when gnomes in high growth make noises, so they can hear something in the distance', \"For good luck with a parent's son living in secret to get housekeeping done.\", 'You get unlocked when a third person claims victory. The third person requires three levels to complete', 'Inlet Shelter 2) had an eight bedroom house with three bedroom bathrooms and only one bathroom', 'He went skiing while in his home in Lancaster, Pennsylvania.', 'All parents pay school-aged children to participate in tests', \"We still haven't had the space to celebrate the birthday of our dear uncle the children of\", 'What do cleaning supplies and clothes do I need?', 'When we weigh many of our lives, life is mental. It should make us feel safe', 'For good dental hygiene, use an effective dentist. Some recommended methods include dentures to reduce', 'You will notice that a waste basket is wet. When you find something wet in a basket', 'So player can wait for his opponent to deliver a counter', 'He loves sport.', \"And what happens when a window is closed while the developer doesn't open it?\", '24. Furniture, especially old wood furniture, should not be heated by heat.', 'As we discovered about a few years ago when we learned about backpacking through Texas, our', \"Chaka Sagami, who's also renowned for being too easily bullied by people, became\", 'I got motivated by a study and put weight on him. I worked hard to work more', 'We discovered the telephone file of Santa Claus in the bathroom', 'The exterior of my van is totally devoted to people and', 'This story originally appeared in The Newsroom<|endoftext|>In January 2015, Hamilton council approved a seven', 'koffinski—.The secret that pops out from any star…that we were meant', 'A woman rushed over, lost in her grief. She looked at Crowder and did not', 'But she insisted at a press conference that none of this would have happened if she had still', \"~He'll prepare him/her for his workloads when he's sick or getting away from\", 'He should leave all the places he had ruined to live and they should stay with his parents', 'Yukimura returned, having too much praise in hand.', 'I played her on top of the piano', 'But there were several of them.', 'Xavier had placed his room before him.', 'In 2 seconds I was able to take pictures of my aircraft. What did I do then', \"We've talked about a lot of things recently, so we don't feel like we're\", \"I'm using purple fabric, but I don't like yellow..\", 'If you caught him, you would have assumed he was wearing his sunglasses at the time', 'It was suspected that a link was made between the producer and the marketplace that makes it highly', \"I later attended an SAA at the dean's office and when we arrived he found me\", 'We got knocked down, but we still survived. We actually got stuck with 3 people on', 'I saw immediately that a ball of bread landed on my plate. This happened when I sat', 'The equipment worn by a male child on horses is compared to the clothing worn by a female', \"Tears flowed down the girl's eyes from the shower where they stood before her. She\", 'He took videos and an album of old pics to share with his older sister.', 'The boy insisted he be allowed to call boys and girls by their gender - but he didn', \"Lowers Rage 4' Wows Chakra in Weight Loss\", 'But when Huang Hong, an English teacher who has died on the estate in seven years,', 'The mother reportedly kept a cat in the backyard to sleep on. An autopsy later found there', 'In 894 he is given the potential expenditure of attention for his retirement by St. Patrick', 'The price tag was a bit high. Upon initial inspection, the tags showed that the cart', 'The girls ate solid, safe food. Skin was harder when they ate organic food. They', \"The egg burnt after a few days. Once I melted it, it wouldn't show any\", '10:54 Spatology in children speaks of tears in children as snow falls', 'In fact, after a 30-year hiatus, writers said that Batgirl has no plans', \"He'll figure out a way to pay back the debt by phone by pulling out a credit\", \"It was printed for a general audience in 1926. They didn't mind using it in print\", 'Daughter Kikira, also in her twenties, felt worried at times when she was hurt', 'It was mainly Japan, but it was mainly in Lake Sweden. To avoid possible harm to', 'But she declined in a meeting with all 41 police officers at the Rajouri Police Station and', 'The 48th minute, after the ball crashed into the wall of the tunnel, was stre', \"I've never gotten excited, but I don't want to give up because it's so cool\", 'It was Mrs Baker, who had just celebrated that birthday and had ordered Christmas presents.', 'The Star tracked down a woman with an unnatural face but she has several distinctive features. she', 'The incident triggered problems. In the early afternoon, Spanish reports said Berthiaume was receiving', 'The boy drank too much and said he liked to eat ice cream with cold beer.', 'In part three of a four-part documentary on Dallas music, Ferguson asks people to share', 'The older gentleman wanted a safe to pay back the loan I made from him.', 'The boy approached us, got his hands around his waist and asked if we would like to', 'Oddly enough, a local snapper accidentally shot my brownie too close to the milk', 'In her misery I was trying to sit still. There was no chair or chair not available', 'In New Hampshire, a woman was arrested after she attempted to break into an apartment in early', 'A couple laughed to themselves as he flew away.', 'The girl drank water with her meal because her meal was', 'In human spinal cord, at least two cerebral nerves never enter the spinal cord.', \"You know those ads, they're good fun. They call for ads that don't hold\", \"He starts pouring beer, which he didn't get before he left this morning.\", 'He put down his mitten to sit next to his bed. He loved this way of', \"He says Mr Roth, who is only 46, doesn't know many deaf people.\", 'But what happens when a student or student loses his job or does something funny in his job', \"This doesn't mean a bad thing. Personally, I find it annoying when people don't\", '20:54 It is very early in June. After another 15 hours and another day of', \"And if you're a Muslim, you might also want to take some inspiration from the prophet\", \"Stray Cats are a common dog's handler. There are no specialized breeders in California\", 'In almost all dialects it is always worn by women. It', 'He looks straight ahead, does not start staring at something and does not rise to do anything', \"You may notice that a lot of people prefer to sleep one day every week. It's\", 'He says she comes to work with her grandmother in home without her permission and she would never', 'New features include restaurant items', \"You may notice that a number of things occur in your joint day out that don't affect\", \"11. Butcher does the animal's job properly. He makes sure there are no unduly\", 'The library collapsed as a piece of wood sank on its side. His birthday came at his', \"Some fans describe fans' desire to read Billy Jazzy Jeff or Billy Ray Cyrus as\", '...the murderer does not even know his motive. He still took money from his friends and', 'The credit cards may be taken as evidence but we find out it does not mean that they', 'He says there are a lot of food frogs that live in the shoreline.', 'In many scenarios the \"wing\" is swinging and flying at the same time. In our', 'The corn harvests were done in 2014', 'This season saw 17(!) months of sunshine. 16 years of showers were felt in three', \"If you wouldn't be able to find dinner in an apartment, say goodbye to the restaurant\", 'For those viewing death, they will need UV light and reflect it through skin.', 'Clones fly off. The final boy learns to fly with his toy drone.', \"The driver broke into a girl's room while they were driving. She broke into the female\", 'We give money to charity to help their hospital', 'You heard me say you could hear French as you walk', \"But there are already a lot of countries playing in World Cup there that don't do well\", 'There are roughly 40% more human visitors than there are cars. More cars than cars.\"', \"You see everyone else, but you're invisible. You're not putting food on the table\", 'What do You Think, Gameshark Gamers vs Your Games \"Would you play about', 'He added half of a cup of cold cream to his dish. He threw it on the', 'In our Halloween concert, we only play drums. No instruments. Not lessons.', 'You can eat monsters, see living things (weird things that walk around on the screen', '…the breeze makes a wind really quick…', 'Shocking how Steven breaks his wallet and leaves the store', ', and if they bleed over, they get cold blooded,\" says University of Arkansas spokesman Scott Bart']\n"
     ]
    }
   ],
   "source": [
    "%cd transformers/examples/\n",
    "\n",
    "preds=[]\n",
    "\n",
    "test_df = pd.read_csv('task3_data/taskC_test.txt', header=None)\n",
    "\n",
    "to_predict=int(test_df.shape[0])\n",
    "print(to_predict)\n",
    "               \n",
    "for i in range(to_predict):\n",
    "     text = test_df.loc[i][0]\n",
    "\n",
    "     result =! python run_generation.py \\\n",
    "            --model_type=gpt2 \\\n",
    "            --model_name_or_path=\"../model_output\" \\\n",
    "            --prompt \"$text\"\n",
    "\n",
    "     print(\"Predicting Input : \" + str(i) + \" - \" + text)\n",
    "     result = result[-3:]\n",
    "     predicted = max(result, key=len)\n",
    "     print(\"Predicted Output: \" + predicted + \"\\n\")\n",
    "    \n",
    "     preds.append(predicted)\n",
    "    \n",
    "\n",
    "print(preds)\n",
    "\n",
    "output_dataframe = pd.DataFrame(preds) \n",
    "output_dataframe.to_csv('task3_data/taskC_predoutput.csv', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3 Prediction\n",
    "\n",
    "Above is presented each input sentence from the test data, with its corresponding automatic output.\n",
    "\n",
    "The next step is the integration with the Evaluation Tools, for evaluation the output with BLEU.\n",
    "\n",
    "#### Bleu Score with Maximum n-gram of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 0.2977.\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "! python evaluation_tools/taskC_scorer.py --references evaluation_tools/taskC_trial_references.csv --predictions evaluation_tools/taskC_predoutput.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bleu Score with Maximum n-gram of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 2.5991.\r\n"
     ]
    }
   ],
   "source": [
    "! python evaluation_tools/taskC_scorer.py --references evaluation_tools/taskC_trial_references.csv --predictions evaluation_tools/taskC_predoutput.csv --max_order 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bleu Score with Maximum n-gram of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score: 9.9449.\r\n"
     ]
    }
   ],
   "source": [
    "! python evaluation_tools/taskC_scorer.py --references evaluation_tools/taskC_trial_references.csv --predictions evaluation_tools/taskC_predoutput.csv --max_order 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
